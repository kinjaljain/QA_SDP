<PAPER>
  <S sid="0">Mixing Multiple Translation Models in Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain.</S>
    <S sid="2" ssid="2">We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step.</S>
    <S sid="3" ssid="3">In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain.</S>
    <S sid="4" ssid="4">Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Statistical machine translation (SMT) systems require large parallel corpora in order to be able to obtain a reasonable translation quality.</S>
    <S sid="6" ssid="2">In statistical learning theory, it is assumed that the training and test datasets are drawn from the same distribution, or in other words, they are from the same domain.</S>
    <S sid="7" ssid="3">However, bilingual corpora are only available in very limited domains and building bilingual resources in a new domain is usually very expensive.</S>
    <S sid="8" ssid="4">It is an interesting question whether a model that is trained on an existing large bilingual corpus in a specific domain can be adapted to another domain for which little parallel data is present.</S>
    <S sid="9" ssid="5">Domain adaptation techniques aim at finding ways to adjust an out-of-domain (OUT) model to represent a target domain (in-domain or IN).</S>
    <S sid="10" ssid="6">Common techniques for model adaptation adapt two main components of contemporary state-of-theart SMT systems: the language model and the translation model.</S>
    <S sid="11" ssid="7">However, language model adaptation is a more straight-forward problem compared to translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain.</S>
    <S sid="12" ssid="8">As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation.</S>
    <S sid="13" ssid="9">It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation.</S>
    <S sid="14" ssid="10">In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments.</S>
    <S sid="15" ssid="11">We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation.</S>
    <S sid="16" ssid="12">In this paper, a new approach for adapting the translation model is proposed.</S>
    <S sid="17" ssid="13">We use a novel system combination approach called ensemble decoding in order to combine two or more translation models with the goal of constructing a system that outperforms all the component models.</S>
    <S sid="18" ssid="14">The strength of this system combination method is that the systems are combined in the decoder.</S>
    <S sid="19" ssid="15">This enables the decoder to pick the best hypotheses for each span of the input.</S>
    <S sid="20" ssid="16">The main applications of ensemble models are domain adaptation, domain mixing and system combination.</S>
    <S sid="21" ssid="17">We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models.</S>
    <S sid="22" ssid="18">We compare the results of ensemble decoding with a number of baselines for domain adaptation.</S>
    <S sid="23" ssid="19">In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al., X&#955;&#65533; =argmax &#65533;p(e, f) log M &#955;mpm(&#65533;e |f) 2010) for conditional phrase-pair probabilities over a fe, X IN and OUT.</S>
    <S sid="24" ssid="20">Furthermore, within the framework of m ensemble decoding, we study and evaluate various methods for combining translation tables.</S>
  </SECTION>
  <SECTION title="2 Baselines" number="2">
    <S sid="25" ssid="1">The natural baseline for model adaption is to concatenate the IN and OUT data into a single parallel corpus and train a model on it.</S>
    <S sid="26" ssid="2">In addition to this baseline, we have experimented with two more sophisticated baselines which are based on mixture techniques.</S>
    <S sid="27" ssid="3">Log-linear translation model (TM) mixtures are of the form: where m ranges over IN and OUT, pm(e |f) is an estimate from a component phrase table, and each &#955;m is a weight in the top-level log-linear model, set so as to maximize dev-set BLEU using minimum error rate training (Och, 2003).</S>
    <S sid="28" ssid="4">We learn separate weights for relative-frequency and lexical estimates for both pm(e |f) and pm(&#65533;f|e).</S>
    <S sid="29" ssid="5">Thus, for 2 component models (from IN and OUT training corpora), there are 4 * 2 = 8 TM weights to tune.</S>
    <S sid="30" ssid="6">Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to a small epsilon value.</S>
    <S sid="31" ssid="7">Our technique for setting &#955;m is similar to that outlined in Foster et al. (2010).</S>
    <S sid="32" ssid="8">We first extract a joint phrase-pair distribution p(e, f) from the development set using standard techniques (HMM word alignment with grow-diag-and symmeterization (Koehn et al., 2003)).</S>
    <S sid="33" ssid="9">We then find the set of weights &#955;&#65533; that minimize the cross-entropy of the mixture p(e |f) with respect to ,5(e, f): For efficiency and stability, we use the EM algorithm to find &#65533;&#955;, rather than L-BFGS as in (Foster et al., 2010).</S>
    <S sid="34" ssid="10">Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to 0; pairs in p(e, f) that do not appear in at least one component table are discarded.</S>
    <S sid="35" ssid="11">We learn separate linear mixtures for relative-frequency and lexical estimates for both p(e|&#65533;f) and p(&#65533;f|e).</S>
    <S sid="36" ssid="12">These four features then appear in the top-level model as usual &#8211; there is no runtime cost for the linear mixture.</S>
  </SECTION>
  <SECTION title="3 Ensemble Decoding" number="3">
    <S sid="37" ssid="1">Ensemble decoding is a way to combine the expertise of different models in one single model.</S>
    <S sid="38" ssid="2">The current implementation is able to combine hierarchical phrase-based systems (Chiang, 2005) as well as phrase-based translation systems (Koehn et al., 2003).</S>
    <S sid="39" ssid="3">However, the method can be easily extended to support combining a number of heterogeneous translation systems e.g. phrase-based, hierarchical phrase-based, and/or syntax-based systems.</S>
    <S sid="40" ssid="4">This section explains how such models can be combined during the decoding.</S>
    <S sid="41" ssid="5">Given a number of translation models which are already trained and tuned, the ensemble decoder uses hypotheses constructed from all of the models in order to translate a sentence.</S>
    <S sid="42" ssid="6">We use the bottomup CKY parsing algorithm for decoding.</S>
    <S sid="43" ssid="7">For each sentence, a CKY chart is constructed.</S>
    <S sid="44" ssid="8">The cells of the CKY chart are populated with appropriate rules from all the phrase tables of different components.</S>
    <S sid="45" ssid="9">As in the Hiero SMT system (Chiang, 2005), the cells which span up to a certain length (i.e. the maximum span length) are populated from the phrasetables and the rest of the chart uses glue rules as defined in (Chiang, 2005).</S>
    <S sid="46" ssid="10">The rules suggested from the component models are combined in a single set.</S>
    <S sid="47" ssid="11">Some of the rules may be unique and others may be common with other component model rule sets, though with different scores.</S>
    <S sid="48" ssid="12">Therefore, we need to combine the scores of such common rules and assign a single score to them.</S>
    <S sid="49" ssid="13">Depending on the mixture operation used for combining the scores, we would get different mixture scores.</S>
    <S sid="50" ssid="14">The choice of mixture operation will be discussed in Section 3.1.</S>
    <S sid="51" ssid="15">Figure 1 illustrates how the CKY chart is filled with the rules.</S>
    <S sid="52" ssid="16">Each cell, covering a span, is populated with rules from all component models as well as from cells covering a sub-span of it.</S>
    <S sid="53" ssid="17">In the typical log-linear model SMT, the posterior probability for each phrase pair (e, f) is given by: other models&#8217; rules are discarded.</S>
    <S sid="54" ssid="18">This is based on the hypothesis that each component model is an expert on certain parts of sentence.</S>
    <S sid="55" ssid="19">In this method, we need to define a binary indicator function &#948;(&#65533;f, m) for each span and component model to specify rules of which model to retain for each span.</S>
    <S sid="56" ssid="20">&#8211; Sum: Instead of comparing only the scores of the best rules, the model with the highest weighted sum of the probabilities of the rules wins.</S>
    <S sid="57" ssid="21">This sum has to take into account the translation table limit (ttl), on the number of rules suggested by each model for each cell: The probability of each phrase-pair (e, f) is computed as: Ensemble decoding uses the same framework for each individual system.</S>
    <S sid="58" ssid="22">Therefore, the score of a phrase-pair (e, f) in the ensemble model is: where &#8853; denotes the mixture operation between two or more model scores.</S>
    <S sid="59" ssid="23">Mixture operations receive two or more scores (probabilities) and return the mixture score (probability).</S>
    <S sid="60" ssid="24">In this section, we explore different options for mixture operation and discuss some of the characteristics of these mixture operations. where m denotes the index of component models, M is the total number of them and &#955;i is the weight for component i. generally known as Logarithmic Opinion Pools (LOPs) where: Product models have been used in combining LMs and TMs in SMT as well as some other NLP tasks such as ensemble parsing (Petrov, 2010).</S>
    <S sid="61" ssid="25">Each of these mixture operations has a specific property that makes it work in specific domain adaptation or system combination scenarios.</S>
    <S sid="62" ssid="26">For instance, LOPs may not be optimal for domain adaptation in the setting where there are two or more models trained on heterogeneous corpora.</S>
    <S sid="63" ssid="27">As discussed in (Smith et al., 2005), LOPs work best when all the models accuracies are high and close to each other with some degree of diversity.</S>
    <S sid="64" ssid="28">LOPs give veto power to any of the component models and this perfectly works for settings such as the one in (Petrov, 2010) where a number of parsers are trained by changing the randomization seeds but having the same base parser and using the same training set.</S>
    <S sid="65" ssid="29">They noticed that parsers trained using different randomization seeds have high accuracies but there are some diversities among them and they used product models for their advantage to get an even better parser.</S>
    <S sid="66" ssid="30">We assume that each of the models is expert in some parts and so they do not necessarily agree on correct hypotheses.</S>
    <S sid="67" ssid="31">In other words, product models (or LOPs) tend to have intersection-style effects while we are more interested in union-style effects.</S>
    <S sid="68" ssid="32">In Section 4.2, we compare the BLEU scores of different mixture operations on a French-English experimental setup.</S>
    <S sid="69" ssid="33">Since in log-linear models, the model scores are not normalized to form probability distributions, the scores that different models assign to each phrasepair may not be in the same scale.</S>
    <S sid="70" ssid="34">Therefore, mixing their scores might wash out the information in one (or some) of the models.</S>
    <S sid="71" ssid="35">We experimented with two different ways to deal with this normalization issue.</S>
    <S sid="72" ssid="36">A practical but inexact heuristic is to normalize the scores over a shorter list.</S>
    <S sid="73" ssid="37">So the list of rules coming from each model for a cell in CKY chart is normalized before getting mixed with other phrase-table rules.</S>
    <S sid="74" ssid="38">However, experiments showed changing the scores with the normalized scores hurts the BLEU score radically.</S>
    <S sid="75" ssid="39">So we use the normalized scores only for pruning and the actual scores are intact.</S>
    <S sid="76" ssid="40">We could also globally normalize the scores to obtain posterior probabilities using the inside-outside algorithm.</S>
    <S sid="77" ssid="41">However, we did not try it as the BLEU scores we got using the normalization heuristic was not promissing and it would impose a cost in decoding as well.</S>
    <S sid="78" ssid="42">More investigation on this issue has been left for future work.</S>
    <S sid="79" ssid="43">A more principled way is to systematically find the most appropriate model weights that can avoid this problem by scaling the scores properly.</S>
    <S sid="80" ssid="44">We used a publicly available toolkit, CONDOR (Vanden Berghen and Bersini, 2005), a direct optimizer based on Powell&#8217;s algorithm, that does not require explicit gradient information for the objective function.</S>
    <S sid="81" ssid="45">Component weights for each mixture operation are optimized on the dev-set using CONDOR.</S>
  </SECTION>
  <SECTION title="4 Experiments &amp; Results" number="4">
    <S sid="82" ssid="1">We carried out translation experiments using the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus1 as OUT, for French to English translation.</S>
    <S sid="83" ssid="2">The dev and test sets were randomly chosen from the EMEA corpus.2 The details of datasets used are summarized in For the mixture baselines, we used a standard one-pass phrase-based system (Koehn et al., 2003), Portage (Sadat et al., 2005), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count.</S>
    <S sid="84" ssid="3">The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7.</S>
    <S sid="85" ssid="4">It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model.</S>
    <S sid="86" ssid="5">For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty.</S>
    <S sid="87" ssid="6">GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7.</S>
    <S sid="88" ssid="7">In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram language model and Kneser-Ney smoothing was used in all the experiments.</S>
    <S sid="89" ssid="8">We used SRILM (Stolcke, 2002) as the langugage model toolkit.</S>
    <S sid="90" ssid="9">Fixing the language model allows us to compare various translation model combination techniques.</S>
  </SECTION>
  <SECTION title="4.2 Results" number="5">
    <S sid="91" ssid="1">Table 2 shows the results of the baselines.</S>
    <S sid="92" ssid="2">The first group are the baseline results on the phrase-based system discussed in Section 2 and the second group are those of our hierarchical MT system.</S>
    <S sid="93" ssid="3">Since the Hiero baselines results were substantially better than those of the phrase-based model, we also implemented the best-performing baseline, linear mixture, in our Hiero-style MT system and in fact it achieves the hights BLEU score among all the baselines as shown in Table 2.</S>
    <S sid="94" ssid="4">This baseline is run three times the score is averaged over the BLEU scores with standard deviation of 0.34.</S>
    <S sid="95" ssid="5">Table 3 shows the results of ensemble decoding with different mixture operations and model weight settings.</S>
    <S sid="96" ssid="6">Each mixture operation has been evaluated on the test-set by setting the component weights uniformly (denoted by uniform) and by tuning the weights using CONDOR (denoted by tuned) on a held-out set.</S>
    <S sid="97" ssid="7">The tuned scores (3rd column in Table 3) are averages of three runs with different initial points as in Clark et al. (2011).</S>
    <S sid="98" ssid="8">We also reported the BLEU scores when we applied the span-wise normalization heuristic.</S>
    <S sid="99" ssid="9">All of these mixture operations were able to significantly improve over the concatenation baseline.</S>
    <S sid="100" ssid="10">In particular, Switching:Max could gain up to 2.2 BLEU points over the concatenation baseline and 0.39 BLEU points over the best performing baseline (i.e.</S>
    <S sid="101" ssid="11">linear mixture model implemented in Hiero) which is statistically significant based on Clark et al. (2011) (p = 0.02).</S>
    <S sid="102" ssid="12">Prod when using with uniform weights gets the lowest score among the mixture operations, however after tuning, it learns to bias the weights towards one of the models and hence improves by 1.31 BLEU points.</S>
    <S sid="103" ssid="13">Although Switching:Sum outperforms the concatenation baseline, it is substantially worse than other mixture operations.</S>
    <S sid="104" ssid="14">One explanation that Switching:Max is the best performing operation and Switching:Sum is the worst one, despite their similarities, is that Switching:Max prefers more peaked distributions while Switching:Sum favours a model that has fewer hypotheses for each span.</S>
    <S sid="105" ssid="15">An interesting observation based on the results in Table 3 is that uniform weights are doing reasonably well given that the component weights are not optimized and therefore model scores may not be in the same scope (refer to discussion in &#167;3.2).</S>
    <S sid="106" ssid="16">We suspect this is because a single LM is shared between both models.</S>
    <S sid="107" ssid="17">This shared component controls the variance of the weights in the two models when combined with the standard L-1 normalization of each model&#8217;s weights and hence prohibits models to have too varied scores for the same input.</S>
    <S sid="108" ssid="18">Though, it may not be the case when multiple LMs are used which are not shared.</S>
    <S sid="109" ssid="19">Two sample sentences from the EMEA test-set along with their translations by the IN, OUT and Ensemble models are shown in Figure 2.</S>
    <S sid="110" ssid="20">The boxes show how the Ensemble model is able to use ngrams from the IN and OUT models to construct a better translation than both of them.</S>
    <S sid="111" ssid="21">In the first example, there are two OOVs one for each of the IN and OUT models.</S>
    <S sid="112" ssid="22">Our approach is able to resolve the OOV issues by taking advantage of the other model&#8217;s presence.</S>
    <S sid="113" ssid="23">Similarly, the second example shows how ensemble decoding improves lexical choices as well as word re-orderings.</S>
  </SECTION>
  <SECTION title="5 Related Work" number="6">
    <S sid="114" ssid="1">Early approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005).</S>
    <S sid="115" ssid="2">Foster et al. (2010), however, uses a different approach to select related sentences from OUT.</S>
    <S sid="116" ssid="3">They use language model perplexities from IN to select relavant sentences from OUT.</S>
    <S sid="117" ssid="4">These sentences are used to enrich the IN training set.</S>
    <S sid="118" ssid="5">Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum&#180;e and Marcu, 2006).</S>
    <S sid="119" ssid="6">Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation.</S>
    <S sid="120" ssid="7">This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instances.</S>
    <S sid="121" ssid="8">Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality.</S>
    <S sid="122" ssid="9">Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
    <S sid="123" ssid="10">Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences.</S>
    <S sid="124" ssid="11">A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs.</S>
    <S sid="125" ssid="12">Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2.</S>
    <S sid="126" ssid="13">Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009).</S>
    <S sid="127" ssid="14">Among these approaches are sentence-based, phrase-based and word-based output combination methods.</S>
    <S sid="128" ssid="15">In a similar approach, Koehn and Schroeder (2007) use a feature of the factored translation model framework in Moses SMT system (Koehn and Schroeder, 2007) to use multiple alternative decoding paths.</S>
    <S sid="129" ssid="16">Two decoding paths, one for each translation table (IN and OUT), were used during decoding.</S>
    <S sid="130" ssid="17">The weights are set with minimum error rate training (Och, 2003).</S>
    <S sid="131" ssid="18">Our work is closely related to Koehn and Schroeder (2007) but uses a different approach to deal with multiple translation tables.</S>
    <S sid="132" ssid="19">The Moses SMT system implements (Koehn and Schroeder, 2007) and can treat multiple translation tables in two different ways: intersection and union.</S>
    <S sid="133" ssid="20">In intersection, for each span only the hypotheses would be used that are present in all phrase tables.</S>
    <S sid="134" ssid="21">For each set of hypothesis with the same source and target phrases, a new hypothesis is created whose feature-set is the union of feature sets of all corresponding hypotheses.</S>
    <S sid="135" ssid="22">Union, on the other hand, uses hypotheses from all the phrase tables.</S>
    <S sid="136" ssid="23">The feature set of these hypotheses are expanded to include one feature set for each table.</S>
    <S sid="137" ssid="24">However, for the corresponding feature values of those phrase-tables that did not have a particular phrase-pair, a default log probability value of 0 is assumed (Bertoldi and Federico, 2009) which is counter-intuitive as it boosts the score of hypotheses with phrase-pairs that do not belong to all of the translation tables.</S>
    <S sid="138" ssid="25">Our approach is different from Koehn and Schroeder (2007) in a number of ways.</S>
    <S sid="139" ssid="26">Firstly, unlike the multi-table support of Moses which only supports phrase-based translation table combination, our approach supports ensembles of both hierarchical and phrase-based systems.</S>
    <S sid="140" ssid="27">With little modification, it can also support ensemble of syntax-based systems with the other two state-of-the-art SMT systems.</S>
    <S sid="141" ssid="28">Secondly, our combining method uses the union option, but instead of preserving the features of all phrase-tables, it only combines their scores using various mixture operations.</S>
    <S sid="142" ssid="29">This enables us to experiment with a number of different operations as opposed to sticking to only one combination method.</S>
    <S sid="143" ssid="30">Finally, by avoiding increasing the number of features we can add as many translation models as we need without serious performance drop.</S>
    <S sid="144" ssid="31">In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al., 2008).</S>
    <S sid="145" ssid="32">Our approach differs from the model combination approach of DeNero et al. (2010), a generalization of consensus or minimum Bayes risk decoding where the search space consists of those of multiple systems, in that model combination uses forest of derivations of all component models to do the combination.</S>
    <S sid="146" ssid="33">In other words, it requires all component models to fully decode each sentence, compute n-gram expectations from each component model and calculate posterior probabilities over translation derivations.</S>
    <S sid="147" ssid="34">While, in our approach we only use partial hypotheses from component models and the derivation forest is constructed by the ensemble model.</S>
    <S sid="148" ssid="35">A major difference is that in the model combination approach the component search spaces are conjoined and they are not intermingled as opposed to our approach where these search spaces are intermixed on spans.</S>
    <S sid="149" ssid="36">This enables us to generate new sentences that cannot be generated by component models.</S>
    <S sid="150" ssid="37">Furthermore, various combination methods can be explored in our approach.</S>
    <S sid="151" ssid="38">Finally, main techniques used in this work are orthogonal to our approach such as Minimum Bayes Risk decoding, using n-gram features and tuning using MERT.</S>
    <S sid="152" ssid="39">Finally, our work is most similar to that of Liu et al. (2009) where max-derivation and maxtranslation decoding have been used.</S>
    <S sid="153" ssid="40">Maxderivation finds a derivation with highest score and max-translation finds the highest scoring translation by summing the score of all derivations with the same yield.</S>
    <S sid="154" ssid="41">The combination can be done in two levels: translation-level and derivation-level.</S>
    <S sid="155" ssid="42">Their derivation-level max-translation decoding is similar to our ensemble decoding with wsum as the mixture operation.</S>
    <S sid="156" ssid="43">We did not restrict ourself to this particular mixture operation and experimented with a number of different mixing techniques and as Table 3 shows we could improve over wsum in our experimental setup.</S>
    <S sid="157" ssid="44">Liu et al. (2009) used a modified version of MERT to tune max-translation decoding weights, while we use a two-step approach using MERT for tuning each component model separately and then using CONDOR to tune component weights on top of them.</S>
  </SECTION>
  <SECTION title="6 Conclusion &amp; Future Work" number="7">
    <S sid="158" ssid="1">In this paper, we presented a new approach for domain adaptation using ensemble decoding.</S>
    <S sid="159" ssid="2">In this approach a number of MT systems are combined at decoding time in order to form an ensemble model.</S>
    <S sid="160" ssid="3">The model combination can be done using various mixture operations.</S>
    <S sid="161" ssid="4">We showed that this approach can gain up to 2.2 BLEU points over its concatenation baseline and 0.39 BLEU points over a powerful mixture model.</S>
    <S sid="162" ssid="5">Future work includes extending this approach to use multiple translation models with multiple language models in ensemble decoding.</S>
    <S sid="163" ssid="6">Different mixture operations can be investigated and the behaviour of each operation can be studied in more details.</S>
    <S sid="164" ssid="7">We will also add capability of supporting syntax-based ensemble decoding and experiment how a phrase-based system can benefit from syntax information present in a syntax-aware MT system.</S>
    <S sid="165" ssid="8">Furthermore, ensemble decoding can be applied on domain mixing settings in which development sets and test sets include sentences from different domains and genres, and this is a very suitable setting for an ensemble model which can adapt to new domains at test time.</S>
    <S sid="166" ssid="9">In addition, we can extend our approach by applying some of the techniques used in other system combination approaches such as consensus decoding, using n-gram features, tuning using forest-based MERT, among other possible extensions.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="167" ssid="1">This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the last author.</S>
    <S sid="168" ssid="2">We would like to thank Philipp Koehn and the anonymous reviewers for their valuable comments.</S>
    <S sid="169" ssid="3">We also thank the developers of GIZA++ and Condor which we used for our experiments.</S>
  </SECTION>
</PAPER>
