<PAPER>
  <S sid="0">Effective Selection of Translation Model Training Data</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.</S>
    <S sid="2" ssid="2">Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.</S>
    <S sid="3" ssid="3">By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.</S>
    <S sid="4" ssid="4">In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.</S>
    <S sid="5" ssid="5">The results show that our methods outperform previous methods.</S>
    <S sid="6" ssid="6">When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Statistical machine translation depends heavily on large scale parallel corpora.</S>
    <S sid="8" ssid="2">The corpora are necessary priori knowledge for training effective translation model.</S>
    <S sid="9" ssid="3">However, domain-specific machine translation has few parallel corpora for translation model training in the domain of interest.</S>
    <S sid="10" ssid="4">For this, an effective approach is to automatically select and expand domain-specific sentence pairs from large scale general-domain parallel corpus.</S>
    <S sid="11" ssid="5">The approach is named Data Selection.</S>
    <S sid="12" ssid="6">Current data selection methods mostly use language models trained on small scale indomain data to measure domain relevance and select domain-relevant parallel sentence pairs to expand training corpora.</S>
    <S sid="13" ssid="7">Related work in literature has proven that the expanded corpora can substantially improve the performance of machine translation (Duh et al., 2010; Haddow and Koehn, 2012).</S>
    <S sid="14" ssid="8">However, the methods are still far from satisfactory for real application for the following reasons: In a word, current data selection methods can&#8217;t well maintain both parallelism and domainrelevance of bitext.</S>
    <S sid="15" ssid="9">To overcome the problem, we first propose the method combining translation model with language model in data selection.</S>
    <S sid="16" ssid="10">The language model measures the domainspecific generation probability of sentences, being used to select domain-relevant sentences at both sides of source and target language.</S>
    <S sid="17" ssid="11">Meanwhile, the translation model measures the translation probability of sentence pair, being used to verify the parallelism of the selected domainrelevant bitext.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="18" ssid="1">The existing data selection methods are mostly based on language model.</S>
    <S sid="19" ssid="2">Yasuda et al. (2008) and Foster et al.</S>
    <S sid="20" ssid="3">(2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models.</S>
    <S sid="21" ssid="4">Axelrod et al. (2011) improved the perplexitybased approach and proposed bilingual crossentropy difference as a ranking function with inand general- domain language models.</S>
    <S sid="22" ssid="5">Duh et al. (2013) employed the method of (Axelrod et al., Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 569&#8211;573, Baltimore, Maryland, USA, June 23-25 2014. c&#65533;2014 Association for Computational Linguistics 2011) and further explored neural language model for data selection rather than the conventional n-gram language model.</S>
    <S sid="23" ssid="6">Although previous works in data selection (Duh et al., 2013; Koehn and Haddow, 2012; Axelrod et al., 2011; Foster et al., 2010; Yasuda et al., 2008) have gained good performance, the methods which only adopt language models to score the sentence pairs are sub-optimal.</S>
    <S sid="24" ssid="7">The reason is that a sentence pair contains a source language sentence and a target language sentence, while the existing methods are incapable of evaluating the mutual translation probability of sentence pair in the target domain.</S>
    <S sid="25" ssid="8">Thus, we propose novel methods which are based on translation model and language model for data selection.</S>
  </SECTION>
  <SECTION title="3 Training Data Selection Methods" number="3">
    <S sid="26" ssid="1">We present three data selection methods for ranking and selecting domain-relevant sentence pairs from general-domain corpus, with an eye towards improving domain-specific translation model performance.</S>
    <S sid="27" ssid="2">These methods are based on language model and translation model, which are trained on small in-domain parallel data.</S>
    <S sid="28" ssid="3">Translation model is a key component in statistical machine translation.</S>
    <S sid="29" ssid="4">It is commonly used to translate the source language sentence into the target language sentence.</S>
    <S sid="30" ssid="5">However, in this paper, we adopt the translation model to evaluate the translation probability of sentence pair and develop a simple but effective variant of translation model to rank the sentence pairs in the generaldomain corpus.</S>
    <S sid="31" ssid="6">The formulations are detailed as below: Where ( ) is the translation model, which is IBM Model 1 in this paper, it represents the translation probability of target language sentence conditioned on source language sentence . and are the number of words in sentence and respectively.</S>
    <S sid="32" ssid="7">( ) is the translation probability of word conditioned on word and is estimated from the small in-domain parallel data.</S>
    <S sid="33" ssid="8">The parameter is a constant and is assigned with the value of 1.0. is the lengthnormalized IBM Model 1, which is used to score general-domain sentence pairs.</S>
    <S sid="34" ssid="9">The sentence pair with higher score is more likely to be generated by in-domain translation model, thus, it is more relevant to the in-domain corpus and will be remained to expand the training data.</S>
    <S sid="35" ssid="10">As described in section 1, the existing data selection methods which only adopt language model to score sentence pairs are unable to measure the mutual translation probability of sentence pairs.</S>
    <S sid="36" ssid="11">To solve the problem, we develop the second data selection method, which is based on the combination of translation model and language model.</S>
    <S sid="37" ssid="12">Our method and ranking function are formulated as follows: Where ( ) is a joint probability of sentence and according to the translation model ( ) and language model ( ), whose parameters are estimated from the small in-domain text. is the improved ranking function and used to score the sentence pairs with the length-normalized translation model ( )and language model ( ).</S>
    <S sid="38" ssid="13">The sentence pair with higher score is more similar to in-domain corpus, and will be picked out.</S>
    <S sid="39" ssid="14">Combining Translation and Language Models As presented in subsection 3.2, the method combines translation model and language model to rank the sentence pairs in the general-domain corpus.</S>
    <S sid="40" ssid="15">However, it does not evaluate the inverse translation probability of sentence pair and the probability of target language sentence.</S>
    <S sid="41" ssid="16">Thus, we take bidirectional scores into account and simply sum the scores in both directions.</S>
    <S sid="42" ssid="17">Again, the sentence pairs with higher scores are presumed to be better and will be selected to incorporate into the domain-specific training data.</S>
    <S sid="43" ssid="18">This approach makes full use of two translation models and two language models for sentence pairs ranking.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="44" ssid="1">We conduct our experiments on the Spoken Language Translation English-to-Chinese task.</S>
    <S sid="45" ssid="2">Two corpora are needed for the data selection.</S>
    <S sid="46" ssid="3">The indomain data is collected from CWMT09, which consists of spoken dialogues in a travel setting, containing approximately 50,000 parallel sentence pairs in English and Chinese.</S>
    <S sid="47" ssid="4">Our generaldomain corpus mined from the Internet contains 16 million sentence pairs.</S>
    <S sid="48" ssid="5">Both the in- and general- domain corpora are identically tokenized (in English) and segmented (in Chinese)1.</S>
    <S sid="49" ssid="6">The details of corpora are listed in Table 1.</S>
    <S sid="50" ssid="7">Additionally, we evaluate our work on the 2004 test set of &#8220;863&#8221; Spoken Language Translation task (&#8220;863&#8221; SLT), which consists of 400 English sentences with 4 Chinese reference translations for each.</S>
    <S sid="51" ssid="8">Meanwhile, the 2005 test set of &#8220;863&#8221; SLT task, which contains 456 English sentences with 4 references each, is used as the development set to tune our systems.</S>
    <S sid="52" ssid="9">We use the NiuTrans 2 toolkit which adopts GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation system.</S>
    <S sid="53" ssid="10">As NiuTrans integrates the mainstream translation engine, we select hierarchical phrasebased engine (Chiang, 2007) to extract the translation rules and carry out our experiments.</S>
    <S sid="54" ssid="11">Moreover, in the decoding process, we use the NiuTrans decoder to produce the best outputs, and score them with the widely used NIST mteval131a3 tool.</S>
    <S sid="55" ssid="12">This tool scores the outputs in several criterions, while the case-insensitive BLEU-4 (Papineni et al., 2002) is used as the evaluation for the machine translation system.</S>
    <S sid="56" ssid="13">Our work relies on the use of in-domain language models and translation models to rank the sentence pairs from the general-domain bilingual training set.</S>
    <S sid="57" ssid="14">Here, we employ ngram language model and IBM Model 1 for data selection.</S>
    <S sid="58" ssid="15">Thus, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train the in-domain 4-gram language model with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1998).</S>
    <S sid="59" ssid="16">The language model is only used to score the general-domain sentences.</S>
    <S sid="60" ssid="17">Meanwhile, we use the language model training scripts integrated in the NiuTrans toolkit to train another 4-gram language model, which is used in MT tuning and decoding.</S>
    <S sid="61" ssid="18">Additionally, we adopt GIZA++ to get the word alignment of in-domain parallel data and form the word translation probability table.</S>
    <S sid="62" ssid="19">This table will be used to compute the translation probability of general-domain sentence pairs.</S>
    <S sid="63" ssid="20">As described above, by using the NiuTrans toolkit, we have built two baseline systems to fulfill &#8220;863&#8221; SLT task in our experiments.</S>
    <S sid="64" ssid="21">The In-domain baseline trained on spoken language corpus has 1.05 million rules in its hierarchicalphrase table.</S>
    <S sid="65" ssid="22">While, the General-domain baseline trained on 16 million sentence pairs has a hierarchical phrase table containing 1.7 billion translation rules.</S>
    <S sid="66" ssid="23">These two baseline systems are equipped with the same language model which is trained on large-scale monolingual target language corpus.</S>
    <S sid="67" ssid="24">The BLEU scores of the Indomain and General-domain baseline system are listed in Table 2.</S>
    <S sid="68" ssid="25">The results show that General-domain system trained on a larger amount of bilingual resources outperforms the system trained on the in-domain corpus by over 12 BLEU points.</S>
    <S sid="69" ssid="26">The reason is that large scale parallel corpus maintains more bilingual knowledge and language phenomenon, while small in-domain corpus encounters data sparse problem, which degrades the translation performance.</S>
    <S sid="70" ssid="27">However, the performance of General-domain baseline can be improved further.</S>
    <S sid="71" ssid="28">We use our three methods to refine the generaldomain corpus and improve the translation performance in the domain of interest.</S>
    <S sid="72" ssid="29">Thus, we build several contrasting systems trained on refined training data selected by the following different methods.</S>
    <S sid="73" ssid="30">We adopt five methods for extracting domainrelevant parallel data from general-domain corpus.</S>
    <S sid="74" ssid="31">Using the scoring methods, we rank the sentence pairs of the general-domain corpus and select only the top N = {50k, 100k, 200k, 400k, 600k, 800k, 1000k} sentence pairs as refined training data.</S>
    <S sid="75" ssid="32">New MT systems are then trained on these small refined training data.</S>
    <S sid="76" ssid="33">Figure 1 shows the performances of systems trained on selected corpora from the general-domain corpus.</S>
    <S sid="77" ssid="34">The horizontal coordinate represents the number of selected sentence pairs and vertical coordinate is the BLEU scores of MT systems.</S>
    <S sid="78" ssid="35">From Figure 1, we conclude that these five data selection methods are effective for domainspecific translation.</S>
    <S sid="79" ssid="36">When top 600k sentence pairs are picked out from general-domain corpus to train machine translation systems, the systems perform higher than the General-domain baseline trained on 16 million parallel data.</S>
    <S sid="80" ssid="37">The results indicate that more training data for translation model is not always better.</S>
    <S sid="81" ssid="38">When the domainspecific bilingual resources are deficient, the domain-relevant sentence pairs will play an important role in improving the translation performance.</S>
    <S sid="82" ssid="39">Additionally, it turns out that our methods (TM, TM+LM and Bidirectional TM+LM) are indeed more effective in selecting domainrelevant sentence pairs.</S>
    <S sid="83" ssid="40">In the end-to-end SMT evaluation, TM selects top 600k sentence pairs of general-domain corpus, but increases the translation performance by 2.7 BLEU points.</S>
    <S sid="84" ssid="41">Meanwhile, the TM+LM and Bidirectional TM+LM have gained 3.66 and 3.56 BLEU point improvements compared against the generaldomain baseline system.</S>
    <S sid="85" ssid="42">Compared with the mainstream methods (Igram and Ieural net), our methods increase translation performance by nearly 3 BLEU points, when the top 600k sentence pairs are picked out.</S>
    <S sid="86" ssid="43">Although, in the figure 1, our three methods are not performing better than the existing methods in all cases, their overall performances are relatively higher.</S>
    <S sid="87" ssid="44">We therefore believe that combining in-domain translation model and language model to score the sentence pairs is well-suited for domainrelevant sentence pair selection.</S>
    <S sid="88" ssid="45">Furthermore, we observe that the overall performance of our methods is gradually improved.</S>
    <S sid="89" ssid="46">This is because our methods are combining more statistical characteristics of in-domain data in ranking and selecting sentence pairs.</S>
    <S sid="90" ssid="47">The results have proven the effectiveness of our methods again.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="5">
    <S sid="91" ssid="1">We present three novel methods for translation model training data selection, which are based on the translation model and language model.</S>
    <S sid="92" ssid="2">Compared with the methods which only employ language model for data selection, we observe that our methods are able to select high-quality domain-relevant sentence pairs and improve the translation performance by nearly 3 BLEU points.</S>
    <S sid="93" ssid="3">In addition, our methods make full use of the limited in-domain data and are easily implemented.</S>
    <S sid="94" ssid="4">In the future, we are interested in applying our methods into domain adaptation task of statistical machine translation in model level.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="95" ssid="1">This research work has been sponsored by two NSFC grants, No.61373097 and No.61272259, and one National Science Foundation of Suzhou (Grants No.</S>
    <S sid="96" ssid="2">SH201212).</S>
  </SECTION>
</PAPER>
