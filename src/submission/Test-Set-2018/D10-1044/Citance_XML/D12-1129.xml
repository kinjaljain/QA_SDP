<PAPER>
  <S sid="0">A New Minimally-Supervised Framework for Domain Word Sense Disambiguation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD).</S>
    <S sid="2" ssid="2">Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique.</S>
    <S sid="3" ssid="3">The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD.</S>
    <S sid="4" ssid="4">Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Domain information pervades most of the text we read every day.</S>
    <S sid="6" ssid="2">If we just think of the Web, the vast majority of its textual content is domain oriented.</S>
    <S sid="7" ssid="3">A case in point is Wikipedia, which provides encyclopedic coverage for a huge number of knowledge domains (Medelyan et al., 2009), but most blogs, Web sites and newspapers also provide a great deal of information focused on specific areas of knowledge.</S>
    <S sid="8" ssid="4">When it comes to automatic text understanding, then, it is crucial to take into account the domain specificity of a piece of text, so as to perform a focused and as-precise-as-possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction.</S>
    <S sid="9" ssid="5">Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010).</S>
    <S sid="10" ssid="6">Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now.</S>
    <S sid="11" ssid="7">Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011).</S>
    <S sid="12" ssid="8">More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010).</S>
    <S sid="13" ssid="9">High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998).</S>
    <S sid="14" ssid="10">Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains.</S>
    <S sid="15" ssid="11">While Wikipedia has recently been considered a valid alternative (Mihalcea, 2007), it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexicographic sense).</S>
    <S sid="16" ssid="12">In this paper we provide three main contributions: &#8226; We tackle the above issues by introducing a new framework based on the minimallysupervised acquisition of specialized glossaries for dozens of domains. as a sense inventory for domain WSD.</S>
    <S sid="17" ssid="13">As a result, we redefine the domain WSD task as one of picking out the most appropriate gloss (finegrained setting) or domain (coarse-grained setting) from a multi-domain glossary.</S>
    <S sid="18" ssid="14">&#8226; We show that our framework represents a considerable departure from the common usage of a general-purpose sense inventory such as WordNet, in that, thanks to the wide coverage of domain meanings, it enables highperformance unsupervised WSD on many domains in the range of 69-80% F1.</S>
    <S sid="19" ssid="15">Furthermore, our approach can be customized to any set of domains of interest, and new senses, i.e., glosses, can be added at any time (either manually or automatically) to the multi-domain sense inventory.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="20" ssid="1">Domain WSD has been the focus of much interest in the last few years.</S>
    <S sid="21" ssid="2">An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007).</S>
    <S sid="22" ssid="3">Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007).</S>
    <S sid="23" ssid="4">Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004).</S>
    <S sid="24" ssid="5">Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005).</S>
    <S sid="25" ssid="6">However, their performance is typically lower than supervised systems.</S>
    <S sid="26" ssid="7">On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain.</S>
    <S sid="27" ssid="8">Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010).</S>
    <S sid="28" ssid="9">However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory.</S>
    <S sid="29" ssid="10">But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains.</S>
    <S sid="30" ssid="11">Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al., 2005) show that domain WSD can achieve accuracy in the 5060% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011).</S>
    <S sid="31" ssid="12">In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms.</S>
    <S sid="32" ssid="13">Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora.</S>
    <S sid="33" ssid="14">To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web.</S>
    <S sid="34" ssid="15">Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010).</S>
    <S sid="35" ssid="16">Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system.</S>
    <S sid="36" ssid="17">Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed.</S>
    <S sid="37" ssid="18">In contrast, our approach tackles crossdomain ambiguity, by working with virtually any set of domains and minimizing the requirements by harvesting domain terms and definitions from the Web, bootstrapped using a small number of seeds.</S>
    <S sid="38" ssid="19">The existing approach closest to ours is that of Huang and Riloff (2010), who devised a bootstrapping approach to induce semantic class taggers from domain text.</S>
    <S sid="39" ssid="20">The semantic classes are associated with arbitrary NPs and must be established beforehand.</S>
    <S sid="40" ssid="21">Our objective, instead, is to perform domain disambiguation at the word level.</S>
    <S sid="41" ssid="22">To do this, we redefine the domain WSD problem as one of selecting the most suitable gloss from those available in our full-fledged multi-domain glossary.</S>
  </SECTION>
  <SECTION title="3 A Minimally-Supervised Framework for Domain WSD" number="3">
    <S sid="42" ssid="1">In this section we present our new framework for performing domain WSD.</S>
    <S sid="43" ssid="2">The framework consists of two phases: glossary bootstrapping (Section 3.1) and domain WSD (Section 3.2).</S>
    <S sid="44" ssid="3">The objective of the first phase is to acquire a multidomain glossary from the Web with minimal supervision.</S>
    <S sid="45" ssid="4">We initially select a set D of domains of interest.</S>
    <S sid="46" ssid="5">For each individual domain d E D we start with an empty set of HTML patterns Pd (i.e., Pd := 0), used for gloss harvesting.</S>
    <S sid="47" ssid="6">During this phase we iteratively populate the pattern set by means of six steps, described in the next six subsections and depicted in Figure 1.</S>
    <S sid="48" ssid="7">The final output of this phase will be a glossary Gd consisting of domain terms and their automatically-harvested glosses.</S>
    <S sid="49" ssid="8">First, given the domain d, we manually pick out K hypernymy relation seeds 5d = {(ti, hi), ... , (tK, hK)}, where the pair (ti, hi) contains a domain term ti and its generalization hi (e.g., (firewall, security system)).</S>
    <S sid="50" ssid="9">The only constraint we impose is that the selected relations must be distinctive for the domain d of interest.</S>
    <S sid="51" ssid="10">The chosen hypernymy relations have to be as topical and representative as possible for the given domain (e.g., (compiler, computer program) is an appropriate pair for computer science, while (byte, unit of measurement) is not, as it might cause the extraction of several glossaries of various units and measures).</S>
    <S sid="52" ssid="11">Note that this is the only human intervention in the entire glossary acquisition process.</S>
    <S sid="53" ssid="12">We now set the iteration counter k to 1 and start the first iteration of the process (steps 2-5).</S>
    <S sid="54" ssid="13">After each iteration k, we keep track of the set of glosses Gd, acquired during iteration k. For each seed pair (ti, hi), we submit the following three queries to a Web search engine: &#8220;ti&#8221; &#8220;hi&#8221; glossary1, &#8220;ti&#8221; &#8220;hi&#8221; definition, &#8220;ti&#8221; &#8220;hi&#8221; dictionary and collect the 64 top-ranking results for each query2.</S>
    <S sid="55" ssid="14">Each resulting page is a candidate glossary for the domain d identified by our relation seeds 5d.</S>
    <S sid="56" ssid="15">We initialize the glossary for iteration k as follows: Gd := 0.</S>
    <S sid="57" ssid="16">Next, from each resulting page, we harvest all the text snippets s starting with ti and ending with hi (e.g., firewall&lt;/b&gt; -- a &lt;i&gt;security system), i.e., s = ti ... hi.</S>
    <S sid="58" ssid="17">For each such text snippet s, we perform five substeps: start from ti and move right until we extract the longest sequence pM of HTML tags and non-alphanumeric characters, which we call the term/gloss separator, between ti and the glossary definition (e.g., &#8220;&lt;/b&gt; --&#8221; between &#8220;firewall&#8221; and &#8220;a&#8221; in the above example); b) gloss extraction: we expand the snippet s to the right of hi in search of the entire gloss of ti, i.e., until we reach a non-formatting tag element (e.g., &lt;span&gt;, &lt;p&gt;, &lt;div&gt;), while ignoring formatting elements such as &lt;b&gt;, &lt;i&gt; and &lt;a&gt; which are typically included within a definition sentence.</S>
    <S sid="59" ssid="18">As a result, we obtain the sequence ti pM glosss(ti) pR, where glosss(ti) is our gloss for seed term ti in snippet s (which includes hi by construction) and pR is the nonformatting HTML tag element to the right of the extracted gloss.</S>
    <S sid="60" ssid="19">For example, we extend the above definition for firewall to: &#8220;a &lt;i&gt;security system&lt;/i&gt; for protecting against illegal entry to a local area network.&#8221;. c) pattern instance extraction: we extract the following pattern instance: where pL and pR are, respectively, the left boundary of ti and the right boundary of glosss(ti), and pM is the term/gloss separator extracted at step 3(a).</S>
    <S sid="61" ssid="20">The two boundaries pL and pR are obtained by extracting the longest sequence of HTML tags and non-alphanumeric characters obtained when moving to the left of ti and the right of glosss(ti), respectively3.</S>
    <S sid="62" ssid="21">For the above example, we extract the following pattern instance: 3The minimum and maximum length of both PL and PR are set to 4 and 50 characters, respectively, as a result of a tuning phase described in Section 4.1. pL = &#8220;&lt;p&gt;&lt;b&gt;&#8221;, ti = &#8220;firewall&#8221;, pM = &#8220;&lt;/b&gt; --&#8221;, glosss(ti) = &#8220;a &lt;i&gt;security system&lt;/i&gt; for protecting against illegal entry to a local area network.&#8221;, pR =&#8220;&lt;/p&gt;&#8221;. d) pattern extraction: we generalize the above pattern instance to the following pattern: i.e., we replace ti and glosss(ti) with *.</S>
    <S sid="63" ssid="22">In the above example, we obtain the following pattern: &lt;p&gt;&lt;b&gt; * &lt;/b&gt; -- * &lt;/p&gt;.</S>
    <S sid="64" ssid="23">Finally, we add the generalized pattern to the set of patterns Pd, i.e., we set Pd := Pd U {pL * pM * pR}.</S>
    <S sid="65" ssid="24">We also add the first sentence of the retrieved definition glosss(ti) to our glossary Gkd, i.e., Gkd := Gkd U {(ti, first(glosss(ti)))}, where first(g) returns the first sentence of gloss g. e) pattern matching: we look for additional pairs of terms/glosses in the Web page containing the snippet s by matching the page against the generalized pattern pL * pM * pR.</S>
    <S sid="66" ssid="25">We then add to Gkd the new (term, gloss) pairs matching the generalized pattern.</S>
    <S sid="67" ssid="26">As a result of this step, we obtain a glossary Gkd for the terms discovered at iteration k. Importantly not all the extracted definitions pertain to the domain of interest.</S>
    <S sid="68" ssid="27">In order to rank by domain pertinence the glosses obtained at iteration k, we define the terminology Tk&#8722;1 1 of the terms accumulated up until iteration k &#8722; 1 as follows: Gloss Domain Measures undertaken to return a degraded ecosystem&#8217;s functions and values, including its hydrology, plant and...</S>
    <S sid="69" ssid="28">BIOLOGY The renewing or repairing of a natural system so that its functions and qualities are comparable to its original... GEOGRAPHY The reign of Charles II in England.</S>
    <S sid="70" ssid="29">ROYALTY A goal of criminal sentencing that attempts to make the victim &#8221;whole again.&#8221; LAW The process and work of improving the degraded quality of the sound or image in terms of video and audio preservation.</S>
    <S sid="71" ssid="30">MEDIA A process used by radio astronomers to eliminate the smoothing effect observed in radio maps that is caused by... PHYSICS For the base step k = 1, we define T10 := T1, i.e., we use the first-iteration terminology itself.</S>
    <S sid="72" ssid="31">To rank the glosses, we first transform each acquired gloss g to its bag-of-words representation Bag(g), which contains all the single- and multi-word expressions in g. We then score each gloss g by the ratio of domain terms found in its bag of words: In Table 1 we show some glosses in the computer science domain (second column, domain terms are underlined) together with their score (last column).</S>
    <S sid="73" ssid="32">Next, we use a threshold 0 (tuned on a held-out domain, described in Section 4.1) to remove from Gd those glosses g whose score(g) &lt; 0.</S>
    <S sid="74" ssid="33">We now aim at selecting the new set of hypernymy relation seeds to be used to start the next iteration.</S>
    <S sid="75" ssid="34">We perform three substeps: a) Hypernym extraction: for each newly-acquired term/gloss pair (t, g) E Gd, we automatically extract a candidate hypernym h from the textual gloss g. To do this we use a simple unsupervised heuristic which just selects the first term in the gloss.</S>
    <S sid="76" ssid="35">More sophisticated, supervised approaches could have been used for hypernym extraction from glosses (Navigli and Velardi, 2010).</S>
    <S sid="77" ssid="36">However, note that, for the purposes of our glossary extraction task, it is not crucial to extract accurate hypernyms, but rather to harvest terms h which are very likely to occur in the glosses of t. We show an example of hypernym extraction for some terms in Table 1 (we report the term in column 1, the gloss in column 2 and the hypernyms extracted by our hypernym extraction technique in column 3). b) (Term, Hypernym)-ranking: we sort all the glosses in Gd by the number of seed terms found in each gloss.</S>
    <S sid="78" ssid="37">In the case of ties (i.e., glosses with the same number of seed terms), we further sort the glosses by the score shown in Formula 1.</S>
    <S sid="79" ssid="38">We show the number of seed terms and the scores for some glosses in Table 1 (columns 4 and 5, respectively), where seed terms are in bold and domain terms (i.e., in T&#65533;&#8722;1 If k equals the maximum number of iterations, we stop.</S>
    <S sid="80" ssid="39">Else, we increment the iteration counter (i.e., k := k + 1) and jump to step (2) of our glossary bootstrapping algorithm after replacing 5d with the new set of seeds.</S>
    <S sid="81" ssid="40">The output of the glossary bootstrapping phase is a domain glossary Gd := Ui=1,...,max Gid, where max is the total number of iterations.</S>
    <S sid="82" ssid="41">Given the nature of Web domain glossaries one can rarely find terms and definitions for general terms (e.g., jurisprudence for the LAW domain).</S>
    <S sid="83" ssid="42">In order to cover this gap, we apply domain filtering (see Section 3.1.4) to all the glosses contained in a general-purpose dictionary (we use WordNet).</S>
    <S sid="84" ssid="43">We then add the surviving term/gloss pairs to Gd.</S>
    <S sid="85" ssid="44">Now that we have acquired a glossary for each domain in our set D, we can create a multi-domain glossary !9 := {((t, g), d) : d E D, (t, g) E Gd}.</S>
    <S sid="86" ssid="45">Our glossary !9 is thus a set of term/gloss pairs for many domains.</S>
    <S sid="87" ssid="46">Note that one pair might individually belong to more than one domain, as glossary bootstrapping is performed separately for each domain.</S>
    <S sid="88" ssid="47">In Table 2 we show an example of the glosses acquired for the term restoration.</S>
    <S sid="89" ssid="48">We observe that 5 out of 6 senses are not available in WordNet (namely: the BIOLOGY, GEOGRAPHY, LAW, MEDIA and PHYSICS senses).</S>
    <S sid="90" ssid="49">Many of them are domainspecific meanings for the general concept of &#8220;the act of restoring&#8221;, with the BIOLOGY and GEOGRAPHY senses being very similar.</S>
    <S sid="91" ssid="50">However, this is a perfectly acceptable phenomenon as any of the two senses, i.e., glosses, would be equally valid when disambiguating a domain text dealing with ecosystem restoration.</S>
    <S sid="92" ssid="51">We redefine the task of domain WSD as one of selecting the most suitable gloss, if one exists, for an input term t. For instance, consider the sentence: &#8220;He performed the restoration of heavily corrupted images&#8221;.</S>
    <S sid="93" ssid="52">An appropriate option for this occurrence would be the MEDIA sense of restoration in Table 2.</S>
    <S sid="94" ssid="53">Our gloss-driven WSD paradigm has the desirable property of automatically providing two levels of sense granularity: a domain, coarse-grained level, similar in spirit to Word Domain Disambiguation (Sanfilippo et al., 2006), in which the sense inventory of a term t is just the set of domains for which t is covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW, MEDIA, PHYSICS in the example of Table 2), and a fine-grained level, which requires the selection of the gloss which best describes the sense denoted by the given word occurrence.</S>
    <S sid="95" ssid="54">A second desirable property of our gloss-driven WSD paradigm is that it relies on a flexible framework, which allows for the bootstrapping of new domain glossaries or the expansion of existing ones.</S>
    <S sid="96" ssid="55">However, while these two properties &#8211; i.e., double level of granularity distinctions and flexibility &#8211; are naturally inherent in the gloss-driven paradigm, the same cannot be said for mainstream open-text WSD in which generalpurpose static dictionaries are typically used.</S>
    <S sid="97" ssid="56">In order to evaluate our framework for domain WSD, we propose two fully unsupervised algorithms for gloss-driven domain WSD.</S>
    <S sid="98" ssid="57">Ideally, high performance could be obtained using state-of-the-art supervised WSD systems.</S>
    <S sid="99" ssid="58">However, in order to train such systems, a wide-coverage sense-labeled corpus should be available for each domain, a heavy task which we leave to future work.</S>
    <S sid="100" ssid="59">Instead, our objective is to show that high-performance domain WSD can be enabled with little effort by our framework.</S>
    <S sid="101" ssid="60">Domain Glossaries as Graphs For each domain d E D, we create an undirected graph Nd = (Vd, Ed) as follows: Vd is the set of concepts identified by term/gloss pairs in the domain glossary Gd, i.e., Vd := Gd; Ed is the set of edges between pairs of concepts, where an edge {(t, g), (t', g')} exists if and only if t' is such that t' =&#65533; t and t' occurs in the bag of words of the gloss g of t. In other words, t is connected to all the domain senses of words used in its definition g. Graph-based WSD Given an input text, for each domain d E D, we produce its bag of domain content words Cd = {w1, w2, ... , w,,,} by performing tokenization, lemmatization and compounding based on the lexicon of domain d. Then, given a target word t, we use Cd \ {t} as the context to disambiguate t within the domain d. In order to carry out domain WSD, i.e., to pick out the most suitable sense of t across domains, we apply a state-ofthe-art graph-based algorithm, namely Personalized PageRank (Haveliwala, 2002, PPR), to each domain graph Nd.</S>
    <S sid="102" ssid="61">PPR is a variant of the popular PageRank algorithm (Brin and Page, 1998) in which the damping probability mass is concentrated on a selected number of graph nodes, instead of being uniformly distributed across all nodes.</S>
    <S sid="103" ssid="62">Specifically, following Agirre and Soroa (2009) we concentrate the probability mass on the nodes (t', g') E Vd for which the term t' is a context word, i.e., t' E Cd.</S>
    <S sid="104" ssid="63">Next, for each domain d E D, we run PPR for a given number of iterations and obtain as output a probability distribution PPVd over the graph nodes.</S>
    <S sid="105" ssid="64">Finally, we select the most suitable gloss of t as follows: where PPVd(t, g) is the PPR probability for the term/gloss pair (t, g) and SensePPR(t) contains the best interpretation of t across all the domains D.</S>
  </SECTION>
  <SECTION title="3.2.3 Algorithm 2: PPR Boosted with Domain Distribution Information" number="4">
    <S sid="106" ssid="1">The words in a given text do not typically deal with a single domain.</S>
    <S sid="107" ssid="2">Instead, they touch different areas of knowledge which are intertwined with each other within the discourse.</S>
    <S sid="108" ssid="3">For example, a text dealing with VIDEOGAMES will often concern domains such as BUSINESS, COMPUTING, SPORTS, etc.</S>
    <S sid="109" ssid="4">Given an input text, we can capture its relevance for each domain by calculating the following domain score: where, as above, Cd is the set of content words from the input text which are covered by domain d. We thus propose a second algorithm which synergistically combines the spreading effect of PPR with the domain distribution information.</S>
    <S sid="110" ssid="5">The best sense for a given term t is calculated as follows: (4) that is, we select as the most suitable gloss for t the one which maximizes the product of its domain relevance score by its domain PPVd value.</S>
    <S sid="111" ssid="6">Note that the same gloss can occur in multiple domains and that it might obtain different scores depending on the domain.</S>
    <S sid="112" ssid="7">Again, since the approach is gloss-driven, we do not see this as a problem, but rather as a natural characteristic of our framework.</S>
  </SECTION>
  <SECTION title="4 Experimental Setup" number="5">
    <S sid="113" ssid="1">We selected 30 domains starting from the Wikipedia featured articles4.</S>
    <S sid="114" ssid="2">We show the domain labels in TaFrom the Web From WordNet From both Total Terms 74,295 83,904 18,313 176,512 Glosses 153,920 68,731 596 223,247 ble 3 (some labels have been conveniently shortened, e.g., PHYSICS should read PHYSICS &amp; ASTRONOMY).</S>
    <S sid="115" ssid="3">We manually identified 8 hypernym/hyponym seeds for each domain, totalizing 240 seeds.</S>
    <S sid="116" ssid="4">We used two criteria for selecting a seed: i) it covers a separate segment of the domain, and ii) it has to be specialized enough to avoid ambiguity.</S>
    <S sid="117" ssid="5">We show the seeds used in four of our domains in Table 4.</S>
    <S sid="118" ssid="6">We bootstrapped our glossary acquisition technique (cf.</S>
    <S sid="119" ssid="7">Section 3.1) on each domain and performed 5 iterations.</S>
    <S sid="120" ssid="8">For increasing the coverage of domain terms we used WordNet glosses (see Section 3.1.6).</S>
    <S sid="121" ssid="9">As a result, we obtained 30 domain glossaries.</S>
    <S sid="122" ssid="10">We also kept aside a 31st domain, namely FASHION, which we employed for tuning the minimum and maximum length of both pL and pR in Section 3.1.3 and the threshold 0 used to filter out non-domain glosses in Section 3.1.4.</S>
    <S sid="123" ssid="11">In Table 5 we show the statistics for the acquired multi-domain glossary by distinguishing Web-derived and WordNet terms and glosses.</S>
    <S sid="124" ssid="12">Our sense inventory is given by the 30-domain glossary obtained as a result of our glossary bootstrapping phase.</S>
    <S sid="125" ssid="13">Overall we collected 176,512 and 223,247 distinct terms and glosses, respectively, with an important contribution from both the Web and WordNet (see Table 5).</S>
    <S sid="126" ssid="14">The average number of glosses per term in our inventory is 1.9 (3.6 glosses on polysemous terms).</S>
    <S sid="127" ssid="15">However, note that a monosemous word in our domain sense inventory does not necessarily make disambiguation easier, as i) we might have missed other domain-specific senses, ii) an uncovered, non-domain sense might fit a word occurrence (in this case, the domain WSD algorithms might be (wrongly) biased towards returning the only possible choice if a non-zero disambiguation score is calculated for it).</S>
    <S sid="128" ssid="16">In order to determine the suitability of our multidomain sense inventory, we compared it with the latest version of WordNet Domains (Magnini et al., 2002, WND 3.2), a well-known resource which provides domain labels for almost 65,000 nominal WordNet synsets (we removed all the synsets tagged with the FACTOTUM label, which indicates no domain specificity).</S>
    <S sid="129" ssid="17">Since WND uses about 160 finer-grained domain labels, we manually mapped them to our 30 labels when possible (e.g.</S>
    <S sid="130" ssid="18">SOCCER and SWIMMING were mapped to SPORTS), totalizing 62,100 domain-labeled synsets.</S>
    <S sid="131" ssid="19">We calculated the coverage of our sense inventory against WND at the synset and the sense level, for each non-FACTOTUM synset.</S>
    <S sid="132" ssid="20">Given a WordNet synset 5, let d = UsES ds be the union of the domains ds provided for each synonym s E 5 by our sense inventory (ds = 0 if not present), and let d&#65533; be the domain labels assigned to 5 by WND.</S>
    <S sid="133" ssid="21">A synset is covered if d and d&#65533; intersect.</S>
    <S sid="134" ssid="22">At the sense level, instead, we consider a synonym s E 5 to be covered if ds and d' intersect.</S>
    <S sid="135" ssid="23">Our synset and sense coverage is 65.9% (40,969/62,100) and 63.7% (71,950/112,875), respectively.</S>
    <S sid="136" ssid="24">We also calculated an extra-coverage of 203.2% (229,384/112,875), that is the fraction of domain senses which are not available in WND, but we are able to provide in our sense inventory (see e.g. the example in Table 2) over the total number of senses in WND.</S>
    <S sid="137" ssid="25">While coverage and extra-coverage provide a good indicator of the completeness of our sense inventory, we need to calculate its precision to determine its correctness.</S>
    <S sid="138" ssid="26">To do so, we randomly sampled 500 domain glosses of terms for which no WordNet sense was tagged with the same domain in WND.</S>
    <S sid="139" ssid="27">A manual validation of this sample resulted in an 87.0% (435/500) estimate of the precision of our sense inventory.</S>
    <S sid="140" ssid="28">A dataset for 30 domains We used the Gigaword corpus (Graff and Cieri, 2003) to extract a 6paragraph text snippet for each of the 30 domains.</S>
    <S sid="141" ssid="29">As a result, we obtained a domain dataset made up of 180 paragraphs to which we applied tokenization, lemmatization and compounding, totaling 1432 domain content words overall (47.7 content words per domain on average).</S>
    <S sid="142" ssid="30">The average polysemy of the words in the dataset was of 9.7 glosses and 4.4 domains per word.</S>
    <S sid="143" ssid="31">Each content word was manually tagged with the most suitable glosses from our multi-domain glossary (3.9 glosses, i.e., senses per word were assigned on average).</S>
    <S sid="144" ssid="32">The annotation task was performed by two annotators with adjudication.</S>
    <S sid="145" ssid="33">Sports and Finance We also experimented with the gold standard produced by Koeling et al. (2005).</S>
    <S sid="146" ssid="34">The dataset covers two domains: SPORTS and FINANCE.</S>
    <S sid="147" ssid="35">The dataset comprises 41 ambiguous words (with an average polysemy of 6.7 senses), many of which express different meanings in the two domains.</S>
    <S sid="148" ssid="36">In each domain, and for each word, around 100 sentences were sense-annotated with WordNet.</S>
    <S sid="149" ssid="37">Environment Finally, we also carried out an experiment on the ENVIRONMENT dataset from the Semeval-2010 domain WSD task (Agirre et al., 2010).</S>
    <S sid="150" ssid="38">The dataset includes 1,398 content words (of which 1,032 content nouns) tagged with WordNet senses.</S>
    <S sid="151" ssid="39">We applied the two algorithms proposed in Section 3.2, namely vanilla PPR and domain-boosted PPR.</S>
    <S sid="152" ssid="40">For both versions of PPR we employed UKB, a readily-available implementation of PPR for WSD5, successfully experimented by Agirre and Soroa (2009) and Agirre et al. (2009).</S>
    <S sid="153" ssid="41">Random baseline We compared our algorithms with the random baseline, which associates a random gloss among those available for each word occurrence according to a uniform distribution.</S>
    <S sid="154" ssid="42">Predominant domain We also compared our algorithms with a predominant sense baseline which assigns to each word occurrence the domain label with the highest domain score Qd among those available for the word (cf.</S>
    <S sid="155" ssid="43">Formula 3).</S>
    <S sid="156" ssid="44">Note that this is a strong baseline, because it aims at identifying the domain covered by the majority of terms in the input text, however it can disambiguate only at a coarsegrained level, i.e., at the domain level.</S>
  </SECTION>
  <SECTION title="5 Experimental Results" number="6">
    <S sid="157" ssid="1">30 domains We ran our WSD systems and the baselines on our 30-domain dataset, on a sentenceby-sentence basis.</S>
    <S sid="158" ssid="2">We calculated results at the two levels of granularity enabled by our WSD framework: a coarse-grained setting where systems output the most appropriate domain label for each word item to be disambiguated; a fine-grained setting where systems are required to output the most suitable gloss for the input word.</S>
    <S sid="159" ssid="3">The results are shown in Table 6.</S>
    <S sid="160" ssid="4">Domain PPR outperforms Vanilla PPR by some points in precision, recall and F1 in both the coarse-grained and the fine-grained setting, achieving an F1 around 80% and 69%, respectively (differences in recall performance are statistically significant using a x2 test).</S>
    <S sid="161" ssid="5">The predominant domain baseline, available only in the coarse-grained setting, lags behind Domain PPR by more than 3 points in precision and 2 in recall.</S>
    <S sid="162" ssid="6">While these differences are not statistically significant, the variance across domains is much higher, thus suggesting lower reliability of the method.</S>
    <S sid="163" ssid="7">These results were obtained in a fully unsupervised setting in which no structured knowledge was provided, unlike previous applications of PPR to WSD (Agirre et al., 2009; Agirre and Soroa, 2009) which relied on the underlying WordNet graph, a manually created resource.</S>
    <S sid="164" ssid="8">Furthermore, our graph contains &#8220;noisy&#8221; semantic relations, as we connect each gloss to all the senses of its gloss words (cf.</S>
    <S sid="165" ssid="9">Section 3.2.2).</S>
    <S sid="166" ssid="10">Finally, we note that the results shown in Table 6 could never have been obtained with WordNet.</S>
    <S sid="167" ssid="11">In fact, drawing on our domain mapping, we calculated that the correct domain sense is not in WordNet for about 68% of the words in the dataset.</S>
    <S sid="168" ssid="12">Instead, the results in Table 6 show that our framework enables high-performance unsupervised (&#8224; differences between the two systems are statistically significant using a x2 test, p &lt; 0.05).</S>
    <S sid="169" ssid="13">WSD thanks to the wide coverage of domain meanings.</S>
    <S sid="170" ssid="14">As regards the random baseline, this performs 42.5% and 44.1% in the two settings.</S>
    <S sid="171" ssid="15">Despite the higher polysemy of glosses (9.7 glosses vs. 4.4 domains per word in the dataset), the performance is higher in the fine-grained setting because often there is more than one gloss covering the same meaning of a domain word.</S>
    <S sid="172" ssid="16">Sports, Finance and Environment For the SPORTS, FINANCE and ENVIRONMENT datasets (cf.</S>
    <S sid="173" ssid="17">Section 4.3) we did not have gloss-based sense annotations, so we could not perform a fine-grained evaluation.</S>
    <S sid="174" ssid="18">Therefore, we first studied the different systems at a coarse level on the basis of the domain distribution of the senses returned for the word items in the dataset.</S>
    <S sid="175" ssid="19">We show the 3 most frequent domain labels for each system and each dataset in Figure 2.</S>
    <S sid="176" ssid="20">The figure seems to confirm our results showing Domain PPR as being more robust than its Vanilla version.</S>
    <S sid="177" ssid="21">Next, to get a more accurate evaluation, we randomly sampled 200 sentences from each dataset and manually validated the coarse-grained senses, i.e., domain assignments, output by each system on this set of sentences.</S>
    <S sid="178" ssid="22">We remark that several words in the datasets did not pertain to the domain of interest.</S>
    <S sid="179" ssid="23">For instance, will and share do not have any sports sense in WordNet, while the same applies to half and chip for the business domain.</S>
    <S sid="180" ssid="24">Table 7 shows the results of our validation, where a domain output by a system was considered correct if a suitable gloss existed for that domain in our inventory.</S>
    <S sid="181" ssid="25">The results show that our framework enables coarse-grained recall in the 70-80% ballpark even on difficult gold standard datasets for which finegrained recall with WordNet struggles to surpass the 50-60% range.</S>
    <S sid="182" ssid="26">For instance, the best performance on the ENVIRONMENT dataset was around 60% recall (Kulkarni et al., 2010) using a semi-supervised WSD system, trained on the domain.</S>
    <S sid="183" ssid="27">Similarly, both the FINANCE and SPORTS datasets are notoriously difficult gold standards on which state-of-the-art recall using WordNet is lower than 60% (Navigli et al., 2011).</S>
    <S sid="184" ssid="28">Interestingly, the predominant domain baseline shows a bias towards BUSINESS, thus performing best on the FINANCE dataset.</S>
    <S sid="185" ssid="29">This is because of the large number of terms covered in our domain glossary, and consequently the high overlap with cue words in context.</S>
    <S sid="186" ssid="30">On the other two domains, we observe performance in line with our 30-domain experiment.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="7">
    <S sid="187" ssid="1">We have here presented a new framework for domain Word Sense Disambiguation.</S>
    <S sid="188" ssid="2">We depart from the use of general-purpose sense inventories like WordNet and propose a bootstrapping approach to the acquisition of sense inventories for virtually any domain.</S>
    <S sid="189" ssid="3">While we selected 30 domains for this study, nothing would prevent us from using a smaller or larger set of these domains, or a set of completely different domains.</S>
    <S sid="190" ssid="4">Our work provides three main contributions: i) we propose a new, flexible approach to glossary bootstrapping which harvests hundreds of thousands of term/gloss pairs; the resulting multidomain glossary is shown to have wide coverage across domains and to include a large amount of terms not available in WordNet; ii) we propose a novel framework for fullyunsupervised domain WSD which uses the multi-domain glossary as our sense inventory; iii) we show that high performance can be achieved by means of simple, unsupervised WSD algorithms (around 80% and 69% in a coarse- and fine-grained setting, respectively).</S>
    <S sid="191" ssid="5">Note that our aim here has not been to determine which system performs best, but rather to show that a reliable, full-fledged framework for domain WSD can be set up with minimal supervision.</S>
    <S sid="192" ssid="6">Additionally, our framework can be applied to any language of interest, provided enough glossaries are available online, by simply translating the keywords used for our queries.</S>
    <S sid="193" ssid="7">The multi-domain glossary (and sense inventory) together with the seeds used for bootstrapping are available from http://lcl.uniroma1. it/dwsd.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="194" ssid="1">The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No.</S>
    <S sid="195" ssid="2">259234.</S>
  </SECTION>
</PAPER>
