<PAPER>
  <S sid="0">All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal.</S>
    <S sid="2" ssid="2">Many supervised WSD systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern.</S>
    <S sid="3" ssid="3">Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora.</S>
    <S sid="4" ssid="4">However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy.</S>
    <S sid="5" ssid="5">Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation.</S>
    <S sid="6" ssid="6">We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain.</S>
    <S sid="7" ssid="7">We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus.</S>
    <S sid="8" ssid="8">Accuracy figures close to self domain training lend credence to the viability of our approach.</S>
    <S sid="9" ssid="9">Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD.</S>
    <S sid="10" ssid="10">Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="11" ssid="1">Amongst annotation tasks, sense marking surely takes the cake, demanding as it does high level of language competence, topic comprehension and domain sensitivity.</S>
    <S sid="12" ssid="2">This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007).</S>
    <S sid="13" ssid="3">Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b).</S>
    <S sid="14" ssid="4">However, the accuracy figures of such systems are low.</S>
    <S sid="15" ssid="5">Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD.</S>
    <S sid="16" ssid="6">It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain.</S>
    <S sid="17" ssid="7">Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007; Agirre and de Lacalle, 2009).</S>
    <S sid="18" ssid="8">To the best of our knowledge there does not exist a system that solves the combined problem of all words domain adapted WSD.</S>
    <S sid="19" ssid="9">We thus propose the following: This procedure tested on four adaptation scenarios, viz., (i) SemCor (Miller et al., 1993) to Tourism, (ii) SemCor to Health, (iii) Tourism to Health and (iv) Health to Tourism has consistently yielded good performance (to be explained in sections 6 and 7).</S>
    <S sid="20" ssid="10">The remainder of this paper is organized as follows.</S>
    <S sid="21" ssid="11">In section 2 we discuss previous work in the area of domain adaptation for WSD.</S>
    <S sid="22" ssid="12">In section 3 we discuss three state of art supervised, unsupervised and knowledge based algorithms for WSD.</S>
    <S sid="23" ssid="13">Section 4 discusses the injection strategy for domain adaptation.</S>
    <S sid="24" ssid="14">In section 5 we describe the dataset used for our experiments.</S>
    <S sid="25" ssid="15">We then present the results in section 6 followed by discussions in section 7.</S>
    <S sid="26" ssid="16">Section 8 examines whether there is any need for intelligent choice of injections.</S>
    <S sid="27" ssid="17">Section 9 concludes the paper highlighting possible future directions.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="28" ssid="1">Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al. (2005) and Agirre et al.</S>
    <S sid="29" ssid="2">(2009b).</S>
    <S sid="30" ssid="3">They report results on three publicly available lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus made available by Koeling et al. (2005).</S>
    <S sid="31" ssid="4">Each of these datasets contains a handful of target words (41-191 words) which are sense marked in the corpus.</S>
    <S sid="32" ssid="5">Our main inspiration comes from the targetword specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009).</S>
    <S sid="33" ssid="6">The former showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data.</S>
    <S sid="34" ssid="7">Agirre and de Lacalle (2009) reported a 22% error reduction when source and target data were combined for training a classifier, as compared to the case when only the target data was used for training the classifier.</S>
    <S sid="35" ssid="8">However, both these works focused on target word specific WSD and do not address all-words domain specific WSD.</S>
    <S sid="36" ssid="9">In the unsupervised setting, McCarthy et al. (2007) showed that their predominant sense acquisition method gives good results on the corpus of Koeling et al.</S>
    <S sid="37" ssid="10">(2005).</S>
    <S sid="38" ssid="11">In particular, they showed that the performance of their method is comparable to the most frequent sense obtained from a tagged corpus, thereby making a strong case for unsupervised methods for domain-specific WSD.</S>
    <S sid="39" ssid="12">More recently, Agirre et al. (2009b) showed that knowledge based approaches which rely only on the semantic relations captured by the Wordnet graph outperform supervised approaches when applied to specific domains.</S>
    <S sid="40" ssid="13">The good results obtained by McCarthy et al. (2007) and Agirre et al.</S>
    <S sid="41" ssid="14">(2009b) for unsupervised and knowledge based approaches respectively have cast a doubt on the viability of supervised approaches which rely on sense tagged corpora.</S>
    <S sid="42" ssid="15">However, these conclusions were drawn only from the performance on certain target words, leaving open the question of their utility in all words WSD.</S>
    <S sid="43" ssid="16">We believe our work contributes to the WSD research in the following way: (i) it shows that there is promise in supervised approach to allword WSD, through the instrument of domain adaptation; (ii) it places in perspective some very recently reported unsupervised and knowledge based techniques of WSD; (ii) it answers some questions arising out of the debate between supervision and unsupervision in WSD; and finally (iv) it explores a convenient middle ground between unsupervised and supervised WSD &#8211; the territory of &#8220;annotate-little and inject&#8221; paradigm.</S>
  </SECTION>
  <SECTION title="3 WSD algorithms employed by us" number="3">
    <S sid="44" ssid="1">In this section we describe the knowledge based, unsupervised and supervised approaches used for our experiments.</S>
    <S sid="45" ssid="2">Agirre et al. (2009b) showed that a graph based algorithm which uses only the relations between concepts in a Lexical Knowledge Base (LKB) can outperform supervised approaches when tested on specific domains (for a set of chosen target words).</S>
    <S sid="46" ssid="3">We employ their method which involves the following steps: We used the publicly available implementation of this algorithm1 for our experiments.</S>
    <S sid="47" ssid="4">McCarthy et al. (2007) used an untagged corpus to construct a thesaurus of related words.</S>
    <S sid="48" ssid="5">They then found the predominant sense (i.e., the most frequent sense) of each target word using pair-wise Wordnet based similarity measures by pairing the target word with its top-k neighbors in the thesaurus.</S>
    <S sid="49" ssid="6">Each target word is then disambiguated by assigning it its predominant sense &#8211; the motivation being that the predominant sense is a powerful, hard-to-beat baseline.</S>
    <S sid="50" ssid="7">We implemented their method using the following steps: Khapra et al. (2010) proposed a supervised algorithm for domain-specific WSD and showed that it beats the most frequent corpus sense and performs on par with other state of the art algorithms like PageRank.</S>
    <S sid="51" ssid="8">We implemented their iterative algorithm which involves the following steps:</S>
  </SECTION>
  <SECTION title="4 Injections for Supervised Adaptation" number="4">
    <S sid="52" ssid="1">This section describes the main interest of our work i.e. adaptation using injections.</S>
    <S sid="53" ssid="2">For supervised adaptation, we use the supervised algorithm described above (Khapra et al., 2010) in the following 3 settings as proposed by Agirre et al. (2009a): tle data as possible.</S>
    <S sid="54" ssid="3">For injecting data from the target domain we randomly select some sense marked words from the target domain and add them to the training data.</S>
    <S sid="55" ssid="4">An obvious question which arises at this point is &#8220;Why were the words selected at random?&#8221; or &#8220;Can selection of words using some active learning strategy yield better results than a random selection?&#8221; We discuss this question in detail in Section 7 and show that a random set of injections performs no worse than a craftily selected set of injections.</S>
  </SECTION>
  <SECTION title="5 DataSet Preparation" number="5">
    <S sid="56" ssid="1">Due to the lack of any publicly available all-words domain specific sense marked corpora we set upon the task of collecting data from two domains, viz., Tourism and Health.</S>
    <S sid="57" ssid="2">The data for Tourism domain was downloaded from Indian Tourism websites whereas the data for Health domain was obtained from two doctors.</S>
    <S sid="58" ssid="3">This data was manually sense annotated by two lexicographers adept in English.</S>
    <S sid="59" ssid="4">Princeton Wordnet 2.13 (Fellbaum, 1998) was used as the sense inventory.</S>
    <S sid="60" ssid="5">A total of 1,34,095 words from the Tourism domain and 42,046 words from the Health domain were manually sense marked.</S>
    <S sid="61" ssid="6">Some files were sense marked by both the lexicographers and the Inter Tagger Agreement (ITA) calculated from these files was 83% which is comparable to the 78% ITA reported on the SemCor corpus considering the domainspecific nature of the corpus.</S>
    <S sid="62" ssid="7">We now present different statistics about the corpora.</S>
    <S sid="63" ssid="8">Table 1 summarizes the number of polysemous and monosemous words in each category.</S>
    <S sid="64" ssid="9">Note that we do not use the monosemous words while calculating precision and recall of our algorithms.</S>
    <S sid="65" ssid="10">Table 2 shows the average number of instances per polysemous word in the 3 corpora.</S>
    <S sid="66" ssid="11">We note that the number of instances per word in the Tourism domain is comparable to that in the SemCor corpus whereas the number of instances per word in the Health corpus is smaller due to the overall smaller size of the Health corpus.</S>
    <S sid="67" ssid="12">Tables 3 and 4 summarize the average degree of Wordnet polysemy and corpus polysemy of the polysemous words in the corpus.</S>
    <S sid="68" ssid="13">Wordnet polysemy is the number of senses of a word as listed in the Wordnet, whereas corpus polysemy is the number of senses of a word actually appearing in the corpus.</S>
    <S sid="69" ssid="14">As expected, the average degree of corpus polysemy (Table 4) is much less than the average degree of Wordnet polysemy (Table 3).</S>
    <S sid="70" ssid="15">Further, the average degree of corpus polysemy (Table 4) in the two domains is less than that in the mixed-domain SemCor corpus, which is expected due to the domain specific nature of the corpora.</S>
    <S sid="71" ssid="16">Finally, Table 5 summarizes the number of unique polysemous words per category in each domain.</S>
    <S sid="72" ssid="17">The data is currently being enhanced by manually sense marking more words from each domain and will be soon freely available4 for research purposes.</S>
  </SECTION>
  <SECTION title="6 Results" number="6">
    <S sid="73" ssid="1">We tested the 3 algorithms described in section 4 using SemCor, Tourism and Health domain corpora.</S>
    <S sid="74" ssid="2">We did a 2-fold cross validation for supervised adaptation and report the average performance over the two folds.</S>
    <S sid="75" ssid="3">Since the knowledge based and unsupervised methods do not need any training data we simply test it on the entire corpus from the two domains.</S>
    <S sid="76" ssid="4">The results obtained by applying the Personalized PageRank (PPR) method to Tourism and Health data are summarized in Table 6.</S>
    <S sid="77" ssid="5">We also report the Wordnet first sense baseline (WFS).</S>
    <S sid="78" ssid="6">The predominant sense for each word in the two domains was calculated using the method described in section 4.2.</S>
    <S sid="79" ssid="7">McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997).</S>
    <S sid="80" ssid="8">Following them, we used k = 50 and observed that the best results for nouns and verbs were obtained using the jcn measure and the best results for adjectives and adverbs were obtained using the lesk measure (Banerjee and Pedersen, 2002).</S>
    <S sid="81" ssid="9">Accordingly, we used jcn for nouns and verbs and lesk for adjectives and adverbs.</S>
    <S sid="82" ssid="10">Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method.</S>
    <S sid="83" ssid="11">We tested this approach only on Tourism domain due to unavailability of large We report results in the source setting, target setting and adaptation setting as described earlier using the following four combinations for source and target data: In each case, the target domain data was divided into two folds.</S>
    <S sid="84" ssid="12">One fold was set aside for testing and the other for injecting data in the adaptation setting.</S>
    <S sid="85" ssid="13">We increased the size of the injected target examples from 1000 to 14000 words in increments of 1000.</S>
    <S sid="86" ssid="14">We then repeated the same experiment by reversing the role of the two folds.</S>
    <S sid="87" ssid="15">Figures 1, 2, 3 and 4 show the graphs of the average F-score over the 2-folds for SC-*T, SC-*H, T-*H and H-*T respectively.</S>
    <S sid="88" ssid="16">The x-axis represents the amount of training data (in words) injected from the target domain and the y-axis represents the F-score.</S>
    <S sid="89" ssid="17">The different curves in each graph are as follows: a. only random : This curve plots the performance obtained using x randomly selected sense tagged words from the target domain and zero sense tagged words from the source domain (x was varied from 1000 to 14000 words in increments of 1000). b. random+source : This curve plots the performance obtained by mixing x randomly selected sense tagged words from the target domain with the entire training data from the source domain (again x was varied from 1000 to 14000 words in increments of 1000). c. source baseline (srcb) : This represents the Fscore obtained by training on the source data alone without mixing any examples from the target domain. d. wordnet first sense (wfs) : This represents the F-score obtained by selecting the first sense from Wordnet, a typically reported baseline. e. target skyline (tsky) : This represents the average 2-fold F-score obtained by training on one entire fold of the target data itself (Health: 15320 polysemous words; Tourism: 47242 polysemous words) and testing on the other fold.</S>
    <S sid="90" ssid="18">These graphs along with other results are discussed in the next section.</S>
  </SECTION>
  <SECTION title="7 Discussions" number="7">
    <S sid="91" ssid="1">We discuss the performance of the three approaches.</S>
    <S sid="92" ssid="2">It is apparent from Tables 6 and 7 that knowledge based and unsupervised approaches do not perform well when compared to the Wordnet first sense (which is freely available and hence can be used for disambiguation).</S>
    <S sid="93" ssid="3">Further, we observe that the performance of these approaches is even less than the source baseline (i.e., the case when training data from a source domain is applied as it is to a target domain - without using any injections).</S>
    <S sid="94" ssid="4">These observations bring out the weaknesses of these approaches when used in an all-words setting and clearly indicate that they come nowhere close to replacing a supervised system.</S>
    <S sid="95" ssid="5">1.</S>
    <S sid="96" ssid="6">The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) &#8211; F-score of 61.7% on Tourism and Fscore of 65.5% on Health &#8211; is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing happens on a mixed-domain corpus (Snyder and Palmer, 2004)).</S>
    <S sid="97" ssid="7">This is in contrast to previous studies (Escudero et al., 2000; Agirre and Martinez, 2004) which suggest that instead of adapting from a generic/mixed domain to a specific domain, it is better to completely ignore the generic examples and use hand-tagged data from the target domain itself.</S>
    <S sid="98" ssid="8">The main reason for the contrasting results is that the earlier work focused only on a handful of target words whereas we focus on all words appearing in the corpus.</S>
    <S sid="99" ssid="9">So, while the behavior of a few target words would change drastically when the domain changes, a majority of the words will exhibit the same behavior (i.e., same predominant sense) even when the domain changes.</S>
    <S sid="100" ssid="10">We agree that the overall performance is still lower than that obtained by training on the domainspecific corpora.</S>
    <S sid="101" ssid="11">However, it is still better than the performance of unsupervised and knowledge based approaches which tilts the scale in favor of supervised approaches even when only mixed domain sense marked corpora is available.</S>
    <S sid="102" ssid="12">Health) to another specific domain (Health or Tourism) gives the same performance as that obtained by adapting from a mixed-domain (SemCor) to a specific domain (Tourism, Health).</S>
    <S sid="103" ssid="13">This is an interesting observation as it suggests that as long as data from one domain is available it is easy to build a WSD engine that works for other domains by injecting a small amount of data from these domains.</S>
    <S sid="104" ssid="14">To verify that the results are consistent, we randomly selected 5 different sets of injections from fold-1 and tested the performance on fold-2.</S>
    <S sid="105" ssid="15">We then repeated the same experiment by reversing the roles of the two folds.</S>
    <S sid="106" ssid="16">The results were indeed consistent irrespective of the set of injections used.</S>
    <S sid="107" ssid="17">Due to lack of space we have not included the results for these 5 different sets of injections.</S>
    <S sid="108" ssid="18">To correctly quantify the benefit of adding injections from the target domain, we calculated the amount of target data (peak size) that is needed to reach the skyline F-score (peak F) in the absence of any data from the source domain.</S>
    <S sid="109" ssid="19">The peak size was found to be 35000 (Tourism) and 14000 (Health) corresponding to peak F values of 74.2% (Tourism) and 73.4% (Health).</S>
    <S sid="110" ssid="20">We then plotted a graph (Figure 5) to capture the relation between the size of injections (expressed as a percentage of the peak size) and the F-score (expressed as a percentage of the peak F).</S>
    <S sid="111" ssid="21">We observe that by mixing only 20-40% of the peak size with the source domain we can obtain up to 95% of the performance obtained by using the entire target data (peak size).</S>
    <S sid="112" ssid="22">In absolute terms, the size of the injections is only 7000-9000 polysemous words which is a very small price to pay considering the performance benefits.</S>
    <S sid="113" ssid="23">Table 8 summarizes the percentage of words that fall in each category in each of the three adaptation scenarios.</S>
    <S sid="114" ssid="24">The fact that nearly 50-60% of the words fall in the &#8220;conformist&#8221; category once again makes a strong case for reusing sense tagged data from one domain to another domain.</S>
    <S sid="115" ssid="25">8 Does the choice of injections matter?</S>
    <S sid="116" ssid="26">An obvious question which arises at this point is &#8220;Why were the words selected at random?&#8221; or &#8220;Can selection of words using some active learning strategy yield better results than a random selection?&#8221; An answer to this question requires a more thorough understanding of the sensebehavior exhibited by words across domains.</S>
    <S sid="117" ssid="27">In any scenario involving a shift from domain D1 to domain D2, we will always encounter words belonging to the following 4 categories: The above characterization suggests that an ideal domain adaptation strategy should focus on injecting WD2 and WD1D2non&#8722;conformists as these would yield maximum benefits if injected into the training data.</S>
    <S sid="118" ssid="28">While it is easy to identify the WD2 words, &#8220;identifying non-conformists&#8221; is a hard problem which itself requires some type of WSD5.</S>
    <S sid="119" ssid="29">However, just to prove that a random injection strategy does as good as an ideal strategy we assume the presence of an oracle which identifies the WD1D2non&#8722;conformists.</S>
    <S sid="120" ssid="30">We then augment the training data with 5-8 instances for WD2 and WD1D2non&#8722;conformists words thus identified.</S>
    <S sid="121" ssid="31">We observed that adding more than 5-8 instances per word does not improve the performance.</S>
    <S sid="122" ssid="32">This is due to the &#8220;one sense per domain&#8221; phenomenon &#8211; seeing only a few instances of a word is sufficient to identify the predominant sense of the word.</S>
    <S sid="123" ssid="33">Further, to ensure a better overall performance, the instances of the most frequent words are injected first followed by less frequent words till we exhaust the total size of the injections (1000, 2000 and so on).</S>
    <S sid="124" ssid="34">We observed that there was a 7580% overlap between the words selected by random strategy and oracle strategy.</S>
    <S sid="125" ssid="35">This is because oracle selects the most frequent words which also have a high chance of getting selected when a random sampling is done.</S>
    <S sid="126" ssid="36">Figures 6, 7, 8 and 9 compare the performance of the two strategies.</S>
    <S sid="127" ssid="37">We see that the random strategy does as well as the oracle strategy thereby supporting our claim that if we have sense marked corpus from one domain then simply injecting ANY small amount of data from the target domain will</S>
  </SECTION>
  <SECTION title="9 Conclusion and Future Work" number="8">
    <S sid="128" ssid="1">Based on our study of WSD in 4 domain adaptation scenarios, we make the following conclusions: ing finding with the following implication: as long as one has sense marked corpus - be it from a mixed or specific domain - simply injecting ANY small amount of data from the target domain suffices to beget good accuracy.</S>
    <S sid="129" ssid="2">As future work, we would like to test our work on the Environment domain data which was released as part of the SEMEVAL 2010 shared task on &#8220;Allwords Word Sense Disambiguation on a Specific Domain&#8221;.</S>
  </SECTION>
</PAPER>
