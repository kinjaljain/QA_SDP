<PAPER>
	<S sid="0">Instance Based Lexical Entailment for Ontology Population</S><ABSTRACT>
		<S sid="1" ssid="1">In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text.</S>
		<S sid="2" ssid="2">The approach is fully unsupervised and based on kernel methods.</S>
		<S sid="3" ssid="3">We demonstrate the effectiveness of our technique largelysurpassing both the random and most fre quent baselines and outperforming current state-of-the-art unsupervised approaches ona benchmark ontology available in the liter ature.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">Textual entailment is formally defined as a relationship between a coherent text T and a language ex pression, the hypothesis H . T is said to entail H ,denoted by T ? H , if the meaning of H can be in ferred from the meaning of T (Dagan et al, 2005;Dagan and Glickman., 2004).</S>
			<S sid="5" ssid="5">Even though this no tion has been recently proposed in the computational linguistics literature, it has already attracted a greatattention due to the very high generality of its settings and to the indubitable usefulness of its (poten tial) applications.</S>
			<S sid="6" ssid="6">In this paper, we concentrate on the problem of lexical entailment, a textual entailment subtask inwhich the system is asked to decide whether the sub stitution of a particular word w with the word e in a coherent text Hw = H lwHr generates a sentence He = H leHr such that Hw ? He, where H l andHr denote the left and the right context of w, re spectively.</S>
			<S sid="7" ssid="7">For example, given the word ?weapon?</S>
			<S sid="8" ssid="8">a system may substitute it with the synonym ?arm?, in order to identify relevant texts that denote the sought concept using the latter term.</S>
			<S sid="9" ssid="9">A particular case of lexical entailment is recognizing synonymy, where both Hw ? He and He ? Hw hold.</S>
			<S sid="10" ssid="10">In the literature, slight variations of this problem are also referred to as sense matching (Dagan et al, 2006), lexical reference (Glickman et al, 2006a) and lexical substitution (Glickman et al, 2006b).</S>
			<S sid="11" ssid="11">They have been applied to a wide variety of tasks, such as semantic matching, subtitle generation andWord Sense Disambiguation (WSD).</S>
			<S sid="12" ssid="12">Modeling lex ical entailment is also a prerequisite to approach the SemEval-2007 lexical substitution task1, consisting of finding alternative words that can occur in given context.</S>
			<S sid="13" ssid="13">In this paper, we propose to apply an approach for lexical entailment to the ontology population task.</S>
			<S sid="14" ssid="14">The basic idea is that if a word entails another one in a given context then the former is an instance or a subclass of the latter.</S>
			<S sid="15" ssid="15">This approach is intuitively appealing because lexical entailment is intrinsically an unsupervised task, therefore it does not requirelexical resources, seed examples or manually annotated data sets.</S>
			<S sid="16" ssid="16">Unsupervised approaches are partic ularly suited for ontology population, whose goal is to find instances of concepts from corpora, because both corpus and the ontology sizes can scale up to millions of documents and thousands of concepts, preventing us from applying supervised learning.</S>
			<S sid="17" ssid="17">In addition, the top level part of the ontology (i.e., the Tbox in the Description Logics terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248often modified during the ontology engineering life cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according tothe renewed application needs required by the evo lution of the application domain.</S>
			<S sid="18" ssid="18">It is evident that to preserve the consistency between the Tbox andthe Abox (i.e., the set of instances and their relations) in such a dynamic ontology engineering pro cess, supervised approaches are clearly inadequate, as small changes in the TBox will be reflected into dramatic annotation effort to keep instances in the Abox aligned.The problem of populating a predefined ontol ogy of concepts with novel instances implies a WSDtask, as the entities in texts are ambiguous with respect to the domain ontology.</S>
			<S sid="19" ssid="19">For example, the en tity Washington is both the name of a state and the name of a city.</S>
			<S sid="20" ssid="20">In the ontology population settingstraditional WSD approaches cannot be directly applied since entities are not reported into dictionar ies, making the lexical entailment alternative moreviable.</S>
			<S sid="21" ssid="21">In particular, we model the problem of on tology population as the problem of recognizing foreach mention of an entity of a particular coarsegrained type (e.g., location) the fine-grained concept (e.g., lake or mountain) that can be substi tuted in texts preserving the meaning.</S>
			<S sid="22" ssid="22">For example, in the sentence ?the first man to climb the Everest without oxygen?, ?Everest?</S>
			<S sid="23" ssid="23">can be substituted with the word mountain preserving the meaning, whilethe sentence is meaningless when ?Everest?</S>
			<S sid="24" ssid="24">is re placed with the word lake.</S>
			<S sid="25" ssid="25">Following the lexical entailment approach, the ontology population task is transformed into the problem of recognizing the term from a fine-grained set of categories (e.g., city,country, river, lake and mountain) that can be substi tuted in the contexts where the entity is mentioned (e.g., Everest in the example above).The main contributions of this paper are summa rized as follows.</S>
			<S sid="26" ssid="26">First, we propose a novel approachto lexical entailment, called Instance Based Lexi cal Entailment (IBLE), that allows approaching the problem as a classification task, in which a giventarget word (i.e., the entailing word) in a particu lar context is judged to entail a different word takenfrom a (pre-defined) set of (possible) candidate en tailed words (see Section 3).</S>
			<S sid="27" ssid="27">Second, we exploit the IBLE approach to model the ontology population task as follows.</S>
			<S sid="28" ssid="28">Given a set of candidate conceptsbelonging to generic ontological types (e.g., people or locations), and a set of pre-recognized men tions of entities of these types in the corpus (e.g., Newton, Ontario), we assign the entity to the class whose lexicalization is more frequently entailed in the corpus.</S>
			<S sid="29" ssid="29">In particular, as training set to learnthe fine-grained category models, we use all the oc currences of their corresponding expressions in the same corpus (e.g., we collected all occurrences in context of the word scientist to describe the concept scientist).</S>
			<S sid="30" ssid="30">Then, we apply the trained model to classify the pre-recognized coarse-grained entities into the fine-grained categories.</S>
			<S sid="31" ssid="31">Our approach is fully unsupervised as for training it only requires occurrences of the candidate entailedwords taken in their contexts.</S>
			<S sid="32" ssid="32">Restricted to the ontology population task, for each coarse-grained en tity (e.g., location), the candidate entailed words are the terms corresponding to the fine-grained classes (e.g., lake or mountain) and the entailing words arementions of entities (e.g., New York, Ontario) be longing to the coarse-grained class, recognized by an entity tagger.Experiments show that our method for recognizing lexical entailment is effective for the on tology population task, reporting improvements over a state-of-the-art unsupervised technique based on contextual similarity measures (Cimiano and Vo?lker, 2005).</S>
			<S sid="33" ssid="33">In addition, we also compared it to a supervised approach (Tanev and Magnini, 2006),that we regarded as an upper bound, obtaining com parable results.</S>
	</SECTION>
	<SECTION title="The Ontology Population Task. " number="2">
			<S sid="34" ssid="1">Populating concepts of a predefined ontology with instances found in a corpus is a primary goal of knowledge management systems.</S>
			<S sid="35" ssid="2">As concepts inthe ontology are generally structured into hierar chies belonging to a common ontological type (e.g.,people or locations), the problem of populating ontologies can be solved hierarchically, firstly identi fying instances in texts as belonging to the topmost concepts, and then assigning them to a fine-grained class.</S>
			<S sid="36" ssid="3">Supervised named entity recognition (NER) systems can be used for accomplishing the first step.</S>
			<S sid="37" ssid="4">State-of-the-art NER systems are characterized by 249 high accuracy, but they require a large amount of training data.</S>
			<S sid="38" ssid="5">However, domain specific ontologies generally contains many ?fine-grained?</S>
			<S sid="39" ssid="6">categories(e.g., particular categories of people, such as writers, scientists, and so on) and, as a consequence, supervised methods cannot be used because the anno tation costs would become prohibitive.Therefore, in the literature, the fine-grained clas sification task has been approached by adoptingweakly supervised (Tanev and Magnini, 2006; Fleis chman and Hovy, 2002) or unsupervised methods (Cimiano and Vo?lker, 2005).</S>
			<S sid="40" ssid="7">Tanev and Magnini (2006) proposed a weakly supervised method thatrequires as training data a list of terms without con text for each class under consideration.</S>
			<S sid="41" ssid="8">Such list can be automatically acquired from existing ontologies or other sources (i.e., database fields, web sites likeWikipedia, etc.) since the approach imposes virtually no restrictions on them.</S>
			<S sid="42" ssid="9">Given a generic syntactically parsed corpus containing at least each train ing entity twice, the algorithm learns, for each class, a feature vector describing the contexts where those entities occur.</S>
			<S sid="43" ssid="10">Then it compares the new (unknown) entity with the so obtained feature vectors, assigning it to the most similar class.</S>
			<S sid="44" ssid="11">Fleischman and Hovy (2002) approached the ontology population problemas a classification task, providing examples of in stances in their context as training examples for their respective fine-grained categories.The aforementioned approaches are clearly inad equate to recognize such fine-grained distinctions, as they would require a time consuming and costly annotation process for each particular class, that is clearly infeasible when the number of concepts in the ontology scales up.</S>
			<S sid="45" ssid="12">Therefore, most of thepresent research in ontology population is focus ing on either unsupervised approaches (Cimiano and Vo?lker, 2005) or weakly supervised approaches (Tanev and Magnini, 2006).</S>
			<S sid="46" ssid="13">Unsupervised approaches are mostly based on term similarity metrics.</S>
			<S sid="47" ssid="14">Cimiano and Vo?lker (2005) assign a particular entity to the fine-grained class such that the contextual similarity is maximal among the set of fine-grained subclasses of a coarse-grained category.</S>
			<S sid="48" ssid="15">Contextual similarity has been measured by adopting lexico-syntactic features provided by a dependency parser, as proposed in (Lin, 1998).</S>
	</SECTION>
	<SECTION title="Instance Based Lexical Entailment. " number="3">
			<S sid="49" ssid="1">Dagan et al (2006) adapted the classical supervisedWSD setting to approach the sense matching prob lem (i.e., the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context) by defining a one-class learning algorithm based onsupport vector machines (SVM).</S>
			<S sid="50" ssid="2">They train a oneclass model for each entailed word (e.g., all the oc currences of the word job in the corpus) and, then,apply it to classify all the occurrences of the entailing words (e.g., the word position), providing a bi nary decision criterion2.</S>
			<S sid="51" ssid="3">Similarly to the WSD case,examples are represented by feature vectors describ ing their contexts, and then compared to the feature vectors describing the context of the target word.In this paper, we adopt a similar strategy to ap proach a multi-class lexical entailment problem.</S>
			<S sid="52" ssid="4">The basic hypothesis is that if a word w entails e in a particular context (Hw ? He), then someof the contexts T je in which e occurs in the train ing corpus are similar to Hw.</S>
			<S sid="53" ssid="5">Given a word w and an (exhaustive) set of candidate entailed words E = {e1, e2, . . .</S>
			<S sid="54" ssid="6">, en}, to which we refer hereafter with the expression ?substitution lexica?, our goal is to select the word ei ? E that can be substituted to w in the context Hw generating a sentence He suchthat Hw ? He.</S>
			<S sid="55" ssid="7">In the multi-class setting, super vised learning approaches can be used.</S>
			<S sid="56" ssid="8">In particular, we can apply a one-versus-all learning methodology, in which each class ei is trained from both positive(i.e., all the occurrences of ei in the corpus) and neg ative examples (i.e., all the occurrences of the words in the set {ej |j 6= i}).</S>
			<S sid="57" ssid="9">Our approach is clearly a simplification of the more general lexical entailment settings, where given two generic words w and e, and a context H = H lwHr, the system is asked to decide whether w entails e or not.</S>
			<S sid="58" ssid="10">In fact, the latter is a binary classification problem, while the former is easier as the system is required to select ?the best?</S>
			<S sid="59" ssid="11">option among the substitution lexicon.</S>
			<S sid="60" ssid="12">Of course providing such set could be problematic in many cases (e.g., it could be incomplete or simply not available for2This approach resembles the pseudo-words technique pro posed to evaluate WSD algorithms at the earlier stages of the WSD studies (Gale et al, 1992), when large scale sense tagged corpora were not available for training supervised algorithms.</S>
			<S sid="61" ssid="13">250 many languages or rare words).</S>
			<S sid="62" ssid="14">On the other hand, such a simplification is practically effective.</S>
			<S sid="63" ssid="15">First ofall, it allows us to provide both positive and negative examples, avoiding the use of one-class classi fication algorithms that in practice perform poorly(Dagan et al, 2006).</S>
			<S sid="64" ssid="16">Second, the large availabil ity of manually constructed substitution lexica, suchas WordNet (Fellbaum, 1998), or the use of reposi tories based on statistical word similarities, such as the database constructed by Lin (1998), allows us to find an adequate substitution lexicon for each target word in most of the cases.</S>
			<S sid="65" ssid="17">For example, as shown in Table 1, the word job has different senses depending on its context, some of them entailing its direct hyponym position (e.g., ?looking for permanent job?), others entailing theword task (e.g., ?the job of repairing?).</S>
			<S sid="66" ssid="18">The prob lem of deciding whether a particular instance of job can be replaced by position, and not by the wordplace, can be solved by looking for the most simi lar contexts where either position or place occur in the training data, and then selecting the class (i.e., the entailed word) characterized by the most similar ones, in an instance based style.</S>
			<S sid="67" ssid="19">In the first example (see row 1), the word job is strongly associated to the word position, because the contexts of the latter in the examples 1 and 2 are similar to the contextof the former, and not to the word task, whose con texts (4, 5 and 6) are radically different.</S>
			<S sid="68" ssid="20">On the other hand, the second example (see row 2) of the word job is similar to the occurrences 4 and 5 of the word task, allowing its correct substitution.It is worthwhile to remark that, due to the ambi guity of the entailed words (e.g., position could alsoentail either perspective or place), not every occur rence of them should be taken into account, in orderto avoid misleading predictions caused by the irrele vant senses.</S>
			<S sid="69" ssid="21">Therefore, approaches based on a more classical contextual similarity technique (Lin, 1998; Dagan, 2000), where words are described ?globally?by context vectors, are doomed to fail.</S>
			<S sid="70" ssid="22">We will provide empirical evidence of this in the evaluation sec tion.</S>
			<S sid="71" ssid="23">Choosing an appropriate similarity function forthe contexts of the words to be substituted is a primary issue.</S>
			<S sid="72" ssid="24">In this work, we exploited similar ity functions already defined in the WSD literature,relying on the analogy between the lexical entailment and the WSD task.</S>
			<S sid="73" ssid="25">The state-of-the-art super vised WSD methodology, reporting the best resultsin most of the Senseval-3 lexical sample tasks in different languages, is based on a combination of syn tagmatic and domain kernels (Gliozzo et al, 2005) in a SVM classification framework.</S>
			<S sid="74" ssid="26">Therefore, we adopted exactly the same strategy for our purposes.</S>
			<S sid="75" ssid="27">A great advantage of this methodology is that itis totally corpus based, as it does not require nei ther the availability of lexical databases, nor the use of complex preprocessing steps such as parsing oranaphora resolution, allowing us to apply it on dif ferent languages and domains once large corpora areavailable for training.</S>
			<S sid="76" ssid="28">Therefore, we exploited exactly the same strategy to implement the IBLE clas sifier required for our purposes, defining a kernel composed by n simple kernels, each representing a different aspect to be considered when estimating contextual similarity among word occurrences.</S>
			<S sid="77" ssid="29">In fact, by using the closure properties of the kernelfunctions, it is possible to define the kernel combi nation schema as follows3: KC(xi, xj) = n?</S>
			<S sid="78" ssid="30">l=1 Kl(xi, xj) ? Kl(xj , xj)Kl(xi, xi) , (1)where Kl are valid kernel functions, measuring sim ilarity between the objects xi and xj from different perspectives4.One means to satisfy both the WSD and the lexical entailment requirements is to consider two dif ferent aspects of similarity: domain aspects, mainly related to the topic (i.e., the global context) of thetexts in which the word occurs, and syntagmatic as pects, concerning the lexico-syntactic pattern in the local context.</S>
			<S sid="79" ssid="31">Domain aspects are captured by thedomain kernel, described in Section 3.1, while syntagmatic aspects are taken into account by the syn tagmatic kernel, presented in Section 3.2.</S>
			<S sid="80" ssid="32">3Some recent works (Zhao and Grishman, 2005; Gliozzoet al, 2005) empirically demostrate the effectiveness of com bining kernels in this way, showing that the combined kernelalways improves the performance of the individual ones.</S>
			<S sid="81" ssid="33">In addition, this formulation allows evaluating the individual contri bution of each information source.</S>
			<S sid="82" ssid="34">4An exhaustive discussion about kernel methods for NLP can be found in (Shawe-Taylor and Cristianini, 2004).</S>
			<S sid="83" ssid="35">251 Entailed job Training position ... looking for permanent academic job in ...</S>
			<S sid="84" ssid="36">1 ... from entry-level through permanent positions.</S>
			<S sid="85" ssid="37">2 My academic position ....</S>
			<S sid="86" ssid="38">3 ... put the lamp in the left position ... task The job of repairing 4 The task of setting up ...</S>
			<S sid="87" ssid="39">5 Repairing the engine is an hard task..</S>
			<S sid="88" ssid="40">6 ... task based evaluation.</S>
			<S sid="89" ssid="41">Table 1: IBLE example.</S>
			<S sid="90" ssid="42">3.1 The Domain Kernel.</S>
			<S sid="91" ssid="43">(Magnini et al, 2002) claim that knowing the domain of the text in which the word is located is a cru cial information forWSD.</S>
			<S sid="92" ssid="44">For example the (domain) polysemy among the Computer Science and the Medicine senses of the word virus can besolved by simply considering the domain of the con text in which it is located.</S>
			<S sid="93" ssid="45">Domain aspects are alsocrucial in recognizing lexical entailment.</S>
			<S sid="94" ssid="46">For exam ple, the term virus entails software agent inthe Computer Science domain (e.g., ?The lap top has been infected by a virus?), while it entails bacterium when located in the Medicine domain (e.g., ?HIV is a virus?).</S>
			<S sid="95" ssid="47">As argued in (Magnini etal., 2002), domain aspects can be considered by an alyzing the lexicon in a large context of the word to be disambiguated, regardless of the actual wordorder.</S>
			<S sid="96" ssid="48">We refer to (Gliozzo et al, 2005) for a detailed description of the domain kernel.</S>
			<S sid="97" ssid="49">The simplest methodology to estimate the domain similar ity among two texts is to represent them by means of vectors in the Vector Space Model (VSM), andto exploit the cosine similarity.</S>
			<S sid="98" ssid="50">The VSM is a kdimensional space Rk, in which the text tj is rep resented by means of the vector ~tj such that the ith component of ~tj is the term frequency of the term wi in it.</S>
			<S sid="99" ssid="51">The similarity between two texts in the VSM is estimated by computing the cosine between them, providing the kernel function KV SM that can be used as a basic tool to estimate domain similarity between texts5.</S>
			<S sid="100" ssid="52">5In (Gliozzo et al, 2005), in addition to the standard VSM, a domain kernel, exploiting external information acquired fromunlabeled data, has been also used to reduce the amount of (labeled) training data.</S>
			<S sid="101" ssid="53">Here, given that our approach is fully un supervised, i.e., we can obtain as many examples as we need, we do not use the domain kernel.</S>
			<S sid="102" ssid="54">3.2 The Syntagmatic Kernel.</S>
			<S sid="103" ssid="55">Syntagmatic aspects are probably the most impor tant evidence for recognizing lexical entailment.</S>
			<S sid="104" ssid="56">In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigramsof collocated words as features to describe local con texts (Yarowsky, 1994).</S>
			<S sid="105" ssid="57">The main drawback of thisapproach is that non contiguous or shifted collocations cannot be identified, decreasing the generalization power of the learning algorithm.</S>
			<S sid="106" ssid="58">For example, suppose that the word job has to be disam biguated into the sentence ?.</S>
			<S sid="107" ssid="59">permanent academicjob in.</S>
			<S sid="108" ssid="60">?, and that the occurrence ?We offer per manent positions.</S>
			<S sid="109" ssid="61">is provided for training.</S>
			<S sid="110" ssid="62">Atraditional feature mapping would extract the con text words w?1:academic, w?2:permanent to represent the former, and w?1:permanent,w?2:offer to index the latter.</S>
			<S sid="111" ssid="63">Evidently such features will not match, leading the algorithm to a mis classification.</S>
			<S sid="112" ssid="64">The syntagmatic kernel, proposed by Gliozzo et al.</S>
			<S sid="113" ssid="65">(2005), is an attempt to solve this problem.</S>
			<S sid="114" ssid="66">It is based on a gap-weighted subsequences kernel (Shawe-Taylor and Cristianini, 2004).</S>
			<S sid="115" ssid="67">In the spirit of kernel methods, this kernel is able to compare sequences directly in the input space, avoiding anyexplicit feature mapping.</S>
			<S sid="116" ssid="68">To perform this opera tion, it counts how many times a (non-contiguous) subsequence of symbols u of length n occurs inthe input string s, and penalizes non-contiguous oc currences according to the number of the contained gaps.</S>
			<S sid="117" ssid="69">To define our syntagmatic kernel, we adapted the generic definition of the sequence kernels to the problem of recognizing collocations in local word contexts.</S>
			<S sid="118" ssid="70">We refer to (Giuliano et al, 2006) for a detailed description of the syntagmatic kernel.</S>
			<S sid="119" ssid="71">252</S>
	</SECTION>
	<SECTION title="Lexical Entailment for Ontology. " number="4">
			<S sid="120" ssid="1">PopulationIn this section, we apply the IBLE technique, de scribed in Section 3, to recognize lexical entailmentfor ontology population.</S>
			<S sid="121" ssid="2">To this aim, we cast ontol ogy population as a lexical entailment task, where the fine-grained categories are the candidate entailed words, and the named entities to be subcategorized are the entailing words.</S>
			<S sid="122" ssid="3">Below, we present the main steps of our algorithm in details.</S>
			<S sid="123" ssid="4">Step 1 By using a state-of-the-art supervised NER system, we recognize the named entities belonging to a set of coarse-grained categories (e.g., location and people) of interest for the domain.</S>
			<S sid="124" ssid="5">Step 2 For all fine-grained categories belonging tothe same coarse-grained type, we extract from a domain corpus all the occurrences of their lexicaliza tions in context (e.g., for the category actor, we extract all contexts where the term actor occurs), and use them as input to train the IBLE classifier.</S>
			<S sid="125" ssid="6">In this way, we obtain a multi-class classifier for eachontological type.</S>
			<S sid="126" ssid="7">Then, we classify all the occur rences of the named entities recognized in the first step.</S>
			<S sid="127" ssid="8">The output of this process is a list of tagged named entities; where the elements of the list couldhave been classified into different fine-grained cat egories even though they refer to the same phrase (e.g., the occurrences of the entity ?Jack London?</S>
			<S sid="128" ssid="9">could have been classified both as writer andactor, depending on the contexts where they oc cur).</S>
			<S sid="129" ssid="10">Step 3 A distinct category is finally assigned to the entities referring to the same phrase in the list.</S>
			<S sid="130" ssid="11">Thisis done on the basis of the tags that have been as signed to all its occurrences during the previous step.To this purpose, we implemented a voting mecha nism.</S>
			<S sid="131" ssid="12">The basic idea is that an entity belongs to aspecific category if its occurrences entail a particu lar superclass ?more often than expected by chance?, where the expectation is modeled on the basis of the overall distribution of fine-grained category labels, assigned during the second step, in the corpus.</S>
			<S sid="132" ssid="13">Thisintuition is formalized by applying a statistical reli ability measure, that depends on the distribution of positive assignments for each class, defined by the following formula: R(e, c) = P (c|e)?</S>
			<S sid="133" ssid="14">?c ?c , (2) where P (c|e) is estimated by the relative frequencyof the fine-grained class c among the different oc currences of the entity e, ?c and ?c measure the mean and the standard deviation of the distributionP (c|E), and E is an (unlabeled) training set of in stances of the coarse-grained type classified by the IBLE algorithm.</S>
			<S sid="134" ssid="15">Finally, each entity is assigned to the category c?</S>
			<S sid="135" ssid="16">such that c?</S>
			<S sid="136" ssid="17">= argmax c R(e, c).</S>
			<S sid="137" ssid="18">(3)</S>
	</SECTION>
	<SECTION title="Evaluation. " number="5">
			<S sid="138" ssid="1">Evaluating a lexical entailment algorithm in itself is rather complex.</S>
			<S sid="139" ssid="2">Therefore, we performed a taskdriven evaluation of our system, measuring its use fulness in an ontology population task, for which evaluation benchmarks are available, allowing us to compare our technique to existing state-of-the-art approaches.As introduced in Section 4, the ontology popu lation task can be modeled as a lexical entailment problem, in which the fine-grained classes are the entailed words and the named entities belonging to the coarse-grained ontological type are the entailing words.In the following, we first introduce the experimental settings (Section 5.1).</S>
			<S sid="140" ssid="3">Then we evaluate our technique by comparing it to state-of-the-art unsuper vised approaches for ontology population (Section 5.2).</S>
			<S sid="141" ssid="4">5.1 Experimental Settings.</S>
			<S sid="142" ssid="5">For all experiments, we adopted the evaluation benchmark proposed in (Tanev and Magnini, 2006).It considers two high-level named entity cate gories both having five fine-grained sub-classes (i.e., mountain, lake, river, city, and country as subtypes of LOCATION; statesman, writer, athlete, actor, and inventor are subtypes of PERSON).</S>
			<S sid="143" ssid="6">The authors usedWordNet andWikipedia as primary data sources for populating the evaluation ontology.</S>
			<S sid="144" ssid="7">In total, the ontology is populated with280 instances which were not ambiguous (with re spect to the ontology) and appeared at least twice in 253 the English CLEF corpus6.</S>
			<S sid="145" ssid="8">Even the evaluation task is rather small and can be perceived as an artificialexperimental setting, it is the best available bench mark we can use to compare our system to existing approaches in the literature, as we are not aware of other available resources.</S>
			<S sid="146" ssid="9">To perform NER we used CRFs (Lafferty et al, 2001).</S>
			<S sid="147" ssid="10">We trained a first-order CRF on the MUC data set to annotate locations and people.</S>
			<S sid="148" ssid="11">In our experiments, we used the implementation providedin MALLET (McCallum, 2002).</S>
			<S sid="149" ssid="12">We used a stan dard feature set inspired by the literature on textchunking and NER (Tjong Kim Sang and Buch holz, 2000; Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) to train a first-order CRFs.</S>
			<S sid="150" ssid="13">Each instance is represented by encoding all thefollowing families of features, all time-shifted by 2,-1,0,1,2: (a) the word itself, (b) the PoS tag ofthe token, (c) orthographic predicates, such as cap italization, upper-case, numeric, single character, and punctuation, (d) gazetteers of locations, peoplenames and organizations, (e) character-n-gram pred icates for 2 6 n 6 3.As an (unsupervised) training set for the fine grained categories, we exploited all occurrences in context of their corresponding terms we found in the CLEF corpus (e.g., for the category actor we used all the occurrences of the term actor).</S>
			<S sid="151" ssid="14">We did not use any prior estimation of the class frequency, adopting a pure unsupervised approach.</S>
			<S sid="152" ssid="15">Table 2 lists the fine-grained concepts and the number of the training examples found for each of them in the CLEF corpus.</S>
			<S sid="153" ssid="16">As a reference for a comparison of the outcomes of this study, we used the results presented in (Tanevand Magnini, 2006) for the Class-Word and ClassExample approaches.</S>
			<S sid="154" ssid="17">The Class-Word approach exploits a similarity metric between terms and con cepts based on the comparison of the contexts where they appear.</S>
			<S sid="155" ssid="18">Details of this technique can be found in (Cimiano and Vo?lker, 2005).</S>
			<S sid="156" ssid="19">Tanev and Magnini(2006) proposed a variant of the Class-Word algo rithm, called Class-Example, that relies on syntacticfeatures extracted from corpus and uses as an addi tional input a set of training examples for each class.</S>
			<S sid="157" ssid="20">Overall, it required 1, 194 examples to accomplish 6http://www.clef-campaign.org this task.</S>
			<S sid="158" ssid="21">All experiments were performed using the SVM package LIBSVM7 customized to embed our own kernel.</S>
			<S sid="159" ssid="22">In all the experiments, we used the default parameter setting.</S>
			<S sid="160" ssid="23">location person mountain 1681 statesman 119 lake 730 writer 3436 river 1411 athlete 642 city 35000 actor 2356 country 15037 inventor 105 Table 2: Number of training examples for each class.</S>
			<S sid="161" ssid="24">5.2 Results.</S>
			<S sid="162" ssid="25">Table 4 shows our results compared with two base lines (i.e., random and most frequent, estimatedfrom the test data) and the two alternative ap proaches for ontology population described in the previous section.</S>
			<S sid="163" ssid="26">Our system outperforms bothbaselines and largely surpasses the Class-Word un supervised method.</S>
			<S sid="164" ssid="27">It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004).</S>
			<S sid="165" ssid="28">In addition, oursystem is also competitive when compared to super vised approaches, being it only 5 points lower than the Class-Example method, while it does not requireseed examples and syntactic parsing.</S>
			<S sid="166" ssid="29">This charac teristic makes our system flexible and adaptable to different languages and domains.</S>
			<S sid="167" ssid="30">System Micro F1 Macro F1 RND Baseline 0.20 0.20 Class-Word 0.42 0.33 MF baseline 0.52 NA IBLE 0.57 0.47 Class-Example 0.62 0.68Table 3: Comparison of different ontology popula tion techniques.</S>
			<S sid="168" ssid="31">7http://www.csie.ntu.edu.tw/?cjlin/ libsvm/ 254 Finally, we performed a disaggregated evaluationof our system, assessing the performance for different ontological types and different concepts.</S>
			<S sid="169" ssid="32">Re sults show that our method performs better on larger fine-grained classes (i.e., writer and country), while the results on smaller categories are affected by low recall, even if the predictions provided by the system tends to be highly accurate.</S>
			<S sid="170" ssid="33">Taking into consideration that our system is fully unsupervised, this behavior is highly desirable because it implies that it is somehow able to identify the predominant class.</S>
			<S sid="171" ssid="34">In addition the high precision on the smallerclasses can be explained by our instance based ap proach.</S>
			<S sid="172" ssid="35">Person N Prec Rec F1 Inventor 11 1 0.18 0.31 Statesman 20 1.0 0.05 0.10 Writer 88 0.61 0.89 0.72 Actor 25 0.57 0.68 0.62 Athlete 20 1 0.1 0.18 Micro 164 0.61 0.61 0.61 Macro 5 0.83 0.38 0.52Table 4: Performance of the IBLE approach on peo ple.</S>
			<S sid="173" ssid="36">Location N Prec Rec F1 City 23 0.35 0.26 0.30 Country 40 0.61 0.70 0.65 River 10 0.8 0.4 0.53 Mountain 5 0.25 0.2 0.22 Lake 4 0.2 0.5 0.29 Micro 82 0.50 0.50 0.50 Macro 5 0.44 0.41 0.42Table 5: Performance of the IBLE approach on lo cations.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number="6">
			<S sid="174" ssid="1">In this paper, we presented a novel unsupervised technique for recognizing lexical entailment in texts, namely instance based lexical entailment, and we exploited it to approach an ontology population task.</S>
			<S sid="175" ssid="2">The basic assumption is that if a word is entailed by another in a given context, then some of the contexts of the entailed word should be similar tothat of the word to be disambiguated.</S>
			<S sid="176" ssid="3">Our tech nique is effective, as it largely surpasses both the random and most frequent baselines.</S>
			<S sid="177" ssid="4">In addition, it improves over the state-of-the-art for unsupervisedapproaches, achieving performances close to the su pervised rivaling techniques requiring hundreds of examples for each class.</S>
			<S sid="178" ssid="5">Ontology population is only one of the possible applications of lexical entailment.</S>
			<S sid="179" ssid="6">For the future, we plan to apply our instance based approach to a wide variety of tasks, e.g., lexical substitution, word sense disambiguation and information retrieval.</S>
			<S sid="180" ssid="7">In addition, we plan to exploit our lexical entailment asa subcomponent of a more complex system to rec ognize textual entailment.</S>
			<S sid="181" ssid="8">Finally, we are going toexplore more elaborated kernel functions to recog nize lexical entailment and more efficient learning strategies to apply our method to web-size corpora.</S>
			<S sid="182" ssid="9">Acknowledgments The authors would like to thank Bernardo Magnini and Hristo Tanev for providing the benchmark, Ido Dagan for useful discussions and commentsregarding the connections between lexical entail ment and ontology population, and Alberto Lavellifor his thorough review.</S>
			<S sid="183" ssid="10">Claudio Giuliano is sup ported by the X-Media project (http://www.x-media-project.org), sponsored by the European Commission as part of the Information So ciety Technologies (IST) program under EC grantnumber IST-FP6-026978.</S>
			<S sid="184" ssid="11">Alfio Gliozzo is sup ported by the FIRB-Israel research project N. RBIN045PXH.</S>
	</SECTION>
</PAPER>
