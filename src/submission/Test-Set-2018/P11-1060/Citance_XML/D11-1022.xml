<PAPER>
  <S sid="0">Dual Decomposition with Many Overlapping Components</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power.</S>
    <S sid="2" ssid="2">However, in cases where lightweight decomare not readily available due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient.</S>
    <S sid="3" ssid="3">We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes.</S>
    <S sid="4" ssid="4">We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010).</S>
    <S sid="6" ssid="2">The predictive power of such models stems from their ability to break locality assumptions.</S>
    <S sid="7" ssid="3">The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference.</S>
    <S sid="8" ssid="4">In this paper, we focus on parsers built from linear programming relaxations, the so-called &#8220;turbo parsers&#8221; (Martins et al., 2009a; Martins et al., 2010).</S>
    <S sid="9" ssid="5">Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable.</S>
    <S sid="10" ssid="6">This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm.</S>
    <S sid="11" ssid="7">While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its success is strongly tied to the ability of finding a &#8220;good&#8221; decomposition, i.e., one involving few overlapping components (or slaves).</S>
    <S sid="12" ssid="8">With many components, the subgradient algorithm exhibits extremely slow convergence (cf.</S>
    <S sid="13" ssid="9">Fig.</S>
    <S sid="14" ssid="10">2).</S>
    <S sid="15" ssid="11">Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components.</S>
    <S sid="16" ssid="12">Examples include features generated by statements in first-order logic, features that violate Markov assumptions, or history features such as the ones employed in transition-based parsers.</S>
    <S sid="17" ssid="13">To tackle the kind of problems above, we adopt DD-ADMM (Alg.</S>
    <S sid="18" ssid="14">1), a recently proposed algorithm that accelerates dual decomposition (Martins et al., 2011).</S>
    <S sid="19" ssid="15">DD-ADMM retains the modularity of the subgradient-based method, but it speeds up consensus by regularizing each slave subproblem towards the averaged votes obtained in the previous round (cf.</S>
    <S sid="20" ssid="16">Eq.</S>
    <S sid="21" ssid="17">14).</S>
    <S sid="22" ssid="18">While this yields more involved subproblems (with a quadratic term), we show that exact solutions can still be efficiently computed for all cases of interest, by using sort operations.</S>
    <S sid="23" ssid="19">As a result, we obtain parsers that can handle very rich features, do not require specifying a decomposition, and can be heavily parallelized.</S>
    <S sid="24" ssid="20">We demonstrate the success of the approach by presenting experiments in dependency parsing with state-of-the-art results.</S>
  </SECTION>
  <SECTION title="2 Background" number="2">
    <S sid="25" ssid="1">Let x E x be an input object (e.g., a sentence), from which we want to predict a structured output y E &#65533; (e.g., a parse tree).</S>
    <S sid="26" ssid="2">The output set &#65533; is assumed too large for exhaustive search to be tractable.</S>
    <S sid="27" ssid="3">We assume to have a model that assigns a score f(y) to each candidate output, based on which we predict Designing the model must obey certain practical considerations.</S>
    <S sid="28" ssid="4">If efficiency is the major concern, a simple model is usually chosen so that Eq.</S>
    <S sid="29" ssid="5">1 can be solved efficiently, at the cost of limited expressive power.</S>
    <S sid="30" ssid="6">If we care more about accuracy, a model with richer features and more involved score functions may be designed.</S>
    <S sid="31" ssid="7">Decoding, however, will be more expensive, and approximations are often necessary.</S>
    <S sid="32" ssid="8">A typical source of intractability comes from the combinatorial explosion inherent in the composition of two or more tractable models (Bar-Hillel et al., 1964; Tromble and Eisner, 2006).</S>
    <S sid="33" ssid="9">Recently, Rush et al. (2010) have proposed a dual decomposition framework to address NLP problems in which the global score decomposes as f(y) = f1(z1)+f2(z2), where z1 and z2 are two overlapping &#8220;views&#8221; of the output, so that Eq.</S>
    <S sid="34" ssid="10">1 becomes: Above, the notation z1 &#8212; z2 means that z1 and z2 &#8220;agree on their overlaps,&#8221; and an isomorphism Y ^- {(z1, z2) E Y1 x Y2 1 z1 &#8212; z2} is assumed.</S>
    <S sid="35" ssid="11">We next formalize these notions and proceed to compositions of an arbitrary number of models.</S>
    <S sid="36" ssid="12">Of special interest is the unexplored setting where this number is very large and each component very simple.</S>
    <S sid="37" ssid="13">A crucial step in the design of structured predictors is that of decomposing outputs into parts (Taskar et al., 2003).</S>
    <S sid="38" ssid="14">We assume the following setup: Basic parts.</S>
    <S sid="39" ssid="15">We let R be a set of basic parts, such that each element y E Y can be identified with a subset of R. The exact meaning of a &#8220;basic part&#8221; is problem dependent.</S>
    <S sid="40" ssid="16">For example, in dependency parsing, R can be the set of all possible dependency arcs (see Fig.</S>
    <S sid="41" ssid="17">1); in phrase-based parsing, it can be the set of possible spans; in sequence labeling, it can be the set of possible labels at each position.</S>
    <S sid="42" ssid="18">Our only assumption is that we can &#8220;read out&#8221; y from the basic parts it contains.</S>
    <S sid="43" ssid="19">For convenience, we represent y as a binary vector, y = (y(r))r&#8712;R, where y(r) = 1 if part r belongs to y, and 0 otherwise.</S>
    <S sid="44" ssid="20">Decomposition.</S>
    <S sid="45" ssid="21">We generalize the decomposition in Eq.</S>
    <S sid="46" ssid="22">2 by considering sets Y1, ... , YS for S &gt; 2.</S>
    <S sid="47" ssid="23">Each Ys is associated with its own set of parts Rs, in the same sense as above; we represent the elements of Ys as binary vectors zs = (zs(r))r&#8712;R,.</S>
    <S sid="48" ssid="24">Examples are vectors indicating a tree structure, a sequence, or an assignment of variables to a factor, in which case it may happen that only some binary vectors are legal.</S>
    <S sid="49" ssid="25">Some parts in Rs are basic, while others are not.</S>
    <S sid="50" ssid="26">We denote by &#175;Rs = Rs n R the subset of the ones that are.</S>
    <S sid="51" ssid="27">In addition, we assume that: Fig.</S>
    <S sid="52" ssid="28">1 shows several parts used in dependency parsing models; in phrase-based parsing, these could be spans and production rules anchored in the surface string; in sequence labeling, they can be unigram, bigram, and trigram labels.1 Global consistency.</S>
    <S sid="53" ssid="29">We want to be able to read out y E Y by &#8220;gluing&#8221; together the components (z1, ... , zS).</S>
    <S sid="54" ssid="30">This is only meaningful if they are &#8220;globally consistent,&#8221; a notion which we make precise.</S>
    <S sid="55" ssid="31">Two components zs E Ys and zt E Yt are said to be consistent (denoted zs ti zt) if they agree on their overlaps, i.e., if zs(r) = zt(r), dr E Rs n Rt.</S>
    <S sid="56" ssid="32">A complete assignment (z1, ... , zS) is globally consistent if all pairs of components are consistent.</S>
    <S sid="57" ssid="33">This is equivalent to the existence of a witness vector (u(r))r&#8712;R such that zs(r) = u(r), ds, r E &#175;Rs.</S>
    <S sid="58" ssid="34">With this setup, assuming that the score function decomposes as f(z) = PSs=1 fs(zs), the decoding problem (which extends Eq.</S>
    <S sid="59" ssid="35">2 for S &gt; 2) becomes: We call the equality constraints expressed in the last line the &#8220;agreement constraints.&#8221; It is these constraints that complicate the problem, which would otherwise be exactly separable into S subproblems.</S>
    <S sid="60" ssid="36">The dual decomposition method (Komodakis et al., 2007; Rush et al., 2010) builds an approximation by dualizing out these constraints, as we describe next.</S>
    <S sid="61" ssid="37">We describe dual decomposition in a slightly different manner than Rush et al. (2010): we will first build a relaxation of P (called P0), in which the entire approximation is enclosed.</S>
    <S sid="62" ssid="38">Then, we dualize P0, yielding problem D. In the second step, the duality gap is zero, i.e., P0 and D are equivalent.2 Relaxation.</S>
    <S sid="63" ssid="39">For each s E {1, ... , S} we consider the convex hull of Ys, We have that Ys = Zs n Z|Rs|; hence, problem P (Eq.</S>
    <S sid="64" ssid="40">3) is equivalent to one in which each Ys is replaced by Zs and the z-variables are constrained to be integer.</S>
    <S sid="65" ssid="41">By dropping the integer constraints, we obtain the following relaxed problem: If the score functions fs are convex, P0 becomes a convex program (unlike P, which is discrete); being a relaxation, it provides an upper bound of P. Lagrangian.</S>
    <S sid="66" ssid="42">Introducing a Lagrange multiplier &#955;s(r) for each agreement constraint in Eq.</S>
    <S sid="67" ssid="43">5, one obtains the Lagrangian function and the dual problem (the master) where the gs(&#955;s) are the solution values of the following subproblems (the slaves): We assume that strong duality holds (w.r.t.</S>
    <S sid="68" ssid="44">Eqs.</S>
    <S sid="69" ssid="45">5&#8211; 7), hence we have P G P0 = D.3 Solving the dual.</S>
    <S sid="70" ssid="46">Why is the dual formulation D (Eqs.</S>
    <S sid="71" ssid="47">7&#8211;8) more appealing than P0 (Eq.</S>
    <S sid="72" ssid="48">5)?</S>
    <S sid="73" ssid="49">The answer is that the components 1, ... , S are now decoupled, which makes things easier provided each slave subproblem (Eq.</S>
    <S sid="74" ssid="50">8) can be solved efficiently.</S>
    <S sid="75" ssid="51">In fact, this is always a concern in the mind of the model&#8217;s designer when she chooses a decomposition (the framework that we describe in &#167;3, in some sense, alleviates her from this concern).</S>
    <S sid="76" ssid="52">If the score functions are linear, i.e., of the form fs(zs) = Pr&#8712;Rs &#952;s(r)zs(r) for some vector &#952;s = (&#952;s (r))r&#8712;Rs, then Eq.</S>
    <S sid="77" ssid="53">8 becomes a linear program, for which a solution exists at a vertex of Zs (which in turn is an element of Ys).</S>
    <S sid="78" ssid="54">Depending on the structure of the problem, Eq.</S>
    <S sid="79" ssid="55">8 may be solved by brute force, dynamic programming, or specialized combinatorial algorithms (Rush et al., 2010; Koo et al., 2010; Rush and Collins, 2011).</S>
    <S sid="80" ssid="56">Applying the projected subgradient method (Komodakis et al., 2007; Rush et al., 2010) to the master problem (Eq.</S>
    <S sid="81" ssid="57">7) yields a remarkably simple algorithm, which at each round t solves the subproblems in Eq.</S>
    <S sid="82" ssid="58">8 for s = 1, ... , S, and then gathers these solutions (call them zt+1 s ) to compute an &#8220;averaged&#8221; vote for each basic part, where &#948;(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, where &#951;t is a stepsize.</S>
    <S sid="83" ssid="59">Intuitively, the algorithm pushes for a consensus among the slaves (Eq.</S>
    <S sid="84" ssid="60">9), via an adjustment of the Lagrange multipliers which takes into consideration deviations from the average (Eq.</S>
    <S sid="85" ssid="61">10).</S>
    <S sid="86" ssid="62">The subgradient method is guaranteed to converge to the solution of D (Eq.</S>
    <S sid="87" ssid="63">7), for suitably chosen stepsizes (Shor, 1985; Bertsekas et al., 1999); it also provides a certificate of optimality in case the relaxation is tight (i.e., P = D) and the exact solution has been found.</S>
    <S sid="88" ssid="64">However, convergence is slow when S is large (as we will show in the experimental section), and no certificates are available when there is a relaxation gap (P &lt; P0).</S>
    <S sid="89" ssid="65">In the next section, we describe the DD-ADMM algorithm (Martins et al., 2011), which does not have these drawbacks and shares a similar simplicity.</S>
  </SECTION>
  <SECTION title="3 Alternating Directions Method" number="3">
    <S sid="90" ssid="1">There are two reasons why subgradient-based dual decomposition is not completely satisfying: 4Our main concern is P; however solving P' is often a useful step towards that goal, either because a good rounding scheme exists, or because one may build tighter relaxations to approach P (Sontag et al., 2008; Rush and Collins, 2011).</S>
    <S sid="91" ssid="2">Taking a look back at the relaxed primal problem P0 (Eq.</S>
    <S sid="92" ssid="3">5), we see that any primal feasible solution must satisfy the agreement constraints.</S>
    <S sid="93" ssid="4">This suggests that penalizing violations of these constraints could speed up consensus.</S>
    <S sid="94" ssid="5">Augmented Lagrangian.</S>
    <S sid="95" ssid="6">By adding a penalty term to Eq.</S>
    <S sid="96" ssid="7">6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): where the parameter &#961; &gt; 0 controls the intensity of the penalty.</S>
    <S sid="97" ssid="8">Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), &#167;4.2).</S>
    <S sid="98" ssid="9">They alternate updates to the &#955;-variables, while seeking to maximize A&#961; with respect to z and u.</S>
    <S sid="99" ssid="10">In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables.</S>
    <S sid="100" ssid="11">The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue by performing alternate maximizations, followed by an update of the Lagrange multipliers as in Eq.</S>
    <S sid="101" ssid="12">10.</S>
    <S sid="102" ssid="13">Recently, ADMM has attracted interest, being applied in a variety of problems; see the recent book by Boyd et al. (2011) for an overview.</S>
    <S sid="103" ssid="14">As derived in the App.</S>
    <S sid="104" ssid="15">A, the u-updates in Eq.</S>
    <S sid="105" ssid="16">13 have a closed form, which is precisely the averaging operation performed by the subgradient method (Eq.</S>
    <S sid="106" ssid="17">9).</S>
    <S sid="107" ssid="18">We are left with the problem of computing the z-updates.</S>
    <S sid="108" ssid="19">Like in the subgradient approach, the maximization in Eq.</S>
    <S sid="109" ssid="20">12 can be separated into S independent slave subproblems, which now take the form: Comparing Eq.</S>
    <S sid="110" ssid="21">8 and Eq.</S>
    <S sid="111" ssid="22">14, we observe that the only difference is the presence in the latter of a quadratic term which regularizes towards the previous averaged votes ut(r).</S>
    <S sid="112" ssid="23">Because of this term, the solution of Eq.</S>
    <S sid="113" ssid="24">14 for linear score functions may not be at a vertex (in contrast to the subgradient method).</S>
    <S sid="114" ssid="25">We devote &#167;4 to describing exact and efficient ways of solving the problem in Eq.</S>
    <S sid="115" ssid="26">14 for important, widely used slaves.</S>
    <S sid="116" ssid="27">Before going into details, we mention another advantage of ADMM over the subgradient algorithm: it knows when to stop.</S>
    <S sid="117" ssid="28">Primal and dual residuals.</S>
    <S sid="118" ssid="29">Recall that the subgradient method provides optimality certificates when the relaxation is tight (P = P') and an exact solution of P has been found.</S>
    <S sid="119" ssid="30">While this is good enough when tight relaxations are frequent, as in the settings explored by Rush et al. (2010), Koo et al.</S>
    <S sid="120" ssid="31">(2010), and Rush and Collins (2011), it is hard to know when to stop when a relaxation gap exists.</S>
    <S sid="121" ssid="32">We would like to have similar guarantees concerning the relaxed primal P'.5 A general weakness of subgradient algorithms is that they do not have this capacity, and so are usually stopped by specifying a maximum number of iterations.</S>
    <S sid="122" ssid="33">In contrast, ADMM allows to keep track of primal and dual residuals (Boyd et al., 2011).</S>
    <S sid="123" ssid="34">This allows providing certificates not only for the exact solution of P (when the relaxation is tight), but also to terminate when a near optimal solution of the relaxed problem P' has been found.</S>
    <S sid="124" ssid="35">Theprimal residual rt P measures the amount by which the agreement constraints are violated: the dual residual rt D is the amount by which a dual optimality condition is violated (see Boyd et al. (2011), p.18, for details).</S>
    <S sid="125" ssid="36">It is computed via: Our stopping criterion is thus that these two residuals are below a threshold, e.g., 1 x 10&#8722;3.</S>
    <S sid="126" ssid="37">The complete algorithm is depicted as Alg.</S>
    <S sid="127" ssid="38">1.</S>
    <S sid="128" ssid="39">As stated in 5This problem is more important than it may look.</S>
    <S sid="129" ssid="40">Problems with many slaves tend to be less exact, hence relaxation gaps are frequent.</S>
    <S sid="130" ssid="41">Also, when decoding is embedded in training, it is useful to obtain the fractional solution of the relaxed primal P (rather than an approximate integer solution).</S>
    <S sid="131" ssid="42">See Kulesza and Pereira (2007) and Martins et al. (2009b) for details.</S>
    <S sid="132" ssid="43">Algorithm 1 ADMM-based Dual Decomposition Martins et al. (2011), convergence to the solution of P' is guaranteed with a fixed stepsize &#951;t = &#964;&#961;, with &#964; E [1, 1.618] (Glowinski and Le Tallec, 1989, Thm.</S>
    <S sid="133" ssid="44">4.2).</S>
    <S sid="134" ssid="45">In our experiments, we set &#964; = 1.5, and adapt &#961; as described in (Boyd et al., 2011, p.20).6</S>
  </SECTION>
  <SECTION title="4 Solving the Subproblems" number="4">
    <S sid="135" ssid="1">In this section, we address the slave subproblems of DD-ADMM (Eq.</S>
    <S sid="136" ssid="2">14).</S>
    <S sid="137" ssid="3">We show how these subproblems can be solved efficiently for several important cases that arise in NLP applications.</S>
    <S sid="138" ssid="4">Throughout, we assume that the score functions fs are linear, i.e., they can be written as fs(zs) = P rERs &#952;s(r)zs(r).</S>
    <S sid="139" ssid="5">This is the case whenever a linear model is used, in which case &#952;s(r) = 1 &#948;(r)w &#183; O(x, r), where w is a weight vector and O(x, r) is a feature vector.</S>
    <S sid="140" ssid="6">It is also the scenario studied in previous work in dual decomposition (Rush et al., 2010).</S>
    <S sid="141" ssid="7">Under this assumption, and discarding constant terms, the slave subproblem in Eq.</S>
    <S sid="142" ssid="8">14 becomes: where as(r) = ut(r)+&#961;&#8722;1(&#952;s(r)+&#955;ts(r)).</S>
    <S sid="143" ssid="9">Since Zs is a polytope, Eq.</S>
    <S sid="144" ssid="10">17 is a quadratic program, which can be solved with a general purpose solver.</S>
    <S sid="145" ssid="11">However, that does not exploit the structure of Zs and is inefficient when |Rs |is large.</S>
    <S sid="146" ssid="12">We next show that for many cases, a closed-form solution is available and 6Briefly, we initialize p = 0.03 and then increase/decrease p by a factor of 2 whenever the primal residual becomes &gt; 10 times larger/smaller than the dual residual. can be computed in O(|Rs|) time, up to log factors.7 Pairwise Factors.</S>
    <S sid="147" ssid="13">This is the case where RPAIR = {r1, r2, r12}, where r1 and r2 are basic parts and r12 is their conjunction, i.e., we have YPAIR = {hz1, z2, z12i  |z12 = z1 &#8743; z2}.</S>
    <S sid="148" ssid="14">This factor is useful to make conjunctions of variables participate in the score function (see e.g. the grandparent, sibling, and head bigram parts in Fig.</S>
    <S sid="149" ssid="15">1).</S>
    <S sid="150" ssid="16">The convex hull of YPAIR is the polytope ZPAIR = {hz1, z2, z12i &#8712; [0, 1]3  |z12 &#8804; z1, z12 &#8804; z2, z12 &#8805; z1 + z2 &#8722; 1}, as shown by Martins et al. (2010).</S>
    <S sid="151" ssid="17">In this case, problem (17) can be written as and has a closed form solution (see App.</S>
    <S sid="152" ssid="18">B).</S>
    <S sid="153" ssid="19">Uniqueness Quantification and XOR.</S>
    <S sid="154" ssid="20">Many problems involve constraining variables to take a single value: for example, in dependency parsing, a modifier can only take one head.</S>
    <S sid="155" ssid="21">This can be expressed as the statement &#8707;!y : Q(y) in first-order logic,8 or as a one-hot XOR factor in a factor graph (Smith and Eisner, 2008; Martins et al., 2010).</S>
    <S sid="156" ssid="22">In this case, RXOR = {r1, ... , rn}, and YXOR = {hz1, ... , zni &#8712; {0, 1}n  |Pni=1 zi = 1}.</S>
    <S sid="157" ssid="23">The convex hull of YXOR is ZXOR = {hz1, ... , zni &#8712; [0, 1]n  |Pn i=1 zi = 1}.</S>
    <S sid="158" ssid="24">Assume for the sake of simplicity that all parts in RXOR are basic.9 Up to a constant, the slave subproblem becomes: This is the problem of projecting onto the probability simplex, which can be done in O(n log n) time via a sort operation (see App.</S>
    <S sid="159" ssid="25">C).10 Existential Quantification and OR.</S>
    <S sid="160" ssid="26">Sometimes, only existence is required, not necessarily uniqueness.</S>
    <S sid="161" ssid="27">This can be expressed with disjunctions, existential quantifiers in first-order logic (&#8707;y : Q(y)), or as a OR factor.</S>
    <S sid="162" ssid="28">In this case, ROR = {r1, ... , rn}, YOR = {hz1, ... , zni &#8712; {0,1}n  |Wni=1 zi = 1}, and the convex hull is ZOR = {hz1, ... , zni &#8712; [0, 1]n  |Pni=1 zi &#8805; 1} (see Tab.</S>
    <S sid="163" ssid="29">1 in Martins et al. (2010)).</S>
    <S sid="164" ssid="30">The slave subproblem becomes: We derive a procedure in App.</S>
    <S sid="165" ssid="31">D to compute this projection in O(n log n) runtime, also with a sort.</S>
    <S sid="166" ssid="32">Negations.</S>
    <S sid="167" ssid="33">The two cases above can be extended to allow some of their inputs to be negated.</S>
    <S sid="168" ssid="34">By a change of variables in Eqs.</S>
    <S sid="169" ssid="35">19&#8211;20 it is possible to reuse the same black box that solves those problems.</S>
    <S sid="170" ssid="36">The procedure is as follows: The ability to handle negated variables adds a great degree of flexibility.</S>
    <S sid="171" ssid="37">From De Morgan&#8217;s laws, we can now handle conjunctions and implications (since Vni=1 Qi(x) &#8658; R(x) is equivalent to Wni=1 &#172;Qi(x) &#8744; R(x)).</S>
    <S sid="172" ssid="38">Logical Variable Assignments.</S>
    <S sid="173" ssid="39">All previous examples involve taking a group of existing variables and defining a constraint.</S>
    <S sid="174" ssid="40">Alternatively, we may want to define a new variable which is the result of an operation involving other variables.</S>
    <S sid="175" ssid="41">For example, R(x) := &#8707;!y : Q(x, y).</S>
    <S sid="176" ssid="42">This corresponds to the XOR-WITH-OUTPUT factor in Martins et al. (2010).</S>
    <S sid="177" ssid="43">Interestingly, this can be expressed as a XOR where R(x) is negated (i.e., either &#172;R(x) holds or exactly one y satisfies Q(x, y), but not both).</S>
    <S sid="178" ssid="44">A more difficult problem is that of the OR-WITHOUTPUT factor, expressed by the formula R(x) := &#8707;y : Q(x, y).</S>
    <S sid="179" ssid="45">We have ROR-OUT = {r0, ... , rn},{ and YOR-OUT = hz0, ... , zni &#8712; {0,1}n  |z0 = a to descendant d), nextsibl(h, m, s) (indicating that (h, m) and (h, s) are consecutive siblings), nonproj(h, m) (indicating that (h, m) is a non-projective arc), as well as the auxiliary variables flow(h, m, d) (indicating that arc (h, m) carries flow to d), and lastsibl(h, m, k) (indicating that, up to position k, the last seen modifier of h occurred at position m).</S>
    <S sid="180" ssid="46">The non-basic parts are the pairwise factors sibl(h, m, s), grand(g, h, m), and bigram(b, h, m); as well as each logical formula.</S>
    <S sid="181" ssid="47">Columns 3&#8211;4 indicate the number of parts of each kind, and the time complexity for solving each subproblem.</S>
    <S sid="182" ssid="48">For a sentence of length n, there are O(n3) parts and the total complexity is O(n3 log n).</S>
    <S sid="183" ssid="49">Vni=1 zi}.</S>
    <S sid="184" ssid="50">The convex hull of YOR-OUT is the following set: ZOR-OUT = I(z0, ... , zn) E [0, 1]n I z0 &gt; En i=1 zi, z0 &#65533; zi, bi = 1, .</S>
    <S sid="185" ssid="51">.</S>
    <S sid="186" ssid="52">.</S>
    <S sid="187" ssid="53">,n} (Martins et al., 2010, Tab.1).</S>
    <S sid="188" ssid="54">The slave subproblem is: The problem in Eq.</S>
    <S sid="189" ssid="55">21 is more involved than the ones in Eqs.</S>
    <S sid="190" ssid="56">19&#8211;20.</S>
    <S sid="191" ssid="57">Yet, there is still an efficient procedure with runtime O(n log n) (see App.</S>
    <S sid="192" ssid="58">E).</S>
    <S sid="193" ssid="59">By using the result above for negated variables, we are now endowed with a procedure for many other cases, such that AND-WITH-OUTPUT and formulas with universal quantifiers (e.g., R(x) := by : Q(x, y)).</S>
    <S sid="194" ssid="60">Up to a log-factor, the runtimes will be linear in the number of predicates.</S>
    <S sid="195" ssid="61">Larger Slaves.</S>
    <S sid="196" ssid="62">The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq.</S>
    <S sid="197" ssid="63">14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model.</S>
    <S sid="198" ssid="64">Hence, our method seems to be more suitable for decompositions which involve &#8220;simple slaves,&#8221; even if their number is large.</S>
    <S sid="199" ssid="65">However, this does not rule out the possibility of using this method otherwise.</S>
    <S sid="200" ssid="66">Eckstein and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact.</S>
    <S sid="201" ssid="67">Hence the method may still work if the slaves are solved numerically up to some accuracy.</S>
    <S sid="202" ssid="68">We defer this to future investigation.</S>
  </SECTION>
  <SECTION title="5 Experiments: Dependency Parsing" number="5">
    <S sid="203" ssid="1">We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008).</S>
    <S sid="204" ssid="2">We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data.</S>
    <S sid="205" ssid="3">We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b).</S>
    <S sid="206" ssid="4">Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010).</S>
    <S sid="207" ssid="5">To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections &#167;02&#8211;21, use &#167;22 as validation data, and test on &#167;23.</S>
    <S sid="208" ssid="6">We ran SVMTool (Gim&#180;enez and Marquez, 2004) to obtain automatic part-of-speech tags for &#167;22&#8211;23.</S>
    <S sid="209" ssid="7">solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time).</S>
    <S sid="210" ssid="8">The parts used in our full model are the ones depicted in Fig.</S>
    <S sid="211" ssid="9">1.</S>
    <S sid="212" ssid="10">Note that a subgradient-based method could handle some of those parts efficiently (arcs, consecutive siblings, grandparents, and head bigrams) by composing arc-factored models, head automata, and a sequence labeler.</S>
    <S sid="213" ssid="11">However, no lightweight decomposition seems possible for incorporating parts for all siblings, directed paths, and non-projective arcs.</S>
    <S sid="214" ssid="12">Tab.</S>
    <S sid="215" ssid="13">1 shows the first-order logical formulae that encode the constraints in our model.</S>
    <S sid="216" ssid="14">Each formula gives rise to a subproblem which is efficiently solvable (see &#167;4).</S>
    <S sid="217" ssid="15">By ablating some of rows of Tab.</S>
    <S sid="218" ssid="16">1 we recover known methods: The experimental results are shown in Tab.</S>
    <S sid="219" ssid="17">2.</S>
    <S sid="220" ssid="18">For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010).</S>
    <S sid="221" ssid="19">Our full model achieved the best reported scores for 7 datasets.</S>
    <S sid="222" ssid="20">The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al.</S>
    <S sid="223" ssid="21">(2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs.</S>
    <S sid="224" ssid="22">See App.</S>
    <S sid="225" ssid="23">F. 13Note however that the actual results of Koo et al. (2010) are higher than our reproduction, as can be seen in the second column.</S>
    <S sid="226" ssid="24">The differences are due to the features that were used and on the way the models were trained.</S>
    <S sid="227" ssid="25">The cause is not search error: exact decoding with an ILP solver (CPLEX) revealed no significant difference with respect to our G+CS column.</S>
    <S sid="228" ssid="26">We leave further analysis for future work.</S>
    <S sid="229" ssid="27">et al. (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al.</S>
    <S sid="230" ssid="28">(2010), and [Ko10] is Koo et al. (2010).</S>
    <S sid="231" ssid="29">In columns 3&#8211;4, &#8220;Full&#8221; is our full model, and &#8220;G+CS&#8221; is our reproduction of the model of Koo et al. (2010), i.e., the same as &#8220;Full&#8221; but with all features ablated excepted for grandparents and consecutive siblings.</S>
    <S sid="232" ssid="30">Feature ablation and error analysis.</S>
    <S sid="233" ssid="31">We conducted a simple ablation study by training several models on the English PTB with different sets of features.</S>
    <S sid="234" ssid="32">Tab.</S>
    <S sid="235" ssid="33">3 shows the results.</S>
    <S sid="236" ssid="34">As expected, performance keeps increasing as we use models with greater expressive power.</S>
    <S sid="237" ssid="35">We show some concrete examples in App.</S>
    <S sid="238" ssid="36">G of sentences that the full model parsed correctly, unlike less expressive models.</S>
    <S sid="239" ssid="37">Convergence speed and optimality.</S>
    <S sid="240" ssid="38">Fig.</S>
    <S sid="241" ssid="39">2 compares the performance of DD-ADMM and the subgradient algorithms in the validation section of the PTB.14 For the second order model, the subgradient 14The learning rate in the subgradient method was set as 77t = 770/(1+Nincr(t)), as in Koo et al. (2010), where Nincr(t) is the number of dual increases up to the tth iteration, and 770 is chosen to maximize dual decrease after 20 iterations (in a per sentence basis).</S>
    <S sid="242" ssid="40">Those preliminary iterations are not plotted in Fig.</S>
    <S sid="243" ssid="41">2. method has more slaves than in Koo et al. (2010): it has a slave imposing the TREE constraint (whose subproblems consists on finding a minimum spanning tree) and several for the all-sibling parts, yielding an average number of 310.5 and a maximum of 4310 slaves.</S>
    <S sid="244" ssid="42">These numbers are still manageable, and we observe that a &#8220;good&#8221; UAS is achieved relatively quickly.</S>
    <S sid="245" ssid="43">The ADMM method has many more slaves due to the multicommodity flow constraints (average 1870.8, maximum 65446), yet it attains optimality sooner, as can be observed in the right plot.</S>
    <S sid="246" ssid="44">For the full model, the subgradient-based method becomes extremely slow, and the UAS score severely degrades (after 1000 iterations it is 2% less than the one obtained with the ADMM-based method, with very few instances having been solved to optimality).</S>
    <S sid="247" ssid="45">The reason is the number of slaves: in this configuration and dataset the average number of slaves per instance is 3327.4, and the largest number is 113207.</S>
    <S sid="248" ssid="46">On the contrary, the ADMM method keeps a robust performance, with a large fraction of optimality certificates in early iterations.</S>
    <S sid="249" ssid="47">Runtime and caching strategies.</S>
    <S sid="250" ssid="48">Despite its suitability to problems with many overlapping components, our parser is still 1.6 times slower than Koo et al. (2010) (0.34 against 0.21 sec./sent. in PTB &#167;23), and is far beyond the speed of transition-based parsers (e.g., Huang and Sagae (2010) take 0.04 sec./sent. on the same data, although accuracy is lower, 92.1%).</S>
    <S sid="251" ssid="49">Our implementation, however, is not fully optimized.</S>
    <S sid="252" ssid="50">We next describe how considerable speed-ups are achieved by caching the subproblems, following a strategy similar to Koo et al. (2010).</S>
    <S sid="253" ssid="51">Fig.</S>
    <S sid="254" ssid="52">3 illustrates the point.</S>
    <S sid="255" ssid="53">After a few iterations, many variables u(r) see a consensus being achieved (i.e., ut(r) = zt+1 s (r), &#8704;s) and enter an idle state: they are left unchanged by the u-update in Eq.</S>
    <S sid="256" ssid="54">9, and so do the Lagrange variables At+1 s (r) (Eq.</S>
    <S sid="257" ssid="55">10).</S>
    <S sid="258" ssid="56">If by iteration t all variables in a subproblem s are idle, then zt+1 s (r) = zts(r), hence the subproblem does not need to be resolved.15 Fig.</S>
    <S sid="259" ssid="57">3 shows that many variables and subproblems are left untouched after the first few rounds.</S>
    <S sid="260" ssid="58">Finally, Fig.</S>
    <S sid="261" ssid="59">4 compares the runtimes of our implementation of DD-ADMM with those achieved by a state-of-the-art LP solver, CPLEX, in its best performing configuration: the simplex algorithm applied to the dual LP.</S>
    <S sid="262" ssid="60">We observe that DD-ADMM is faster in some regimes but slower in others.</S>
    <S sid="263" ssid="61">For short sentences (&lt; 15 words), DD-ADMM tends to be faster.</S>
    <S sid="264" ssid="62">For longer sentences, CPLEX is quite effective as it uses good heuristics for the pivot steps in the simplex algorithm; however, we observed that it sometimes gets trapped on large problems.</S>
    <S sid="265" ssid="63">Note also that DD-ADMM is not fully optimized, and that it is much more amenable to parallelization than the simplex algorithm, since it is composed of many independent slaves.</S>
    <S sid="266" ssid="64">This suggests potentially significant speed-ups in multi-core environments.</S>
  </SECTION>
  <SECTION title="6 Related Work" number="6">
    <S sid="267" ssid="1">Riedel and Clarke (2006) first formulated dependency parsing as an integer program, along with logical constraints.</S>
    <S sid="268" ssid="2">The multicommodity flow formulation was introduced by Martins et al. (2009a), along with some of the parts considered here.</S>
    <S sid="269" ssid="3">Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010).</S>
    <S sid="270" ssid="4">DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs.</S>
    <S sid="271" ssid="5">The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011).</S>
    <S sid="272" ssid="6">Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal).</S>
    <S sid="273" ssid="7">While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery.</S>
    <S sid="274" ssid="8">Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-lightly-decomposable constrained problems (e.g., exploiting linguistic knowledge).</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="7">
    <S sid="275" ssid="1">We have introduced new feature-rich turbo parsers.</S>
    <S sid="276" ssid="2">Since exact decoding is intractable, we solve an LP relaxation through a recently proposed consensus algorithm, DD-ADMM, which is suitable for problems with many overlapping components.</S>
    <S sid="277" ssid="3">We study the empirical runtime and convergence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011).</S>
    <S sid="278" ssid="4">DD-ADMM compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions.</S>
    <S sid="279" ssid="5">While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors.</S>
    <S sid="280" ssid="6">DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011).</S>
    <S sid="281" ssid="7">Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011).</S>
    <S sid="282" ssid="8">Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011).</S>
    <S sid="283" ssid="9">We defer this for future work.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="284" ssid="1">We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code.</S>
    <S sid="285" ssid="2">A. M. was supported by a FCT/ICTI grant through the CMU-Portugal Program, and by Priberam.</S>
    <S sid="286" ssid="3">This work was partially supported by the FET programme (EU FP7), under the SIMBAD project (contract 213250).</S>
    <S sid="287" ssid="4">N. S. was supported by NSF CAREER IIS-1054319.</S>
  </SECTION>
</PAPER>
