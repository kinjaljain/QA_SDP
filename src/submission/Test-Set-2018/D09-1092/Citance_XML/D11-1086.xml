<PAPER>
	<S sid="0">Improving Bilingual Projections via Sparse Covariance Matrices</S><ABSTRACT>
		<S sid="1" ssid="1">Mapping documents into an interlingual representation can help bridge the language bar rier of cross-lingual corpora.</S>
		<S sid="2" ssid="2">Many existing approaches are based on word co-occurrencesextracted from aligned training data, repre sented as a covariance matrix.</S>
		<S sid="3" ssid="3">In theory, sucha covariance matrix should represent seman tic equivalence, and should be highly sparse.</S>
		<S sid="4" ssid="4">Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations.</S>
		<S sid="5" ssid="5">In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways.</S>
		<S sid="6" ssid="6">First, we explore word association measures and bilingual dictionaries to weigh the word pairs.</S>
		<S sid="7" ssid="7">Later, we explore different selection strategies to remove the noisy pairsbased on the association scores.</S>
		<S sid="8" ssid="8">Our experimental results on the task of aligning compa rable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="9" ssid="9">Aligning documents from different languages arisesin a range of tasks such as parallel phrase extrac tion (Gale and Church, 1991; Rapp, 1999), miningtranslations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarla mudi, 2011) and document retrieval (Ballesteros and Croft, 1996; Munteanu and Marcu, 2005).</S>
			<S sid="10" ssid="10">In this task, we are given a comparable corpora and some documents in one language are assumed to have a comparable document in the other language and thegoal is to recover this hidden alignment.</S>
			<S sid="11" ssid="11">In this paper, we address this problem by mapping the documents into a common subspace (interlingual repre sentation).</S>
			<S sid="12" ssid="12">This common subspace generalizes thenotion of vector space model for cross-lingual ap plications (Turney and Pantel, 2010).</S>
			<S sid="13" ssid="13">Most of the existing approaches use manually aligned document pairs to find a common subspace in which the aligned document pairs are maximallycorrelated.</S>
			<S sid="14" ssid="14">The sub-space can be found using ei ther generative approaches based on topic modeling (Mimno et al, 2009; Jagarlamudi and Daume?</S>
			<S sid="15" ssid="15">III,2010; Zhang et al, 2010; Vu et al, 2009) or discriminative approaches based on variants of Principal Component Analysis (PCA) and Canonical Cor relation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al, 2003; Platt et al, 2010; Haghighi et al, 2008).</S>
			<S sid="16" ssid="16">Both styles rely on document level term co-occurrences to find the latent representation.</S>
			<S sid="17" ssid="17">The discriminative approaches capture essential word co-occurrences in terms of two monolingual covariance matrices and a cross-covariance matrix.</S>
			<S sid="18" ssid="18">Subsequently, they use these covariance matrices to find projection directions in each language such that aligned documents lie close to each other (Sec.</S>
			<S sid="19" ssid="19">2).The strong reliance of these approaches on the co variance matrices leads to problems, especially with the noisy data caused either by the noisy words in a document or the noisy document alignments.</S>
			<S sid="20" ssid="20">Noisy data is not uncommon and is usually the case with data collected from community based resources such as Wikipedia.</S>
			<S sid="21" ssid="21">This degrades performance of a 930variety of tasks, such as transliteration Mining (Kle mentiev and Roth, 2006; Hermjakob et al, 2008; Ravi and Knight, 2009) and multilingual web search (Gao et al, 2009).In this paper, we address the problem of identi fying and removing noisy entries in the covariance matrices.</S>
			<S sid="22" ssid="22">We address this problem in two stages.In the first stage, we explore the use of word asso ciation measures such as Mutual Information (MI) and Yule?s ?</S>
			<S sid="23" ssid="23">(Reis and Judd, 2000) in computing the strength of a word pair (Sec.</S>
			<S sid="24" ssid="24">3.1).</S>
			<S sid="25" ssid="25">We also explore the use of bilingual dictionaries developed from cleaner resources such as parallel data.</S>
			<S sid="26" ssid="26">In thesecond stage, we use the association strengths in fil tering out the noisy word pairs from the covariancematrices.</S>
			<S sid="27" ssid="27">We pose this as a word pair selection prob lem and explore multiple strategies (Sec.</S>
			<S sid="28" ssid="28">3.2).We evaluate the utility of sparse covariance matrices in improving the bilingual projections incrementally (Sec.</S>
			<S sid="29" ssid="29">4).</S>
			<S sid="30" ssid="30">We first report results on synthetic multi-view data where the true correspondences between features of different views are available.</S>
			<S sid="31" ssid="31">Moreover, this also lets us systematically ex plore the effect of noise level on the accuracy.</S>
			<S sid="32" ssid="32">Our experimental results show a significant improvement when the true correspondences are available.</S>
			<S sid="33" ssid="33">Later, we report our experimental results on the document alignment task on Europarl and Wikipedia data setsand on two language pairs.</S>
			<S sid="34" ssid="34">We found that sparsifying the covariance matrices helps in general, but using cleaner resource such bilingual dictionaries per formed best.</S>
	</SECTION>
	<SECTION title="Canonical Correlation Analysis (CCA). " number="2">
			<S sid="35" ssid="1">In this section, we describe how Canonical Correlation Analysis is used to solve the problem of aligning bilingual documents.</S>
			<S sid="36" ssid="2">We mainly focus on repre senting the solution of CCA in terms of covariance matrices.</S>
			<S sid="37" ssid="3">Since most of the existing discriminativeapproaches are variants of CCA, showing the advantage of recovering sparseness in CCA makes it appli cable to the other variants as well.</S>
			<S sid="38" ssid="4">Given a training data of n aligned document pairs, CCA finds projection directions for each language,so that the documents when projected along these di rections are maximally correlated (Hotelling, 1936).</S>
			<S sid="39" ssid="5">Let X (d1?n) and Y (d2?n) be the representation of data in both the languages and further assume that the data is centered (subtract the mean vector from each document i.e. xi?xi??x and yi ? yi??y).</S>
			<S sid="40" ssid="6">Then CCA finds projection directions a and b which maximize: aTXY Tb?</S>
			<S sid="41" ssid="7">aTXXTa ? bTY Y Tb s.t. aTXXTa = 1 &amp; bTY Y Tb = 1 The projection directions are obtained by solving the generalized eigen system: [ 0 Cxy.</S>
			<S sid="42" ssid="8">Cyx 0 ] [ a b ] = [ (1-?)Cxx+?I 0 0 (1-?)Cyy+?I ] [ a b ] (1)where Cxx = XXT , Cyy = Y Y T are the monolingual covariance matrices, Cxy = XY T is the crosscovariance matrix and ? is the regularization param eter.</S>
			<S sid="43" ssid="9">Using these eigenvectors as columns, we form the projection matrices A and B. These projectionmatrices are used to map documents in both the lan guages into interlingual representation.</S>
			<S sid="44" ssid="10">Given any new pair of documents, their similarityis computed by first mapping them into the lower di mensions space and computing the cosine similarity between their projections.</S>
			<S sid="45" ssid="11">In general, using all the eigenvectors is sub optimal and thus retaining top eigenvectors leads to better generalizability.</S>
	</SECTION>
	<SECTION title="Covariance Selection. " number="3">
			<S sid="46" ssid="1">As shown above, the underlying objective function in most of the discriminative approaches is of the form aTXY Tb.</S>
			<S sid="47" ssid="2">This can be rewritten as : aTXY Tb = n?</S>
			<S sid="48" ssid="3">k=1 ?xk,a??yk,b? = n?</S>
			<S sid="49" ssid="4">k=1 ( d1?</S>
			<S sid="50" ssid="5">i=1 Xi,kai ? d2?</S>
			<S sid="51" ssid="6">j=1 Yj,kbj ) = d1?</S>
			<S sid="52" ssid="7">i=1 d2?</S>
			<S sid="53" ssid="8">j=1 aibj ( n?</S>
			<S sid="54" ssid="9">k=1 Xi,kYj,k ) = d1,d2?</S>
			<S sid="55" ssid="10">i,j=1 aibjCxyij (2) Similarly, the constraints can also be rewritten as?d1 i,j=1 aiajCxxij = 1 and ?d2 i,j=1 bibjC yy ij = 1.</S>
			<S sid="56" ssid="11">931 Maximizing this objective function, under theconstraints, involves a careful selection of the vec tors a and b such that aibj is high whenever Cxyijis high.</S>
			<S sid="57" ssid="12">So, every non-zero entry of the crosscovariance matrix restricts the choice of the pro jection directions.</S>
			<S sid="58" ssid="13">While this may not be a severe problem when the training data is clean, but this isvery uncommon especially in the case of high di mensional data like text documents.</S>
			<S sid="59" ssid="14">Moreover, the inherent ambiguity of natural languages increasesthe chances of seeing a noisy word in any docu ment.</S>
			<S sid="60" ssid="15">Every occurrence of a noisy word will have a non-zero contribution towards the covariance matrix making it dense, which in turn prevents the selection of appropriate projection directions.</S>
			<S sid="61" ssid="16">In this section, we describe some techniques to recover the sparsity by removing the noisy entries from the covariance matrices.</S>
			<S sid="62" ssid="17">We break this task into two sub problems: computing an associationscore for every word pair and then using an appro priate strategy to identify the noisy pairs based on their weights.</S>
			<S sid="63" ssid="18">We explore multiple ways to address both the steps in the following two sections.</S>
			<S sid="64" ssid="19">For the sake of convenience and clarity, we describe ourtechniques in the context of cross-covariance ma trix between English and Spanish language pair.</S>
			<S sid="65" ssid="20">Butthese techniques extend directly to monolingual co variance matrices, and to different language pairs as well.</S>
			<S sid="66" ssid="21">3.1 Computing Word Pair Association.</S>
			<S sid="67" ssid="22">The first step in filtering out the noisy word cooccurrences is to use an appropriate measure to compute the strength of word pairs (English and Spanish words).</S>
			<S sid="68" ssid="23">This is a well studied problem and sev eral association measures have been proposed in the NLP literature (Dunning, 1993; Inkpen and Hirst, 2002; Moore, 2004).</S>
			<S sid="69" ssid="24">These association measures can be divided into groups based on the statistics they use (Hoang et al, 2009).</S>
			<S sid="70" ssid="25">Here we explore a few of them for sparsifying the cross-covariance matrix.</S>
			<S sid="71" ssid="26">3.1.1 Covariance The first option is to use the cross-covariancematrix itself.</S>
			<S sid="72" ssid="27">As noted above, when the data ma trix is centered, the cross-covariance of an English word (ei) with a Spanish word (fj) is given by?n k=1 XikYjk.</S>
			<S sid="73" ssid="28">It measures the strength with whichtwo words co-occur together.</S>
			<S sid="74" ssid="29">This measure uses in formation about the occurrence of a word pair in aligned documents and doesn?t use other statisticssuch as ?how often this pair doesn?t co-occur to gether?</S>
			<S sid="75" ssid="30">and so on.</S>
			<S sid="76" ssid="31">3.1.2 Mutual InformationAssociation measures like covariance and Pointwise Mutual Information, which only use the fre quency with which a word pair co-occurs, often overestimate the strength of low frequent words (Moore, 2004).</S>
			<S sid="77" ssid="32">On the other hand, measureslike Log-likelihood ratio (Dunning, 1993) and Mu tual Information (MI) use other statistics like the marginal probabilities of each of the words.</S>
			<S sid="78" ssid="33">For any two words, ei and fj , let n11, n10, n01 and n00 denote the number of documents in which both the words co-occur, only English word occurs,only Spanish word occurs and none of the words oc cur.</S>
			<S sid="79" ssid="34">Then the Mutual Information of this word pair is given by: MI(ei, fj) = 1 n ? i,j?{0,1} nij log nij ? n ninj (3) where ni and nj denote the number of documents in which the English and the Spanish word occurs and n is the total number of documents.</S>
			<S sid="80" ssid="35">We treatthe occurrence of a word in a document slightly dif ferent from others, we treat a word as occurring in a document if it has occurred more than its average frequency in the corpus.</S>
			<S sid="81" ssid="36">Log-likelihood ratio and the MI differ only in terms of the constant they use, so we use only MI in our experiments.</S>
			<S sid="82" ssid="37">3.1.3 Yule?s ? Yule?s ? is another popular association measure used in psychology (Reis and Judd, 2000).</S>
			<S sid="83" ssid="38">It usessame statistics used by Mutual Information but differs in the way in which they are combined.</S>
			<S sid="84" ssid="39">MI converts the frequencies into probabilities before com puting the association measure where as Yule?s ? uses the observed frequencies directly, and doesn?tmake any assumptions about the underlying proba bility distributions.</S>
			<S sid="85" ssid="40">Given the same interpretation of the variables as introduced in the previous section, the Yule?s ? is estimated as: ? = ?n00n11 ? ?n01n10?n00n11 + ?n01n10 (4) 932This way of combining the frequencies bears simi larity with the log-odds ratio.</S>
			<S sid="86" ssid="41">3.1.4 Bilingual Dictionary The above three association measures use the same training data that is available to compute the covariance matrices in CCA.</S>
			<S sid="87" ssid="42">Thus, their utility inbringing additional information, which is not cap tured by the covariance matrices, is arguable (our experiments show that they are indeed helpful).</S>
			<S sid="88" ssid="43">Moreover, they use document level co-occurrenceinformation which is coarse compared to the cooccurrence at sentence level or the translational in formation provided by a bilingual dictionary.</S>
			<S sid="89" ssid="44">So, we use bilingual dictionaries as our final resource to weigh the word co-occurrences.</S>
			<S sid="90" ssid="45">Notice that, using bilingual information brings in information gleaned from an external corpus.</S>
			<S sid="91" ssid="46">We use translation tables learnt using Giza++ (Och and Ney, 2003) on Europarl data set.</S>
			<S sid="92" ssid="47">Since thetranslation tables are asymmetric, we combine trans lation tables from both the directions.</S>
			<S sid="93" ssid="48">We first use a threshold on the conditional probability to filter out the low probability ones and then convert them into joint probabilities before combining.</S>
			<S sid="94" ssid="49">For each word pair (ei, fj), we compute the score as: 1 2 ( P (ei|fj)P (fj) + P (fj|ei)P (ei) ) While the first three association measures can also be applied to monolingual data, bilingual dictionary can?t be used for weighting monolingual word pairs.</S>
			<S sid="95" ssid="50">So in this case, we use either of the above mentioned techniques for weighting monolingual word pairs.</S>
			<S sid="96" ssid="51">3.2 Selection Strategies.</S>
			<S sid="97" ssid="52">The next step after computing association measure for all word pairs is to use them in selecting the pairs that need to be retained.</S>
			<S sid="98" ssid="53">In this section, we describe some approaches such as thresholding and matching for the word pair selection.</S>
			<S sid="99" ssid="54">3.2.1 Thresholding A straight forward way to remove the noisy word co-occurrences is to zero out the entries of thecross-covariance matrix that are lower than a thresh old.</S>
			<S sid="100" ssid="55">To understand the motivation, consider the rewritten objective function of CCA, aTXY Tb = ? ij C xyij aibj . This is linear in terms of the individ ual components of the cross-covariance matrix.</S>
			<S sid="101" ssid="56">So,if we want to remove some of the entries of the co variance matrix with minimal change in the value of the objective function, then the optimal choice is to sort the entries of the covariance matrix and filter out the less confident word pairs.</S>
			<S sid="102" ssid="57">3.2.2 Relative Thresholding While the thresholding strategy described in the above section is very simple, it is often biased by the frequent words.</S>
			<S sid="103" ssid="58">Since a frequent word co-occurs with other words often, it naturally tends to have high association with most of the other words.</S>
			<S sid="104" ssid="59">As a result, absolute thresholding tends to remove allthe less frequent word pairs while leaving the cooccurrences of the frequent words untouched.</S>
			<S sid="105" ssid="60">Even tually, this may lead to zeroing out some of the rows or the columns of the cross-covariance matrix.</S>
			<S sid="106" ssid="61">To circumvent this, we try thresholding at word level.</S>
			<S sid="107" ssid="62">For every English word, we choose a few Spanish words that have high association and viceversa.</S>
			<S sid="108" ssid="63">Since the nearest neighbour property is asym metric, we take the union of all the selected word pairs.</S>
			<S sid="109" ssid="64">That is, we retain a word pair, if either the Spanish word is in the top ranked list of the English word or vice versa.</S>
			<S sid="110" ssid="65">3.2.3 Maximal MatchingThough relative thresholding overcomes the prob lem of zeroing out entire rows or columns posed by direct thresholding, it is still biased by the frequent words.</S>
			<S sid="111" ssid="66">The high association measure of a frequent English word with many Spanish words, makes it a nearest neighbour for lot of Spanish words.</S>
			<S sid="112" ssid="67">One way to prevent this is to discourage an already selected English word from associating with a new Spanish word.</S>
			<S sid="113" ssid="68">This requires a global knowledge of all the selected pairs and can not be done by looking at theindividual words, as is the case with the greedy strat egy employed by the relative thresholding.We use matching to solve this problem.</S>
			<S sid="114" ssid="69">We for mulate the selection of the word pairs as a networkflow problem (Jagarlamudi et al, 2011).</S>
			<S sid="115" ssid="70">The objec tive is to select word pairs that have high associationmeasure while constraining each word to be asso ciated with only a few words from other language.</S>
			<S sid="116" ssid="71">Let Iij denote an indicator variable taking a value of 9330 or 1 depending on if the word pair (ei, fj) is se lected or not.</S>
			<S sid="117" ssid="72">We want each word to be associated with k words from other language, i.e.?j Iij = k and ? i Iij = k. Moreover, we want word pairs with high association score to be selected.</S>
			<S sid="118" ssid="73">We canencode this objective and the constraints as the fol lowing optimization problem: argmax I d1,d2?</S>
			<S sid="119" ssid="74">i,j=1 Cxyij Iij (5) ?i ? j Iij = k; ?j ? i Iij = k; ?i, j Iij ? {0, 1}If k = 1, then this problem reduces to a linear assignment problem and can be solved optimally us ing the Hungarian algorithm (Jonker and Volgenant, 1987).</S>
			<S sid="120" ssid="75">For other values of k, this can be solved by relaxing the constraint Iij ? {0, 1} to 0 ? Iij ? 1.</S>
			<S sid="121" ssid="76">The optimal solution of the relaxed problem can befound efficiently using linear programming (Ravin dra et al, 1993).</S>
			<S sid="122" ssid="77">The uni-modular nature of theconstraints guarantees an integral solution (Schri jver, 2003), so relaxing the original integer problem doesn?t introduce any error in the optimal solution.</S>
			<S sid="123" ssid="78">3.2.4 Monolingual Augmentation The above three selection strategies operate on the covariance matrices independently.</S>
			<S sid="124" ssid="79">In this sectionwe propose to combine them.</S>
			<S sid="125" ssid="80">Specifically, we pro pose to augment the set of selected bilingual word pairs using the monolingual word pairs.</S>
			<S sid="126" ssid="81">We first useany of the above mentioned strategies to select bilin gual and monolingual word pairs.</S>
			<S sid="127" ssid="82">Let Ixy, Ixx and Iyy be the binary matrices that indicate the selected word pairs based on the bilingual and monolingualassociation scores.</S>
			<S sid="128" ssid="83">Then the monolingual augmen tation strategy updates Ixy in the following way: Ixy ? Binarize(IxxIxyIyy) i.e., we multiply Ixy with the monolingual selection matrices and then binarize the resulting matrix.</S>
			<S sid="129" ssid="84">Ourmonolingual augmentation is motivated by the fol lowing probabilistic interpretation: P (x, y) = ? x?,y? P (x|x?)P (y|y?)P (x?, y?)</S>
			<S sid="130" ssid="85">which can be rewritten as P ? T xP (T y)T where T x and T y are monolingual state transition matrices.</S>
			<S sid="131" ssid="86">3.3 Our Approach.</S>
			<S sid="132" ssid="87">In this section we summarize our approach for thetask of finding aligned documents from a crosslingual comparable corpora.</S>
			<S sid="133" ssid="88">The training phase in volves finding projection directions for documents of both the languages.</S>
			<S sid="134" ssid="89">We compute the covariance matrices using the training data.</S>
			<S sid="135" ssid="90">Then we use any of the word association measures (Sec.</S>
			<S sid="136" ssid="91">3.1) along with a selection criteria (Sec.</S>
			<S sid="137" ssid="92">3.2) to recover the sparseness in either only the cross-covariance or all of the covariance matrices.</S>
			<S sid="138" ssid="93">Let Ixy, Ixx and Iyy be the binary matrices which represent the wordpairs that are selected based on the chosen sparsi fication technique.</S>
			<S sid="139" ssid="94">Now, we replace the covariance matrices in Eq.</S>
			<S sid="140" ssid="95">1 as follows: Cxx ? Cxx ? Ixx, Cyy ? Cyy ? Iyy and Cxy ? Cxy ? Ixy where?</S>
			<S sid="141" ssid="96">denotes the element-wise matrix product.</S>
			<S sid="142" ssid="97">Subsequently, we solve the generalized eigenvalue problem shown in Eq.</S>
			<S sid="143" ssid="98">1 to obtain the projection direc tions.</S>
			<S sid="144" ssid="99">Let A and B be the matrices formed with topeigenvectors of Eq.</S>
			<S sid="145" ssid="100">1 as the columns.</S>
			<S sid="146" ssid="101">These pro jection matrices are used to map documents into theinterlingual representation.</S>
			<S sid="147" ssid="102">Such an interlingual rep resentation is useful in many tasks like cross-lingual text categorization (Bel et al, 2003) multilingual web search (Gao et al, 2009) and so on.</S>
			<S sid="148" ssid="103">During the testing, given an English document x,finding an aligned Spanish document involves solv ing: argmax y xT ( (ABT )?</S>
			<S sid="149" ssid="104">Ixy ) y ? xT ( (AAT )?</S>
			<S sid="150" ssid="105">Ixx ) x ? yT ( (BBT )?</S>
			<S sid="151" ssid="106">Iyy ) y If the documents are normalized before hand, then the above equation reduces to computing only the numerator.</S>
	</SECTION>
	<SECTION title="Experiments. " number="4">
			<S sid="152" ssid="1">4.1 Experimental Setup.</S>
			<S sid="153" ssid="2">We experiment with the task of finding aligned doc uments from a cross-lingual comparable corpora.</S>
			<S sid="154" ssid="3">Inthis task, we are given comparable corpora consisting of two document collections, each in a differ ent language.</S>
			<S sid="155" ssid="4">As the corpora are comparable, somedocuments in one collection have a comparable doc ument in the other collection.</S>
			<S sid="156" ssid="5">The task is to recover 934 this hidden alignment.</S>
			<S sid="157" ssid="6">The recovered alignment is compared against the ground truth.We evaluate our idea of sparsifying the covari ance matrices incrementally.</S>
			<S sid="158" ssid="7">We first evaluate the effectiveness of our approach on synthetic data, as it enables us to systematically study the effect of noise.</S>
			<S sid="159" ssid="8">Subsequently, we evaluate each of the above discussed sparsification strategies on real world data sets.</S>
			<S sid="160" ssid="9">We have discussed four possible ways forcomputing word association measure and three ap proaches for word pair selection.</S>
			<S sid="161" ssid="10">That leaves us 12different ways for sparsifying the covariance matri ces, with each method having parameters to control the amount of sparseness.</S>
			<S sid="162" ssid="11">We use a small amount of development data for model selection and parameter tuning and choose a few promising models.</S>
			<S sid="163" ssid="12">Finally,we compare these selected models with state-of-theart baselines on two language pairs and on two dif ferent data sets.</S>
			<S sid="164" ssid="13">In each case, we use the training data to learn the projection directions.</S>
			<S sid="165" ssid="14">And then, for each of the test documents, we find the aligned document from other language.</S>
			<S sid="166" ssid="15">We report average accuracy of the top ranked document and also the Mean Reciprocal Rank (MRR) of the true aligned document.</S>
			<S sid="167" ssid="16">4.2 Synthetic Data.</S>
			<S sid="168" ssid="17">We follow the generative story introduced in Bach and Jordan (2005) to generate synthetic multi-viewdata.</S>
			<S sid="169" ssid="18">Their method does not assume any correspon dence between the feature dimensions of both the views.</S>
			<S sid="170" ssid="19">We modify their approach slightly so thatwe know the actual correspondence between the fea tures.</S>
			<S sid="171" ssid="20">We use these true feature correspondences for sparsification of the cross-covariance matrix.</S>
			<S sid="172" ssid="21">We first generate a d dimensional vector in the common latent space and then use the projection matrices to map it into the individual feature spaces as follows: z ? N (0, Id) x|z ?</S>
			<S sid="173" ssid="22">(W1z + ?1) + ? N (0, Id1) y|z ?</S>
			<S sid="174" ssid="23">(W1z + ?2) + ? N (0, Id2) Notice that we use the same projection matrix W1for both the views, this ensures a one-to-one corre spondence between the features of both the spaces.</S>
			<S sid="175" ssid="24">Moreover, we also introduce a parameter ? which controls the amount of noise in the data.</S>
			<S sid="176" ssid="25">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 1.5 2 2.5 3 3.5 4 Sparse MRR Sparse Accuracy CCA MRR CCA Accuracy Figure 1: Accuracy of CCA and our sparsified version with the noise parameter.</S>
			<S sid="177" ssid="26">We generate a total of 3000 pairs of points and use 2000 of them for training the models and the restfor evaluation.</S>
			<S sid="178" ssid="27">We use the true feature correspondences to form the cross-covariance selection ma trix Ixy (Sec.</S>
			<S sid="179" ssid="28">3.3).</S>
			<S sid="180" ssid="29">For this experiment, we use the full monolingual covariance matrices.</S>
			<S sid="181" ssid="30">We train both CCA and our sparse version on the training data andevaluate them on the test data.</S>
			<S sid="182" ssid="31">We repeat this mul tiple times and report the average accuracies.</S>
			<S sid="183" ssid="32">Fig.</S>
			<S sid="184" ssid="33">1 shows the performance of CCA and our sparse CCA, as we vary the noise parameter ? from 1 to 4.</S>
			<S sid="185" ssid="34">Itis very clear that the sparse version performs sig nificantly better than CCA.</S>
			<S sid="186" ssid="35">As the noise increases,the performance of CCA drops quickly.</S>
			<S sid="187" ssid="36">This exper iment demonstrates a significant performance gain when the true correspondences are available.</S>
			<S sid="188" ssid="37">But this information is not available in the case of real world data sets, so we try to approximate it.</S>
			<S sid="189" ssid="38">4.3 Model Selection.</S>
			<S sid="190" ssid="39">As we have discussed, there are several choices for computing the association measure and for selecting the word pairs to be retained.</S>
			<S sid="191" ssid="40">And each of them have sparsity parameters, giving raise to many possible models.</S>
			<S sid="192" ssid="41">For model selection, we use approximately 5000 document pairs collected from the Wikipediabetween English and Spanish.</S>
			<S sid="193" ssid="42">We use the crosslanguage links provided as the ground truth.</S>
			<S sid="194" ssid="43">We to kenize the documents, retain only the most frequent2000 words in each language and convert the docu 935 0.4 0.45 0.5 0.55 0.6 0.65 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 MI+Match Yule+Match Cov+Match MI+RelThreshold Yule+RelThreshold Cov+RelThreshold MI+Threshold Yule+Threshold Cov+Threshold CCA Figure 2: Comparison of the word association measures along with different selection criteria.</S>
			<S sid="195" ssid="44">The x-axis plots the number of non-zero entries in the covariance matrices and the y-axis plots the accuracy of top-ranked document.</S>
			<S sid="196" ssid="45">ments into TFIDF vectors.</S>
			<S sid="197" ssid="46">We use 60% of the datafor training different models and the rest for evaluat ing the models.</S>
			<S sid="198" ssid="47">We choose a few promising models based on this development set results and evaluate them on bigger data sets.</S>
			<S sid="199" ssid="48">4.3.1 Selection Strategies In the first experiment, we combine the three association measures, Covariance (Cov), MI andYule?s ?, with the three selection criteria, Threshold, Relative Threshold (RelThreshold) and Match ing (Match).</S>
			<S sid="200" ssid="49">Fig.</S>
			<S sid="201" ssid="50">2 shows the performance of thesedifferent combinations with varying levels of spar sity in the covariance matrices.</S>
			<S sid="202" ssid="51">The horizontal line represents the performance of CCA on this data set.We start with 2000 non-zero entries in the covari ance matrices and experiment up to 20,000 non-zero entries.</S>
			<S sid="203" ssid="52">Since our data set has 2000 words in eachlanguage, 2000 non-zero entries in a covariance matrix implies that, on an average, every word is as sociated with only one word.</S>
			<S sid="204" ssid="53">This results in highly sparse covariance matrices.</S>
			<S sid="205" ssid="54">Overall, we observe that reducing the level of sparsity , i.e. selecting more number of elements in the covariance matrices, increases the performance slightly and then decreases again.</S>
			<S sid="206" ssid="55">From the figure, it seems that sparsifying the covariance matrices might help in improving the performance of the task.</S>
			<S sid="207" ssid="56">Butit is interesting to note that not all the models per form better than CCA.</S>
			<S sid="208" ssid="57">In fact, both the models that achieve better scores use Matching as the selection criteria.</S>
			<S sid="209" ssid="58">This suggests that, apart from the weighting of the word pairs, appropriate selection of the wordpairs is also equally important.</S>
			<S sid="210" ssid="59">In the rest of the ex periments we mainly report results with Matching as the selection criterion.</S>
			<S sid="211" ssid="60">From this figure, we observethat Mutual Information and Yule?s ? perform com petitively but they consistently outperform models that use covariance as the association measure.</S>
			<S sid="212" ssid="61">So in the rest of the experiments we report results with MI or Yule?s ?.</S>
			<S sid="213" ssid="62">4.3.2 Amount of Sparsity In the previous experiment, we used same level of sparsity for all the covariance matrices, i.e. same number of associations were selected for each word in all the three covariance matrices.</S>
			<S sid="214" ssid="63">In the following experiment, we use different levels of sparsity for the individual covariance matrices.</S>
			<S sid="215" ssid="64">Fig.</S>
			<S sid="216" ssid="65">3 shows the performance of Yule+Match and Dictionary+Match combinations with different levels of sparsity.</S>
			<S sid="217" ssid="66">Inthe Yule+Match combination, we use Yule?s ? as sociation measure for weighting the word pairs and use matching for selection.</S>
			<S sid="218" ssid="67">In the Dictionary+Matchcombination, we use bilingual dictionary for sparsi fying cross-covariance matrix, i.e. we keep all theword pairs whose conditional translation probabil ity is above a threshold.</S>
			<S sid="219" ssid="68">And for monolingual word pairs, we use MI for weighting and matching for word pair selection.</S>
			<S sid="220" ssid="69">For each level of sparsity of the cross-covariancematrix, we experiment with different levels of spar sity on the monolingual covariance matrices.</S>
			<S sid="221" ssid="70">?OnlyXY?</S>
			<S sid="222" ssid="71">indicates we use the full monolingual covari ance matrices.</S>
			<S sid="223" ssid="72">In ?Match(k)?</S>
			<S sid="224" ssid="73">runs, we allow each word to be associated with a total of k words (Eq.</S>
			<S sid="225" ssid="74">5).?Aug? indicates that we use monolingual augmen tation to refine the sparsity of the cross-covariance matrix (Sec.</S>
			<S sid="226" ssid="75">3.2.4).</S>
			<S sid="227" ssid="76">From both the figures 3(a) and 3(b), we observe that ?Only XY?</S>
			<S sid="228" ssid="77">run (dark blue) performs poorlycompared to the other runs, indicating that sparsifying all the covariance matrices is better than spar sifying only the cross-covariance matrix.</S>
			<S sid="229" ssid="78">In the 936 (a) Performance of Yule+Match combination.</S>
			<S sid="230" ssid="79">The x-axis plots the number of Spanish words selected per each English word and vice versa.</S>
			<S sid="231" ssid="80">This determines the sparsity of Cxy.</S>
			<S sid="232" ssid="81">Matching is used as selection criteria for all the covariance matrices.</S>
			<S sid="233" ssid="82">(b) Performance of Dictionary+Match combination.</S>
			<S sid="234" ssid="83">The x-axisplots the threshold on bilingual translation probability and it determines the sparsity of Cxy.</S>
			<S sid="235" ssid="84">Matching is used to select only the mono lingual sparsity.</S>
			<S sid="236" ssid="85">Figure 3: Comparison of Yule+Match and Dictionary+Match combination with different levels of sparsity for the covariance matrices.</S>
			<S sid="237" ssid="86">In both the figures, the x-axis plots the sparsity of the cross-covariance matrix and for each value we try different levels of sparsity on the monolingual covariance matrices (which are grouped together).</S>
			<S sid="238" ssid="87">The description of these individual runs is provided in the relevant parts of the text.</S>
			<S sid="239" ssid="88">The y-axis plots the accuracy of the top-ranked document.</S>
			<S sid="240" ssid="89">CCA achieves 61% accuracy on this data set.</S>
			<S sid="241" ssid="90">Yule+Match combination, Fig.</S>
			<S sid="242" ssid="91">3(a), all the runs seem to be performing better when each English word is allowed to associate with 2 or 3 Spanishwords and vice versa.</S>
			<S sid="243" ssid="92">Among different ways of se lecting the monolingual word pairs, Match(2)+Aug performs better than the remaining runs.</S>
			<S sid="244" ssid="93">So we useMatch(2)+Aug combination for the Yule?s ? mea sure.</S>
			<S sid="245" ssid="94">Unlike the Yule+Match combinations, there is no clear winner for Dictionary+Match combinations.</S>
			<S sid="246" ssid="95">First of all, the performance increase as we increasethe translation probability threshold and then decreases again (indicated by the ?Average?</S>
			<S sid="247" ssid="96">performance in Fig.</S>
			<S sid="248" ssid="97">3(b)).</S>
			<S sid="249" ssid="98">On an average, all the sys tems perform better with a threshold of 0.01, which we use in our final experiments.</S>
			<S sid="250" ssid="99">In this case, both Match(1) and Match(2)+Aug runs (orange and green bars respectively) perform competitively so we use both of these models in our final experiments.</S>
			<S sid="251" ssid="100">In both the above experiments, the performance bars are very similar when we use MI instead of Yule and vice versa for weighting monolingual word pairs.</S>
			<S sid="252" ssid="101">Thus, to illustrate the main ideas we chose Yule?s ? for the former combination and MI for the latter combination.</S>
			<S sid="253" ssid="102">4.3.3 Promising Models Based on the above experiments, we choose the following combinations for our final experiments.</S>
			<S sid="254" ssid="103">Yule(l)+Match(k), where l ? {2, 3} is the number of Spanish words allowed for each English wordand vice versa and k=2 is the number of monolin gual word associations for each word.</S>
			<S sid="255" ssid="104">We also runboth these combinations with monolingual augmen tation, indicated by Yule(l)+Match(k)+Aug. For dictionary based weighting, Dictionary+Match(k), we choose a translation probability threshold of 0.01and try k ? {1, 2}.</S>
			<S sid="256" ssid="105">Again, we run these combina tions with monolingual augmentation identified by Dictionary+Match(k)+Aug. 4.4 Results.</S>
			<S sid="257" ssid="106">For our final results, we choose data in two language pairs (English-Spanish and English-German) from two different resources, Europarl (Koehn, 2005) and Wikipedia.</S>
			<S sid="258" ssid="107">For Europarl data sets, we artificially make them comparable by considering the first half 937 Wikipedia Europarl English-Spanish English-German English-Spanish English-German Acc.</S>
			<S sid="259" ssid="108">MRR Acc.</S>
			<S sid="260" ssid="109">MRR Acc.</S>
			<S sid="261" ssid="110">MRR Acc.</S>
			<S sid="262" ssid="111">MRR CCA 0.776 0.852 0.570 0.699 0.872 0.920 0.748 0.831 OPCA 0.781 0.856 0.570 0.700 0.870 0.920 0.748 0.831 Yule(2)+Match(2) 0.798?</S>
			<S sid="263" ssid="112">0.866?</S>
			<S sid="264" ssid="113">0.576 0.703 0.901?</S>
			<S sid="265" ssid="114">0.939?</S>
			<S sid="266" ssid="115">0.780?</S>
			<S sid="267" ssid="116">0.853?</S>
			<S sid="268" ssid="117">Yule(2)+Match(2)+Aug 0.811?</S>
			<S sid="269" ssid="118">0.876?</S>
			<S sid="270" ssid="119">0.602?</S>
			<S sid="271" ssid="120">0.723?</S>
			<S sid="272" ssid="121">0.883 0.927 0.771?</S>
			<S sid="273" ssid="122">0.847?</S>
			<S sid="274" ssid="123">Yule(3)+Match(2) 0.803?</S>
			<S sid="275" ssid="124">0.870?</S>
			<S sid="276" ssid="125">0.572 0.700 0.856 0.907 0.747 0.830 Yule(3)+Match(2)+Aug 0.793?</S>
			<S sid="277" ssid="126">0.861?</S>
			<S sid="278" ssid="127">0.610?</S>
			<S sid="279" ssid="128">0.726?</S>
			<S sid="280" ssid="129">0.878+ 0.925+ 0.763+ 0.843?</S>
			<S sid="281" ssid="130">Dictionary+Match(1) 0.811?</S>
			<S sid="282" ssid="131">0.875?</S>
			<S sid="283" ssid="132">0.656?</S>
			<S sid="284" ssid="133">0.762?</S>
			<S sid="285" ssid="134">0.928?</S>
			<S sid="286" ssid="135">0.957?</S>
			<S sid="287" ssid="136">0.874?</S>
			<S sid="288" ssid="137">0.922?</S>
			<S sid="289" ssid="138">Dictionary+Match(2) 0.811?</S>
			<S sid="290" ssid="139">0.876?</S>
			<S sid="291" ssid="140">0.623?</S>
			<S sid="292" ssid="141">0.736?</S>
			<S sid="293" ssid="142">0.923?</S>
			<S sid="294" ssid="143">0.955?</S>
			<S sid="295" ssid="144">0.853?</S>
			<S sid="296" ssid="145">0.907?</S>
			<S sid="297" ssid="146">Dictionary+Match(2)+Aug 0.825?</S>
			<S sid="298" ssid="147">0.885?</S>
			<S sid="299" ssid="148">0.630?</S>
			<S sid="300" ssid="149">0.735?</S>
			<S sid="301" ssid="150">0.897?</S>
			<S sid="302" ssid="151">0.935?</S>
			<S sid="303" ssid="152">0.866?</S>
			<S sid="304" ssid="153">0.917?</S>
			<S sid="305" ssid="154">Table 1: Performance of our models in comparison with CCA and OPCA on English-Spanish and English-German language pairs.</S>
			<S sid="306" ssid="155">and + indicate statistical significance measured by paired t-test at p=0.01 and 0.05 levels respectively.</S>
			<S sid="307" ssid="156">When an improvement is significant at p=0.01 it is automatically significant at p=0.05 and hence is not shown.</S>
			<S sid="308" ssid="157">of English document and the second half of its aligned foreign language document (Mimno et al,2009).</S>
			<S sid="309" ssid="158">For Wikipedia data set, we use the cross language link as the ground truth.</S>
			<S sid="310" ssid="159">For each of these data sets, we choose approximately 5000 aligned document pairs.</S>
			<S sid="311" ssid="160">We remove the stop words and keep all the words that occur in at least five documents.</S>
			<S sid="312" ssid="161">After the preprocessing, on an average, we are left with 4700 words in each language.</S>
			<S sid="313" ssid="162">Subsequently weconvert the documents into their TFIDF representa tion.In Platt et al (2010), the authors compare differ ent systems on the comparable document retrieval task and show that discriminative approaches work better compared to their generative counter parts.So, here we compare only with the state-of-the art discriminative systems such as CCA and OPCA(Platt et al, 2010).</S>
			<S sid="314" ssid="163">For each of the systems, we re port the average results of five-fold cross validation.We divide the data into 3:1:1 ratio for training, vali dation and test sets.</S>
			<S sid="315" ssid="164">The validation data set is used to select the best number of dimensions of the common sub space.</S>
			<S sid="316" ssid="165">For both CCA and our models, we set the regularization parameter ? to 0.3 which we found works well in a relevant but different experiments.For OPCA, we manually tried different regulariza tion parameters ranging from 0.0001 to 1 and found that a value of 0.001 worked best.</S>
			<S sid="317" ssid="166">The results are shown in Table 1.</S>
			<S sid="318" ssid="167">On these data sets, both CCA and OPCA performed competitively.</S>
			<S sid="319" ssid="168">OPCA takes advantage of the common vocabularyin both the languages.</S>
			<S sid="320" ssid="169">But in our data sets, vocab ulary of both the languages is treated differently, so it is not surprising that they give almost the sameresults.</S>
			<S sid="321" ssid="170">From the results, it is clear that sparsifying the covariance matrices helps improving the ac curacies significantly.</S>
			<S sid="322" ssid="171">In all the four data sets, the best performing method always used dictionary for cross-lingual sparsity selection.</S>
			<S sid="323" ssid="172">This indicates that using fine granular information such as a bilingual dictionary gleaned from an external source is very helpful in improving the accuracies.</S>
			<S sid="324" ssid="173">Among the models that rely solely on the training data, modelsthat use monolingual augmentation performed bet ter on Wikipedia data set, while models that do not use augmentation performed better on Europarl data sets.</S>
			<S sid="325" ssid="174">This suggests that, when the aligned documentsare clean (closer to being parallel) the statistics com puted from cross-lingual corpora are trustworthy.</S>
			<S sid="326" ssid="175">As the documents become comparable, we need to usemonolingual statistics to refine the bilingual statis tics.</S>
			<S sid="327" ssid="176">Moreover, these models achieve higher gains in the case of Wikipedia data set compared to the gains in Europarl.</S>
			<S sid="328" ssid="177">This conforms with our initial hunch that, when the training data is clean the covariance matrices tend to be less noisy.</S>
	</SECTION>
	<SECTION title="Discussion. " number="5">
			<S sid="329" ssid="1">In this paper, we have proposed the idea of sparsifyng covariance matrices to improve bilingual pro 938 jection directions.</S>
			<S sid="330" ssid="2">We are not aware of any NLP research that attempts to recover the sparseness of the covariance matrices to improve the projection directions.</S>
			<S sid="331" ssid="3">Our work is different from the sparse CCA (Hardoon and Shawe-Taylor, 2011; Rai and Daume?</S>
			<S sid="332" ssid="4">III, 2009) proposed in the Machine Learningliterature.</S>
			<S sid="333" ssid="5">Their objective is to find projection directions such that the original documents are repre sented as a sparse vectors in the common sub-space.</S>
			<S sid="334" ssid="6">Another seemingly relevant but different direction is the sparse covariance matrix selection research (Banerjee et al, 2005).</S>
			<S sid="335" ssid="7">The objective in this workis to find matrices such that the inverse of the co variance matrix is sparse which has applications in Gaussian processes.In this paper, we tried sparsification in the con text of CCA only but our technique is general andcan be applied to its variants like OPCA.</S>
			<S sid="336" ssid="8">Our experimental results show that using external informa tion such as bilingual dictionaries which is gleanedfrom cleaner resources brings significant improve ments.</S>
			<S sid="337" ssid="9">Moreover, we also observe that computingword pair association measures from the same train ing data along with an appropriate selection criteriacan also yield significant improvements.</S>
			<S sid="338" ssid="10">This is cer tainly encouraging and in future we would like to explore more sophisticated techniques to recover the sparsity based on the training data itself.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number="6">
			<S sid="339" ssid="1">We thank the anonymous reviewers for their help ful comments.</S>
			<S sid="340" ssid="2">This material is partially supported by the National Science Foundation under Grant No. 1139909.</S>
	</SECTION>
</PAPER>
