<PAPER>
  <S sid="0">DCU-Symantec Submission for the WMT 2012 Quality Estimation Task</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper describes the features and the machine learning methods used by Dublin City (DCU) and the WMT 2012 quality estimation task.</S>
    <S sid="2" ssid="2">Two sets features are proposed: one i.e. respecting the data limitation suggested by the organisers, and one i.e. using data or tools trained on data that was not provided by the workshop organisers.</S>
    <S sid="3" ssid="3">In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data.</S>
    <S sid="4" ssid="4">In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers.</S>
    <S sid="5" ssid="5">We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">For the first time, the WMT organisers this year propose a Quality Estimation (QE) shared task, which is divided into two sub-tasks: scoring and ranking automatic translations.</S>
    <S sid="7" ssid="2">The aim of this workshop is to define useful sets of features and machine learning techniques in order to predict the quality of a machine translation (MT) output T (Spanish) given a source segment 5 (English).</S>
    <S sid="8" ssid="3">Quality is measured using a 5-point likert scale which is based on postediting effort, following the scoring scheme: The final score is a combination of the scores assigned by three evaluators.</S>
    <S sid="9" ssid="4">The use of a 5-point scale makes the scoring task more difficult than a binary classification task where a translation is considered to be either good or bad.</S>
    <S sid="10" ssid="5">However, if the task is successfully carried out, the score produced is more useful.</S>
    <S sid="11" ssid="6">Dublin City University and Symantec jointly address the scoring task.</S>
    <S sid="12" ssid="7">For each pair (5, T) of source segment 5 and machine translation T, we train three classifiers and one classifier combination using the training data provided by the organisers to predict 5-point Likert scores.</S>
    <S sid="13" ssid="8">In this paper, we present the classification results on the test set along with additional results obtained using regression techniques.</S>
    <S sid="14" ssid="9">We evaluate the usefulness of two new sets of features: The remainder of this paper is organised as follows.</S>
    <S sid="15" ssid="10">In Section 2, we give an overview of all the features employed in our QE system.</S>
    <S sid="16" ssid="11">Then, in Section 3, we describe the topic and syntax-based features in more detail.</S>
    <S sid="17" ssid="12">Section 4 presents the various classification and regression techniques we explored.</S>
    <S sid="18" ssid="13">Our results are presented and discussed in Section 5.</S>
    <S sid="19" ssid="14">Finally, we summarise and outline our plans in Section 6.</S>
  </SECTION>
  <SECTION title="2 Features Overview" number="2">
    <S sid="20" ssid="1">In this section, we describe the features used in our QE system.</S>
    <S sid="21" ssid="2">In the first subsection, the features included in our constrained system are presented.</S>
    <S sid="22" ssid="3">In the second subsection, we detail the features included in our unconstrained system.</S>
    <S sid="23" ssid="4">Both of these systems include the 17 baseline features provided for the shared task.</S>
    <S sid="24" ssid="5">The constrained system is based only on the data provided by the organisers.</S>
    <S sid="25" ssid="6">We extracted 70 features in total (including the baseline features) and we present them here according to the type of information they capture.</S>
    <S sid="26" ssid="7">Word and Phrase-Level Features All the language models (LMs) used in our work are n-gram LMs with Kneser-Ney smoothing built with the SRI Toolkit (Stolcke, 2002).</S>
    <S sid="27" ssid="8">More details about these two features are provided in Section 3.1.</S>
    <S sid="28" ssid="9">In addition to the features used for the constrained system, a further 238 unconstrained features were included in our unconstrained system.</S>
  </SECTION>
  <SECTION title="MT System Features" number="3">
    <S sid="29" ssid="1">As for our constrained system, we use MT output back-translation from Spanish to English, but this time using Bing Translator' in addition to Moses.</S>
    <S sid="30" ssid="2">Each back-translated segment is scored with TER, BLEU and the Levenshtein distance, based on the source segments as a translation reference.</S>
    <S sid="31" ssid="3">Wagner et al. (2007; 2009) propose a series of features to measure sentence grammaticality.</S>
    <S sid="32" ssid="4">These features rely on a part-of-speech tagger, a probabilistic parser and a precision grammar/parser.</S>
    <S sid="33" ssid="5">We have at our disposal these tools for English and so we apply them to the source data.</S>
    <S sid="34" ssid="6">The features themselves are described in more detail in Section 3.2.</S>
    <S sid="35" ssid="7">We use a part-of-speech tagger trained on Spanish to extract from the target data the subset of grammaticality features proposed by Wagner et al. (2007; 2009) that are based on POS n-grams.</S>
    <S sid="36" ssid="8">In addition we extract features which reflect the prevalence of particular POS tags in each target segment.</S>
    <S sid="37" ssid="9">These are explained in more detail in Section 3.2 below.</S>
    <S sid="38" ssid="10">LANGUAGETOOL (based on (Naber, 2003)) is an open-source grammar and style proofreading tool that finds errors based on pre-defined, languagespecific rules.</S>
    <S sid="39" ssid="11">The latest version of the tool can be run in server mode, so individual sentences can be checked and assigned a total number of errors (which may or may not be true positives).2 This number is used as a feature for each source segment and its corresponding MT output.</S>
  </SECTION>
  <SECTION title="3 Topic and Syntax-based Features" number="4">
    <S sid="40" ssid="1">In this section, we focus on the set of features that aim to capture adequacy using topic modelling and grammaticality using POS tagging and syntactic parsing.</S>
    <S sid="41" ssid="2">We extract source and target features based on a topic model built using LDA.</S>
    <S sid="42" ssid="3">The main idea in topic modelling is to produce a set of thematic word clusters from a collection of documents.</S>
    <S sid="43" ssid="4">Using the parallel corpus provided for the task, a bilingual corpus is built where each line is composed of a source segment and its translation separated by a space.</S>
    <S sid="44" ssid="5">Each pair of segments is considered as a bilingual document.</S>
    <S sid="45" ssid="6">This corpus is used to train a bilingual topic model after stopwords removal.</S>
    <S sid="46" ssid="7">The resulting model is one set of bilingual topics z containing words w with a probability p(wn|zn, Q) (with n equal to the vocabulary size in the whole parallel corpus).</S>
    <S sid="47" ssid="8">This model can be used to infer the probability distribution of unseen source and target segments over bilingual topics.</S>
    <S sid="48" ssid="9">During the test step, each source segment and its translation are considered individually, as two monolingual documents.</S>
    <S sid="49" ssid="10">This method allows us to compare the source and target topic distributions.</S>
    <S sid="50" ssid="11">We assume that a source segment and its translation share topic similarities.</S>
    <S sid="51" ssid="12">We propose two ways of using topic-based features for quality estimation: keeping source and target topic vectors as two sets of k features, or computing a vector distance between these two vectors and using one feature only.</S>
    <S sid="52" ssid="13">To measure the proximity of two vectors, we decided to used the Cosine distance, as it leads to the best results in terms of classification accuracy.</S>
    <S sid="53" ssid="14">However, we plan to study different metrics in further experiments, like the Manhattan or the Euclidean distances.</S>
    <S sid="54" ssid="15">Some parameters related to LDA have to be studied more carefully too, such as the number of topics (dimensions in the topic space), the number of words per topic, the Dirichlet hyperparameter &#945;, etc.</S>
    <S sid="55" ssid="16">In our experiments, we built a topic model composed of 10 dimensions using Gibbs sampling with 1000 iterations.</S>
    <S sid="56" ssid="17">We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics.</S>
    <S sid="57" ssid="18">Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009).</S>
    <S sid="58" ssid="19">In the field of machine translation, Tam et al. (2007) propose to adapt a translation and a language model to a specific topic using Latent Semantic Analysis (LSA, or Latent Semantic Indexing, LSI (Deerwester et al., 1990)).</S>
    <S sid="59" ssid="20">More recently, some studies were conducted on the use of LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011).</S>
    <S sid="60" ssid="21">Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation.</S>
    <S sid="61" ssid="22">Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures.</S>
    <S sid="62" ssid="23">Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output.</S>
    <S sid="63" ssid="24">Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length.</S>
    <S sid="64" ssid="25">Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation.</S>
    <S sid="65" ssid="26">Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus.</S>
    <S sid="66" ssid="27">Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation.</S>
    <S sid="67" ssid="28">Owczarzak et al. (2007) use labelled dependencies together with WordNet to avoid penalising valid syntactic and lexical variations in MT evaluation.</S>
    <S sid="68" ssid="29">In what follows, we describe how we make use of syntactic information in the QE task, i.e. evaluating MT output without a reference translation.</S>
    <S sid="69" ssid="30">Wagner et al. (2007; 2009) use three sources of linguistic information in order to extract features which they use to judge the grammaticality of English sentences: coverage precision grammar of English (Butt et al., 2002) and a Lexical Functional Grammar parser (Maxwell and Kaplan, 1996).</S>
    <S sid="70" ssid="31">These include whether or not a sentence could be parsed without resorting to robustness measures, the number of analyses found and the parsing time.</S>
    <S sid="71" ssid="32">3.</S>
    <S sid="72" ssid="33">Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al., 1993), one trained on a distorted version of the treebank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions.</S>
    <S sid="73" ssid="34">These features were originally designed to distinguish grammatical sentences from ungrammatical ones and were tested on sentences from learner corpora by Wagner et al. (2009) and Wagner (2012).</S>
    <S sid="74" ssid="35">In this work we extract all three sets of features from the source side of our data and the POS-based subset from the target side.3 We use the publicly available pre-trained TreeTagger models for English and Spanish4.</S>
    <S sid="75" ssid="36">The reference corpus used to obtain POS n-gram frequences is the MT translation model training data.5 In addition to the POS-based features described in Wagner et al. (2007; 2009), we also extract the following features from the Spanish POS-tagged data: for each POS tag P and target segment T, we extract a feature which is the proportion of words in T that are tagged as P. Two additional features are extracted to represent the proportion of words in T that are assigned more than one tag by the tagger, 3Unfortunately, due to time constraints, we were unable to source a suitable probabilistic phrase-structure parser and a precision grammar for Spanish and were thus unable to extract parser-based features for Spanish.</S>
    <S sid="76" ssid="37">We expect that these features would be more useful on the target side than the source side. and the proportion of words in T that are unknown to the tagger.</S>
  </SECTION>
  <SECTION title="4 Machine Learning" number="5">
    <S sid="77" ssid="1">In this section, we describe the machine learning methods that we experimented with.</S>
    <S sid="78" ssid="2">Our final systems submitted for the shared task are based on classification methods.</S>
    <S sid="79" ssid="3">However, we also performed some experiments with regression methods.</S>
    <S sid="80" ssid="4">We evaluate the systems on the test set using the official evaluation script and the reference scores.</S>
    <S sid="81" ssid="5">We report the evaluation results as Mean Average Error (MAE) and Root Mean Squared Error (RMSE).</S>
    <S sid="82" ssid="6">In order to apply classification algorithms to the set of features associated with each source and target segment, we rounded the training data scores to the closest integer.</S>
    <S sid="83" ssid="7">We tested several classifiers and empirically chose three algorithms: Support Vector Machine using sequential minimal optimization and RBF kernel (parameters optimized by gridsearch) (Platt, 1999), Naive Bayes (John and Langley, 1995) and Random Forest (Breiman, 2001) (the latter two techniques were applied with default parameters).</S>
    <S sid="84" ssid="8">We use the Weka toolkit (Hall et al., 2009) to train the classifiers and predict the scores on the test set.</S>
    <S sid="85" ssid="9">Each method is evaluated individually and then combined by averaging the predicted scores.</S>
    <S sid="86" ssid="10">We applied three different regression techniques: SVM epsilon-SVR with RBF kernel, Linear Regression and M5P (Quinlan, 1992; Wang and Witten, 1997).</S>
    <S sid="87" ssid="11">The two latter algorithms were used with default parameters, whereas SVM parameters ('y, c and E) were optimized by grid-search.</S>
    <S sid="88" ssid="12">We also performed a combination of the three algorithms by averaging the predicted scores.</S>
    <S sid="89" ssid="13">We apply a linear function on the predicted scores S in order to keep them in the correct range (from 1 to 5) as detailed in (1), where S' is the rescaled sentence score, Smin is the lowest predicted score and Smax is the highest predicted score.</S>
    <S sid="90" ssid="14">Smax &#8722; Smin</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="6">
    <S sid="91" ssid="1">Table 1 shows the results obtained by our classification approach on various feature subsets.</S>
    <S sid="92" ssid="2">Note that the two submitted systems used the combined classifier approach with the constrained and unconstrained feature sets.</S>
    <S sid="93" ssid="3">Table 2 shows the results for the same feature combinations, this time using regression rather than classification.</S>
    <S sid="94" ssid="4">The results of quality estimation using classification methods show that the baseline and the syntaxbased features with the classifier combination leads to the best results with an MAE of 0.71 and an RMSE of 0.87.</S>
    <S sid="95" ssid="5">However, these scores are substantially lower than the ones obtained using regression, where the unconstrained set of features with SVM leads to an MAE of 0.62 and an RMSE of 0.78.</S>
    <S sid="96" ssid="6">It seems that the classification methods are not suitable for this task according to the different sets of features studied.</S>
    <S sid="97" ssid="7">Furthermore, the topic-distance feature is not correlated with the quality scores, according to the regression results.</S>
    <S sid="98" ssid="8">On the other hand, the syntax-based features appear to be the most informative and lead to an MAE of 0.70.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="7">
    <S sid="99" ssid="1">We presented in this paper our submission for the WMT12 Quality Estimation shared task.</S>
    <S sid="100" ssid="2">We also presented further experiments using different machine learning techniques and we evaluated the impact of two sets of features - one set which is based on linguistic features extracted using POS tagging and parsing, and a second set which is based on topic modelling.</S>
    <S sid="101" ssid="3">The best results are obtained by our unconstrained system containing all features and using an E-SVR regression method with a Radial Basis Function kernel.</S>
    <S sid="102" ssid="4">This setup leads to a Mean Average Error of 0.62 and a Root Mean Squared Error of 0.78.</S>
    <S sid="103" ssid="5">Unfortunately, we did not submit our best configuration for the shared task.</S>
    <S sid="104" ssid="6">We plan to continue working on the task of machine translation quality estimation.</S>
    <S sid="105" ssid="7">Our immediate next steps are to continue to investigate the contribution of individual features, to explore feature selection in a more detailed fashion and to apply our best system to other types of data including sentences taken from an online discussion forum.</S>
  </SECTION>
</PAPER>
