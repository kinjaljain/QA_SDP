<PAPER>
  <S sid="0">Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training.</S>
    <S sid="2" ssid="2">During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language&#8217;s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM).</S>
    <S sid="3" ssid="3">We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task.</S>
    <S sid="4" ssid="4">Our topic modeling approach is simpler to construct than its counterparts.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Adaptation is usually applied to reduce the performance drop of Statistical Machine Translation (SMT) systems when translating documents that deviate from training and tuning conditions.</S>
    <S sid="6" ssid="2">In this paper, we focus primarily on language model (LM) adaptation.</S>
    <S sid="7" ssid="3">In SMT, LMs are used to promote fluent translations.</S>
    <S sid="8" ssid="4">As probabilistic models of sequences of words, language models guide the selection and ordering of phrases in translation.</S>
    <S sid="9" ssid="5">With respect to *This work was carried out during an internship period at Fondazione Bruno Kessler.</S>
    <S sid="10" ssid="6">LM training, LM adaptation for SMT tries to improve an existing LM by using smaller amounts of texts.</S>
    <S sid="11" ssid="7">When adaptation data represents the translation task domain one generally refers to domain adaptation, while when they just represent the content of the single document to be translated one typically refers to topic adaptation.</S>
    <S sid="12" ssid="8">We propose a cross-language topic adaptation method, enabling the adaptation of a LM based on the topic distribution of the source document during translation.</S>
    <S sid="13" ssid="9">We train a latent semantic topic model on a collection of bilingual documents, in which each document contains both the source and target language.</S>
    <S sid="14" ssid="10">During inference, a latent topic distribution of words across both the source and target languages is inferred from a source document to be translated.</S>
    <S sid="15" ssid="11">After inference, we remove all source language words from the topic-word distributions and construct a unigram language model which is used to adapt our background LM via Minimum Discrimination Information (MDI) estimation (Federico, 1999, 2002; Kneser et al., 1997).</S>
    <S sid="16" ssid="12">We organize the paper as follows: In Section 2, we discuss relevant previous work.</S>
    <S sid="17" ssid="13">In Section 3, we review topic modeling.</S>
    <S sid="18" ssid="14">In Section 4, we review MDI adaptation.</S>
    <S sid="19" ssid="15">In Section 5, we describe our new bilingual topic modeling based adaptation technique.</S>
    <S sid="20" ssid="16">In Section 6, we report adaptation experiments, followed by conclusions and future work in Section 7.</S>
  </SECTION>
  <SECTION title="2 Previous work" number="2">
    <S sid="21" ssid="1">Zhao et al. (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model.</S>
    <S sid="22" ssid="2">In Sethy et al. (2006), domain-specific language models are obtained by including only the sentences that are similar to the ones in the target domain via a relative entropy based criterion.</S>
    <S sid="23" ssid="3">Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation.</S>
    <S sid="24" ssid="4">Foster and Kuhn (2007) use a mixture model approach that involves splitting a training corpus into different components, training separate models on each component, and applying mixture weights as a function of the distances of each component to the source text.</S>
    <S sid="25" ssid="5">Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and outof-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.</S>
    <S sid="26" ssid="6">Although the application of mixture models yields significant results, the number of mixture weights to learn grows linearly with the number of independent language models applied.</S>
    <S sid="27" ssid="7">Most works focus on monolingual language model adaptation in the context of automatic speech recognition.</S>
    <S sid="28" ssid="8">Federico (2002) combines Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) for topic modeling with the minimum discrimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER).</S>
    <S sid="29" ssid="9">Latent Dirichlet Allocation (LDA) techniques have been proposed as an alternative to PLSA to construct purely generative models.</S>
    <S sid="30" ssid="10">LDA techniques include variational Bayes (Blei et al., 2003) and HMM-LDA (Hsu and Glass, 2006).</S>
    <S sid="31" ssid="11">Recently, bilingual approaches to topic modeling have also been proposed.</S>
    <S sid="32" ssid="12">A Hidden Markov Bilingual Topic AdMixture (HM-BiTAM) model is proposed by Zhao and Xing (2008), which constructs a generative model in which words from a target language are sampled from a mixture of topics drawn from a Dirichlet distribution.</S>
    <S sid="33" ssid="13">Foreign words are sampled via alignment links from a first-order Markov process and a topic specific translation lexicon.</S>
    <S sid="34" ssid="14">While HM-BiTAM has been used for bilingual topic extraction and topic-specific lexicon mapping in the context of SMT, Zhao and Xing (2008) note that HM-BiTAM can generate unigram language models for both the source and target language and thus can be used for language model adaptation through MDI in a similar manner as outlined in Federico (2002).</S>
    <S sid="35" ssid="15">Another bilingual LSA approach is proposed by Tam et al. (2007), which consists of two hierarchical LDA models, constructed from parallel document corpora.</S>
    <S sid="36" ssid="16">A one-to-one correspondence between LDA models is enforced by learning the hyperparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by fixing the hyperparameters.</S>
    <S sid="37" ssid="17">The technique is based on the assumption that the topic distributions of the source and target documents are identical.</S>
    <S sid="38" ssid="18">It is shown by Tam et al. (2007) that the bilingual LSA framework is also capable of adapting the translation model.</S>
    <S sid="39" ssid="19">Their work is extended in Tam and Schultz (2009) by constructing parallel document clusters formed by monolingual documents using M parallel seed documents.</S>
    <S sid="40" ssid="20">Additionally, Gong et al. (2010) propose translation model adaptation via a monolingual LDA training.</S>
    <S sid="41" ssid="21">A monolingual LDA model is trained from either the source or target side of the training corpus and each phrase pair is assigned a phrase-topic distribution based on: where Mj is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles).</S>
    <S sid="42" ssid="22">Documents are grouped into tuples w = (w1, , wL) for each language l = &#65533;, &#65533;&#65533;&#65533;, L. Each document wl in tuple w is assumed to have the same topic distribution, drawn from an asymmetric Dirichlet prior.</S>
    <S sid="43" ssid="23">Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters ,(jl.</S>
    <S sid="44" ssid="24">Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora.</S>
  </SECTION>
  <SECTION title="3 Topic Modeling" number="3">
    <S sid="45" ssid="1">The original idea of LSA is to map documents to a latent semantic space, which reduces the dimensionality by means of singular value decomposition (Deerwester et al., 1990).</S>
    <S sid="46" ssid="2">A word-document matrix A is decomposed by the formula A = UEVt, where U and V are orthogonal matrices with unit-length columns and E is a diagonal matrix containing the singular values of A. LSA approximates E by casting all but the largest k singular values in E to zero.</S>
    <S sid="47" ssid="3">PLSA is a statistical model based on the likelihood principle that incorporates mixing proportions of latent class variables (or topics) for each observation.</S>
    <S sid="48" ssid="4">In the context of topic modeling, the latent class variables z &#8712; Z = {z1, ..., zk} correspond to topics, from which we can derive probabilistic distributions of words w &#8712; W = {w1, ..., wm} in a document d &#8712; D = {d1, ..., dn} with k &lt;&lt; n. Thus, the goal is to learn P(z  |d) and P(w|z) by maximizing the log-likelihood function: (4) where Oz,d' = P(z  |d').</S>
    <S sid="49" ssid="5">(4) can be maximized by performing Expectation Maximization on document d' by keeping fixed the word-topic distributions already estimated on the training data.</S>
    <S sid="50" ssid="6">Consequently, a word-document distribution can be inferred by applying the mixture model (3) (see Federico, 2002 for details).</S>
  </SECTION>
  <SECTION title="4 MDI Adaptation" number="4">
    <S sid="51" ssid="1">An n-gram language model approximates the probability of a sequence of words in a text W1T = w1, ..., wT drawn from a vocabulary V by the following equation: L(W, D) = wEW 1 words preceding wi.</S>
    <S sid="52" ssid="2">Given a training corpus B, dED we can compute the probability of a n-gram from a smoothed model via interpolation as: where n(w, d) is the term frequency of w in d. Using Bayes&#8217; formula, the conditional probability Using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), we estimate the parameters P(z|d) and P(w|z) via an iterative process that alternates two steps: (i) an expectation step (E) in which posterior probabilities are computed for each latent topic z; and (ii) a maximization (M) step, in which the parameters are updated for the posterior probabilities computed in the previous E-step.</S>
    <S sid="53" ssid="3">Details of how to efficiently implement the re-estimation formulas can be found in Federico (2002).</S>
    <S sid="54" ssid="4">Iterating the E- and M-steps will lead to a convergence that approximates the maximum likelihood equation in (2).</S>
    <S sid="55" ssid="5">A document-topic distribution B can be inferred on a new document d' by maximizing the following where fB(w|h) is the discounted frequency of sequence hw, h' is the lower order history, where |h|&#8722;1 = |h'|, and AB(h) is the zero-frequency probability of h, defined as: Federico (1999) has shown that MDI Adaptation is useful to adapt a background language model with a small adaptation text sample A, by assuming to have only sufficient statistics on unigrams.</S>
    <S sid="56" ssid="6">Thus, we can reliably estimate PA(w) constraints on the marginal distribution of an adapted language model PA(h, w) which minimizes the KullbackLeibler distance from B, i.e.</S>
    <S sid="57" ssid="7">: The joint distribution in (7) can be computed using Generalized Iterative Scaling (Darroch and Ratcliff, 1972).</S>
    <S sid="58" ssid="8">Under the unigram constraints, the GIS algorithm reduces to the closed form: PA(h, w) = PB(h, w)&#945;(w), (8) where In order to estimate the conditional distribution of the adapted LM, we rewrite (8) and simplify the equation to: The adaptation model can be improved by smoothing the scaling factor in (9) by an exponential where 0 &lt; &#947; &lt; 1.</S>
    <S sid="59" ssid="9">Empirically, &#947; values less than one decrease the effect of the adaptation ratio to reduce the bias.</S>
    <S sid="60" ssid="10">As outlined in Federico (2002), the adapted language model can also be written in an interpolation which permits to efficiently compute the normalization term for high order n-grams recursively and by just summing over observed n-grams.</S>
    <S sid="61" ssid="11">The recursion ends with the following initial values for the empty history E: MDI adaptation is one of the adaptation methods provided by the IRSTLM toolkit and was applied as explained in the following section.</S>
  </SECTION>
  <SECTION title="5 Bilingual Latent Semantic Models" number="5">
    <S sid="62" ssid="1">Similar to the treatment of documents in HMBiTAM (Zhao and Xing, 2008), we combine parallel texts into a document-pair (E, F) containing n parallel sentence pairs (ei, fi), 1 &lt; i &lt; n, corresponding to the source and target languages, respectively.</S>
    <S sid="63" ssid="2">Based on the assumption that the topics in a parallel text share the same semantic meanings across languages, the topics are sampled from the same topicdocument distribution.</S>
    <S sid="64" ssid="3">We make the additional assumption that stop-words and punctuation, although having high word frequencies in documents, will generally have a uniform topic distribution across documents; therefore, it is not necessary to remove them prior to model training, as they will not adversely affect the overall topic distribution in each document.</S>
    <S sid="65" ssid="4">In order to ensure the uniqueness between word tokens between languages, we annotate E with special characters.</S>
    <S sid="66" ssid="5">We perform PLSA training, as described in Section 3.1 and receive wordtopic distributions P(w|z), w E VE U VF Given an untranslated text &#710;E, we split E&#710; into a sequence of documents D. For each document di E D, we infer a full word-document distribution by learning &#952;&#710; via (4).</S>
    <S sid="67" ssid="6">Via (3), we can generate the full word-document distribution P(w  |d) for w E VF.</S>
    <S sid="68" ssid="7">We then convert the word-document probabilities into pseudo-counts via a scaling function: where &#916; is a scaling factor to raise the probability ratios above 1.</S>
    <S sid="69" ssid="8">Since our goal is to generate a unigram language model on the target language for adaptation, we remove the source words generated in (17) prior to building the language model.</S>
    <S sid="70" ssid="9">From our newly generated unigram language model, we perform MDI adaptation on the background LM to yield an adapted LM for translating the source document used for the PLSA inference step.</S>
  </SECTION>
  <SECTION title="6 Experiments" number="6">
    <S sid="71" ssid="1">Our experiments were done using the TED Talks collection, used in the IWSLT 2010 evaluation task1. we didn &#8217;t have money, so we had a cheap, little ad , but we wanted college students for a study of prison life .</S>
    <S sid="72" ssid="2">75 people volunteered , took personality tests . we did interviews . picked two dozen : the most normal , the most healthy .</S>
    <S sid="73" ssid="3">We perform MDI adaptation with each of the unigram language models to update the background TED language model.</S>
    <S sid="74" ssid="4">We configure the adaptation rate parameter y to 0.3, as recommended in Federico (2002).</S>
    <S sid="75" ssid="5">The baseline LM is replaced with each adapted LM, corresponding to the document to be translated.</S>
    <S sid="76" ssid="6">We then calculate the mean perplexity of the adapted LMs and the baseline, respectively.</S>
    <S sid="77" ssid="7">The perplexity scores are shown in Table 2.</S>
    <S sid="78" ssid="8">We observe a 15.3% relative improvement in perplexity score over the baseline.</S>
    <S sid="79" ssid="9">We perform MT experiments on the IWSLT 2010 evaluation set to compare the baseline and adapted LMs.</S>
    <S sid="80" ssid="10">In the evaluation, we notice a 0.85 improvement in BLEU (%), yielding a 3% improvement over the baseline.</S>
    <S sid="81" ssid="11">The same performance trend in NIST is observed with a 2.4% relative improvement compared to the unadapted baseline.</S>
    <S sid="82" ssid="12">Our PLSA and MDI-based adaptation method not only improves fluency but also improves adequacy: the topicbased adaptation approach is attempting to suggest more appropriate words based on increased unigram probabilities than that of the baseline LM.</S>
    <S sid="83" ssid="13">Table 3 demonstrates a large improvement in unigram selection for the adapted TED model in terms of the individual contribution to the NIST score, with diminishing effects on larger n-grams.</S>
    <S sid="84" ssid="14">The majority of the overall improvements are on individual word selection.</S>
    <S sid="85" ssid="15">Examples of improved fluency and adequacy are shown in Figure 3.</S>
    <S sid="86" ssid="16">Line 285 shows an example of a translation that doesn&#8217;t provide much of an n-gram improvement, but demonstrates more fluent output, due to the deletion of the first comma and the movement of the second comma to the end of the clause.</S>
    <S sid="87" ssid="17">While &#8220;installation&#8221; remains an inadequate noun in this clause, the adapted model reorders the root words &#8220;rehab&#8221; and &#8220;installation&#8221; (in comparison with the baseline) and improves the grammaticality of the sentence; however, the number does not match between the determiner and the noun phrase.</S>
    <S sid="88" ssid="18">Line 597 demonstrates a perfect phrase translation with respect to the reference translation using semantic paraphrasing.</S>
    <S sid="89" ssid="19">The baseline phrase &#8220;d&#8217;origine&#8221; is transformed and attributed to the noun.</S>
    <S sid="90" ssid="20">Instead of translating &#8220;original&#8221; as a phrase for &#8220;home&#8221;, the adapted model captures the original meaning of the word in the translation.</S>
    <S sid="91" ssid="21">Line 752 demonstrates an improvement in adequacy through the replacement of the word &#8220;quelque&#8221; with &#8220;autre.&#8221; Additionally, extra words are removed.</S>
    <S sid="92" ssid="22">These lexical changes result in the improvement in translation quality due to topic-based adaptation via PLSA.</S>
  </SECTION>
  <SECTION title="7 Conclusions" number="7">
    <S sid="93" ssid="1">An alternative approach to bilingual topic modeling has been presented that integrates the PLSA framework with MDI adaptation that can effectively adapt a background language model when given a document in the source language.</S>
    <S sid="94" ssid="2">Rather than training two topic models and enforcing a one-to-one correspondence for translation, we use the assumption that parallel texts refer to the same topics and have a very similar topic distribution.</S>
    <S sid="95" ssid="3">Preliminary experiments show a reduction in perplexity and an overall improvement in BLEU and NIST scores on speech translation.</S>
    <S sid="96" ssid="4">We also note that, unlike previous works involving topic modeling, we did not remove stop words and punctuation, but rather assumed that these features would have a relatively uniform topic distribution.</S>
    <S sid="97" ssid="5">One downside to the MDI adaptation approach is that the computation of the normalization term z(h) is expensive and potentially prohibitive during continuous speech translation tasks.</S>
    <S sid="98" ssid="6">Further investigation is needed to determine if there is a suitable approximation that avoids computing probabilities across all n-grams.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="99" ssid="1">This work was supported by the T4ME network of excellence (IST-249119), funded by the DG INFSO of the European Commission through the Seventh Framework Programme.</S>
    <S sid="100" ssid="2">The first author received a grant under the Erasmus Mundus Language &amp; Communication Technologies programme.</S>
  </SECTION>
</PAPER>
