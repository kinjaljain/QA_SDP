<PAPER>
  <S sid="0">A Structured Vector Space Model for Word Meaning in Context</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context.</S>
    <S sid="2" ssid="2">This task is a crucial step towards a robust, vector-based compositional account of sentence meaning.</S>
    <S sid="3" ssid="3">We argue that existing models for this task do not take syntactic structure sufficiently into account. present a novel vector space model that addresses these issues by incorporating the selectional preferences for words&#8217; argument positions.</S>
    <S sid="4" ssid="4">This makes it possible to integrate syntax into the computation of word meaning in context.</S>
    <S sid="5" ssid="5">In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors.</S>
    <S sid="7" ssid="2">In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus.</S>
    <S sid="8" ssid="3">These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001).</S>
    <S sid="9" ssid="4">Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007).</S>
    <S sid="10" ssid="5">In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996).</S>
    <S sid="11" ssid="6">Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context.</S>
    <S sid="12" ssid="7">There have been several approaches in the literature (Smolensky, 1990; Sch&#168;utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors.</S>
    <S sid="13" ssid="8">Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a O b.</S>
    <S sid="14" ssid="9">The context b can consist of as little as one word, as shown in Example (1).</S>
    <S sid="15" ssid="10">In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract.</S>
    <S sid="16" ssid="11">Conversely, verbs can influence the interpretation of nouns: In (1a), ball is understood as a spherical object, and in (1c) as a dancing event.</S>
    <S sid="17" ssid="12">In this paper, we argue that models of word meaning relying on this procedure of vector composition are limited both in their scope and scalability.</S>
    <S sid="18" ssid="13">The underlying shortcoming is a failure to consider syntax in two important ways.</S>
    <S sid="19" ssid="14">The syntactic relation is ignored.</S>
    <S sid="20" ssid="15">The first problem concerns the manner of vector composition, which ignores the relation between the target a and its context b.</S>
    <S sid="21" ssid="16">This relation can have a decisive influence on their interpretation, as Example (2) shows: (2) a. a horse draws b. draw a horse In (2a), the meaning of the verb draw can be paraphrased as pull, while in (2b) it is similar to sketch.</S>
    <S sid="22" ssid="17">This difference in meaning is due to the difference in relation: in (2a), horse is the subject, while in (2b) it is the object.</S>
    <S sid="23" ssid="18">On the modeling side, however, a vector combination function that ignores the relation will assign the same representation to (2a) and (2b).</S>
    <S sid="24" ssid="19">Thus, existing models are systematically unable to capture this class of phenomena.</S>
    <S sid="25" ssid="20">Single vectors are too weak to represent phrases.</S>
    <S sid="26" ssid="21">The second problem arises in the context of the important open question of how semantic spaces can &#8220;scale up&#8221; to provide interesting meaning representations for entire sentences.</S>
    <S sid="27" ssid="22">We believe that the current vector composition methods, which result in a single vector c, are not informative enough for this purpose.</S>
    <S sid="28" ssid="23">One proposal for &#8220;scaling up&#8221; is to straightforwardly interpret c = a O b as the meaning of the phrase a + b (Kintsch, 2001; Mitchell and Lapata, 2008).</S>
    <S sid="29" ssid="24">The problem is that the vector c can only encode a fixed amount of structural information if its dimensionality is fixed, but there is no upper limit on sentence length, and hence on the amount of structure to be encoded.</S>
    <S sid="30" ssid="25">It is difficult to conceive how c could encode deeper semantic properties, like predicateargument structure (distinguishing &#8220;dog bites man&#8221; and &#8220;man bites dog&#8221;), that are crucial for sentencelevel semantic tasks such as the recognition of textual entailment (Dagan et al., 2006).</S>
    <S sid="31" ssid="26">An alternative approach to sentence meaning would be to use the vector space representation only for representing word meaning, and to represent sentence structure separately.</S>
    <S sid="32" ssid="27">Unfortunately, present models cannot provide this grounding either, since they compute a single vector c that provides the same representations for both the meanings of a and b in context.</S>
    <S sid="33" ssid="28">In this paper, we propose a new, structured vector space model for word meaning (SVS) that addresses these problems.</S>
    <S sid="34" ssid="29">A SVS representation of a lemma comprises several vectors representing the word&#8217;s lexical meaning as well as the selectional preferences that it has for its argument positions.</S>
    <S sid="35" ssid="30">The meaning of word a in context b is computed by combining a with b&#8217;s selectional preference vector specific to the relation between a and b, addressing the first problem above.</S>
    <S sid="36" ssid="31">In an expression a + b, the meanings of a and b in this context are computed as two separate vectors a' and b&#65533;.</S>
    <S sid="37" ssid="32">These vectors can then be combined with a representation of the structure&#8217;s expression (e.g., a parse tree), to address the second problem discussed above.</S>
    <S sid="38" ssid="33">We test the SVS model on the task of recognizing contextually appropriate paraphrases, finding that SVS performs at and above the state-ofthe-art.</S>
    <S sid="39" ssid="34">Plan of the paper.</S>
    <S sid="40" ssid="35">Section 2 reviews related work.</S>
    <S sid="41" ssid="36">Section 3 presents the SVS model for word meaning in context.</S>
    <S sid="42" ssid="37">Sections 4 to 6 relate experiments on the paraphrase appropriateness task.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="43" ssid="1">In this section we give a short overview over existing vector space based approaches to computing word meaning in context.</S>
    <S sid="44" ssid="2">General context effects.</S>
    <S sid="45" ssid="3">The first category of models aims at integrating the widest possible range of context information without recourse to linguistic structure.</S>
    <S sid="46" ssid="4">The best-known work in this category is Sch&#168;utze (1998).</S>
    <S sid="47" ssid="5">He first computes &#8220;first-order&#8221; vector representations for word meaning by collecting co-occurrence counts from the entire corpus.</S>
    <S sid="48" ssid="6">Then, he determines &#8220;second-order&#8221; vectors for individual word instances in their context, which is taken to be a simple surface window, by summing up all first-order vectors of the words in this context.</S>
    <S sid="49" ssid="7">The resulting vectors form sense clusters.</S>
    <S sid="50" ssid="8">McDonald and Brew (2004) present a similar model.</S>
    <S sid="51" ssid="9">They compute the expectation for a word wz in a sequence by summing the first-order vectors for the words wi to wz_i and showed that the distance between expectation and first-order vector for wz correlates with human reading times.</S>
    <S sid="52" ssid="10">Predicate-argument combination.</S>
    <S sid="53" ssid="11">The second category of prior studies concentrates on contexts consisting of a single word only, typically modeling the combination of a predicate p and an argument a. Kintsch (2001) uses vector representations of p and a to identify the set of words that are similar to both p and a.</S>
    <S sid="54" ssid="12">After this set has been narrowed down in a self-inhibitory network, the meaning of the predicateargument combination is obtained by computing the centroid of its members&#8217; vectors.</S>
    <S sid="55" ssid="13">The procedure does not take the relation between p and a into account.</S>
    <S sid="56" ssid="14">Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: R is the relation holding between p and a, and K additional knowledge.</S>
    <S sid="57" ssid="15">This framework allows sensitivity to the relation.</S>
    <S sid="58" ssid="16">However, the concrete instantiations that Mitchell and Lapata consider disregards K and R, thus sharing the other models&#8217; limitations.</S>
    <S sid="59" ssid="17">They focus instead on methods for the direct combination of p and a: In a comparison between component-wise addition and multiplication of p and a, they find far superior results for the multiplication approach.</S>
    <S sid="60" ssid="18">Tensor product-based models.</S>
    <S sid="61" ssid="19">Smolensky (1990) uses tensor product to combine two word vectors a and b into a vector c representing the expression a+b.</S>
    <S sid="62" ssid="20">The vector c is located in a very high-dimensional space and is thus capable of encoding the structure of the expression; however, this makes the model infeasible in practice, as dimensionality rises with every word added to the representation.</S>
    <S sid="63" ssid="21">Jones and Mewhort (2007) represent lemma meaning by using circular convolution to encode n-gram co-occurrence information into vectors of fixed dimensionality.</S>
    <S sid="64" ssid="22">Similar to Brew and McDonald (2004), they predict most likely next words in a sequence, without taking syntax into account.</S>
    <S sid="65" ssid="23">Kernel methods.</S>
    <S sid="66" ssid="24">One of the main tests for the quality of models of word meaning in context is the ability to predict the appropriateness of paraphrases in given a context.</S>
    <S sid="67" ssid="25">Typically, a paraphrase applies only to some senses of a word, not all, as can be seen in the paraphrases &#8220;grab&#8221; and &#8220;contract&#8221; of &#8220;catch&#8221;.</S>
    <S sid="68" ssid="26">Vector space models generally predict paraphrase appropriateness based on the similarity between vectors.</S>
    <S sid="69" ssid="27">This task can also be addressed with kernel methods, which project items into an implicit feature space for efficient similarity computation.</S>
    <S sid="70" ssid="28">Consequently, vector space methods and kernel methods have both been used for NLP tasks based on similarity, notably Information Retrieval and Textual Entailment.</S>
    <S sid="71" ssid="29">Nevertheless, they place their emphasis on different types of information.</S>
    <S sid="72" ssid="30">Current kernels are mostly tree kernels that compare syntactic structure, and use semantic information mostly for smoothing syntactic similarity (Moschitti and Quarteroni, 2008).</S>
    <S sid="73" ssid="31">In contrast, vector-space models focus on the interaction between the lexical meaning of words in composition.</S>
  </SECTION>
  <SECTION title="3 A structured vector space model for" number="3">
    <S sid="74" ssid="1">In this section, we define the structured vector space (SVS) model of word meaning.</S>
    <S sid="75" ssid="2">The main intuition behind our model is to view the interpretation of a word in context as guided by expectations about typical events.</S>
    <S sid="76" ssid="3">For example, in (1a), we assume that upon hearing the phrase &#8220;catch a ball&#8221;, the hearer will interpret the meaning of &#8220;catch&#8221; to match typical actions that can be performed with a ball.</S>
    <S sid="77" ssid="4">Similarly, the interpretation of &#8220;ball&#8221; will reflect the hearer&#8217;s expectations about typical things that can be caught.</S>
    <S sid="78" ssid="5">This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds.</S>
    <S sid="79" ssid="6">In cognitive science, the central role of expectations about typical events for human language processing is well-established.</S>
    <S sid="80" ssid="7">Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pad&#180;o et al., 2006).</S>
    <S sid="81" ssid="8">Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005).</S>
    <S sid="82" ssid="9">In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003).</S>
    <S sid="83" ssid="10">Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002).</S>
    <S sid="84" ssid="11">Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad&#180;o et al., 2007).</S>
    <S sid="85" ssid="12">We first present the SVS model of word meaning that integrates lexical information with selectional preferences.</S>
    <S sid="86" ssid="13">Then, we show how the SVS model provides a new way of computing meaning in context.</S>
    <S sid="87" ssid="14">Representing lemma meaning.</S>
    <S sid="88" ssid="15">We abandon the traditional choice of representing word meaning as a single vector.</S>
    <S sid="89" ssid="16">Instead, we encode each word as a combination of (a) one vector that models the lexical meaning of the word, and (b) a set of vectors, each of which represents the semantic expectations/selectional preferences for one particular relation that the word supports.1 The idea is illustrated in Fig.</S>
    <S sid="90" ssid="17">1.</S>
    <S sid="91" ssid="18">In the representation of the verb catch, the central square stands for the lexical vector of catch itself.</S>
    <S sid="92" ssid="19">The three arrows link it to catch&#8217;s preferences for its subjects (subj), its objects (obj), and for verbs for which it appears as a complement (comp&#8722;1).</S>
    <S sid="93" ssid="20">The figure shows the selectional preferences as word lists for readability; in practice, each selectional preference is a single vector (cf.</S>
    <S sid="94" ssid="21">Section 4).</S>
    <S sid="95" ssid="22">Likewise, ball is represented by one vector for ball itself, one for ball&#8217;s preferences for its modifiers (mod), one vector for the verbs of which it is a subject (subj&#8722;1), and one for the verbs of which is an object (obj&#8722;1).</S>
    <S sid="96" ssid="23">This representation includes selectional preferences (like subj, obj, mod) exactly parallel to inverse selectional preferences (subj&#8722;1, obj&#8722;1, comp&#8722;1).</S>
    <S sid="97" ssid="24">To our knowledge, preferences of the latter kind have not been studied in computational linguistics.</S>
    <S sid="98" ssid="25">However, their existence is supported in psycholinguistics by priming effects from nouns to typical verbs (McRae et al., 2005).</S>
    <S sid="99" ssid="26">Formally, let D be a vector space (the set of possible vectors), and let R be some set of relation labels.</S>
    <S sid="100" ssid="27">In the structured vector space (SVS) model, we represent the meaning of a lemma w as a triple where v E D is a lexical vector describing the word w itself, R : R &#8212;* D maps each relation label onto a vector that describes w&#8217;s selectional preferences, and R&#8722;1 : R &#8212;* D maps from role labels to vectors describing inverse selectional preferences of w. Both R and R&#8722;1 are partial functions.</S>
    <S sid="101" ssid="28">For example, the direct object preference would be undefined for intransitive verbs.</S>
    <S sid="102" ssid="29">Computing meaning in context.</S>
    <S sid="103" ssid="30">The SVS model of lemma meaning permits us to compute the meaning of a word a in the context of another word b in a new way, via their selectional preferences.</S>
    <S sid="104" ssid="31">Let (va, Ra, R&#8722;1 a ) and (vb, Rb, R&#8722;1 b ) be the representations of the two words, and let r E R be the relation linking a to b.</S>
    <S sid="105" ssid="32">Then, we define the meaning of a and b in this context as a pair (a0, b0) of vectors, where a0 is the meaning of a in the context of b, and b0 the meaning of b in the context of a: where v1 O v2 is a direct vector combination function as in traditional models, e.g. addition or componentwise multiplication.</S>
    <S sid="106" ssid="33">If either Ra(r) or R&#8722;1 b (r) are not defined, the combination fails.</S>
    <S sid="107" ssid="34">Afterwards, the argument position r is considered filled, and is deleted from Ra and R&#8722;1 Figure 2 illustrates this procedure on the representations from Figure 1.</S>
    <S sid="108" ssid="35">The dotted lines indicate that the lexical vector for catch is combined with the inverse object preference of ball.</S>
    <S sid="109" ssid="36">Likewise, the lexical vector for ball is combined with the object preference vector of catch.</S>
    <S sid="110" ssid="37">Note that our procedure for computing meaning in context can be expressed within the framework of Mitchell and Lapata (Eq.</S>
    <S sid="111" ssid="38">(3)).</S>
    <S sid="112" ssid="39">We can encode the expectations of a and b as additional knowledge K. The combined representation c is the pair (a', b') that is computed according to our model (Eq.</S>
    <S sid="113" ssid="40">(4)).</S>
    <S sid="114" ssid="41">The SVS scheme we have proposed incorporates syntactic information in a more general manner than previous models, and thus addresses the issues we have discussed in Section 1.</S>
    <S sid="115" ssid="42">Since the representation retains individual selectional preferences for all relations, combining the same words through different relations can (and will in general) result in different adapted representations.</S>
    <S sid="116" ssid="43">For instance, in the case of Example (2), we would expect the inverse subject preference of horse (&#8220;things that a horse typically does&#8221;) to push the lexical vector of draw into the direction of pulling, while its inverse object preference (&#8220;things that are done to horses&#8221;) suggest a different interpretation.</S>
    <S sid="117" ssid="44">Rather than yielding a single, joint vector for the whole expression, our procedure for computing meaning in context results in one context-adapted meaning representation per word, similar to the output of a WSD system.</S>
    <S sid="118" ssid="45">As a consequence, our model can be combined with any formalism representing the structure of an expression.</S>
    <S sid="119" ssid="46">(The formalism used then determines the set R of relations.)</S>
    <S sid="120" ssid="47">For example, combining SVS with a dependency tree would yield a tree in which each node is labeled by a SVS tuple that represents the word&#8217;s meaning in context.</S>
  </SECTION>
  <SECTION title="4 Experimental setup" number="4">
    <S sid="121" ssid="1">This section provides the background to the following experimental evaluation of SVS, including parameters used for computing the SVS representations that will be used in the experiments.</S>
    <S sid="122" ssid="2">In this paper, we evaluate the SVS model against the task of predicting, given a predicate-argument pair, how appropriate a paraphrase (of either the predicate or the argument) is in that context.</S>
    <S sid="123" ssid="3">We perform two experiments that both use the paraphrase task, but differ in their emphasis.</S>
    <S sid="124" ssid="4">Experiment 1 replicates an existing evaluation against human judgments.</S>
    <S sid="125" ssid="5">This evaluation uses synthetic dataset, limited to one particular construction, and constructed to provide maximally distinct paraphrase candidates.</S>
    <S sid="126" ssid="6">Experiment 2 considers a broader class of constructions along with annotator-generated paraphrase candidates that are not screened for distinctness.</S>
    <S sid="127" ssid="7">In both experiments, we compare the SVS model against the state-of-theart model by Mitchell and Lapata 2008 (henceforth M&amp;L; cf.</S>
    <S sid="128" ssid="8">Sec.</S>
    <S sid="129" ssid="9">2 for model details).</S>
    <S sid="130" ssid="10">Vector space.</S>
    <S sid="131" ssid="11">In our parameterization of the vector space, we largely follow M&amp;L because their model has been rigorously evaluated and found to outperform a range of other models.</S>
    <S sid="132" ssid="12">Our first space is a traditional &#8220;bag-of-words&#8221; vector space (BOW, (Lund and Burgess, 1996)).</S>
    <S sid="133" ssid="13">For each pair of a target word and context word, the BOW space records a function of their co-occurrence frequency within a surface window of size 10.</S>
    <S sid="134" ssid="14">The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions.</S>
    <S sid="135" ssid="15">We also consider a &#8220;dependency-based&#8221; vector space (SYN, (Pad&#180;o and Lapata, 2007)).</S>
    <S sid="136" ssid="16">In this space, target and context words have to be linked by a &#8220;valid&#8221; dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993).</S>
    <S sid="137" ssid="17">For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&amp;L.</S>
    <S sid="138" ssid="18">Selectional preferences.</S>
    <S sid="139" ssid="19">We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional preference through similarity to seen filler vectors va,: We compute the selectional preference vector for word b and relation r as the weighted centroid of seen filler vectors va. We collect seen fillers from the Minipar-parse of the BNC.</S>
    <S sid="140" ssid="20">Let f(a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC, then verb subject landmark sim judgment slump shoulder slouch high 7 slump shoulder decline low 2 slump value slouch low 3 slump value decline high 7 We call this base model SELPREF.</S>
    <S sid="141" ssid="21">We will also study two variants of SELPREF, based on two different hypotheses about what properties of the selectional preferences are particularly important for meaning adaption.</S>
    <S sid="142" ssid="22">The first model aims specifically at alleviating noise introduced by infrequent fillers, a common problem in data-driven approaches.</S>
    <S sid="143" ssid="23">It only uses fillers seen more often than a threshold 0.</S>
    <S sid="144" ssid="24">We call this model SELPREF-CUT: Our second variant again aims at alleviating noise, but noise introduced by low-valued dimensions rather than infrequent fillers.</S>
    <S sid="145" ssid="25">It achieves this by taking each component of the selectional preference vector to the nth power.</S>
    <S sid="146" ssid="26">In this manner, dimensions with high counts are further inflated, while dimensions with low counts are depressed.3 This model, SELPREF-POW, is defined as follows: If Rb(r)SELPREF = (v1, .</S>
    <S sid="147" ssid="27">.</S>
    <S sid="148" ssid="28">.</S>
    <S sid="149" ssid="29">, vM), The inverse selectional preferences R&#65533;1 b are defined analogously for all three model variants.</S>
    <S sid="150" ssid="30">We instantiate the vector combination function O as component-wise multiplication, following M&amp;L.</S>
    <S sid="151" ssid="31">Baselines and significance testing.</S>
    <S sid="152" ssid="32">All tasks that we consider below involve judgments for the meaning of a word a in the context of a word b.</S>
    <S sid="153" ssid="33">A first baseline that every model must beat is simply using the original vector for a.</S>
    <S sid="154" ssid="34">We call this baseline &#8220;target only&#8221;.</S>
    <S sid="155" ssid="35">Since we assume that the selectional preferences of b model the expectations for a, we use b&#8217;s selectional preference vector for the given relation as a second baseline, &#8220;selpref only&#8221;.</S>
    <S sid="156" ssid="36">3Since we focus on the size-invariant cosine similarity, the use of this model does not require normalization.</S>
    <S sid="157" ssid="37">Differences between the performance of models were tested for significance using a stratified shuffling-based randomization test (Yeh, 2000).4.</S>
  </SECTION>
  <SECTION title="5 Exp." number="5">
    <S sid="158" ssid="1">In our first experiment, we attempt to predict human similarity judgments.</S>
    <S sid="159" ssid="2">This experiment is a replication of the evaluation of M&amp;L on their dataset5.</S>
    <S sid="160" ssid="3">Dataset.</S>
    <S sid="161" ssid="4">The M&amp;L dataset comprises a total of 3,600 human similarity judgements for 120 experimental items.</S>
    <S sid="162" ssid="5">Each item, as shown in Figure 3, consists of an intransitive verb and a subject noun that are combined with a &#8220;landmark&#8221;, a synonym of the verb that is chosen to be either similar or dissimilar to the verb in the context of the given subject.</S>
    <S sid="163" ssid="6">The dataset was constructed by extracting pairs of subjects and intransitive verbs from a parsed version of the BNC.</S>
    <S sid="164" ssid="7">Each item was paired with two landmarks, chosen to be as dissimilar as possible according to a WordNet similarity measure.</S>
    <S sid="165" ssid="8">All nouns and verbs were subjected to a pretest, where only those with highly significant variations in human judgments across landmarks were retained.</S>
    <S sid="166" ssid="9">For each item of the final dataset, judgements on a 7-point scale were elicited.</S>
    <S sid="167" ssid="10">For example, judges considered the compatible landmark &#8220;slouch&#8221; to be much more similar to &#8220;shoulder slumps&#8221; than the incompatible landmark &#8220;decline&#8221;.</S>
    <S sid="168" ssid="11">In Figure 3, the column sim shows whether the experiment designers considered the respective landmark to have high or low similarity to the verb, and the column judgment shows a participant&#8217;s judgments.</S>
    <S sid="169" ssid="12">Experimental procedure.</S>
    <S sid="170" ssid="13">We used cosine to compute similarity to the lexical vector of the landmark.</S>
    <S sid="171" ssid="14">&#8220;Target only&#8221; compares the landmark against the lexical vector of the verb, and &#8220;selpref only&#8221; compares it to the noun&#8217;s subj&#8722;1 preference.</S>
    <S sid="172" ssid="15">For the M&amp;L model, the comparison is to the combined lexical vectors of verb and noun.</S>
    <S sid="173" ssid="16">For our models SELPREF, SELPREF-CUT and SELPREF-POW, we combine the verb&#8217;s lexical vector with the subj&#8722;1 preference of the noun.</S>
    <S sid="174" ssid="17">We used a held-out dataset of 10% of the data to optimize the parameters of 8 of SELPREF-CUT and n of SELPREF-POW.</S>
    <S sid="175" ssid="18">Vectors with PMI components could model the data, while raw frequency components could not; we report only the former.</S>
    <S sid="176" ssid="19">We use the same two evaluation scores as M&amp;L: The first score is the average similarity to compatible landmarks (high) and incompatible landmarks (low).</S>
    <S sid="177" ssid="20">The second is Spearman&#8217;s p, a nonparametric correlation coefficient.</S>
    <S sid="178" ssid="21">We compute p between individual human similarity scores and our predictions.</S>
    <S sid="179" ssid="22">Based on agreement between human judges, M&amp;L estimate an upper bound p of 0.4 for the dataset.</S>
    <S sid="180" ssid="23">Results and discussion.</S>
    <S sid="181" ssid="24">Table 1 shows the results of Exp.</S>
    <S sid="182" ssid="25">1 on the test set.</S>
    <S sid="183" ssid="26">In the upper half (BOW), we replicate M&amp;L&#8217;s main finding that simple componentwise multiplication of the predicate and argument vectors results in a highly significant correlation of p = 0.2, significantly outperforming both baselines.</S>
    <S sid="184" ssid="27">It is interesting, though, that the subj&#8722;1 preference itself (&#8220;Selpref only&#8221;) is already highly significantly correlated with the human judgments.</S>
    <S sid="185" ssid="28">A comparison of the upper half (BOW) with the lower half (SYN) shows that the dependency-based space generally shows better correlation with human judgements.</S>
    <S sid="186" ssid="29">This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces (Lin, 1998; Pad&#180;o and Lapata, 2007).</S>
    <S sid="187" ssid="30">All instances of the SELPREF model show highly significant correlations.</S>
    <S sid="188" ssid="31">SELPREF and SELPREF-CUT show very similar performance.</S>
    <S sid="189" ssid="32">They do better than both baselines in the BOW space; however, in the cleaner SYN space, their performance is numerically lower than using selectional preferences only (p = 0.13 vs. 0.16).</S>
    <S sid="190" ssid="33">SELPREF-POW is always significantly better than SELPREF and SELPREF-CUT, and shows the best result of all tested models (p = 0.27, BOW space).</S>
    <S sid="191" ssid="34">The performance is somewhat lower in the SYN space (p = 0.22).</S>
    <S sid="192" ssid="35">However, this difference, and the difference to the best M&amp;L model at p = 0.24, are not statistically significant.</S>
    <S sid="193" ssid="36">The SVS model computes meaning in context by combining a word&#8217;s lexical representation with the preference vector of its context.</S>
    <S sid="194" ssid="37">In this, it differs from previous models, including that by M&amp;L, which used what we have been calling &#8220;direct combination&#8221;.</S>
    <S sid="195" ssid="38">So it is important to ask to what extent this difference in method translate to a difference in predictions.</S>
    <S sid="196" ssid="39">We analyzed this by measuring the similarity by the nouns&#8217; lexical vectors, used by direct combination methods, and their inverse subject preferences, which SVS uses.</S>
    <S sid="197" ssid="40">The result is shown in the first column in Table 2, computed as mean cosine similarities and standard deviations between noun vectors and selectional preferences.</S>
    <S sid="198" ssid="41">The table shows that these vectors have generally low similarity, which is further reduced by applying cutoff and potentiation.</S>
    <S sid="199" ssid="42">Thus, the predictions of SVS will differ from those of direct combination models like M&amp;L.</S>
    <S sid="200" ssid="43">A related question is whether syntax-aware vector combination makes a difference: Does the model encode different expectations for different syntactic relations (cf.</S>
    <S sid="201" ssid="44">Example 2)?</S>
    <S sid="202" ssid="45">The second column of Table 2 explores this question by comparing inverse selectional preferences for the subject and object slots.</S>
    <S sid="203" ssid="46">We observe that the similarity is very high for raw preferences, but becomes lower when noise is eliminated.</S>
    <S sid="204" ssid="47">Since the SELPREF-POW model performed best in our evaluation, we read this as evidence that potentiation helps to suppress noise introduced by mis-identified subject and object fillers.</S>
    <S sid="205" ssid="48">In Experiment 1, all experimental items were verbs, which means that all disambiguation was done through inverse selectional preferences.</S>
    <S sid="206" ssid="49">As inverse selectional preferences are currently largely unexplored, it is interesting to note that the evidence that they provide for the paraphrase task is as strong as that of the context nouns themselves.</S>
  </SECTION>
  <SECTION title="6 Exp." number="6">
    <S sid="207" ssid="1">This section reports on a second, more NLP-oriented experiment whose task is to distinguish between appropriate and inappropriate paraphrases on a broader range of constructions.</S>
    <S sid="208" ssid="2">Dataset.</S>
    <S sid="209" ssid="3">For this experiment, we use the SemEval1 lexical substitution (lexsub) dataset (McCarthy and Navigli, 2007), which contains 10 instances each of 200 target words in sentential contexts, drawn from Sharoff&#8217;s (2006) English Internet Corpus.</S>
    <S sid="210" ssid="4">Contextually appropriate paraphrases for each instance of each target word were elicited from up to 6 participants.</S>
    <S sid="211" ssid="5">Fig.</S>
    <S sid="212" ssid="6">4 shows two instances for the verb to work.</S>
    <S sid="213" ssid="7">The distribution over paraphrases can be seen as a characterization of the target word&#8217;s meaning in each context.</S>
    <S sid="214" ssid="8">Experimental procedure.</S>
    <S sid="215" ssid="9">In this paper, we predict appropriate paraphrases solely on the basis of a single context word that stands in a direct predicateargument relation to the target word.</S>
    <S sid="216" ssid="10">We extracted all instances from the lexsub test data with such a relation.</S>
    <S sid="217" ssid="11">After parsing all sentences with verbal and nominal targets with Minipar, this resulted in three Sentence Substitutes By asking people who work be employed 4; there, I have since determined labour 1 that he didn&#8217;t.</S>
    <S sid="218" ssid="12">(# 2002) Remember how hard your an- toil 4; labour 3; cestors worked.</S>
    <S sid="219" ssid="13">(# 2005) task 1 sets of sentences: (a), target intransitive verbs with noun subjects (V-SUBJ, 48 sentences); (b), target transitive verbs with noun objects (V-OBJ, 213 sent.</S>
    <S sid="220" ssid="14">); and (c), target nouns occurring as objects of verbs (N-OBJ, 102 sent.</S>
    <S sid="221" ssid="15">).6 Note that since we use only part of the lexical substitution dataset in this experiment, a direct comparison with results from the SemEval task is not possible.</S>
    <S sid="222" ssid="16">As in the original SemEval task, we phrase the task as a ranking problem.</S>
    <S sid="223" ssid="17">For each target word, the paraphrases given for all 10 instances are pooled.</S>
    <S sid="224" ssid="18">The task is to rank the list for each item so that appropriate paraphrases (such as &#8220;be employed&#8221; for # 2002) rank higher than paraphrases not given (e.g., &#8220;toil&#8221;).</S>
    <S sid="225" ssid="19">Our model ranks paraphrases by their similarity to the following combinations (Eq.</S>
    <S sid="226" ssid="20">(4)): for V-SUBJ, verb plus the noun&#8217;s subj&#8722;1 preferences; for V-OBJ, verb plus the noun&#8217;s obj&#8722;1 preferences; and for NOBJ, the noun plus the verb&#8217;s obj preferences.</S>
    <S sid="227" ssid="21">Our comparison model, M&amp;L, ranks all paraphrases by their similarity to the direct noun-verb combination.</S>
    <S sid="228" ssid="22">To avoid overfitting, we consider only the two models that performed optimally in in the SYN space in Experiment 1 (SELPREF-POW with n=30 and M&amp;L).</S>
    <S sid="229" ssid="23">However, since we found that vectors with raw frequency components could model the data, while PMI components could not, we only report the former.</S>
    <S sid="230" ssid="24">For evaluation, we adopt the SemEval &#8220;out of ten&#8221; precision metric POOT.</S>
    <S sid="231" ssid="25">It uses the model&#8217;s ten top-ranked paraphrases as its guesses for appropriate paraphrases.</S>
    <S sid="232" ssid="26">Let Gi be the gold paraphrases for item i, Mi the model&#8217;s top ten paraphrases for i, and f(s, i) the frequency of s as paraphrase for i: McCarthy and Navigli propose this metric for the dataset for robustness.</S>
    <S sid="233" ssid="27">Due to the sparsity of paraphrases, a metric that considers fewer guesses leads to artificially low results when a &#8220;good&#8221; paraphrase was not mentioned by the annotators by chance but is ranked highly by a model.</S>
    <S sid="234" ssid="28">Results and discussion.</S>
    <S sid="235" ssid="29">Table 6 shows the mean out-of-ten precision for all models.</S>
    <S sid="236" ssid="30">The behavior is fairly uniform across all three datasets.</S>
    <S sid="237" ssid="31">Unsurprisingly, &#8220;target only&#8221;, which uses the same ranking for all instances of a target, yields the worst results.7 M&amp;L&#8217;s direct combination model outperforms &#8220;target only&#8221; significantly (p &lt; 0.05).</S>
    <S sid="238" ssid="32">However, on both the V-SUBJ and the N-OBJ the &#8220;selpref only&#8221; baseline does better than direct combination.</S>
    <S sid="239" ssid="33">The best results on all datasets are obtained by SELPREF-POW.</S>
    <S sid="240" ssid="34">The difference between SELPREF-POW and the &#8220;target only&#8221; baseline is highly significant (p &lt; 0.01).</S>
    <S sid="241" ssid="35">The difference to M&amp;L&#8217;s model is significant at p = 0.05.</S>
    <S sid="242" ssid="36">We interpret these results as encouraging evidence for the usefulness of selectional preferences for judging substitutability in context.</S>
    <S sid="243" ssid="37">Knowledge about the selectional preferences of a single context word can already lead to a significant improvement in precision.</S>
    <S sid="244" ssid="38">We find this overall effect even though the word is not informative in all cases.</S>
    <S sid="245" ssid="39">For instance, the subject of item 2002 in Fig.</S>
    <S sid="246" ssid="40">4, &#8220;who&#8221;, presumably helps little in determining the verb&#8217;s context-adapted meaning.</S>
    <S sid="247" ssid="41">It is interesting that the improvement of SELPREFPOW over &#8220;selpref only&#8221; is smallest for the N-OBJ dataset (1.9% POOT).</S>
    <S sid="248" ssid="42">N-OBJ uses selectional preferences for nouns that may fill the direct object position, , while V-SUBJ and V-OBJ use inverse selectional preferences for verbs (cf. the two graphs in Fig.</S>
    <S sid="249" ssid="43">1).</S>
    <S sid="250" ssid="44">7&#8220;Target only&#8221; still does very much better than a random baseline, which performs at 22% POOT.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="7">
    <S sid="251" ssid="1">In this paper, we have considered semantic space models that can account for the meaning of word occurrences in context.</S>
    <S sid="252" ssid="2">Arguing that existing models do not sufficiently take syntax into account, we have introduced the new structured vector space (SVS) model of word meaning.</S>
    <S sid="253" ssid="3">In addition to a vector representing a word&#8217;s lexical meaning, it contains vectors representing the word&#8217;s selectional preferences.</S>
    <S sid="254" ssid="4">These selectional preferences play a central role in the computation of meaning in context.</S>
    <S sid="255" ssid="5">We have evaluated the SVS model on two datasets on the task of predicting the felicitousness of paraphrases in given contexts.</S>
    <S sid="256" ssid="6">On the M&amp;L dataset, SVS outperforms the state-of-the-art model of M&amp;L, though the difference is not significant.</S>
    <S sid="257" ssid="7">On the Lexical Substitution dataset, SVS significantly outperforms the state-of-the-art.</S>
    <S sid="258" ssid="8">This is especially interesting as the Lexical Substitution dataset, in contrast to the M&amp;L data, uses &#8220;realistic&#8221; paraphrase candidates that are not necessarily maximally distinct.</S>
    <S sid="259" ssid="9">The most important limitation of the evaluation that we have given in this paper is that we have only considered single words as context.</S>
    <S sid="260" ssid="10">Our next step will be to integrate information from multiple relations (such as both the subject and object positions of a verb) into the computation of context-specific meaning.</S>
    <S sid="261" ssid="11">Our eventual aim is a model that can give a compositional account of a word&#8217;s meaning in context, where all words in an expression disambiguate one another according to the relations between them.</S>
    <S sid="262" ssid="12">We will explore the usability of vector space models of word meaning in NLP applications, formulated as the question of how to perform inferences on them in the context of the Textual Entailment task (Dagan et al., 2006).</S>
    <S sid="263" ssid="13">Paraphrase-based inference rules play a large role in several recent approaches to Textual Entailment (e.g.</S>
    <S sid="264" ssid="14">Szpektor et al (2008)); appropriateness judgments of paraphrases in context, the task of Experiments 1 and 2 above, can be viewed as testing the applicability of these inferences rules.</S>
    <S sid="265" ssid="15">Acknowledgments.</S>
    <S sid="266" ssid="16">Many thanks for helpful discussion to Jason Baldridge, David Beaver, Dedre Gentner, James Hampton, Dan Jurafsky, Alexander Koller, Brad Love, and Ray Mooney.</S>
  </SECTION>
</PAPER>
