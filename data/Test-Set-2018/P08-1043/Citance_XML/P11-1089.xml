<PAPER>
  <S sid="0">A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Most previous studies of morphological disambiguation and dependency parsing have been pursued independently.</S>
    <S sid="2" ssid="2">Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the &#8220;pipeline&#8221; approach, assuming that morphological information has been separately obtained.</S>
    <S sid="3" ssid="3">However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other.</S>
    <S sid="4" ssid="4">In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures.</S>
    <S sid="5" ssid="5">In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">To date, studies of morphological analysis and dependency parsing have been pursued more or less independently.</S>
    <S sid="7" ssid="2">Morphological taggers disambiguate morphological attributes such as partof-speech (POS) or case, without taking syntax into account (Hakkani-T&#168;ur et al., 2000; Haji&#711;c et al., 2001); dependency parsers commonly assume the &#8220;pipeline&#8221; approach, relying on morphological information as part of the input (Buchholz and Marsi, 2006; Nivre et al., 2007).</S>
    <S sid="8" ssid="3">This approach serves many languages well, especially those with less morphological ambiguity.</S>
    <S sid="9" ssid="4">In English, for example, accuracy of POS tagging has risen above 97% (Toutanova et al., 2003), and that of dependency parsing has reached the low nineties (Nivre et al., 2007).</S>
    <S sid="10" ssid="5">For these languages, there may be little to be gained to justify the computational cost of incorporating syntactic inference during the morphological tagging task; conversely, it is doubtful that errorful morphological information is a main cause of errors in English dependency parsing.</S>
    <S sid="11" ssid="6">However, the pipeline approach seems more problematic for morphologically-rich languages with substantial interactions between morphology and syntax (Tsarfaty, 2006).</S>
    <S sid="12" ssid="7">Consider the Latin sentence, Una dies omnis potuit praecurrere amantis, &#8216;One day was able to make up for all the lovers&#8217;1.</S>
    <S sid="13" ssid="8">As shown in Table 1, the adjective omnis (&#8216;all&#8217;) is ambiguous in number, gender, and case; there are seven valid analyses.</S>
    <S sid="14" ssid="9">From the perspective of a finitestate morphological tagger, the most attractive analysis is arguably the singular nominative, since omnis is immediately followed by the singular verb potuit (&#8216;could&#8217;).</S>
    <S sid="15" ssid="10">Indeed, the baseline tagger used in this study did make this decision.</S>
    <S sid="16" ssid="11">Given its nominative case, the pipeline parser assigned the verb potuit to be its head; the two words form the typical subjectverb relation, agreeing in number.</S>
    <S sid="17" ssid="12">Unfortunately, as shown in Figure 1, the word omnis in fact modifies the noun amantis, at the end of the sentence.</S>
    <S sid="18" ssid="13">As a result, despite the distance between them, they must agree in number, gender and case, i.e., both must be plural masculine (or feminine) accusative.</S>
    <S sid="19" ssid="14">The pipeline parser, acting on the input that omnis is nominative, naturally did not see this agreement, and therefore did not consider this syntactic relation likely.</S>
    <S sid="20" ssid="15">Such a dilemma is not uncommon in languages with relatively free word order.</S>
    <S sid="21" ssid="16">On the one hand, it appears difficult to improve morphological tagging accuracy on words like omnis without syntactic knowledge; on the other hand, a parser cannot reliably disambiguate syntax unless it has accurate morphological information, in this example the agreement in number, gender, and case.</S>
    <S sid="22" ssid="17">In this paper we propose to attack this chickenand-egg problem with a discriminative model that jointly infers morphological and syntactic properties of a sentence, given its words as input.</S>
    <S sid="23" ssid="18">In evaluations on various highly-inflected languages, the model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.</S>
    <S sid="24" ssid="19">After a description of previous work (&#167;2), the joint model (&#167;3) will be contrasted with the baseline pipeline model (&#167;4).</S>
    <S sid="25" ssid="20">Experimental results (&#167;56) will then be presented, followed by conclusions and future directions.</S>
  </SECTION>
  <SECTION title="2 Previous Work" number="2">
    <S sid="26" ssid="1">Since space does not allow a full review of the vast literature on morphological analysis and parsing, we focus only on past research involving joint morphological and syntactic inference (&#167;2.1); we then discuss Latin (&#167;2.2), a language representative of the challenges that motivated our approach.</S>
    <S sid="27" ssid="2">Most previous work in morphological disambiguation, even when applied on morphologically complex languages with relatively free word order, such as Turkish (Hakkani-T&#168;ur et al., 2000) and Czech (Haji&#711;c et al., 2001), did not consider syntactic relationships between words.</S>
    <S sid="28" ssid="3">In the literature on data-driven parsing, two recent studies attempted joint inference on morphology and syntax, and both considered phrase-structure trees for Modern Hebrew (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008).</S>
    <S sid="29" ssid="4">The primary focus of morphological processing in Modern Hebrew is splitting orthographic words into morphemes: clitics such as prepositions, pronouns, and the definite article must be separated from the core word.</S>
    <S sid="30" ssid="5">Each of the resulting morphemes is then tagged with an atomic &#8220;part-of-speech&#8221; to indicate word class and some morphological features.</S>
    <S sid="31" ssid="6">Similarly, the English POS tags in the Penn Treebank combine word class information with morphologidies day una cal attributes such as &#8220;plural&#8221; or &#8220;past tense&#8221;.</S>
    <S sid="32" ssid="7">Cohen and Smith (2007) separately train a discriminative conditional random field (CRF) for segmentation and tagging, and a generative probabilistic context-free grammar (PCFG) for parsing.</S>
    <S sid="33" ssid="8">At decoding time, the two models are combined as a product of experts.</S>
    <S sid="34" ssid="9">Goldberg and Tsarfaty (2008) propose a generative joint model.</S>
    <S sid="35" ssid="10">This paper is the first to use a fully discriminative model for joint morphological and syntactic inference on dependency trees.</S>
    <S sid="36" ssid="11">Unlike Modern Hebrew, Latin does not require extensive morpheme segmentation2.</S>
    <S sid="37" ssid="12">However, it does have a relatively free word order, and is also highly inflected, with each word having up to nine morphological attributes, listed in Table 2.</S>
    <S sid="38" ssid="13">In addition to its absolute numbers of cases, moods, and tenses, Latin morphology is fusional.</S>
    <S sid="39" ssid="14">For instance, the suffix &#8722;is in omnis cannot be segmented into morphemes that separately indicate gender, number, and case.</S>
    <S sid="40" ssid="15">According to the Latin morphological database encoded in MORPHEUS (Crane, 1991), 30% of Latin nouns can be parsed as another part-of-speech, and on average each has 3.8 possible morphological interpretations.</S>
    <S sid="41" ssid="16">We know of only one previous attempt in datadriven dependency parsing for Latin (Bamman and Crane, 2008), with the goal of constructing a dynamic lexicon for a digital library.</S>
    <S sid="42" ssid="17">Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005).</S>
    <S sid="43" ssid="18">Head selection accuracy was 61.49%, and rose to 64.99% with oracle morphological tags.</S>
    <S sid="44" ssid="19">Of the nine morphological attributes, gender and especially case had the lowest accuracy.</S>
    <S sid="45" ssid="20">This observation echoes the findings for Czech (Smith et al., 2005), where case was also the most difficult to disambiguate.</S>
  </SECTION>
  <SECTION title="3 Joint Model" number="3">
    <S sid="46" ssid="1">This section describes a model that jointly infers morphological and syntactic properties of a sentence.</S>
    <S sid="47" ssid="2">It will be presented as a graphical model, Ancient Greek has the same attributes; Czech and Hungarian lack some of them.</S>
    <S sid="48" ssid="3">In all categories except POS, a value of null (&#8216;-&#8217;) may also be assigned.</S>
    <S sid="49" ssid="4">For example, a noun has &#8216;-&#8217; for the tense attribute. starting with the variables and then the factors, which represents constraints on the variables.</S>
    <S sid="50" ssid="5">Let n be the number of words and m be the number of possible values for a morphological attribute.</S>
    <S sid="51" ssid="6">The variables are: 3The TAG variables were actually implemented as multinomials, but are presented here as booleans for ease of understanding.</S>
    <S sid="52" ssid="7">We define a probability distribution over all joint assignments A to the above variables, where Z is a normalizing constant.</S>
    <S sid="53" ssid="8">The assignment A is subject to a hard constraint, represented in Figure 2 as TREE, requiring that the values of the LINK variables must yield a tree, which may be non-projective.</S>
    <S sid="54" ssid="9">The factors Fk(A) represent soft constraints evaluating various aspects of the &#8220;goodness&#8221; of the tree structure implied by A.</S>
    <S sid="55" ssid="10">We say a factor &#8220;fires&#8221; when all its neighboring variables are true and it evaluates to a non-negative real number; otherwise, it evaluates to 1 and has no effect on the product in equation (1).</S>
    <S sid="56" ssid="11">Soft constraints in the model are divided into local and link factors, to which we now turn.</S>
    <S sid="57" ssid="12">The local factors consult either one word or two neighboring words, and their morphological attributes.</S>
    <S sid="58" ssid="13">These factors express the desirability of the assignments of morphological attributes based on local context.</S>
    <S sid="59" ssid="14">There are three types: It is clear that so far, none of these factors are aware of the morphological agreement between omnis and amantis, crucial for inferring their syntactic relation.</S>
    <S sid="60" ssid="15">We now turn our attention to link factors, which serve this purpose.</S>
    <S sid="61" ssid="16">The link factors consult all pairs of words, possibly separated by a long distance, that may have a dependency link.</S>
    <S sid="62" ssid="17">These factors model the likelihood of such a link based on the word identities and their morphological attributes: tures&#8221; used by McDonald et al. (2005) that involve POS.</S>
    <S sid="63" ssid="18">These factors are not shown in Figure 2, but would have exactly the same structure as the CASE-LINK factors.</S>
    <S sid="64" ssid="19">Beyond these basic features, McDonald et al. (2005) also utilize POS trigrams and POS 4grams.</S>
    <S sid="65" ssid="20">Both include the POS of two linked words, wi and wj.</S>
    <S sid="66" ssid="21">The third component in the trigrams is the POS of each word wk located between wi and wj, i &lt; k &lt; j.</S>
    <S sid="67" ssid="22">The two additional components that make up the 4-grams are subsets of the POS of words located to the immediate left and right of wi and wj.</S>
    <S sid="68" ssid="23">If fully implemented in our joint model, these features would necessitate two separate families of link factors: O(n3m3) factors for the POS trigrams, and O(n2m4) factors for the POS 4-grams.</S>
    <S sid="69" ssid="24">To avoid this substantial increase in model complexity, these features are instead approximated: the POS of all words involved in the trigrams and 4-grams, except those of wi and wj, are regarded as fixed, their values being taken from the output of a morphological tagger (&#167;4.1), rather than connected to the appropriate TAG variables.</S>
    <S sid="70" ssid="25">This approximation allows these features to be incorporated in the POS-LINK factors. ternary factors, each connected to the variables Li,j, Ti,a,vi and Tj,a,vj, for every attribute a other than POS.</S>
    <S sid="71" ssid="26">The factor fires when all three variables are true, and both vi and vj are nonnull; i.e., it fires when the parent word wi has vi as its morphological attribute a, and the child wj has vj.</S>
    <S sid="72" ssid="27">Features include the combination of vi and vj themselves, and agreement between them.</S>
    <S sid="73" ssid="28">The CASE-LINK factors in Figure 2 are an example of this family of factors.</S>
  </SECTION>
  <SECTION title="4 Baselines" number="4">
    <S sid="74" ssid="1">To ensure a meaningful comparison with the joint model, our two baselines are both implemented in the same graphical model framework, and trained with the same machine-learning algorithm.</S>
    <S sid="75" ssid="2">Roughly speaking, they divide up the variables and factors of the joint model and train them separately.</S>
    <S sid="76" ssid="3">For morphological disambiguation, we use the baseline tagger described in &#167;4.1.</S>
    <S sid="77" ssid="4">For dependency parsing, our baseline is a &#8220;pipeline&#8221; parser (&#167;4.2) that infers syntax upon the output of the baseline tagger.</S>
    <S sid="78" ssid="5">The tagger is a graphical model with the WORD and TAG variables, connected by the local factors TAG-UNIGRAM, TAG-BIGRAM, and TAGCONSISTENCY, all used in the joint model (&#167;3).</S>
    <S sid="79" ssid="6">The parser has no local factors, but has the same variables as the joint model and the same features from all three families of link factors (&#167;3).</S>
    <S sid="80" ssid="7">However, since it takes as input the morphological attributes predicted by the tagger, the TAG variables are now observed.</S>
    <S sid="81" ssid="8">This leads to a change in the structure of the link factors &#8212; all features from the POSLINK factors now belong to the WORD-LINK factors, since the POS of all words are observed.</S>
    <S sid="82" ssid="9">In short, the features of the parser are a replication of (McDonald et al., 2005), but also extended beyond POS to the other morphological attributes, with the features in the MORPH-LINK factors incorporated into WORD-LINK for similar reasons.</S>
  </SECTION>
  <SECTION title="5 Experimental Set-up" number="5">
    <S sid="83" ssid="1">Our evaluation focused on the Latin Dependency Treebank (Bamman and Crane, 2006), created at the Perseus Digital Library by tailoring the Prague Dependency Treebank guidelines for the Latin language.</S>
    <S sid="84" ssid="2">It consists of excerpts from works by eight Latin authors.</S>
    <S sid="85" ssid="3">We randomly divided the 53K-word treebank into 10 folds of roughly equal sizes, with an average of 5314 words (347 sentences) per fold.</S>
    <S sid="86" ssid="4">We used one fold as the development set and performed cross-validation on the other nine.</S>
    <S sid="87" ssid="5">To measure how well our model generalizes to other highly-inflected, relatively free-word-order languages, we considered Ancient Greek, Hungarian, and Czech.</S>
    <S sid="88" ssid="6">Their respective datasets consist of 8000 sentences from the Ancient Greek Dependency Treebank (Bamman et al., 2009), 5800 from the Hungarian Szeged Dependency Treebank (Vincze et al., 2010), and a subset of 3100 from the Prague Dependency Treebank (B&#168;ohmov&#180;a et al., 2003).</S>
    <S sid="89" ssid="7">We define each factor in (1) as a log-linear function: Given an assignment A and words W, fh is an indicator function describing the presence or absence of the feature, and Bh is the corresponding set of weights learned using stochastic gradient ascent, with the gradients inferred by loopy belief propagation (Smith and Eisner, 2008).</S>
    <S sid="90" ssid="8">The variance of the Gaussian prior is set to 1.</S>
    <S sid="91" ssid="9">The other two parameters in the training process, the number of belief propagation iterations and the number of training rounds, were tuned on the development set.</S>
    <S sid="92" ssid="10">The output of the joint model is the assignment to the TAG and LINK variables.</S>
    <S sid="93" ssid="11">Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables.</S>
    <S sid="94" ssid="12">For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP.</S>
    <S sid="95" ssid="13">We produced head attachments by first calculating the posteriors of the LINK variables with BP and then passing them to an edgefactored tree decoder.</S>
    <S sid="96" ssid="14">This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008).</S>
    <S sid="97" ssid="15">This MBR decoding procedure enforces the hard constraint that the output be a tree but sums over possible morphological assignments.5 In principle, the joint model should consider every possible combination of morphological attributes for every word.</S>
    <S sid="98" ssid="16">In practice, to reduce the complexity of the model, we used a pre-existing morphological database, MORPHEUS (Crane, 1991), to constrain the range of possible values of the attributes listed in Table 2; more precisely, we add a hard constraint, requiring that assignments to the TAG variables be compatible with MORPHEUS.</S>
    <S sid="99" ssid="17">This constraint significantly reduces the value of m in the big-O notation for the number of variables and factors described in &#167;3.</S>
    <S sid="100" ssid="18">To illustrate the effect, the graphical model of the sentence in Table 1, whose six words are all covered by the database, has 1,866 factors; without the benefit of the database, the full model would have 31,901 factors.</S>
    <S sid="101" ssid="19">The MORPHEUS database was automatically generated from a list of stems, inflections, irregular forms and morphological rules.</S>
    <S sid="102" ssid="20">It covers about 99% of the distinct words in the Latin Dependency Treebank.</S>
    <S sid="103" ssid="21">At decoding time, for each fold, the database is further augmented with tags seen in training data.</S>
    <S sid="104" ssid="22">After this augmentation, an average of 44 words are &#8220;unseen&#8221; in each fold.</S>
    <S sid="105" ssid="23">Similarly, we constructed morphological dictionaries for Czech, Ancient Greek, and Hungarian from words that occurred at least five times in the training data; words that occurred fewer times were unrestricted in the morphological attributes they could take on.</S>
  </SECTION>
  <SECTION title="6 Experimental Results" number="6">
    <S sid="106" ssid="1">We compare the performance of the pipeline model (&#167;4) and the joint model (&#167;3) on morphological disambiguation and unlabeled dependency parsing.</S>
    <S sid="107" ssid="2">As seen in Table 3, the joint model outperforms6 the baseline tagger in all attributes in Latin morphological disambiguation.</S>
    <S sid="108" ssid="3">Among words not covered by the morphological database, accuracy in POS is slightly better, but lower for case, gender and number.</S>
    <S sid="109" ssid="4">The joint model made the most gains on adjectives and participles.</S>
    <S sid="110" ssid="5">Both parts-of-speech are particularly ambiguous: according to MORPHEUS, 43% of the adjectives can be interpreted as another POS, most frequently nouns; while participles have an average of 5.5 morphological interpretations.</S>
    <S sid="111" ssid="6">Both also often have identical forms for different genders, numbers and cases.</S>
    <S sid="112" ssid="7">In these situations, syntactic considerations help nudge the joint model to the correct interpretations.</S>
    <S sid="113" ssid="8">Experiments on the other three languages bear out similar results: the joint model improves morphological disambiguation.</S>
    <S sid="114" ssid="9">The performance of Czech (Table 4) exhibits the closest analogue to Latin: gender, number, and case are much less accurately predicted than are the other morphological attributes.</S>
    <S sid="115" ssid="10">Like Latin, Czech lacks definite and indefinite articles to provide high-confidence cues for noun phrase boundaries.</S>
    <S sid="116" ssid="11">The Ancient Greek treebank comprises both archaic texts, before the development of a definite article, and later classic Greek, which has a definite article; Hungarian has both a definite and an indefinite article.</S>
    <S sid="117" ssid="12">In both languages (Tables 5 and 6), noun and adjective gender, number, and case are more accurately predicted than in Czech and Latin.</S>
    <S sid="118" ssid="13">The verbal system of ancient Greek, in contrast, is more complex than that of the other languages, so mood, voice, and tense accuracy are lower.</S>
    <S sid="119" ssid="14">In addition to morphological disambiguation, we also measured the performance of the joint model on dependency parsing of Latin and the other languages.</S>
    <S sid="120" ssid="15">The baseline pipeline parser (&#167;4.2) yielded 61.00% head selection accuracy (i.e., unlabeled attachment score, UAS), outperformed7 by the joint model at 61.88%.</S>
    <S sid="121" ssid="16">The joint model showed similar improvements in Ancient Greek, Hungarian, and Czech.</S>
    <S sid="122" ssid="17">Wrong decisions made by the baseline tagger often misled the pipeline parser.</S>
    <S sid="123" ssid="18">For adjectives, the example shown in Table 1 and Figure 1 is a typical scenario, where an accusative adjective was tagged as nominative, and was then misanalyzed by the parser as modifying a verb (as a subject) rather than modifying an accusative noun.</S>
    <S sid="124" ssid="19">For participles modifying a noun, the wrong noun was often chosen based on inaccurate morphological information.</S>
    <S sid="125" ssid="20">In these cases, the joint model, entertaining all morphological possibilities, was able to find the combination of links and morphological analyses that are collectively more likely.</S>
    <S sid="126" ssid="21">The accuracy figures of our baselines are comparable, but not identical, to their counterparts reported in (Bamman and Crane, 2008).</S>
    <S sid="127" ssid="22">The differences may partially be attributed to the different morphological tagger used, and the different learning algorithm, namely Margin Infused Relaxed Algorithm (MIRA) in (McDonald et al., 2005) rather than maximum likelihood.</S>
    <S sid="128" ssid="23">More importantly, the Latin Dependency Treebank has grown from about 30K at the time of the previous work to 53K at present, resulting in significantly different training and testing material.</S>
    <S sid="129" ssid="24">Gold Pipeline Parser When given perfect morphological information, the Latin parser performs at 65.28% accuracy in head selection.</S>
    <S sid="130" ssid="25">Despite the oracle morphology, the head selection accuracy is still below other languages.</S>
    <S sid="131" ssid="26">This is hardly surprising, given the relatively small training set, and that the &#8220;the most difficult languages are those that combine a relatively free word order with a high degree of inflection&#8221;, as observed at the recent dependency parsing shared task (Nivre et al., 2007); both of these are characteristics of Latin.</S>
    <S sid="132" ssid="27">A particularly troublesome structure is coordination; the most frequent link errors all involve either a parent or a child as a conjunction.</S>
    <S sid="133" ssid="28">In a list of words, all words and coordinators depend on the final coordinator.</S>
    <S sid="134" ssid="29">Since the factors in our model consult only one link at a time, they do not sufficiently capture this kind of structures.</S>
    <S sid="135" ssid="30">Higher-order features, particularly those concerned with links with grandparents and siblings, have been shown to benefit dependency parsing (Smith and Eisner, 2008) and may be able to address this issue.</S>
  </SECTION>
  <SECTION title="7 Conclusions and Future Work" number="7">
    <S sid="136" ssid="1">We have proposed a discriminative model that jointly infers morphological properties and syntactic structures.</S>
    <S sid="137" ssid="2">In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.</S>
    <S sid="138" ssid="3">This model may be refined by incorporating richer features and improved decoding.</S>
    <S sid="139" ssid="4">In particular, we would like to experiment with higher-order features (&#167;6), and with maximum a posteriori decoding, via max-product BP or (relaxed) integer linear programming.</S>
    <S sid="140" ssid="5">Further evaluation on other morphological systems would also be desirable.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="141" ssid="1">We thank David Bamman and Gregory Crane for their feedback and support.</S>
    <S sid="142" ssid="2">Part of this research was performed by the first author while visiting Perseus Digital Library at Tufts University, under the grants A Reading Environment for Arabic and Islamic Culture, Department of Education (P017A060068-08) and The Dynamic Lexicon: Cyberinfrastructure and the Automatic Analysis of Historical Languages, National Endowment for the Humanities (PR-50013-08).</S>
    <S sid="143" ssid="3">The latter two authors were supported by Army prime contract #W911NF07-1-0216 and University of Pennsylvania subaward #103-548106; by SRI International subcontract #27001338 and ARFL prime contract #FA8750-09-C0181; and by the Center for Intelligent Information Retrieval.</S>
    <S sid="144" ssid="4">Any opinions, findings, and conclusions or recommendations expressed in this material are the authors&#8217; and do not necessarily reflect those of the sponsors.</S>
  </SECTION>
</PAPER>
