<PAPER>
  <S sid="0">Efficient parsing with Linear Context-Free Rewriting Systems</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Previous work on treebank parsing with discontinuous constituents using Linear Rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity.</S>
    <S sid="2" ssid="2">There have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible.</S>
    <S sid="3" ssid="3">Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy.</S>
    <S sid="4" ssid="4">The resulting parser has been applied to a discontinuous treebank with favorable results.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Discontinuity in constituent structures (cf. figure 1 &amp; 2) is important for a variety of reasons.</S>
    <S sid="6" ssid="2">For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997).</S>
    <S sid="7" ssid="3">Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005).</S>
    <S sid="8" ssid="4">Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and S&#248;gaard, 2008; Maier and Lichte, 2009).</S>
    <S sid="9" ssid="5">Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank (Marcus et al., 1993) annotation.</S>
    <S sid="10" ssid="6">However, the computational complexity is such that until now, the length of sentences needed to be restricted.</S>
    <S sid="11" ssid="7">In the case of Kallmeyer and Maier (2010) and Evang and Kallmeyer (2011) the limit was 25 words.</S>
    <S sid="12" ssid="8">Maier (2010) and van Cranenburgh et al. (2011) manage to parse up to 30 words with heuristics and optimizations, but no further.</S>
    <S sid="13" ssid="9">Algorithms have been suggested to binarize the grammars in such a way as to minimize parsing complexity, but the current paper shows that these techniques are not sufficient to parse longer sentences.</S>
    <S sid="14" ssid="10">Instead, this work presents a novel form of coarse-to-fine parsing which does alleviate this limitation.</S>
    <S sid="15" ssid="11">The rest of this paper is structured as follows.</S>
    <S sid="16" ssid="12">First, we introduce linear context-free rewriting systems (LCFRS).</S>
    <S sid="17" ssid="13">Next, we discuss and evaluate binarization strategies for LCFRS.</S>
    <S sid="18" ssid="14">Third, we present a technique for approximating an LCFRS by a PCFG in a coarse-to-fine framework.</S>
    <S sid="19" ssid="15">Lastly, we evaluate this technique on a large corpus without the usual length restrictions.</S>
  </SECTION>
  <SECTION title="2 Linear Context-Free Rewriting Systems" number="2">
    <S sid="20" ssid="1">Linear Context-Free Rewriting Systems (LCFRS; Vijay-Shanker et al., 1987; Weir, 1988) subsume a wide variety of mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010).</S>
    <S sid="21" ssid="2">Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009).</S>
    <S sid="22" ssid="3">Since LCFRS subsumes various synchronous grammars, they are also important for machine translation.</S>
    <S sid="23" ssid="4">This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into an LCFRS, similar to the TuLiPa system (Kallmeyer et al., 2008).</S>
    <S sid="24" ssid="5">As all mildly context-sensitive formalisms, LCFRS are parsable in polynomial time, where the degree depends on the productions of the grammar.</S>
    <S sid="25" ssid="6">Intuitively, LCFRS can be seen as a generalization of context-free grammars to rewriting other objects than just continuous strings: productions are context-free, but instead of strings they can rewrite tuples, trees or graphs.</S>
    <S sid="26" ssid="7">We focus on the use of LCFRS for parsing with discontinuous constituents.</S>
    <S sid="27" ssid="8">This follows up on recent work on parsing the discontinuous annotations in German corpora with LCFRS (Maier, 2010; van Cranenburgh et al., 2011) and work on parsing the Wall Street journal corpus in which traces have been converted to discontinuous constituents (Evang and Kallmeyer, 2011).</S>
    <S sid="28" ssid="9">In the case of parsing with discontinuous constituents a nonterminal may cover a tuple of discontinuous strings instead of a single, contiguous sequence of terminals.</S>
    <S sid="29" ssid="10">The number of components in such a tuple is called the fan-out of a rule, which is equal to the number of gaps plus one; the fan-out of the grammar is the maximum fan-out of its production.</S>
    <S sid="30" ssid="11">A context-free grammar is a LCFRS with a fan-out of 1.</S>
    <S sid="31" ssid="12">For convenience we will will use the rule notation of simple RCG (Boullier, 1998), which is a syntactic variant of LCFRS, with an arguably more transparent notation.</S>
    <S sid="32" ssid="13">A LCFRS is a tuple G = (N, T, V, P, 5).</S>
    <S sid="33" ssid="14">N is a finite set of non-terminals; a function dim : N &#8212;* N specifies the unique fan-out for every nonterminal symbol.</S>
    <S sid="34" ssid="15">T and V are disjoint finite sets of terminals and variables.</S>
    <S sid="35" ssid="16">5 is the distinguished start symbol with dim(5) = 1.</S>
    <S sid="36" ssid="17">P is a finite set of rewrite rules (productions) of the form: and &#945;i E (T U V)* for 1 &lt; i &lt; dim(Ai).</S>
    <S sid="37" ssid="18">Productions must be linear: if a variable occurs in a rule, it occurs exactly once on the left hand side (LHS), and exactly once on the right hand side (RHS).</S>
    <S sid="38" ssid="19">A rule is ordered if for any two variables X1 and X2 occurring in a non-terminal on the RHS, X1 precedes X2 on the LHS iff X1 precedes X2 on the RHS.</S>
    <S sid="39" ssid="20">Every production has a fan-out determined by the fan-out of the non-terminal symbol on the lefthand side.</S>
    <S sid="40" ssid="21">Apart from the fan-out productions also have a rank: the number of non-terminals on the right-hand side.</S>
    <S sid="41" ssid="22">These two variables determine the time complexity of parsing with a grammar.</S>
    <S sid="42" ssid="23">A production can be instantiated when its variables can be bound to non-overlapping spans such that for each component &#945;i of the LHS, the concatenation of its terminals and bound variables forms a contiguous span in the input, while the endpoints of each span are non-contiguous.</S>
    <S sid="43" ssid="24">As in the case of a PCFG, we can read off LCFRS productions from a treebank (Maier and S&#248;gaard, 2008), and the relative frequencies of productions form a maximum likelihood estimate, for a probabilistic LCFRS (PLCFRS), i.e., a (discontinuous) treebank grammar.</S>
    <S sid="44" ssid="25">As an example, figure 3 shows the productions extracted from the tree in figure 2.</S>
  </SECTION>
  <SECTION title="3 Binarization" number="3">
    <S sid="45" ssid="1">A probabilistic LCFRS can be parsed using a CKYlike tabular parsing algorithm (cf.</S>
    <S sid="46" ssid="2">Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011), but this requires a binarized grammar.1 Any LCFRS can be binarized.</S>
    <S sid="47" ssid="3">Crescenzi et al. (2011) state &#8220;while CFGs can always be reduced to rank two (Chomsky Normal Form), this is not the case for LCFRS with any fan-out greater than one.&#8221; However, this assertion is made under the assumption of a fixed fan-out.</S>
    <S sid="48" ssid="4">If this assumption is relaxed then it is easy to binarize either deterministically or, as will be investigated in this work, optimally with a dynamic programming approach.</S>
    <S sid="49" ssid="5">Binarizing an LCFRS may increase its fan-out, which results in an increase in asymptotic complexity.</S>
    <S sid="50" ssid="6">Consider the following production: Henceforth, we assume that non-terminals on the right-hand side are ordered by the order of their first variable on the left-hand side.</S>
    <S sid="51" ssid="7">There are two ways to binarize this production.</S>
    <S sid="52" ssid="8">The first is from left to right: This binarization introduces a production with a fan-out of 2, which could have been avoided.</S>
    <S sid="53" ssid="9">After binarization, an LCFRS can be parsed in O(|G |&#183; |w|p) time, where |G |is the size of the grammar, |w |is the length of the sentence.</S>
    <S sid="54" ssid="10">The degree p of the polynomial is the maximum parsing complexity of a rule, defined as: parsing complexity := &#981; + &#981;1 + &#981;2 (6) where &#981; is the fan-out of the left-hand side and &#981;1 and &#981;2 are the fan-outs of the right-hand side of the rule in question (Gildea, 2010).</S>
    <S sid="55" ssid="11">As Gildea (2010) shows, there is no one to one correspondence between fan-out and parsing complexity: it is possible that parsing complexity can be reduced by increasing the fan-out of a production.</S>
    <S sid="56" ssid="12">In other words, there can be a production which can be binarized with a parsing complexity that is minimal while its fan-out is sub-optimal.</S>
    <S sid="57" ssid="13">Therefore we focus on parsing complexity rather than fan-out in this work, since parsing complexity determines the actual time complexity of parsing with a grammar.</S>
    <S sid="58" ssid="14">There has been some work investigating whether the increase in complexity can be minimized effectively (G&#180;omez-Rodr&#180;&#305;guez et al., 2009; Gildea, 2010; Crescenzi et al., 2011).</S>
    <S sid="59" ssid="15">More radically, it has been suggested that the power of LCFRS should be limited to well-nested structures, which gives an asymptotic improvement in parsing time (G&#180;omez-Rodr&#180;&#305;guez et al., 2010).</S>
    <S sid="60" ssid="16">However, there is linguistic evidence that not all language use can be described in wellnested structures (Chen-Main and Joshi, 2010).</S>
    <S sid="61" ssid="17">Therefore we will use the full power of LCFRS in this work&#8212;parsing complexity is determined by the treebank, not by a priori constraints.</S>
    <S sid="62" ssid="18">Apart from optimizing for parsing complexity, for linguistic reasons it can also be useful to parse the head of a constituent first, yielding so-called head-driven binarizations (Collins, 1999).</S>
    <S sid="63" ssid="19">Additionally, such a head-driven binarization can be &#8216;Markovized&#8217;&#8211;i.e., the resulting production can be constrained to apply to a limited amount of horizontal context as opposed to the full context in the original constituent (e.g., Klein and Manning, 2003), which can have a beneficial effect on accuracy.</S>
    <S sid="64" ssid="20">In the notation of Klein and Manning (2003) there are two Markovization parameters: h and v. The first parameter describes the amount of horizontal context for the artificial labels of a binarized production.</S>
    <S sid="65" ssid="21">In a normal form binarization, this parameter equals infinity, because the binarized production should only apply in the exact same context as the context in which it originally belongs, as otherwise the set of strings accepted by the grammar would be affected.</S>
    <S sid="66" ssid="22">An artificial label will have the form XA&#65533;B&#65533;C for a binarized production of a constituent X that has covered children A, B, and C of X.</S>
    <S sid="67" ssid="23">The other extreme, h = 1, enables generalizations by stringing parts of binarized constituents together, as long as they share one non-terminal.</S>
    <S sid="68" ssid="24">In the previous example, the label would become just XA, i.e., the presence of B and C would no longer be required, which enables switching to any binarized production that has covered A as the last node.</S>
    <S sid="69" ssid="25">Limiting the amount of horizontal context on which a production is conditioned is important when the treebank contains many unique constituents which can only be parsed by stringing together different binarized productions; in other words, it is a way of dealing with the data sparseness about n-ary productions in the treebank.</S>
    <S sid="70" ssid="26">The second parameter describes parent annotation, which will not be investigated in this work; the default value is v = 1 which implies only including the immediate parent of the constituent that is being binarized; including grandparents is a way of weakening independence assumptions.</S>
    <S sid="71" ssid="27">Crescenzi et al. (2011) also remark that an optimal head-driven binarization allows for Markovization.</S>
    <S sid="72" ssid="28">However, it is questionable whether such a binarization is worthy of the name Markovization, as the non-terminals are not introduced deterministically from left to right, but in an arbitrary fashion dictated by concerns of parsing complexity; as such there is not a Markov process based on a meaningful (e.g., temporal) ordering and there is no probabilistic interpretation of Markovization in such a setting.</S>
    <S sid="73" ssid="29">To summarize, we have at least four binarization strategies (cf. figure 4 for an illustration): tion with horizontal Markovization.</S>
    <S sid="74" ssid="30">Minimizes parsing complexity.</S>
    <S sid="75" ssid="31">Introduced in and proven to be NP-hard by Crescenzi et al. (2011).</S>
    <S sid="76" ssid="32">An issue with the minimal binarizations is that the algorithm for finding them has a high computational complexity, and has not been evaluated empirically on treebank data.2 Empirical investigation is interesting for two reasons.</S>
    <S sid="77" ssid="33">First of all, the high computational complexity may not be relevant with constant factors of constituents, which can reasonably be expected to be relatively small.</S>
    <S sid="78" ssid="34">Second, it is important to establish whether an asymptotic improvement is actually obtained through optimal binarizations, and whether this translates to an improvement in practice.</S>
    <S sid="79" ssid="35">Gildea (2010) presents a general algorithm to binarize an LCFRS while minimizing a given scoring function.</S>
    <S sid="80" ssid="36">We will use this algorithm with two different scoring functions.</S>
    <S sid="81" ssid="37">The first directly optimizes parsing complexity.</S>
    <S sid="82" ssid="38">Given a (partially) binarized constituent c, the function returns a tuple of scores, for which a linear order is defined by comparing elements starting from the most significant (left-most) element.</S>
    <S sid="83" ssid="39">The tuples contain the parsing complexity p, and the fan-out &#981; to break ties in parsing complexity; if there are still ties after considering the fan-out, the sum of the parsing complexities of the subtrees of c is considered, which will give preference to a binarization where the worst case complexity occurs once instead of twice.</S>
    <S sid="84" ssid="40">The formula is then: The second function is the similar except that only head-driven strategies are accepted.</S>
    <S sid="85" ssid="41">A headdriven strategy is a binarization in which the head is introduced first, after which the rest of the children are introduced one at a time.</S>
    <S sid="86" ssid="42">Given a (partial) binarization c, the score should reflect the maximum complexity and fan-out in that binarization, to optimize for the worst case, as well as the sum, to optimize the average case.</S>
    <S sid="87" ssid="43">This aspect appears to be glossed over by Gildea (2010).</S>
    <S sid="88" ssid="44">Considering only the score of the last production in a binarization produces suboptimal binarizations.</S>
    <S sid="89" ssid="45">As data we use version 2 of the Negra (Skut et al., 1997) treebank, with the common training, development and test splits (Dubey and Keller, 2003).</S>
    <S sid="90" ssid="46">Following common practice, punctuation, which is left out of the phrase-structure in Negra, is reattached to the nearest constituent.</S>
    <S sid="91" ssid="47">In the course of experiments it was discovered that the heuristic method for punctuation attachment used in previous work (e.g., Maier, 2010; van Cranenburgh et al., 2011), as implemented in rparse,3 introduces additional discontinuity.</S>
    <S sid="92" ssid="48">We applied a slightly different heuristic: punctuation is attached to the highest constituent that contains a neighbor to its right.</S>
    <S sid="93" ssid="49">The result is that punctuation can be introduced into the phrase-structure without any additional discontinuity, and thus without artificially inflating the fan-out and complexity of grammars read off from the treebank.</S>
    <S sid="94" ssid="50">This new heuristic provides a significant improvement: instead of a fan-out of 9 and a parsing complexity of 19, we obtain values of 4 and 9 respectively.</S>
    <S sid="95" ssid="51">The parser is presented with the gold part-ofspeech tags from the corpus.</S>
    <S sid="96" ssid="52">For reasons of efficiency we restrict sentences to 25 words (including punctuation) in this experiment: NEGRA-25.</S>
    <S sid="97" ssid="53">A grammar was read off from the training part of NEGRA-25, and sentences of up to 25 words in the development set were parsed using the resulting PLCFRS, using the different binarization schemes.</S>
    <S sid="98" ssid="54">First with aright-branching, right-to-left binarization, and second with the minimal binarization according to parsing complexity andfanout.</S>
    <S sid="99" ssid="55">The last two binarizations are head-driven and Markovized&#8212;the first straightforwardly from left-to-right, the latter optimized for minimal parsing complexity.</S>
    <S sid="100" ssid="56">With Markovization we are forced to add a level of parent annotation to tame the increase in productivity caused by h = 1.</S>
    <S sid="101" ssid="57">The distribution of parsing complexity (measured with eq.</S>
    <S sid="102" ssid="58">6) in the grammars with different binarization strategies is shown in figure 5 and 6.</S>
    <S sid="103" ssid="59">Although the optimal binarizations do seem to have some effect on the distribution of parsing complexities, it remains to be seen whether this can be cashed out as a performance improvement in practice.</S>
    <S sid="104" ssid="60">To this end, we also parse using the binarized grammars.</S>
    <S sid="105" ssid="61">In this work we binarize and parse with disco-dop introduced in van Cranenburgh et al. (2011).4 In this experiment we report scores of the (exact) Viterbi derivations of a treebank PLCFRS; cf. table 1 for the results.</S>
    <S sid="106" ssid="62">Times represent CPU time (single core); accuracy is given with a generalization of PARSEVAL to discontinuous structures, described in Maier (2010).</S>
    <S sid="107" ssid="63">Instead of using Maier&#8217;s implementation of discontinuous Fi scores in rparse, we employ a variant that ignores (a) punctuation, and (b) the root node of each tree.</S>
    <S sid="108" ssid="64">This makes our evaluation incomparable to previous results on discontinuous parsing, but brings it in line with common practice on the Wall street journal benchmark.</S>
    <S sid="109" ssid="65">Note that this change yields scores about 2 or 3 percentage points lower than those of rparse.</S>
    <S sid="110" ssid="66">Despite the fact that obtaining optimal binarizations is exponential (Gildea, 2010) and NPhard (Crescenzi et al., 2011), they can be computed relatively quickly on this data set.5 Importantly, in the first case there is no improvement on fan-out or parsing complexity, while in the head-driven case there is a minimal improvement because of a single production with parsing complexity 15 without optimal binarization.</S>
    <S sid="111" ssid="67">On the other hand, the optimal binarizations might still have a significant effect on the average case complexity, rather than the worst-case complexities.</S>
    <S sid="112" ssid="68">Indeed, in both cases parsing with the optimal grammar is faster; in the first case, however, when the time for binarization is considered as well, this advantage mostly disappears.</S>
    <S sid="113" ssid="69">The difference in Fi scores might relate to the efficacy of Markovization in the binarizations.</S>
    <S sid="114" ssid="70">It should be noted that it makes little theoretical sense to &#8216;Markovize&#8217; a binarization when it is not a left-to-right or right-to-left binarization, because with an optimal binarization the non-terminals of a constituent are introduced in an arbitrary order.</S>
    <S sid="115" ssid="71">More importantly, in our experiments, these techniques of optimal binarizations did not scale to longer sentences.</S>
    <S sid="116" ssid="72">While it is possible to obtain an optimal binarization of the unrestricted Negra corpus, parsing long sentences with the resulting grammar remains infeasible.</S>
    <S sid="117" ssid="73">Therefore we need to look at other techniques for parsing longer sentences.</S>
    <S sid="118" ssid="74">We will stick with the straightforward head-driven, head-outward binarization strategy, despite this being a computationally sub-optimal binarization.</S>
    <S sid="119" ssid="75">One technique for efficient parsing of LCFRS is the use of context-summary estimates (Kallmeyer and Maier, 2010), as part of a best-first parsing algorithm.</S>
    <S sid="120" ssid="76">This allowed Maier (2010) to parse sentences of up to 30 words.</S>
    <S sid="121" ssid="77">However, the calculation of these estimates is not feasible for longer sentences and large grammars (van Cranenburgh et al., 2011).</S>
    <S sid="122" ssid="78">Another strategy is to perform an online approximation of the sentence to be parsed, after which parsing with the LCFRS can be pruned effectively.</S>
    <S sid="123" ssid="79">This is the strategy that will be explored in the current work.</S>
  </SECTION>
  <SECTION title="4 Context-free grammar approximation for coarse-to-fine parsing" number="4">
    <S sid="124" ssid="1">Coarse-to-fine parsing (Charniak et al., 2006) is a technique to speed up parsing by exploiting the information that can be gained from parsing with simpler, coarser grammars&#8212;e.g., a grammar with a smaller set of labels on which the original grammar can be projected.</S>
    <S sid="125" ssid="2">Constituents that do not contribute to a full parse tree with a coarse grammar can be ruled out for finer grammars as well, which greatly reduces the number of edges that need to be explored.</S>
    <S sid="126" ssid="3">However, by changing just the labels only the grammar constant is affected.</S>
    <S sid="127" ssid="4">With discontinuous treebank parsing the asymptotic complexity of the grammar also plays a major role.</S>
    <S sid="128" ssid="5">Therefore we suggest to parse not just with a coarser grammar, but with a coarser grammar formalism, following a suggestion in van Cranenburgh et al. (2011).</S>
    <S sid="129" ssid="6">This idea is inspired by the work of Barth&#180;elemy et al. (2001), who apply it in a non-probabilistic setting where the coarse grammar acts as a guide to the non-deterministic choices of the fine grammar.</S>
    <S sid="130" ssid="7">Within the coarse-to-fine approach the technique becomes a matter of pruning with some probabilistic threshold.</S>
    <S sid="131" ssid="8">Instead of using the coarse grammar only as a guide to solve non-deterministic choices, we apply it as a pruning step which also discards the most suboptimal parses.</S>
    <S sid="132" ssid="9">The basic idea is to extract a grammar that defines a superset of the language we want to parse, but with a fanout of 1.</S>
    <S sid="133" ssid="10">More concretely, a context-free grammar can be read off from discontinuous trees that have been transformed to context-free trees by the procedure introduced in Boyd (2007).</S>
    <S sid="134" ssid="11">Each discontinuous node is split into a set of new nodes, one for each component; for example a node NP2 will be split into two nodes labeled NP*1 and NP*2 (like Barth&#180;elemy et al., we mark components with an index to reduce overgeneration).</S>
    <S sid="135" ssid="12">Because Boyd&#8217;s transformation is reversible, chart items from this grammar can be converted back to discontinuous chart items, and can guide parsing of an LCFRS.</S>
    <S sid="136" ssid="13">This guiding takes the form of a white list.</S>
    <S sid="137" ssid="14">After parsing with the coarse grammar, the resulting chart is pruned by removing all items that fail to meet a certain criterion.</S>
    <S sid="138" ssid="15">In our case this is whether a chart item is part of one of the k-best derivations&#8212;we use k = 50 in all experiments (as in van Cranenburgh et al., 2011).</S>
    <S sid="139" ssid="16">This has similar effects as removing items below a threshold of marginalized posterior probability; however, the latter strategy requires computation of outside probabilities from a parse forest, which is more involved with an LCFRS than with a PCFG.</S>
    <S sid="140" ssid="17">When parsing with the fine grammar, whenever a new item is derived, the white list is consulted to see whether this item is allowed to be used in further derivations; otherwise it is immediately discarded.</S>
    <S sid="141" ssid="18">This coarse-to-fine approach will be referred to as CFG-CTF, and the transformed, coarse grammar will be referred to as a split-PCFG.</S>
    <S sid="142" ssid="19">Splitting discontinuous nodes for the coarse grammar introduces new nodes, so obviously we need to binarize after this transformation.</S>
    <S sid="143" ssid="20">On the other hand, the coarse-to-fine approach requires a mapping between the grammars, so after reversing the transformation of splitting nodes, the resulting discontinuous trees must be binarized (and optionally Markovized) in the same manner as those on which the fine grammar is based.</S>
    <S sid="144" ssid="21">To resolve this tension we elect to binarize twice.</S>
    <S sid="145" ssid="22">The first time is before splitting discontinuous nodes, and this is where we introduce Markovization.</S>
    <S sid="146" ssid="23">This same binarization will be used for the fine grammar as well, which ensures the models make the same kind of generalizations.</S>
    <S sid="147" ssid="24">The second binarization is after splitting nodes, this time with a binary normal form (2NF; all productions are either unary, binary, or lexical).</S>
    <S sid="148" ssid="25">Parsing with this grammar proceeds as follows.</S>
    <S sid="149" ssid="26">After obtaining an exhaustive chart from the coarse stage, the chart is pruned so as to only contain items occurring in the k-best derivations.</S>
    <S sid="150" ssid="27">When parsing in the fine stage, each new item is looked up in this pruned coarse chart, with multiple lookups if the item is discontinuous (one for each component).</S>
    <S sid="151" ssid="28">To summarize, the transformation happens in four steps (cf. figure 7 for an illustration):</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="152" ssid="1">We evaluate on Negra with the same setup as in section 3.3.</S>
    <S sid="153" ssid="2">We report discontinuous Fi scores as well as exact match scores.</S>
    <S sid="154" ssid="3">For previous results on discontinuous parsing with Negra, see table 3.</S>
    <S sid="155" ssid="4">For results with the CFG-CTF method see table 4.</S>
    <S sid="156" ssid="5">We first establish the viability of the CFG-CTF method on NEGRA-25, with a head-driven v = 1, h = 2 binarization, and reporting again the scores of the exact Viterbi derivations from a treebank PLCFRS versus a PCFG using our transformations.</S>
    <S sid="157" ssid="6">Figure 8 compares the parsing times of LCFRS with and without the new CFG-CTF method.</S>
    <S sid="158" ssid="7">The graph shows a steep incline for parsing with LCFRS directly, which makes it infeasible to parse longer sentences, while the CFG-CTF method is faster for sentences of length &gt; 22 despite its overhead of parsing twice.</S>
    <S sid="159" ssid="8">The second experiment demonstrates the CFGCTF technique on longer sentences.</S>
    <S sid="160" ssid="9">We restrict the length of sentences in the training, development and test corpora to 40 words: NEGRA-40.</S>
    <S sid="161" ssid="10">As a first step we apply the CFG-CTF technique to parse with a PLCFRS as the fine grammar, pruning away all items not occurring in the 10,000 best derivations from the PCFG chart.</S>
    <S sid="162" ssid="11">The result shows that the PLCFRS gives a slight improvement over the split-pcfg, which accords with the observation that the latter makes stronger independence assumptions in the case of discontinuity.</S>
    <S sid="163" ssid="12">In the next experiments we turn to an allfragments grammar encoded in a PLCFRS using Goodman&#8217;s (2003) reduction, to realize a (discontinuous) Data-Oriented Parsing (DOP; Scha, 1990) model&#8212;which goes by the name of DiscoDOP (van Cranenburgh et al., 2011).</S>
    <S sid="164" ssid="13">This provides an effective yet conceptually simple method to weaken the independence assumptions of treebank grammars.</S>
    <S sid="165" ssid="14">Table 2 gives statistics on the grammars, including the parsing complexities.</S>
    <S sid="166" ssid="15">The fine grammar has a parsing complexity of 9, which means that parsing with this grammar has complexity O(|w|9).</S>
    <S sid="167" ssid="16">We use the same parameters as van Cranenburgh et al. (2011), except that unlike van Cranenburgh et al., we can use v = 1, h = 1 Markovization, in order to obtain a higher coverage.</S>
    <S sid="168" ssid="17">The DOP grammar is added as a third stage in the coarse-to-fine pipeline.</S>
    <S sid="169" ssid="18">This gave slightly better results than substituting the the DOP grammar for the PLCFRS stage.</S>
    <S sid="170" ssid="19">Parsing with NEGRA-40 took about 11 hours and 4 GB of memory.</S>
    <S sid="171" ssid="20">The same model from NEGRA-40 can also be used to parse the full development set, without length restrictions, establishing that the CFG-CTF method effectively eliminates any limitation of length for parsing with LCFRS.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="172" ssid="1">Our results show that optimal binarizations are clearly not the answer to parsing LCFRS efficiently, as they do not significantly reduce parsing complexity in our experiments.</S>
    <S sid="173" ssid="2">While they provide some efficiency gains, they do not help with the main problem of longer sentences.</S>
    <S sid="174" ssid="3">We have presented a new technique for largescale parsing with LCFRS, which makes it possible to parse sentences of any length, with favorable accuracies.</S>
    <S sid="175" ssid="4">The availability of this technique may lead to a wider acceptance of LCFRS as a syntactic backbone in computational linguistics.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="176" ssid="1">I am grateful to Willem Zuidema, Remko Scha, Rens Bod, and three anonymous reviewers for comments.</S>
  </SECTION>
</PAPER>
