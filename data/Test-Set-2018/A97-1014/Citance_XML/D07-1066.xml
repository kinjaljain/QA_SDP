<PAPER>
	<S sid="0">Treebank Annotation Schemes and Parser Evaluation for German</S><ABSTRACT>
		<S sid="1" ssid="1">Recent studies focussed on the question whether less-configurational languages like German are harder to parse than English, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by Ku?bler et al(2006).</S>
		<S sid="2" ssid="2">This claim is based on the assumption that PARSEVAL metrics fully re flect parse quality across treebank encodingschemes.</S>
		<S sid="3" ssid="3">In this paper we present new ex periments to test this claim.</S>
		<S sid="4" ssid="4">We use thePARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measur ing the effect of controlled error insertion on treebank trees and parser output.</S>
		<S sid="5" ssid="5">Wealso provide extensive past-parsing crosstreebank conversion.</S>
		<S sid="6" ssid="6">The results of the ex periments show that, contrary to Ku?bler etal.</S>
		<S sid="7" ssid="7">(2006), the question whether or not Ger man is harder to parse than English remains undecided.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="8" ssid="8">A long-standing and unresolved issue in the pars ing literature is whether parsing less-configurationallanguages is harder than e.g. parsing English.</S>
			<S sid="9" ssid="9">Ger man is a case in point.</S>
			<S sid="10" ssid="10">Results from Dubey and Keller (2003) suggest that state-of-the-art parsingscores for German are generally lower than those ob tained for English, while recent results from Ku?bler et al (2006) raise the possibility that this might be an artefact of particular encoding schemes and data structures of treebanks, which serve as training resources for probabilistic parsers.</S>
			<S sid="11" ssid="11">Ku?bler (2005) and Maier (2006) show that treebank annotationschemes have considerable influence on parsing results.</S>
			<S sid="12" ssid="12">A comparison of unlexicalised PCFG parsing (Ku?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu?Ba D/Z (Telljohann et al, 2004) treebanks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991).</S>
			<S sid="13" ssid="13">Ku?bler et al (2006) conclude that, contrary to what had been assumed, German is not actually harder to parse than English, but that theNEGRA annotation scheme does not support opti mal PCFG parsing performance.</S>
			<S sid="14" ssid="14">Despite being the standard metric for measuring PCFG parser performance, PARSEVAL has been criticised for not representing ?real?</S>
			<S sid="15" ssid="15">parser quality (Carroll et al, 1998; Brisco et al, 2002; Sampson and Babarbczy, 2003).</S>
			<S sid="16" ssid="16">PARSEVAL checks label and wordspan identity in parser output compared to the original treebank trees.</S>
			<S sid="17" ssid="17">It neither weights results, differentiating between linguistically more or less severe errors, nor does it give credit to constituents where the syntactic categories have been recognised correctly but the phrase boundary is slightly wrong.</S>
			<S sid="18" ssid="18">With this in mind, we question the assumptionthat the PARSEVAL results for NEGRA and Tu?Ba D/Z reflect a real difference in quality between the parser output for parsers trained on the two different treebanks.</S>
			<S sid="19" ssid="19">As a consequence we also question the conclusion that PARSEVAL results for German in the same range as the parsing results for the English 630 Penn-II Treebank prove that German is not harder to parse than the more configurational English.</S>
			<S sid="20" ssid="20">To investigate this issue we present experiments on the German TIGER treebank (Dipper et al, 2001) andthe Tu?Ba-D/Z treebank.</S>
			<S sid="21" ssid="21">TIGER is based on and ex tends the NEGRA data and annotation scheme.</S>
			<S sid="22" ssid="22">Our error insertion and past-parsing treebank-encoding experiments experiments show that the differences in parsing results for the two treebanks are not caused by a higher number of errors in the output of the parser trained on the TIGER treebank, but aredue to the bias of the PARSEVAL metric towards an notation schemes (such as that of Tu?Ba-D/Z) with ahigher ratio of non-terminal/terminal nodes.</S>
			<S sid="23" ssid="23">The ex periments also show that compared to PARSEVALthe Leaf-Ancestor metric is somewhat less susceptible to non-terminal/terminal ratios and that con trary to the PARSEVAL results, dependency-based evaluations score TIGER trained parsers higher than Tu?Ba-D/Z trained parsers.</S>
			<S sid="24" ssid="24">This paper is structured as follows: Section 2 gives an overview of the main features of the two treebanks.</S>
			<S sid="25" ssid="25">Section 3 describes our first experiment, where we systematically insert controlled errors into the original treebank trees and compare the influence of these modifications on the evaluation results inthe PARSEVAL metric and the Leaf-Ancestor met ric against the original, unmodified trees for bothtreebanks.</S>
			<S sid="26" ssid="26">In Section 4 we present the second ex periment, where we extract an unlexicalised PCFGfrom each of the treebanks.</S>
			<S sid="27" ssid="27">Then we convert the out put of the PCFG parser trained on the Tu?Ba-D/Z into a TIGER-style format and evaluate the converted trees.</S>
			<S sid="28" ssid="28">In Section 5 we present a dependency-based evaluation and compare the results to the results of the two other measures.</S>
			<S sid="29" ssid="29">The last section concludes.</S>
	</SECTION>
	<SECTION title="The TIGER Treebank and the Tu?Ba-D/Z. " number="2">
			<S sid="30" ssid="1">The two German treebanks used in our experimentsare the TIGER Treebank (Release 2) and the Tu?baD/Z (Release 2).</S>
			<S sid="31" ssid="2">The Tu?Ba-D/Z consists of approximately 22 000 sentences, while the TIGER Treebank is much larger with more than 50 000 sen tences.</S>
			<S sid="32" ssid="3">Both treebanks contain German newspapertext and are annotated with phrase structure and de pendency (functional) information.</S>
			<S sid="33" ssid="4">Both treebanks use the Stuttgart Tu?bingen POS Tag Set (Schiller et al, 95).</S>
			<S sid="34" ssid="5">TIGER uses 49 different grammatical function labels, while the Tu?Ba-D/Z utilises only 36 function labels.</S>
			<S sid="35" ssid="6">For the encoding of phrasalnode categories the Tu?Ba-D/Z uses 30 different categories, the TIGER Treebank uses a set of 27 cate gory labels.Other major differences between the two treebanks are: in the Tiger Treebank long distance de pendencies are expressed through crossing branches(Figure 1), while in the Tu?Ba-D/Z the same phenomenon is expressed with the help of grammati cal function labels (Figure 2), where the node labelV-MOD encodes the information that the PP modifies the verb.</S>
			<S sid="36" ssid="7">The annotation in the Tiger Tree bank is rather flat and allows no unary branching, whereas the nodes in the Tu?Ba-D/Z do contain unary branches and a more hierarchical structure, resulting in a much deeper tree structure than the trees in the Tiger Treebank.</S>
			<S sid="37" ssid="8">This results in an average highernumber of nodes per sentence for the Tu?Ba-D/Z. Ta ble 1 shows the differences in the ratio of nodes for the Tiger treebank and the Tu?Ba-D/Z. phrasal phrasal words nodes/sent nodes/word /sent TIGER 8.29 0.47 17.60 Tu?Ba-D/Z 20.69 1.20 17.27 Table 1: Average number of phrasal nodes/words in TIGER and Tu?Ba-D/ZFigures 1 and 2 also illustrate the different annota tion of PPs in both annotation schemes.</S>
			<S sid="38" ssid="9">In the Tiger treebank the internal structure of the PP is flat andthe adjective and noun inside the PP are directly attached to the PP, while the Tu?Ba-D/Z is more hier archical and inserts an additional NP node.</S>
			<S sid="39" ssid="10">Another major difference is the annotation of topological fields in the style of Drach (1937) and Ho?hle (1986) in the Tu?Ba-D/Z. The model captures German word order, which accepts three possible sentence configurations (verb first, verb second and verb last), by providing fields like the initial field (VF), the middle field (MF) and the final field (NF).</S>
			<S sid="40" ssid="11">The fields are positioned relative to the verb, which can fill in the left (LK) or the right sentence bracket(VC).</S>
			<S sid="41" ssid="12">The ordering of topological fields is deter mined by syntactic constraints.</S>
			<S sid="42" ssid="13">631 Auch mit staatlichen Auftr?agen sieht es schlecht aus.</S>
			<S sid="43" ssid="14">?It also looks bad for public contracts.?</S>
			<S sid="44" ssid="15">Figure 1: TIGER treebank tree In Wales sieht es besser aus.</S>
			<S sid="45" ssid="16">?Things seem better in Wales.?</S>
			<S sid="46" ssid="17">Figure 2: Tu?Ba-D/Z treebank tree 2.1 Differences between TIGER and NEGRA.</S>
			<S sid="47" ssid="18">To date, most PCFG parsing for German hasbeen done using the NEGRA corpus as a train ing resource.</S>
			<S sid="48" ssid="19">The flat annotation scheme of theTIGER treebank is based on the NEGRA annotation scheme, but it also employs some impor tant extensions, which include the annotation ofverb-subcategorisation, appositions and parenthe ses, coordinations and the encoding of proper nouns (Brants et al, 2002).</S>
	</SECTION>
	<SECTION title="Treebank Preprocessing: Converting. " number="3">
			<S sid="49" ssid="1">TIGER Graphs into CFG TreesThe sentences in the TIGER treebank are repre sented as graphs with LDDs expressed throughcrossing branches.</S>
			<S sid="50" ssid="2">Before being able to insert errors or extract a PCFG we had to resolve these cross ing branches in the TIGER treebank.</S>
			<S sid="51" ssid="3">This was done by attaching the non-head child nodes higher up in the tree, following Ku?bler (2006).</S>
			<S sid="52" ssid="4">For the graph in Figure 1 this would mean that the modifying PP ?Auch mit staatlichen Auftra?gen?</S>
			<S sid="53" ssid="5">(also for public contracts) was attached directly to the S node, while the head of the adjectival phrase (AP) remained in it?s original position.</S>
			<S sid="54" ssid="6">As a side effect this leads to the creation of some unary nodes in the TIGER trees.</S>
			<S sid="55" ssid="7">We also inserted a virtual root node and removed all functional labels from the TIGER and Tu?Ba-D/Z trees.</S>
	</SECTION>
	<SECTION title="Experiment I. " number="4">
			<S sid="56" ssid="1">Experiment I is designed to assess the impact of identical errors on the two treebank encoding schemes and the PARSEVAL1 and Leaf-Ancestor evaluation metrics.</S>
			<S sid="57" ssid="2">4.1 Experimental Setup.</S>
			<S sid="58" ssid="3">The TIGER treebank and the Tu?Ba-D/Z both con tain newspaper text, but from different German newspapers.</S>
			<S sid="59" ssid="4">To support a meaningful comparison we have to compare similar sentences from bothtreebanks.</S>
			<S sid="60" ssid="5">In order to control for similarity we se lected all sentences of length 10 ? n ? 40 from both treebanks.</S>
			<S sid="61" ssid="6">For all sentences with equal length we computed the average number of prepositions, determiners, nouns (and related POS such as propernames and personal pronouns), interrogative pro nouns, finite verbs, infinite verbs, past participles and imperative verb forms.</S>
			<S sid="62" ssid="7">For each sentence length we selected all sentences from both treebanks which showed an average for each of the POS listed abovewhich did not deviate more than 0.8 from the av erage for all sentences for this particular sentence length.</S>
			<S sid="63" ssid="8">From this set we randomly selected 1024 sentences for each of the treebanks.</S>
			<S sid="64" ssid="9">This results in two test sets, comparable in word length, syntactic structure and complexity.</S>
			<S sid="65" ssid="10">Table 2 shows the ratio of phrasal versus terminal nodes in the test sets.We then inserted different types of controlled er rors automatically into the original treebank trees in our test sets and evaluated the modified trees against 1In all our experiments we use the evalb metric (Sekine and Collins, 1997), the most commonly used implementation of the PARSEVAL metric.</S>
			<S sid="66" ssid="11">632 phrasal phrasal nodes words nodes/sent nodes/word /sent TIGER 6.97 0.48 14.49 Tu?Ba-D/Z 19.18 1.30 14.75 Table 2: Average number of phrasal nodes/words in the TIGER and Tu?Ba-D/Z test setthe original treebank trees, in order to assess the impact of similar (controlled for type and number) er rors on the two encoding schemes.</S>
			<S sid="67" ssid="12">4.2 Error Insertion.</S>
			<S sid="68" ssid="13">The errors fall into three types: attachment, span and labeling (Table 3).</S>
			<S sid="69" ssid="14">We carried out the same number of error insertions in both test sets.</S>
			<S sid="70" ssid="15">Error description ATTACH I Attach PPs inside an NP one level higher up in the tree ATTACH II Change verb attachment to noun attachment for PPs on sentence level, inside a VP or in the MF (middle field) LABEL I Change labels of PPs to NP LABEL II Change labels of VPs to PP SPAN I Include adverb to the left of a PP into the PP SPAN II Include NN to the left of a PP into the PP SPAN III Combination of SPANI and SPANII Table 3: Description of inserted error types 4.3 Results for Error Insertion for the Original.</S>
			<S sid="71" ssid="16">Treebank Trees Table 4 shows the impact of the error insertion into the original treebank trees on PARSEVAL results, evaluated against the gold trees.</S>
			<S sid="72" ssid="17">PARSEVAL resultsin all experiments report labelled precision and re call.</S>
			<S sid="73" ssid="18">The first error (PP attachment I, 85 insertions in each test set) leads to a decrease in f-score of 1.16 for the TIGER test set, while for the Tu?Ba-D/Z test set the same error only caused a decrease of 0.43.</S>
			<S sid="74" ssid="19">The effect remains the same for all error types andis most pronounced for the category label errors, be cause the frequency of the labels resulted in a large number of substitutions.</S>
			<S sid="75" ssid="20">The last row lists the total weighted average for all error types, weighted with respect to their frequency of occurrence in the test sets.</S>
			<S sid="76" ssid="21">Table 4 clearly shows that the PARSEVAL measure punishes the TIGER treebank annotation TIGER Tu?Ba # errors PP attachment I 98.84 99.57 85 PP attachment II 98.75 99.55 89 Label I 80.02 92.73 1427 Label II 93.00 97.45 500 SPAN I 99.01 99.64 71 SPAN II 97.47 99.08 181 SPAN III 96.51 98.73 252 total weighted ave. 87.09 95.30Table 4: f-score for PARSEVAL results for error in sertion in the original treebank treesscheme to a greater extent, while the same num ber and type of errors in the Tu?Ba-D/Z annotation scheme does not have an equally strong effect on PARSEVAL results for similar sentences.</S>
			<S sid="77" ssid="22">4.4 Discussion: PARSEVAL and LA. Experiment I shows that the gap between the PAR SEVAL results for the two annotation schemes does not reflect a difference in quality between the trees.</S>
			<S sid="78" ssid="23">Both test sets contain the same number of sentences with the same sentence length and are equivalent in complexity and structure.</S>
			<S sid="79" ssid="24">They contain the same number and type of errors.</S>
			<S sid="80" ssid="25">This suggests that the difference between the results for the TIGER and the Tu?Ba-D/Z test set are due to the higher ratio of non-terminal/terminal nodes in the Tu?Ba-D/Z trees (Table 1).</S>
			<S sid="81" ssid="26">In order to obtain an alternative view on the quality of our annotation schemes we used the leaf-ancestor (LA) metric (Sampson and Babarbczy, 2003), a parser evaluation metric which measures the similarity of the path from each terminal nodein the parse tree to the root node.</S>
			<S sid="82" ssid="27">The path consists of the sequence of node labels between the ter minal node and the root node, and the similarity oftwo paths is calculated by using the Levenshtein dis tance (Levenshtein, 1966).</S>
			<S sid="83" ssid="28">Table 5 shows the results for the leaf-ancestor evaluation metric for our errorinsertion test sets.</S>
			<S sid="84" ssid="29">Here the weighted average re sults for the two test sets are much closer to each other (94.98 vs. 97.18 as against 87.09 vs. 95.30).</S>
			<S sid="85" ssid="30">Only the label errors, due to the large numbers, show a significant difference between the two annotation schemes.</S>
			<S sid="86" ssid="31">Tables 4 and 5 show that compared toPARSEVAL the LA metric is somewhat less sensi tive to the nonterminal/terminal ratio.</S>
			<S sid="87" ssid="32">Figure 3 illustrates the different behaviour of the 633 TIGER Tu?Ba # errors PP attachment I 99.62 99.70 85 PP attachment II 99.66 99.78 89 Label I 92.45 95.24 1427 Label II 96.05 99.28 500 SPAN I 99.82 99.84 71 SPAN II 99.51 99.77 181 SPAN III 99.34 99.62 252 total weighted ave. 94.98 97.18 Table 5: LA results for error insertion in the original treebank trees two evaluation metrics with respect to an example sentence.</S>
			<S sid="88" ssid="33">Sentence 9: Die Stadtverwaltung von Venedig hat erstmals streunende Katzen gez?ahlt.?For the first time the city council of Venice has counted stray ing cats.?</S>
			<S sid="89" ssid="34">(TOP (S (NP (ART Die [the] ) (NN Stadtverwaltung [city counsil] ) (PP (APPR von [of] ) (NE Venedig [Venice] ) ) ) (VAFIN hat [has] ) (VP (ADV erstmals [for the first time] ) (NP (ADJA streunende [straying] ) (NN Katzen [cats] ) ) (VVPP geza?hlt [counted] ) ) ) ($.</S>
			<S sid="90" ssid="35">) Figure 3: Sentence 9 from the TIGER Test Set Table 6 shows that all error types inserted intoSentence 9 in our test set result in the same eval uation score for the PARSEVAL metric, while the LA metric provides a more discriminative treatment of PP attachment errors, label errors and span errorsfor the same sentence (Table 6).</S>
			<S sid="91" ssid="36">However, the dif ferences in the LA results are only indirectly caused by the different error types.</S>
			<S sid="92" ssid="37">They actually reflect the number of terminal nodes affected by the error insertion.</S>
			<S sid="93" ssid="38">For Label I and II the LA results vary considerably, because the substitution of the PP for an NP (Label I) in Figure 3 affects two terminal nodes only (PP von [of] Venedig [Venice]), while the change of the VP into a PP (Label II) alters the paths of four terminal nodes (VP erstmals [for the first time] streunende [straying] Katzen [cats] geza?hlt [counted]) and therefore has a much greater impact on the overall result for the sentence.</S>
			<S sid="94" ssid="39">ERROR PARSEVAL LA PP attachment I 83.33 96.30 Label I 83.33 96.00 Label II 83.33 91.00 SPAN II 83.33 96.40 Table 6: Evaluation results for Sentence 9The Tu?Ba-D/Z benefits from its overall higher ra tio of nodes per sentence, resulting in a higher ratio of non-terminal/terminal nodes per phrase and the effect, that the inserted label error affects a smaller number of terminal nodes than in the TIGER test set for LA testing.</S>
	</SECTION>
	<SECTION title="Experiment II. " number="5">
			<S sid="95" ssid="1">Ku?bler (2005) and Maier (2006) assess the impact of the different treebank annotation schemes on PCFG parsing by conducting a number of modificationsconverting the Tu?Ba-D/Z into a format more sim ilar to the NEGRA (and hence TIGER) treebank.</S>
			<S sid="96" ssid="2">After each modification they extract a PCFG from the modified treebank and measure the effect of the changes on parsing results.</S>
			<S sid="97" ssid="3">They show that with each modification transforming the Tu?Ba-D/Z into a more NEGRA-like format the parsing results also become more similar to the results of the NEGRA treebank, i.e. the results get worse.</S>
			<S sid="98" ssid="4">Maier takes this as evidence that the Tu?Ba-D/Z is more adequate forPCFG parsing.</S>
			<S sid="99" ssid="5">This assumption is based on the belief that PARSEVAL results fully reflect parse qual ity across different treebank encoding schemes.</S>
			<S sid="100" ssid="6">This is not always true, as shown in Experiment I. In our second experiment we crucially change the order of events in the Ku?bler (2005), Maier (2006)and Ku?bler et al (2006) experiments: We first extract an unlexicalised PCFG from each of the orig inal treebanks.</S>
			<S sid="101" ssid="7">We then transform the output of the parser trained on the Tu?Ba-D/Z into a format more similar to the TIGER Treebank.</S>
			<S sid="102" ssid="8">In contrast to Ku?bler (2005) and Maier (2006), who converted the 634 treebank before extracting the grammars in order tomeasure the impact of single features like topological fields or unary nodes on PCFG parsing, we con vert the trees in the parser output of a parser trained on the original unconverted treebank resources.</S>
			<S sid="103" ssid="9">This allows us to preserve the basic syntactic structureand also the errors present in the output trees resulting from a potential bias in the original tree bank training resources.</S>
			<S sid="104" ssid="10">The results for the original parser output evaluated against the unmodified goldtrees should not be crucially different from the re sults for the modified parser output evaluated against the modified gold trees.</S>
			<S sid="105" ssid="11">5.1 Experimental Setup.</S>
			<S sid="106" ssid="12">For Experiment II we trained BitPar (Schmid, 2004), a parser for highly ambiguous PCFG grammars, onthe two treebanks.</S>
			<S sid="107" ssid="13">The Tu?Ba-D/Z training data con sists of the 21067 treebank trees not included in the Tu?Ba-D/Z test set.</S>
			<S sid="108" ssid="14">Because of the different size of the two treebanks we selected 21067 sentences from the TIGER treebank, starting from sentence 10000 (and excluding the sentences in the TIGER test set).</S>
			<S sid="109" ssid="15">Before extracting the grammars we resolved thecrossing branches in the TIGER treebank as de scribed in Section 3.</S>
			<S sid="110" ssid="16">After this preprocessing step we extracted an unlexicalised PCFG from each of our training sets.</S>
			<S sid="111" ssid="17">Our TIGER grammar has a total of 21163 rule types, while the grammar extracted from the Tu?Ba-D/Z treebank consists of 5021 rules only.</S>
			<S sid="112" ssid="18">We parsed the TIGER and Tu?Ba-D/Z test set with the extracted grammars, using the gold POS tags for parser input.</S>
			<S sid="113" ssid="19">We then automatically converted theTu?Ba-D/Z output to a TIGER-like format and com pare the evaluation results for the unmodified treesagainst the gold trees with the results for the con verted parser output against the converted gold trees.</S>
			<S sid="114" ssid="20">5.2 Converting the Tu?Ba-D/Z Trees.</S>
			<S sid="115" ssid="21">The automatic conversion of the Tu?Ba-D/Z-style trees includes the removal of topological fields and unary nodes as well as the deletion of NPs insideof PPs, because the NP child nodes are directly at tached to the PP in the TIGER annotation scheme.</S>
			<S sid="116" ssid="22">As a last step in the conversion process we adapted the Tu?Ba-D/Z node labels to the TIGER categories.</S>
			<S sid="117" ssid="23">5.2.1 The Conversion Process: An Example We demonstrate the conversion process using anexample sentence from the Tu?Ba-D/Z test set (Fig ure 4).</S>
			<S sid="118" ssid="24">The converted tree is given in Figure 5:topological fields, here VF (initial field), MF (mid dle field) and LK (left sentence bracket), as well asunary nodes have been removed.</S>
			<S sid="119" ssid="25">The category la bels have been changed to TIGER-style annotation.</S>
			<S sid="120" ssid="26">Erziehungsurlaub nehmen bisher nur zwei Prozent der M?anner.</S>
			<S sid="121" ssid="27">?Until now only two percent of the men take parental leave.?</S>
			<S sid="122" ssid="28">Figure 4: Original Tu?Ba-D/Z-style gold tree Figure 5: Converted TIGER-style gold tree Figure 6 shows the unmodified parser output from the Tu?Ba-D/Z trained grammar for the same string.</S>
			<S sid="123" ssid="29">The parser incorrectly included all adverbs inside an NP governed by the PP, while in the gold tree (Figure 4) both adverbs are attached to the PP.</S>
			<S sid="124" ssid="30">The modified parser output is shown in Figure 7.</S>
			<S sid="125" ssid="31">5.3 Results for Converted Parser Output.</S>
			<S sid="126" ssid="32">We applied the conversion method described aboveto the original trees and the parser output for the sentences in the TIGER and the Tu?Ba-D/Z test sets.</S>
			<S sid="127" ssid="33">Table 7 shows PARSEVAL and LA results for the mod ified trees, evaluating the converted parser output 635 Figure 6: Parser output (Tu?Ba-D/Z grammar) Figure 7: Converted parser output (Tu?Ba-D/Z) for each treebank against the converted gold trees of the same treebank.</S>
			<S sid="128" ssid="34">Due to the resolved crossing branches in the TIGER treebank we also have some unary nodes in the TIGER test set.</S>
			<S sid="129" ssid="35">Their removalsurprisingly improves both PARSEVAL and LA re sults.</S>
			<S sid="130" ssid="36">For the Tu?Ba-D/Z all conversions lead to a decrease in precision and recall for the PARSEVALmetric.</S>
			<S sid="131" ssid="37">Converting the trees parsed by the Tu?BaD/Z grammar to a TIGER-like format produces an f score which is slightly lower than that for the TIGER trees.</S>
			<S sid="132" ssid="38">The same is true for the LA metric, but not tothe same extent as for PARSEVAL.</S>
			<S sid="133" ssid="39">The LA met ric also gives slightly better results for the originalTIGER trees compared to the result for the unmodi fied Tu?Ba-D/Z trees.</S>
			<S sid="134" ssid="40">The constant decrease in PARSEVAL results for the modified trees is consistent with the results in Ku?bler et al (2005), but our conclusions are slightlydifferent.</S>
			<S sid="135" ssid="41">Our experiment shows that the Tu?Ba D/Z annotation scheme does not generally producehigher quality parser output, but that the PARSEVAL results are highly sensitive to the ratio of non terminal/terminal nodes.</S>
			<S sid="136" ssid="42">However, the parser output for the grammar trained on the Tu?Ba-D/Z yields a EVALB LA prec.</S>
			<S sid="137" ssid="43">recall f-sco.</S>
			<S sid="138" ssid="44">avg.</S>
			<S sid="139" ssid="45">TIGER 83.54 83.65 83.59 94.69 no Unary 84.33 84.48 84.41 94.83 Tu?Ba-D/Z 92.59 89.79 91.17 94.23 Tu?Ba-D/Z? TIGER no Top 92.38 88.76 90.53 93.93 no Unary 89.96 85.67 87.76 93.59 no Top + no U. 88.44 82.24 85.23 92.91 no Top + no U. 87.15 79.52 83.16 92.47 + no NP in PP Table 7: The impact of the conversion process on PARSEVAL and LA higher precision in the PARSEVAL metric against the Tu?Ba-D/Z gold trees than the parser output of the TIGER grammar against the TIGER gold trees.</S>
			<S sid="140" ssid="46">For PARSEVAL recall, the TIGER grammar gives better results.</S>
	</SECTION>
	<SECTION title="Experiment III. " number="6">
			<S sid="141" ssid="1">In Experiment I and II we showed that the tree based PARSEVAL metric is not a reliable measurefor comparing the impact of different treebank an notation schemes on the quality of parser output and that the issue, whether German is harder to parse than English, remains undecided.</S>
			<S sid="142" ssid="2">In Experiment IIIwe report a dependency-based evaluation and com pare the results to the results of the other metrics.</S>
			<S sid="143" ssid="3">6.1 Dependency-Based (DB) Evaluation.</S>
			<S sid="144" ssid="4">The dependency-based evaluation used in the exper iments follows the method of Lin (1998) and Ku?blerand Telljohann (2002), converting the original treebank trees and the parser output into dependency re lations of the form WORD POS HEAD.</S>
			<S sid="145" ssid="5">Functional labels have been omitted for parsing, therefore thedependencies do not comprise functional informa tion.</S>
			<S sid="146" ssid="6">Figure 8 shows the original TIGER Treebank representation for the CFG tree in Figure 3.</S>
			<S sid="147" ssid="7">Square boxes denote grammatical functions.</S>
			<S sid="148" ssid="8">Figure 9 shows the dependency relations for the same tree, indicated by labelled arrows.</S>
			<S sid="149" ssid="9">Converted into a WORD POS HEAD triple format the dependency tree looks as follows (Table 8).Following Lin (1998), our DB evaluation algo rithm computes precision and recall:?</S>
			<S sid="150" ssid="10">Precision: the percentage of dependency re lationships in the parser output that are also 636 Figure 8: TIGER treebank representation for Figure 3 SB NKPGNK NK OA MO OC the city counsil of Venice has for the straying cats counted first time Die Stadtverwaltung von Venedig hat erstmals streunende Katzen gez?hlt ?For the first time the city counsil of Venice has counted straying cats.?</S>
			<S sid="151" ssid="11">Figure 9: Dependency relations for Figure 8 found in the gold triples?</S>
			<S sid="152" ssid="12">Recall: the percentage of dependency relation ships in the gold triples that are also found in the parser output triples.</S>
			<S sid="153" ssid="13">WORD POS HEAD Die [the] ART Stadtverwaltung Stadtverwaltung NN hat [city counsil] von [of] APPR Stadtverwaltung Venedig [Venice] NE vonhat [has] VAFIN erstmals ADV geza?hlt [for the first time] streunende [straying] ADJA Katzen Katzen [cats] NN geza?hlt geza?hlt [counted] VVPP hat Table 8: Dependency triples for Figure 9We assessed the quality of the automatic conver sion methodology by converting the 1024 originaltrees from each of our test sets into dependency rela tions, using the functional labels in the original trees to determine the dependencies.</S>
			<S sid="154" ssid="14">Topological fields in the Tu?Ba-D/Z test set have been removed before extracting the dependency relationships.</S>
			<S sid="155" ssid="15">We then removed all functional information fromthe trees and converted the stripped trees into dependencies, using heuristics to find the head.</S>
			<S sid="156" ssid="16">We eval uated the dependencies for the stripped gold trees against the dependencies for the original gold trees including functional labels and obtained an f-score of 99.64% for TIGER and 99.13% for the Tu?Ba-D/Zdependencies.</S>
			<S sid="157" ssid="17">This shows that the conversion is re liable and not unduly biased to either the TIGER or Tu?Ba-D/Z annotation schemes.</S>
			<S sid="158" ssid="18">6.2 Experimental Setup.</S>
			<S sid="159" ssid="19">For Experiment III we used the same PCFG grammars and test sets as in Experiment II.</S>
			<S sid="160" ssid="20">Before ex tracting the dependency relationships we removed the topological fields in the Tu?Ba-D/Z parser output.</S>
			<S sid="161" ssid="21">As shown in Section 6.1, this does not penalise thedependency-based evaluation results for the Tu?Ba D/Z. In contrast to Experiment II we used raw textas parser input instead of the gold POS tags, allow 637 ing a comparison with the gold tag results in Table 7.</S>
			<S sid="162" ssid="22">6.3 Results.</S>
			<S sid="163" ssid="23">Table 9 shows the evaluation results for the threedifferent evaluation metrics.</S>
			<S sid="164" ssid="24">For the DB evalua tion the parser trained on the TIGER training set achieves about 7% higher results for precision and recall than the parser trained on the Tu?Ba-D/Z. This result is clearly in contrast to the PARSEVAL scores, which show higher results for precision and recall for the Tu?Ba-D/Z. But contrary to the PARSEVAL results on gold POS tags as parser input (Table 7),the gap between the results for TIGER and Tu?Ba D/Z is not as wide as before.</S>
			<S sid="165" ssid="25">PARSEVAL gives a labelled bracketing f-score of 81.12% (TIGER)and 85.47% (Tu?Ba-D/Z) on raw text as parser input, while the results on gold POS tags are more dis tinctive with an f-score of 83.59% for TIGER and 91.17% for Tu?Ba-D/Z. The LA results again give better scores to the TIGER parser output, this timethe difference is more pronounced than for Experi ment II (Table 7).</S>
			<S sid="166" ssid="26">Dependencies PARSEVAL LA Prec Rec Prec Rec Avg TIGER 85.71 85.72 81.21 81.04 93.88 Tu?Ba 76.64 76.63 87.24 83.77 92.58 Table 9: Parsing results for three evaluation metrics The considerable difference between the resultsfor the metrics raises the question which of the met rics is the most adequate for judging parser output quality across treebank encoding schemes.</S>
	</SECTION>
	<SECTION title="Conclusions. " number="7">
			<S sid="167" ssid="1">In this paper we presented novel experiments assess ing the validity of parsing results measured along different dimensions: the tree-based PARSEVAL metric, the string-based Leaf-Ancestor metric anda dependency-based evaluation.</S>
			<S sid="168" ssid="2">By inserting con trolled errors into gold treebank trees and measuring the effects on parser evaluation results we gave new evidence for the downsides of PARSEVAL which,despite severe criticism, is still the standard measure for parser evaluation.</S>
			<S sid="169" ssid="3">We showed that PAR SEVAL cannot be used to compare the output ofPCFG parsers trained on different treebank anno tation schemes, because the results correlate withthe ratio of non-terminal/terminal nodes.</S>
			<S sid="170" ssid="4">Compar ing two different annotation schemes, PARSEVALconsistently favours the one with the higher node ra tio.</S>
			<S sid="171" ssid="5">We examined the influence of treebank annotationschemes on unlexicalised PCFG parsing, and re jected the claim that the German Tu?Ba-D/Z treebankis more appropriate for PCFG parsing than the Ger man TIGER treebank and showed that converting the Tu?Ba-D/Z trained parser output to a TIGER-like format leads to PARSEVAL results slightly worse than the ones for the TIGER treebank trained parser.Additional evidence comes from a dependency based evaluation, showing that, for the output of the parser trained on the TIGER treebank, the mapping from the CFG trees to dependency relations yields better results than for the grammar trained on theTu?Ba-D/Z annotation scheme, even though PARSE VAL scores suggest that the TIGER-based parseroutput trees are substantial worse than Tu?Ba-D/Z based parser output trees.</S>
			<S sid="172" ssid="6">We have shown that different treebank annotation schemes have a strong impact on parsing results forsimilar input data with similar (simulated) parser er rors.</S>
			<S sid="173" ssid="7">Therefore the question whether a particular language is harder to parse than another language or not, can not be answered by comparing parsingresults for parsers trained on treebanks with different annotation schemes.</S>
			<S sid="174" ssid="8">Comparing PARSEVAL based parsing results for a parser trained on the Tu?Ba-D/Z or TIGER to results achieved by a parser trained on the English Penn-II treebank (Marcus et al, 1994) does not provide conclusive evidenceabout the parsability of a particular language, be cause the results show a bias introduced by thecombined effect of annotation scheme and evalua tion metric.</S>
			<S sid="175" ssid="9">This means that the question whether German is harder to parse than English, is still undecided.</S>
			<S sid="176" ssid="10">A possible way forward is perhaps a dependency-based evaluation of TIGER/Tu?Ba-D/Z with Penn-II trained grammars for ?similar?</S>
			<S sid="177" ssid="11">test andtraining sets and cross-treebank and -language con trolled error insertion experiments.</S>
			<S sid="178" ssid="12">Even this is not entirely straightforward as it is not completely clear what constitutes ?similar?</S>
			<S sid="179" ssid="13">test/training sets across languages.</S>
			<S sid="180" ssid="14">We will attempt to pursue this in further research.</S>
			<S sid="181" ssid="15">638 Acknowledgements We would like to thank the anomymous reviewers for many helpful comments.</S>
			<S sid="182" ssid="16">This research has been supported by a Science Foundation Ireland grant 04|IN|I527.</S>
	</SECTION>
</PAPER>
