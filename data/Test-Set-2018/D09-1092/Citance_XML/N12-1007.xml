<PAPER>
  <S sid="0">Entity Clustering Across Languages</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet.</S>
    <S sid="2" ssid="2">When co-referent text mentions appear in different languages, these techniques cannot be easily applied.</S>
    <S sid="3" ssid="3">Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters.</S>
    <S sid="4" ssid="4">Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures.</S>
    <S sid="5" ssid="5">Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown.</S>
    <S sid="6" ssid="6">On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">This paper introduces techniques for clustering coreferent text mentions across documents and languages.</S>
    <S sid="8" ssid="2">On the web today, a breaking news item may instantly result in mentions to a real-world entity in multiple text formats: news articles, blog posts, tweets, etc.</S>
    <S sid="9" ssid="3">Much NLP work has focused on model adaptation to these diverse text genres.</S>
    <S sid="10" ssid="4">However, the diversity of languages in which the mentions appear is a more significant challenge.</S>
    <S sid="11" ssid="5">This was particularly evident during the 2011 popular uprisings in the Arab world, in which electronic media played a prominent role.</S>
    <S sid="12" ssid="6">A key issue for the outside world was the aggregation of information that appeared simultaneously in English, French, and various Arabic dialects.</S>
    <S sid="13" ssid="7">To our knowledge, we are the first to consider clustering entity mentions across languages without a priori knowledge of the quantity or types of real-world entities (a knowledge base).</S>
    <S sid="14" ssid="8">The cross-lingual setting introduces several challenges.</S>
    <S sid="15" ssid="9">First, we cannot assume a prototypical name format.</S>
    <S sid="16" ssid="10">For example, the Anglo-centric first/middle/last prototype used in previous name modeling work (cf.</S>
    <S sid="17" ssid="11">(Charniak, 2001)) does not apply to Arabic names like Abdullah ibn Abd Al-Aziz Al-Saud or Chinese names like Hu Jintao (referred to as Mr. Hu, not Mr. Jintao).</S>
    <S sid="18" ssid="12">Second, organization names often require both transliteration and translation.</S>
    <S sid="19" ssid="13">For example, the Arabic but a translation of &#65533;&#233;&#187;Q&#229;&#65533;... &#8216;Corporation&#8217;.</S>
    <S sid="20" ssid="14">Our models are organized as a pipeline.</S>
    <S sid="21" ssid="15">First, for each document, we perform standard mention detection and coreference resolution.</S>
    <S sid="22" ssid="16">Then, we use pairwise cross-lingual similarity models to measure both mention and context similarity.</S>
    <S sid="23" ssid="17">Finally, we cluster the mentions based on similarity.</S>
    <S sid="24" ssid="18">Our work makes the following contributions: (1) introduction of the task, (2) novel models for crosslingual entity clustering of person and organization entities, (3) cross-lingual annotation of the NIST Automatic Content Extraction (ACE) 2008 Arabic-English evaluation set, and (4) experimental results using both gold and automatic within-document processing.</S>
    <S sid="25" ssid="19">We will release our software and annotations to support future research.</S>
    <S sid="26" ssid="20">Consider the toy corpus in Fig.</S>
    <S sid="27" ssid="21">1.</S>
    <S sid="28" ssid="22">The English documents contain mentions of two people: Steven Paul Jobs and Mark Elliot Zuckerberg.</S>
    <S sid="29" ssid="23">Of course, the surface realization of Mr. Jobs&#8217; last name in English is also an ordinary nominal, hence the ambiguous mention string (absent context) in the second document.</S>
    <S sid="30" ssid="24">The Arabic document introduces an organization entity (Apple Inc.) along with proper and pronominal references to Mr. Jobs.</S>
    <S sid="31" ssid="25">Finally, the French document refers to Mr. Jobs by the honorific &#8216;Monsieur,&#8217; and to Apple without its corporate designation.</S>
    <S sid="32" ssid="26">Our goal is to automatically produce the crosslingual entity clusters E1 (Mark Elliot Zuckerberg), E2 (Apple Inc.), and E3 (Steven Paul Jobs).</S>
    <S sid="33" ssid="27">Both the true number and characteristics of these entities are unobserved.</S>
    <S sid="34" ssid="28">Our models require two pre-processing steps: mention detection and within-document coreference/anaphora resolution, shown in Fig.</S>
    <S sid="35" ssid="29">1 by the text boxes and intra-document links, respectively.</S>
    <S sid="36" ssid="30">For example, in doc3, a within-document coreference system would pre-linker joobz &#8216;Jobs&#8217; with the masculine pronoun o h `his'.</S>
    <S sid="37" ssid="31">In addition, the mention detector determines that the surface form &#8220;Jobs&#8221; in doc2 is not an entity reference.</S>
    <S sid="38" ssid="32">For this within-document pre-processing we use Serif (Ramshaw et al., 2011).1 Our models measure cross-lingual similarity of the coreference chains to make clustering decisions (&#8226; in Fig.</S>
    <S sid="39" ssid="33">1).</S>
    <S sid="40" ssid="34">The similarity models (indicated by the = and =6 operators in Fig.</S>
    <S sid="41" ssid="35">1) consider both mention string and context similarity (&#167;2).</S>
    <S sid="42" ssid="36">We use the mention similarities as hard constraints, and the context similarities as soft constraints.</S>
    <S sid="43" ssid="37">In this work, we investigate two standard constrained clustering algorithms (&#167;3).</S>
    <S sid="44" ssid="38">Our methods can be used to extend existing systems for mono-lingual entity clustering (also known as &#8220;cross-document coreference resolution&#8221;) to the cross-lingual setting.</S>
  </SECTION>
  <SECTION title="2 Mention and Context Similarity" number="2">
    <S sid="45" ssid="1">Our goal is to create cross-lingual sets of co-referent mentions to real-world entities (people, places, organizations, etc.).</S>
    <S sid="46" ssid="2">In this paper, we adopt the following notation.</S>
    <S sid="47" ssid="3">Let M be a set of distinct text mentions in a collection of documents; C is a partitioning of M into document-level sets of co-referent mentions (called coreference chains); E is a partitioning of C into sets of co-referent chains (called entities).</S>
    <S sid="48" ssid="4">Let i, j be nonnegative integers less than or equal to |M |and a, b be non-negative integers less than or equal to |C|.</S>
    <S sid="49" ssid="5">Our experiments use a separate within-document coreference system to create C, which is fixed.</S>
    <S sid="50" ssid="6">We will learn E, which has size no greater than |C |since the set of mono-lingual chains is the largest valid partitioning.</S>
    <S sid="51" ssid="7">We define accessor functions to access properties of mentions and chains.</S>
    <S sid="52" ssid="8">For any mention mi, define the following functions: lang(mi) is the language; doc(mi) is the document containing mi; type(mi) is the semantic type, which is assigned by the withindocument coreference system.</S>
    <S sid="53" ssid="9">We also extract a set of mention contexts S, which are the sentences containing each mention (i.e., |S |= |M|).</S>
    <S sid="54" ssid="10">We learn the partition E by considering mention and context similarity, which are measured with separate component models.</S>
    <S sid="55" ssid="11">We use separate methods for within- and crosslanguage mention similarity.</S>
    <S sid="56" ssid="12">The pairwise similarity thographic representation.</S>
    <S sid="57" ssid="13">&#8220;0&#8221; indicates a null mapping.</S>
    <S sid="58" ssid="14">For English, we also lowercase and remove determiners and punctuation.</S>
    <S sid="59" ssid="15">For Arabic, we remove the determiner &#200;@ Al `the' and the elongation character tatwil `_'. of any two mentions mi and mj is: distance (Porter and Winkler, 1997).</S>
    <S sid="60" ssid="16">Jaro-Winkler rewards matching prefixes, the empirical justification being that less variation typically occurs at the beginning of names.2 The metric produces a score in the range [0,1], where 0 indicates equality.</S>
    <S sid="61" ssid="17">Maxent model (cross-language) When lang(mi) =&#65533; lang(mj), then the two mentions might be in different writing systems.</S>
    <S sid="62" ssid="18">Edit distance calculations no longer apply directly.</S>
    <S sid="63" ssid="19">One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler.</S>
    <S sid="64" ssid="20">However, transliteration systems are complex and require significant training resources.</S>
    <S sid="65" ssid="21">We find that a simpler, low-resource approach works well in practice.</S>
    <S sid="66" ssid="22">First, we deterministically map both languages to a common phonetic representation (Tbl.</S>
    <S sid="67" ssid="23">1).3 Next, we align the mention pairs with the Hungarian algorithm, ken indices.</S>
    <S sid="68" ssid="24">Define the following functions for strings: cbigrams(&#183;) returns the set of character bigrams; len(&#183;) is the token length; Lev(&#183;, &#183;) is the Levenshtein edit distance between two strings.</S>
    <S sid="69" ssid="25">Prior to feature extraction, we add unique start and end symbols to the mention strings. which produces a word-to-word alignment Ary,.i&#65533;...j.4 Finally, we build a simple binary Maxent classifier p(y|mi, mj; A) that extracts features from the aligned mentions (Tbl.</S>
    <S sid="70" ssid="26">2).</S>
    <S sid="71" ssid="27">We learn the parameters A using a quasi-Newton procedure with Li (lasso) regularization (Andrew and Gao, 2007).</S>
    <S sid="72" ssid="28">Mention strings alone are not always sufficient for disambiguation.</S>
    <S sid="73" ssid="29">Consider again the simple example in Fig.</S>
    <S sid="74" ssid="30">1.</S>
    <S sid="75" ssid="31">Both doc3 and doc4 reference &#8220;Steve Jobs&#8221; and &#8220;Apple&#8221; in the same contexts.</S>
    <S sid="76" ssid="32">Context cooccurence and/or similarity can thus disambiguate these two entities from other entities with similar references (e.g., &#8220;Steve Jones&#8221; or &#8220;Apple Corps&#8221;).</S>
    <S sid="77" ssid="33">As with the mention strings, the contexts may originate in different writing systems.</S>
    <S sid="78" ssid="34">We consider both highand low-resource approaches for mapping contexts to a common representation.</S>
    <S sid="79" ssid="35">Machine Translation (MT) For the high-resource setting, if lang(mi) =&#65533; English, then we translate both mi and its context si to English with an MT system.</S>
    <S sid="80" ssid="36">We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module.</S>
    <S sid="81" ssid="37">We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic).</S>
    <S sid="82" ssid="38">Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples&#8212; groups of topically-similar documents&#8212;share a topic distribution.</S>
    <S sid="83" ssid="39">The tuples need not be sentence-aligned, so training data is easier to obtain.</S>
    <S sid="84" ssid="40">For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve Jobs.</S>
    <S sid="85" ssid="41">Let D be a set of document tuples, where there is one document in each tuple for each of L languages.</S>
    <S sid="86" ssid="42">Each language has vocabulary V and each document dlt has Ni tokens.</S>
    <S sid="87" ssid="43">We specify a fixed-size set of topics K. The PLTM generates the document tuples as follows: For cross-lingual context mapping, we infer the 1best topic assignments for each token in all S mention contexts.</S>
    <S sid="88" ssid="44">This technique reduces V = k for all l. Moreover, all languages have a common vocabulary: the set of K topic indices.</S>
    <S sid="89" ssid="45">Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details.</S>
    <S sid="90" ssid="46">After mapping each mention context to a common representation, we measure context similarity based on the choice of clustering algorithm.</S>
  </SECTION>
  <SECTION title="3 Clustering Algorithms" number="3">
    <S sid="91" ssid="1">We incorporate the mention and context similarity measures into a clustering framework.</S>
    <S sid="92" ssid="2">We consider two algorithms.</S>
    <S sid="93" ssid="3">The first is hierarchical agglomerative clustering (HAC), with which we assume basic familiarity (Manning et al., 2008).</S>
    <S sid="94" ssid="4">A shortcoming of HAC is that a stop threshold must be tuned.</S>
    <S sid="95" ssid="5">To avoid this requirement, we also consider non-parametric probabilistic clustering in the form of a Dirichlet process mixture model (DPMM) (Antoniak, 1974) .</S>
    <S sid="96" ssid="6">Both clustering algorithms can be modified to accommodate pairwise constraints.</S>
    <S sid="97" ssid="7">We have observed better results by encoding mention similarity as a hard constraint.</S>
    <S sid="98" ssid="8">Context similarity is thus the cluster distance measure.5 To turn the Jaro-Winkler distance into a hard boolean constraint, we tuned a threshold q on held-out data, i.e., jaro-winkler(mi, mj) G q =&gt;. mi = mj.</S>
    <S sid="99" ssid="9">Likewise, the Maxent model is a binary classifier, so p(y = 1|mi, mj; A) &gt; 0.5 =&gt;. mi = mj.</S>
    <S sid="100" ssid="10">In both clustering algorithms, any two chains Ca and Cb cannot share the same cluster assignment if: The deterministic accessor function repr(Ca) returns the representative mention of a chain.</S>
    <S sid="101" ssid="11">The heuristic we used was &#8220;first mention&#8221;: the function returns the earliest mention that appears in the associated document.</S>
    <S sid="102" ssid="12">In many languages, the first mention is typically more complete than later mentions.</S>
    <S sid="103" ssid="13">This heuristic also makes our system less sensitive to withindocument coreference errors.6 The representative mention only has special status for mention similarity: context similarity considers all mention contexts.</S>
    <S sid="104" ssid="14">HAC iteratively merges the &#8220;nearest&#8221; clusters according to context similarity.</S>
    <S sid="105" ssid="15">In our system, each cluster context is a bag of words W formed from the contexts of all coreference chains in that cluster.</S>
    <S sid="106" ssid="16">For each word in W we estimate a unigram Entity Language Model (ELM) (Raghavan et al., 2004): the corpus7 and p is a smoothing parameter.</S>
    <S sid="107" ssid="17">For any two entity clusters Ea and Eb, the distance between PE. and PEb is given by a metric based on the JensenShannon Divergence (JSD) (Endres and Schindelin, 2003): where KL(PE.||M) is the Kullback-Leibler divergence and M =1&#65533;(PE.</S>
    <S sid="108" ssid="18">+ PEb).</S>
    <S sid="109" ssid="19">We initialize HAC to E = C, i.e., the initial clustering solution is just the set of all coreference chains.</S>
    <S sid="110" ssid="20">Then we remove all links in the HAC proximity matrix that violate pairwise cannot-link constraints.</S>
    <S sid="111" ssid="21">During clustering, we do not merge Ea and Eb if any pair of chains violates a cannot-link constraint.</S>
    <S sid="112" ssid="22">This procedure propagates the cannot-link constraints (Klein et al., 2002).</S>
    <S sid="113" ssid="23">To output E, we stop clustering when the minimum JSD exceeds a stop threshold 'y, which is tuned on a development set.</S>
    <S sid="114" ssid="24">Instead of tuning a parameter like -y, it would be preferable to let the data dictate the number of entity clusters.</S>
    <S sid="115" ssid="25">We thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. Specifically, we consider a DPMM, which automatically infers the number of mixtures.</S>
    <S sid="116" ssid="26">Each Ca has an associated mixture Ba: where &#945; is the concentration parameter of the DP prior and G0 is the base distribution with support V .</S>
    <S sid="117" ssid="27">For our experiments, we set G0 = Dir(7r1,... , 7rV ), where 7ri = PV (wi).</S>
    <S sid="118" ssid="28">For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints.</S>
    <S sid="119" ssid="29">The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E_a, Ca) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei and Frazier, 2010).</S>
    <S sid="120" ssid="30">During sampling, we also learn &#945; using the auxiliary variable procedure of West (1995), so the only fixed parameters are those of the vague Gamma prior.</S>
    <S sid="121" ssid="31">However, we found that these hyperparameters were not sensitive.</S>
  </SECTION>
  <SECTION title="4 Training Data and Procedures" number="4">
    <S sid="122" ssid="1">We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training.</S>
    <S sid="123" ssid="2">Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010).</S>
    <S sid="124" ssid="3">We extracted 12,860 name pairs from the parallel Arabic-English translation treebanks,9 although our experiments show that the model achieves high accuracy with significantly fewer training examples.</S>
    <S sid="125" ssid="4">We generated a uniform distribution of training examples by running a Bernoulli trial for each aligned name pair in the corpus.</S>
    <S sid="126" ssid="5">If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus.</S>
    <S sid="127" ssid="6">MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation.</S>
    <S sid="128" ssid="7">We built a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T07), in addition to all of the target side training data.</S>
    <S sid="129" ssid="8">In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008).</S>
    <S sid="130" ssid="9">PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles.</S>
    <S sid="131" ssid="10">Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia&#65533;s 272 language editions (de Melo and Weikum, 2010).</S>
    <S sid="132" ssid="11">To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus.</S>
    <S sid="133" ssid="12">The topically-aligned tuples served as &#8220;glue&#8221; to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002).</S>
    <S sid="134" ssid="13">We ran the sampler for 10,000 iterations and set the number of topics K = 512.</S>
  </SECTION>
  <SECTION title="5 Task Evaluation Framework" number="5">
    <S sid="135" ssid="1">Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008).</S>
    <S sid="136" ssid="2">We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities.</S>
    <S sid="137" ssid="3">Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base).</S>
    <S sid="138" ssid="4">We report results for both gold and automatic within-document mention detection and coreference resolution.</S>
    <S sid="139" ssid="5">Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions.</S>
    <S sid="140" ssid="6">For the gold setting, we report: Information-theoretic measure that utilizes the entropy of the clusters and their mutual information.</S>
    <S sid="141" ssid="7">Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set.</S>
    <S sid="142" ssid="8">For the automatic setting, we must apply a different metric since the number of system chains may differ from the reference.</S>
    <S sid="143" ssid="9">We use B3sys (Cai and Strube, 2010), a variant of B3 that was shown to penalize both twinless reference chains and spurious system chains more fairly.</S>
    <S sid="144" ssid="10">Evaluation Corpus The automatic evaluation of cross-lingual coreference systems requires annotated 10Mimno et al. (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. multilingual corpora.</S>
    <S sid="145" ssid="11">Cross-document annotation is expensive (Strassel et al., 2008), so we chose the ACE2008 Arabic-English evaluation corpus as a starting point for cross-lingual annotation.</S>
    <S sid="146" ssid="12">The corpus consists of seven genres sampled from independent sources over the course of a decade (Tbl.</S>
    <S sid="147" ssid="13">3).</S>
    <S sid="148" ssid="14">The corpus provides gold mono-lingual cross-document coreference annotations for both PER and ORG entities.</S>
    <S sid="149" ssid="15">Using these annotations as a starting point, we found and annotated 216 cross-lingual entities.11 Because a similar corpus did not exist for development, we split the evaluation corpus into development and test sections.</S>
    <S sid="150" ssid="16">However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split.</S>
    <S sid="151" ssid="17">We thus split the corpus by global entity id.</S>
    <S sid="152" ssid="18">We assigned one-third of the entities to development, and the remaining twothirds to test.</S>
  </SECTION>
  <SECTION title="6 Comparison to Related Tasks and Work" number="6">
    <S sid="153" ssid="1">Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution.</S>
    <S sid="154" ssid="2">The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975).</S>
    <S sid="155" ssid="3">Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering.</S>
    <S sid="156" ssid="4">Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007).</S>
    <S sid="157" ssid="5">However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types.</S>
    <S sid="158" ssid="6">Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do.</S>
    <S sid="159" ssid="7">More 11The annotators were the first author and another fluent speaker of Arabic.</S>
    <S sid="160" ssid="8">The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011).</S>
    <S sid="161" ssid="9">Cross-document work on languages other than English is scarce.</S>
    <S sid="162" ssid="10">Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names.</S>
    <S sid="163" ssid="11">For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al.</S>
    <S sid="164" ssid="12">(2004).</S>
    <S sid="165" ssid="13">They clustered the entities incrementally using a binary classifier.</S>
    <S sid="166" ssid="14">Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness.</S>
    <S sid="167" ssid="15">Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching.</S>
    <S sid="168" ssid="16">To our knowledge, the cross-lingual entity clustering task is novel.</S>
    <S sid="169" ssid="17">However, there is significant prior work on similar tasks: Our work incorporates elements of the first three tasks.</S>
    <S sid="170" ssid="18">Most importantly, we avoid the key element of entity linking: a knowledge base.</S>
  </SECTION>
  <SECTION title="7 Experiments" number="7">
    <S sid="171" ssid="1">We performed intrinsic evaluations for both mention and context similarity.</S>
    <S sid="172" ssid="2">For context similarity, we analyzed mono-lingual entity clustering, which also facilitated comparison to prior work on the ACE2008 set, gold within-document processing).</S>
    <S sid="173" ssid="3">Higher scores (T) are better for CEAF and B3, whereas lower (].) is better for NVI.</S>
    <S sid="174" ssid="4">#gold indicates the number of reference entities, whereas #hyp is the size of E. evaluation set.</S>
    <S sid="175" ssid="5">Our main results are for the new task: cross-lingual entity clustering.</S>
    <S sid="176" ssid="6">Cross-lingual Mention Matching We created a random 80/10/10 (train, development, test) split of the Maxent training corpus and evaluated binary classification accuracy (Tbl.</S>
    <S sid="177" ssid="7">4).</S>
    <S sid="178" ssid="8">Of the mis-classified examples, we observed three major error types.</S>
    <S sid="179" ssid="9">First, the model learns that high edit distance is predictive of a mismatch.</S>
    <S sid="180" ssid="10">However, singleton strings that do not match often have a lower edit distance than longer strings that do match.</S>
    <S sid="181" ssid="11">As a result, singletons often cause false positives.</S>
    <S sid="182" ssid="12">Second, names that originate in a third language tend to violate the phonemic correspondences.</S>
    <S sid="183" ssid="13">For example, the model gives a false negative for a German football team: vim,&#65533; , &#65533;.</S>
    <S sid="184" ssid="14">, v 1 (phonetic mapping: af s kazrslawtrn) versus &#8220;FC Kaiserslautern.&#8221; Finally, names that require translation are problematic.</S>
    <S sid="185" ssid="15">For example, the classifier produces a false negative for (God, gd) ?</S>
    <S sid="186" ssid="16">(ail1, allh).</S>
    <S sid="187" ssid="17">CEAFT NVI]. applied to the subset of target cross-lingual entities in the test set.</S>
    <S sid="188" ssid="18">For CEAF and B3, SINGLEToN is the stronger baseline due to the high proportion of singleton entities in the corpus.</S>
    <S sid="189" ssid="19">Of course, cross-lingual entities have at least two chains, so No-CoNTExT is a better baseline for cross-lingual clustering.</S>
    <S sid="190" ssid="20">Mono-lingual Entity Clustering For comparison, we also evaluated our system on a standard monolingual cross-document coreference task (Arabic and English) (Tbl.</S>
    <S sid="191" ssid="21">5).</S>
    <S sid="192" ssid="22">We configured the system with HAC clustering and Jaro-Winkler (within-language) mention similarity.</S>
    <S sid="193" ssid="23">We built mono-lingual ELMs for context similarity.</S>
    <S sid="194" ssid="24">We used two baselines: fore, E is the set of fully-connected components in C subject to the pairwise constraints.</S>
    <S sid="195" ssid="25">For HAC, we manually tuned the stop threshold -y, the Jaro-Winkler threshold q, and the ELM smoothing parameter p on the development set.</S>
    <S sid="196" ssid="26">For the DPMM, no development tuning was necessary, and we evaluated a single sample of E taken after 3,000 iterations.</S>
    <S sid="197" ssid="27">To our knowledge, Baron and Freedman (2008) reported the only previous results on the ACE2008 data set.</S>
    <S sid="198" ssid="28">However, they only gave gold results for English, and clustered the entire evaluation corpus (test+development).</S>
    <S sid="199" ssid="29">To control for the effect of within-document errors, we considered their gold input (mention detection and within-document coreference resolution) results.</S>
    <S sid="200" ssid="30">They reported B3 for the two entity types separately: ORG (91.5% F1) and PER (94.3% F1).</S>
    <S sid="201" ssid="31">The different experimental designs preclude a precise comparison, but the accuracy of (Serif) within-document processing).</S>
    <S sid="202" ssid="32">For HAC, we used the same parameters as the gold setting. the two systems are at least in the same range.</S>
    <S sid="203" ssid="33">We evaluated four system configurations on the new task: HAC+MT, HAC+PLTM, DPMM+MT, and DPMM+PLTM.</S>
    <S sid="204" ssid="34">First, we established an upper bound by assuming gold within-document mention detection and coreference resolution (Tbl.</S>
    <S sid="205" ssid="35">6).</S>
    <S sid="206" ssid="36">This setting isolated the new cross-lingual clustering methods from within-document processing errors.</S>
    <S sid="207" ssid="37">Then we evaluated with Serif (automatic) within-document processing (Tbl.</S>
    <S sid="208" ssid="38">7).</S>
    <S sid="209" ssid="39">This second experiment replicated an application setting.</S>
    <S sid="210" ssid="40">We used the same baselines and tuning procedures as in the mono-lingual clustering experiment.</S>
    <S sid="211" ssid="41">Results In the gold setting, HAC+MT produces the best results, as expected.</S>
    <S sid="212" ssid="42">The dimensionality reduction of the vocabulary imposed by PLTM significantly reduces accuracy, but HAC+PLTM still exceeds the baseline.</S>
    <S sid="213" ssid="43">We tried increasing the number of PLTM topics k, but did not observe an improvement in task accuracy.</S>
    <S sid="214" ssid="44">For both context-mapping methods, the DPMM suffers from low-recall.</S>
    <S sid="215" ssid="45">Upon inspection, the clustering solution of DPMM+MT contains a high proportion of singleton hypotheses, suggesting that the model finds lower similarity in the presence of a larger vocabulary.</S>
    <S sid="216" ssid="46">When the context vocabulary consists of PLTM topics, larger clusters are discovered (DPMM+PLTM).</S>
    <S sid="217" ssid="47">The effect of dimensionality reduction is also apparent in the clustering solutions of the PLTM models.</S>
    <S sid="218" ssid="48">For example, for the Serif output, DPMM+PLTM produces a cluster consisting of &#8220;White House&#8221;, &#8220;Senate&#8221;, &#8220;House of Representatives&#8221;, and &#8220;Parliament&#8221;.</S>
    <S sid="219" ssid="49">Arabic mentions of the latter three entities pass the pairwise mention similarity constraints due to the word 0_4A &#8216;council&#8217;, which appears in text mentions for all three legislative bodies.</S>
    <S sid="220" ssid="50">A cross-language matching error resulted in the linking of &#8220;White House&#8221;, and the reduced granularity of the contexts precluded further disambiguation.</S>
    <S sid="221" ssid="51">Of course, these entities probably appear in similar contexts.</S>
    <S sid="222" ssid="52">The caveat with the Serif results in Tbl.</S>
    <S sid="223" ssid="53">7 is that 3,251 of the 7,655 automatic coreference chains are not in the reference.</S>
    <S sid="224" ssid="54">Consequently, the evaluation is dominated by the penalty for spurious system coreference chains.</S>
    <S sid="225" ssid="55">Nonetheless, all models except for DPMM+PLTM exceed the baselines, and the relationships between models depicted in the gold experiments hold for the this setting.</S>
  </SECTION>
  <SECTION title="8 Conclusion" number="8">
    <S sid="226" ssid="1">Cross-lingual entity clustering is a natural step toward more robust natural language understanding.</S>
    <S sid="227" ssid="2">We proposed pipeline models that make clustering decisions based on cross-lingual similarity.</S>
    <S sid="228" ssid="3">We investigated two methods for mapping documents in different languages to a common representation: MT and the PLTM.</S>
    <S sid="229" ssid="4">Although MT may achieve more accurate results for some language pairs, the PLTM training resources (e.g., Wikipedia) are readily available for many languages.</S>
    <S sid="230" ssid="5">As for the clustering algorithms, HAC appears to perform better than the DPMM on our dataset, but this may be due to the small corpus size.</S>
    <S sid="231" ssid="6">The instance-level constraints represent tendencies that could be learned from larger amounts of data.</S>
    <S sid="232" ssid="7">With more data, we might be able to relax the constraints and use an exchangeable DPMM, which might be more effective.</S>
    <S sid="233" ssid="8">Finally, we have shown that significant quantities of within-document errors cascade into the cross-lingual clustering phase.</S>
    <S sid="234" ssid="9">As a result, we plan a model that clusters the mentions directly, thus removing the dependence on within-document coreference resolution.</S>
    <S sid="235" ssid="10">In this paper, we have set baselines and proposed models that significantly exceeded those baselines.</S>
    <S sid="236" ssid="11">The best model improved upon the cross-lingual entity baseline by 24.3% F1.</S>
    <S sid="237" ssid="12">This result was achieved without a knowledge base, which is required by previous approaches to cross-lingual entity linking.</S>
    <S sid="238" ssid="13">More importantly, our techniques can be used to extend existing cross-document entity clustering systems for the increasingly multilingual web.</S>
    <S sid="239" ssid="14">Acknowledgments We thank Jason Eisner, David Mimno, Scott Miller, Jim Mayfield, and Paul McNamee for helpful discussions.</S>
    <S sid="240" ssid="15">This work was started during the SCALE 2010 summer workshop at Johns Hopkins.</S>
    <S sid="241" ssid="16">The first author is supported by a National Science Foundation Graduate Fellowship.</S>
  </SECTION>
</PAPER>
