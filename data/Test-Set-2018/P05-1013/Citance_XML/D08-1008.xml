<PAPER>
	<S sid="0">Dependency-based Semantic Role Labeling of PropBank</S><ABSTRACT>
		<S sid="1" ssid="1">We present a PropBank semantic role label ing system for English that is integrated with a dependency parser.</S>
		<S sid="2" ssid="2">To tackle the problemof joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent.</S>
		<S sid="3" ssid="3">The syntactic model is a projective parser using pseudo-projective transfor mations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.</S>
		<S sid="4" ssid="4">The complete syntactic?semanticoutput is selected from a candidate pool gen erated by the subsystems.We evaluate the system on the CoNLL 2005 test sets using segment-based and dependency-based metrics.</S>
		<S sid="5" ssid="5">Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently.</S>
		<S sid="6" ssid="6">Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008.</S>
		<S sid="7" ssid="7">Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="8" ssid="8">Automatic semantic role labeling (SRL), the taskof determining who does what to whom, is a useful intermediate step in NLP applications perform ing semantic analysis.</S>
			<S sid="9" ssid="9">It has obvious applicationsfor template-filling tasks such as information extrac tion and question answering (Surdeanu et al, 2003; Moschitti et al, 2003).</S>
			<S sid="10" ssid="10">It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems(Haghighi et al, 2005; Hickl et al, 2006).</S>
			<S sid="11" ssid="11">In addi tion, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al, 2008).</S>
			<S sid="12" ssid="12">The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras andM?rquez, 2005; Baker et al, 2007; Surdeanu et al, 2008).Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in ma chine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al, 2005; Surdeanu et al, 2007; Punyakanok et al., 2008).</S>
			<S sid="13" ssid="13">With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea andPalmer, 2002; Punyakanok et al, 2008).</S>
			<S sid="14" ssid="14">Most sys tems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcuset al, 1993), although there has also been much re search on the use of shallow syntax (Carreras and M?rquez, 2004) in SRL.</S>
			<S sid="15" ssid="15">In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a moretransparent encoding of predicate?argument relations.</S>
			<S sid="16" ssid="16">Furthermore, the few systems based on de pendencies that have been presented have generally performed much worse than their constituent-based 69counterparts.</S>
			<S sid="17" ssid="17">For instance, Pradhan et al (2005) re ported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation.</S>
			<S sid="18" ssid="18">In a similar vein, Swansonand Gordon (2006) reported that parse tree path fea tures extracted from a rule-based dependency parserare much less reliable than those from a modern con stituent parser.In contrast, we recently carried out a de tailed comparison (Johansson and Nugues, 2008b) between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers.</S>
			<S sid="19" ssid="19">We suggested that the previous lack of progress independency-based SRL was due to low parsing accuracy.</S>
			<S sid="20" ssid="20">The experiments showed that the grammat ical function information available in dependency representations results in a steeper learning curve when training semantic role classifiers, and it also seemed that the dependency-based role classifiers were more resilient to lexical problems caused by change of domain.</S>
			<S sid="21" ssid="21">The recent CoNLL-2008 Shared Task (Surdeanu et al, 2008) was an attempt to show that SRL can beaccurately carried out using only dependency syn tax.</S>
			<S sid="22" ssid="22">However, these results are not easy to compareto previously published results since the task defini tions and evaluation metrics were different.This paper compares the best-performing system in the CoNLL-2008 Shared Task (Johans son and Nugues, 2008a) with previously published constituent-based SRL systems.</S>
			<S sid="23" ssid="23">The system carriesout joint dependency-syntactic and semantic analysis.</S>
			<S sid="24" ssid="24">We first describe its implementation in Sec tion 2, and then compare the system with the best system in the CoNLL-2005 Shared Task in Section 3.</S>
			<S sid="25" ssid="25">Since the outputs of the two systems are differ-.</S>
			<S sid="26" ssid="26">ent, we carry out two types of evaluations: first by using the traditional segment-based metric used in the CoNLL-2005 Shared Task, and then by using the dependency-based metric from the CoNLL-2008Shared Task.</S>
			<S sid="27" ssid="27">Both evaluations require a transformation of the output of one system: For the segmentbased metric, we have to convert the dependencybased output to segments; and for the dependency based metric, a head-finding procedure is needed toselect heads in segments.</S>
			<S sid="28" ssid="28">For the first time for a sys tem using only dependency syntax, we report resultsfor PropBank-based semantic role labeling of En glish that are close to the state of the art, and for some measures even superior.</S>
	</SECTION>
	<SECTION title="Syntactic?Semantic Dependency. " number="2">
			<S sid="29" ssid="1">AnalysisThe training corpus that we used is the dependency annotated Penn Treebank from the 2008 CoNLL Shared Task on joint syntactic?semantic analysis (Surdeanu et al, 2008).</S>
			<S sid="30" ssid="2">Figure 1 shows a sentenceannotated in this framework.</S>
			<S sid="31" ssid="3">The CoNLL task involved semantic analysis of predicates from Prop Bank (for verbs, such as plan) and NomBank (for nouns, such as investment); in this paper, we report the performance on PropBank predicates only since we compare our system with previously published PropBank-based SRL systems.</S>
			<S sid="32" ssid="4">Chrysler plans new investment in Latin America plan.01 LOC PMODNMODNMODOBJ A0 investment.01 A1A0 A2 SBJ ROOT Figure 1: An example sentence annotated with syntactic and semantic dependency structures.We model the problem of constructing a syntac tic and a semantic graph as a task to be solved jointly.</S>
			<S sid="33" ssid="5">Intuitively, syntax and semantics are highly interdependent and semantic interpretation should help syntactic disambiguation, and joint syntactic?semantic analysis has a long tradition in deep linguistic formalisms.</S>
			<S sid="34" ssid="6">Using a discriminative model, we thus formulate the problem of finding a syntactic tree y?syn and a semantic graph y?sem for a sentence x as maximizing a function Fjoint that scores the complete syntactic?semantic structure: ?y?syn, y?sem?</S>
			<S sid="35" ssid="7">= arg max ysyn,ysem Fjoint(x, ysyn, ysem) The dependencies in the feature representation used to compute Fjoint determine the tractability of the 70 rerankingPred?argLinguisticconstraintsSensedisambig.</S>
			<S sid="36" ssid="8">Argumentidentification Argumentlabeling dependencySyntacticparsing Global semantic model Syntactic?semantic reranking Semantic pipeline Figure 2: The architecture of the syntactic?semantic analyzer.search procedure needed to perform the maximiza tion.</S>
			<S sid="37" ssid="9">To be able to use complex syntactic features such as paths when predicting semantic structures, exact search is clearly intractable.</S>
			<S sid="38" ssid="10">This is true even with simpler feature representations ? the problemis a special case of multi-headed dependency analy sis, which is NP-hard even if the number of heads is bounded (Chickering et al, 1994).This means that we must resort to a simplification such as an incremental method or a rerank ing approach.</S>
			<S sid="39" ssid="11">We chose the latter option and thus created syntactic and semantic submodels.</S>
			<S sid="40" ssid="12">The joint syntactic?semantic prediction is selected from a small list of candidates generated by the respective subsystems.</S>
			<S sid="41" ssid="13">Figure 2 shows the architecture.</S>
			<S sid="42" ssid="14">2.1 Syntactic Submodel.</S>
			<S sid="43" ssid="15">We model the process of syntactic parsing of a sentence x as finding the parse tree y?syn = argmaxysyn Fsyn(x, ysyn) that maximizes a scoringfunction Fsyn.</S>
			<S sid="44" ssid="16">The learning problem consists of fit ting this function so that the cost of the predictions is as low as possible according to a cost function ?syn.</S>
			<S sid="45" ssid="17">In this work, we consider linear scoring functions of the following form: Fsyn(x, ysyn) = ?syn(x, ysyn) ?wwhere ?syn(x, ysyn) is a numeric feature represen tation of the pair (x, ysyn) andw a vector of feature weights.</S>
			<S sid="46" ssid="18">We defined the syntactic cost ?syn as the sum of link costs, where the link cost was 0 for a correct dependency link with a correct label, 0.5 for a correct link with an incorrect label, and 1 for an incorrect link.A widely used discriminative framework for fit ting the weight vector is the max-margin model (Taskar et al, 2003), which is a generalization ofthe well-known support vector machines to gen eral cost-based prediction problems.</S>
			<S sid="47" ssid="19">Since the large number of training examples and features in ourcase make an exact solution of the max-margin optimization problem impractical, we used the on line passive?aggressive algorithm (Crammer et al, 2006), which approximates the optimization process in two ways: ? The weight vector w is updated incrementally, one example at a time.?</S>
			<S sid="48" ssid="20">For each example, only the most violated con straint is considered.The algorithm is a margin-based variant of the perceptron (preliminary experiments show that it outperforms the ordinary perceptron on this task).</S>
			<S sid="49" ssid="21">Al gorithm 1 shows pseudocode for the algorithm.</S>
			<S sid="50" ssid="22">Algorithm 1 The Online PA Algorithm input Training set T = {(xt, yt)}Tt=1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for (xt, yt) in T let y?t = argmaxy F (xt, y) + ?(yt, y) let ?t = min ( C, F (xt,y?t)?F (xt,yt)+?(yt,y?t)??(x,yt)??(x,y?t)?2 ) w ? w + ?t(?(x, yt)??(x, y?t)) return waverage We used a C value of 0.01, and the number of iterations was 6.</S>
			<S sid="51" ssid="23">2.1.1 Features and SearchThe feature function?syn is a factored represen tation, meaning that we compute the score of the complete parse tree by summing the scores of its parts, referred to as factors: ?(x, y) ?w = ? f?y ?(x, f) ?w 71 We used a second-order factorization (McDonald and Pereira, 2006; Carreras, 2007), meaning that the factors are subtrees consisting of four links: the governor?dependent link, its sibling link, and theleftmost and rightmost dependent links of the depen dent.This factorization allows us to express useful fea tures, but also forces us to adopt the expensivesearch procedure by Carreras (2007), which extends Eisner?s span-based dynamic programming algorithm (1996) to allow second-order feature depen dencies.</S>
			<S sid="52" ssid="24">This algorithm has a time complexity ofO(n4), where n is the number of words in the sentence.</S>
			<S sid="53" ssid="25">The search was constrained to disallow mul tiple root links.</S>
			<S sid="54" ssid="26">To evaluate the argmax in Algorithm 1 during training, we need to handle the cost function ?syn in addition to the factor scores.</S>
			<S sid="55" ssid="27">Since the cost function ?syn is based on the cost of single links, this can easily be integrated into the factor-based search.</S>
			<S sid="56" ssid="28">2.1.2 Handling Nonprojective Links Although only 0.4% of the links in the trainingset are nonprojective, 7.6% of the sentences con tain at least one nonprojective link.</S>
			<S sid="57" ssid="29">Many of these links represent long-range dependencies ? such aswh-movement ? that are valuable for semantic pro cessing.</S>
			<S sid="58" ssid="30">Nonprojectivity cannot be handled by span-based dynamic programming algorithms.</S>
			<S sid="59" ssid="31">For parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used instead.</S>
			<S sid="60" ssid="32">However, this algorithm cannot be generalized to the second-order setting ?McDonald and Pereira (2006) proved that this problem is NP-hard, and described an approximate greedy search algorithm.</S>
			<S sid="61" ssid="33">To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson,2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and spe cial trace labels are used to enable recovery of the nonprojective links at parse time.</S>
			<S sid="62" ssid="34">The use of trace labels in the pseudo-projective transformation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once.</S>
			<S sid="63" ssid="35">Since the running time of our parser depends on the number of labels, we used only the 20 most frequent trace labels.</S>
			<S sid="64" ssid="36">2.2 Semantic Submodel.</S>
			<S sid="65" ssid="37">Our semantic model consists of three parts: ? A SRL classifier pipeline that generates a list of candidate predicate?argument structures.</S>
			<S sid="66" ssid="38">A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments.</S>
			<S sid="67" ssid="39">A global reranker that assigns scores to predicate?argument structures in the filtered candidate list.</S>
			<S sid="68" ssid="40">Rather than training the models on gold-standard syntactic input, we created an automatically parsed training set by 5-fold cross-validation.</S>
			<S sid="69" ssid="41">Training on automatic syntax makes the semantic classifiers more resilient to parsing errors, in particular adjunct labeling errors.</S>
			<S sid="70" ssid="42">2.2.1 SRL PipelineThe SRL pipeline consists of classifiers for pred icate disambiguation, argument identification, andargument labeling.</S>
			<S sid="71" ssid="43">For the predicate disambigua tion classifiers, we trained one subclassifier for eachlemma.</S>
			<S sid="72" ssid="44">All classifiers in the pipeline were L2regularized linear logistic regression classifiers, im plemented using the efficient LIBLINEAR package (Lin et al, 2008).</S>
			<S sid="73" ssid="45">For multiclass problems, we used the one-vs-all binarization method, which makes it easy to prevent outputs not allowed by the PropBank frame.</S>
			<S sid="74" ssid="46">Since our classifiers were logistic, their outputvalues could be meaningfully interpreted as prob abilities.</S>
			<S sid="75" ssid="47">This allowed us to combine the scores from subclassifiers into a score for the completepredicate?argument structure.</S>
			<S sid="76" ssid="48">To generate the candidate lists used by the global SRL models, we ap plied beam search based on these scores using a beam width of 4.The argument identification classifier was preceded by a pruning step similar to the constituent based pruning by Xue and Palmer (2004).</S>
			<S sid="77" ssid="49">The features used by the classifiers are listed inTable 1, and are described in Appendix A. We selected the feature sets by greedy forward subset se lection.</S>
			<S sid="78" ssid="50">72 Feature PredDis ArgId ArgLab PREDWORD ? PREDLEMMA ? PREDPARENTWORD/POS ? CHILDDEPSET ? ?</S>
			<S sid="79" ssid="51">CHILDWORDSET ? CHILDWORDDEPSET ? CHILDPOSSET ? CHILDPOSDEPSET ? DEPSUBCAT ? PREDRELTOPARENT ? PREDPARENTWORD/POS ? PREDLEMMASENSE ? ?</S>
			<S sid="80" ssid="52">VOICE ? ?</S>
			<S sid="81" ssid="53">POSITION ? ?</S>
			<S sid="82" ssid="54">ARGWORD/POS ? ?</S>
			<S sid="83" ssid="55">LEFTWORD/POS ? RIGHTWORD/POS ? ?</S>
			<S sid="84" ssid="56">LEFTSIBLINGWORD/POS ? PREDPOS ? ?</S>
			<S sid="85" ssid="57">RELPATH ? ?</S>
			<S sid="86" ssid="58">VERBCHAINHASSUBJ ? ?</S>
			<S sid="87" ssid="59">CONTROLLERHASOBJ ? PREDRELTOPARENT ? ?</S>
			<S sid="88" ssid="60">FUNCTION ? Table 1: Classifier features in predicate disambiguation (PredDis), argument identification (ArgId), and argument labeling (ArgLab).</S>
			<S sid="89" ssid="61">2.2.2 Linguistically Motivated Global Constraints The following three global constraints were used to filter the candidates generated by the pipeline.CORE ARGUMENT CONSISTENCY.</S>
			<S sid="90" ssid="62">Core argu ment labels must not appear more than once.DISCONTINUITY CONSISTENCY.</S>
			<S sid="91" ssid="63">If there is a la bel C-X, it must be preceded by a label X.REFERENCE CONSISTENCY.</S>
			<S sid="92" ssid="64">If there is a label R X and the label is inside an attributive relative clause, it must be preceded by a label X. 2.2.3 Predicate?Argument Reranker Toutanova et al (2005) have showed that a global model that scores the complete predicate?argument structure can lead to substantial performance gains.</S>
			<S sid="93" ssid="65">We therefore created a global SRL classifier usingthe following global features in addition to the fea tures from the pipeline:CORE ARGUMENT LABEL SEQUENCE.</S>
			<S sid="94" ssid="66">The com plete sequence of core argument labels.</S>
			<S sid="95" ssid="67">The sequence also includes the predicate and voice, for instance A0+break.01/Active+A1.</S>
			<S sid="96" ssid="68">MISSING CORE ARGUMENT LABELS.</S>
			<S sid="97" ssid="69">The set of core argument labels declared in the PropBank frame that are not present in the predicate?</S>
			<S sid="98" ssid="70">argument structure.</S>
			<S sid="99" ssid="71">Similarly to the syntactic submodel, we trained the global SRL model using the online passive?</S>
			<S sid="100" ssid="72">aggressive algorithm.</S>
			<S sid="101" ssid="73">The cost function ? was defined as the number of incorrect links in thepredicate?argument structure.</S>
			<S sid="102" ssid="74">The number of iter ations was 20 and the regularization parameter C was 0.01.</S>
			<S sid="103" ssid="75">Interestingly, we noted that the global SRL model outperformed the pipeline even when no global features were added.</S>
			<S sid="104" ssid="76">This shows that theglobal learning model can correct label bias prob lems introduced by the pipeline architecture.</S>
			<S sid="105" ssid="77">2.3 Syntactic?Semantic Reranking.</S>
			<S sid="106" ssid="78">As described previously, we carried out reranking on the candidate set of complete syntactic?semantic structures.</S>
			<S sid="107" ssid="79">To do this, we used the top 16 trees from the syntactic module and applied a linear model: Fjoint(x, ysyn, ysem) = ?joint(x, ysyn, ysem) ?wOur baseline joint feature representation?joint con tained only three features: the log probability of thesyntactic tree and the log probability of the seman tic structure according to the pipeline and the global model, respectively.</S>
			<S sid="108" ssid="80">This model was trained on the complete training set using cross-validation.</S>
			<S sid="109" ssid="81">The probabilities were obtained using the multinomial logistic function (?softmax?).</S>
			<S sid="110" ssid="82">We carried out an initial experiment with a more complex joint feature representation, but failed to improve over the baseline.</S>
			<S sid="111" ssid="83">Time prevented us from exploring this direction conclusively.</S>
	</SECTION>
	<SECTION title="Comparisons with Previous Results. " number="3">
			<S sid="112" ssid="1">To compare our results with previously publishedresults in SRL, we carried out an experiment com paring our system to the top system (Punyakanok etal., 2008) in the CoNLL-2005 Shared Task.</S>
			<S sid="113" ssid="2">How ever, comparison is nontrivial since the output ofthe CoNLL-2005 systems was a set of labeled seg ments, while the CoNLL-2008 systems (including ours) produced labeled semantic dependency links.To have a fair comparison of our link-based sys tem against previous segment-based systems, we 73carried out a two-way evaluation: In the first eval uation, the dependency-based output was converted to segments and evaluated using the segment scorer from CoNLL-2005, and in the second evaluation, we applied a head-finding procedure to the output of a segment-based system and scored the result using the link-based CoNLL-2008 scorer.</S>
			<S sid="114" ssid="3">It can be discussed which of the two metrics is most correlated with application performance.</S>
			<S sid="115" ssid="4">The traditional metric used in the CoNLL-2005 task treats SRL as a bracketing problem, meaning that the entities scored by the evaluation procedure are labeled snippets of text; however, it is questionable whether this is the proper way to evaluate a task whose purpose is to find semantic relations between logical entities.</S>
			<S sid="116" ssid="5">We believe that the same criticisms that have been leveled at the PARSEVAL metric for constituent structures are equally valid for thebracket-based evaluation of SRL systems.</S>
			<S sid="117" ssid="6">The in appropriateness of the traditional metric has led to a number of alternative metrics (Litkowski, 2004; Baker et al, 2007; Surdeanu et al, 2008).</S>
			<S sid="118" ssid="7">3.1 Segment-based Evaluation.</S>
			<S sid="119" ssid="8">To be able to score the output of a dependency-based SRL system using the segment scorer, a conversionstep is needed.</S>
			<S sid="120" ssid="9">Algorithm 2 shows how a set of seg ments is constructed from an argument dependencynode.</S>
			<S sid="121" ssid="10">For each argument node, the algorithm com putes the yield Y of the argument node, i.e. the set of dependency nodes to include in the bracketing.</S>
			<S sid="122" ssid="11">This set is then partitioned into contiguous parts, from which the segments are computed.</S>
			<S sid="123" ssid="12">In most cases,the yield is just the subtree dominated by the argu ment node.</S>
			<S sid="124" ssid="13">However, if the argument dominates the predicate, then the branch containing the predicate is removed.</S>
			<S sid="125" ssid="14">Table 2 shows the performance figures of oursystem on the WSJ and Brown corpora: preci sion, recall, F1-measure, and complete proposition accuracy (PP).</S>
			<S sid="126" ssid="15">These figures are compared to the best-performing system in the CoNLL-2005 SharedTask (Punyakanok et al, 2008), referred to as Pun yakanok in the table, and the best result currentlypublished (Surdeanu et al, 2007), referred to as Surdeanu.</S>
			<S sid="127" ssid="16">To validate the sanity of the segment creation algorithm, the table also shows the result of ap plying segment creation to gold-standard syntactic?</S>
			<S sid="128" ssid="17">Algorithm 2 Segment creation from an argument dependency node.</S>
			<S sid="129" ssid="18">input Predicate node p, argument node a if a does not dominate p Y ? {n : a dominates n} else c?</S>
			<S sid="130" ssid="19">the child of a that dominates p Y ? {n : a dominates n} \ {n : c dominates n} end if S ? partition of Y into contiguous subsets return {(min-index s,max-index s) : s ? S} WSJ P R F1 PP Our system 82.22 77.72 79.90 57.24 Punyakanok 82.28 76.78 79.44 53.79 Surdeanu 87.47 74.67 80.56 51.66 Gold standard 97.38 96.77 97.08 93.20 Brown P R F1 PP Our system 68.79 61.87 65.15 32.34 Punyakanok 73.38 62.93 67.75 32.34 Surdeanu 81.75 61.32 70.08 34.33 Gold standard 97.22 96.55 96.89 92.79 WSJ+Brown P R F1 PP Our system 80.50 75.59 77.97 53.94 Punyakanok 81.18 74.92 77.92 50.95 Surdeanu 86.78 72.88 79.22 49.36 Gold standard 97.36 96.75 97.05 93.15 Table 2: Evaluation with unnormalized segments.semantic trees.</S>
			<S sid="131" ssid="20">We see that the two conversion procedures involved (constituent-to-dependency con version by the CoNLL-2008 Shared Task organizers, and our dependency-to-segment conversion) work satisfactorily although the process is not completely lossless.</S>
			<S sid="132" ssid="21">During inspection of the output, we noted thatmany errors arise from inconsistent punctuation attachment in PropBank/Treebank.</S>
			<S sid="133" ssid="22">We therefore nor malized the segments to exclude punctuation at the beginning or end of a segment.</S>
			<S sid="134" ssid="23">The results of this evaluation is shown in Table 3.</S>
			<S sid="135" ssid="24">This table does not include the Surdeanu system since we did not have 74 access to its output.</S>
			<S sid="136" ssid="25">WSJ P R F1 PP Our system 82.95 78.40 80.61 58.65 Punyakanok 82.67 77.14 79.81 54.55 Gold standard 97.85 97.24 97.54 94.34 Brown P R F1 PP Our system 70.84 63.71 67.09 36.94 Punyakanok 74.29 63.71 68.60 34.08 Gold standard 97.46 96.78 97.12 93.41 WSJ+Brown P R F1 PP Our system 81.39 76.44 78.84 55.77 Punyakanok 81.63 75.34 78.36 51.84 Gold standard 97.80 97.18 97.48 94.22 Table 3: Evaluation with normalized segments.</S>
			<S sid="137" ssid="26">The results on the WSJ test set clearly show that dependency-based SRL systems can rival constituent-based systems in terms of performance ? it clearly outperforms the Punyakanok system, andhas a higher recall and complete proposition accu racy than the Surdeanu system.</S>
			<S sid="138" ssid="27">We interpret the highrecall as a result of the dependency syntactic repre sentation, which makes the parse tree paths simpler and thus the arguments easier to find.</S>
			<S sid="139" ssid="28">For the Brown test set, on the other hand, thedependency-based system suffers from a low pre cision compared to the constituent-based systems.</S>
			<S sid="140" ssid="29">Our error analysis indicates that the domain change caused problems with prepositional attachment forthe dependency parser ? it is well-known that prepo sitional attachment is a highly lexicalized problem, and thus sensitive to domain changes.</S>
			<S sid="141" ssid="30">We believe that the reason why the constituent-based systems are more robust in this respect is that they utilize acombination strategy, using inputs from two differ ent full constituent parsers, a clause bracketer, anda chunker.</S>
			<S sid="142" ssid="31">However, caution is needed when draw ing conclusions from results on the Brown test set, which is only 7,585 words, compared to the 59,100 words in the WSJ test set.</S>
			<S sid="143" ssid="32">3.2 Dependency-based Evaluation.</S>
			<S sid="144" ssid="33">It has previously been noted (Pradhan et al, 2005) that a segment-based evaluation may be unfavorableto a dependency-based system, and that an evaluation that scores argument heads may be more indica tive of its true performance.</S>
			<S sid="145" ssid="34">We thus carried out anevaluation using the evaluation script of the CoNLL 2008 Shared Task.</S>
			<S sid="146" ssid="35">In this evaluation method, an ar-.</S>
			<S sid="147" ssid="36">gument is counted as correctly identified if its head and label are correct.</S>
			<S sid="148" ssid="37">Note that this is not equivalentto the segment-based metric: In a perfectly identi fied segment, we may still pick out the wrong head, and if the head is correct, we may infer an incorrect segment.</S>
			<S sid="149" ssid="38">The evaluation script also scores predicate disambiguation performance; we did not include thisscore since the 2005 systems did not output predi cate sense identifiers.Since CoNLL-2005-style segments have no in ternal tree structure, it is nontrivial to extract a head.</S>
			<S sid="150" ssid="39">It is conceivable that the output of the parsers used by the Punyakanok system could be used toextract heads, but this is not recommendable because the Punyakanok system is an ensemble sys tem and a segment does not always exactly match a constituent in a parse tree.</S>
			<S sid="151" ssid="40">Furthermore, the CoNLL-2008 constituent-to-dependency conversionmethod uses a richer structure than just the raw con stituents: empty categories, grammatical functions,and named entities.</S>
			<S sid="152" ssid="41">To recreate this additional infor mation, we would have to apply automatic systems and end up with unreliable results.</S>
			<S sid="153" ssid="42">Instead, we thus chose to find an upper bound on the performance of the segment-based system.We applied a simple head-finding procedure (Algorithm 3) to find a set of head nodes for each segment.</S>
			<S sid="154" ssid="43">Since the CoNLL-2005 output does not in clude dependency information, the algorithm uses gold-standard dependencies and intersects segments with the gold-standard segments.</S>
			<S sid="155" ssid="44">This will give us an upper bound, since if the segment contains the correct head, it will always be counted as correct.</S>
			<S sid="156" ssid="45">The algorithm looks for dependencies leaving the segment, and if multiple outgoing edges are found, a couple of simple heuristics are applied.</S>
			<S sid="157" ssid="46">We found that the best performance is achieved when selecting only one outgoing edge.</S>
			<S sid="158" ssid="47">?Small clauses,?</S>
			<S sid="159" ssid="48">which are split into an object and a predicative complement in the dependency framework, are the only cases where we select two heads.Table 4 shows the results of the dependency based evaluation.</S>
			<S sid="160" ssid="49">In the table, the output of the 75 Algorithm 3 Finding head nodes in a segment.</S>
			<S sid="161" ssid="50">input Argument segment a if a overlaps with a segment in the gold standard a?</S>
			<S sid="162" ssid="51">intersection of a and gold standard F ? {n : governor of n outside a} if |F | = 1 return F remove punctuation nodes from F if |F | = 1 return F if F = {n1, n2, . . .}</S>
			<S sid="163" ssid="52">where n1 is an object and n2 is the predicative part of a small clause return {n1, n2} if F contains a node n that is a subject or an object return {n} else return {n}, where n is the leftmost node in Fdependency-based system is compared to the seman tic dependency links automatically extracted from the segments of the Punyakanok system.</S>
			<S sid="164" ssid="53">WSJ P R F1 PP Our system 88.46 83.55 85.93 61.97 Punyakanok 87.25 81.59 84.32 58.17 Brown P R F1 PP Our system 77.67 69.63 73.43 41.32 Punyakanok 80.29 68.59 73.98 37.28 WSJ+Brown P R F1 PP Our system 87.07 81.68 84.29 59.22 Punyakanok 86.94 80.21 83.45 55.39 Table 4: Dependency-based evaluation.</S>
			<S sid="165" ssid="54">In this evaluation, the dependency-based systemhas a higher F1-measure than the Punyakanok system on both test sets.</S>
			<S sid="166" ssid="55">This suggests that the main ad vantage of using a dependency-based semantic role labeler is that it is better at finding the heads of semantic arguments, rather than finding segments.</S>
			<S sid="167" ssid="56">The results are also interesting in comparison to the multi-view system described by Pradhan et al (2005), which has a reported head F1 measure of 85.2 on the WSJ test set.</S>
			<S sid="168" ssid="57">The figure is not exactly compatible with ours, however, since that system used a different head extraction mechanism.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="4">
			<S sid="169" ssid="1">We have described a dependency-based system1 for semantic role labeling of English in the PropBankframework.</S>
			<S sid="170" ssid="2">Our evaluations show that the perfor mance of our system is close to the state of theart.</S>
			<S sid="171" ssid="3">This holds regardless of whether a segmentbased or a dependency-based metric is used.</S>
			<S sid="172" ssid="4">In terestingly, our system has a complete proposition accuracy that surpasses other systems by nearly 3 percentage points.</S>
			<S sid="173" ssid="5">Our system is the first semantic role labeler based only on syntactic dependency that achieves a competitive performance.</S>
			<S sid="174" ssid="6">Evaluation and comparison is a difficult issuesince the natural output of a dependency-based sys tem is a set of semantic links rather than segments, as is normally the case for traditional systems.</S>
			<S sid="175" ssid="7">To handle this situation fairly to both types of systems, we carried out a two-way evaluation: conversion of dependencies to segments for the dependency-basedsystem, and head-finding heuristics for segment based systems.</S>
			<S sid="176" ssid="8">However, the latter is difficult since no structure is available inside segments, and we had to resort to computing upper-bound results usinggold-standard input; despite this, the dependency based system clearly outperformed the upper bound of the performance of the segment-based system.</S>
			<S sid="177" ssid="9">The comparison can also be slightly misleading since the dependency-based system was optimized for the dependency metric and previous systems for the segment metric.Our evaluations suggest that the dependency based SRL system is biased to finding argument heads, rather than argument text snippets, and thisis of course perfectly logical.</S>
			<S sid="178" ssid="10">Whether this is an advantage or a drawback will depend on the applica tion ? for instance, a template-filling system might need complete segments, while an SRL-based vectorspace representation for text categorization, or a rea soning application, might prefer using heads only.</S>
			<S sid="179" ssid="11">In the future, we would like to further investigatewhether syntactic and semantic analysis could be integrated more tightly.</S>
			<S sid="180" ssid="12">In this work, we used a sim 1Our system is freely available for download at http://nlp.cs.lth.se/lth_srl.</S>
			<S sid="181" ssid="13">76 plistic loose coupling by means of reranking a small set of complete structures.</S>
			<S sid="182" ssid="14">The same criticisms that are often leveled at reranking-based models clearly apply here too: The set of tentative analyses from the submodules is too small, and the correct analysis is often pruned too early.</S>
			<S sid="183" ssid="15">An example of a method to mitigate this shortcoming is the forest reranking byHuang (2008), in which complex features are evalu ated as early as possible.</S>
			<S sid="184" ssid="16">A Classifier Features Features Used in Predicate Disambiguation PREDWORD, PREDLEMMA.</S>
			<S sid="185" ssid="17">The lexical form and lemma of the predicate.</S>
			<S sid="186" ssid="18">PREDPARENTWORD and PREDPARENTPOS.</S>
			<S sid="187" ssid="19">Form and part-of-speech tag of the parent node of the predicate.CHILDDEPSET, CHILDWORDSET, CHILDWORDDEPSET, CHILDPOSSET, CHILD POSDEPSET.</S>
			<S sid="188" ssid="20">These features represent the setof dependents of the predicate using combina tions of dependency labels, words, and parts of speech.DEPSUBCAT.</S>
			<S sid="189" ssid="21">Subcategorization frame: the concatenation of the dependency labels of the pred icate dependents.PREDRELTOPARENT.</S>
			<S sid="190" ssid="22">Dependency relation be tween the predicate and its parent.</S>
			<S sid="191" ssid="23">Features Used in Argument Identification and LabelingPREDLEMMASENSE.</S>
			<S sid="192" ssid="24">The lemma and sense num ber of the predicate, e.g. give.01.</S>
			<S sid="193" ssid="25">VOICE.</S>
			<S sid="194" ssid="26">For verbs, this feature is Active or Passive.</S>
			<S sid="195" ssid="27">For nouns, it is not defined.</S>
			<S sid="196" ssid="28">POSITION.</S>
			<S sid="197" ssid="29">Position of the argument with respect to the predicate: Before, After, or On.ARGWORD and ARGPOS.</S>
			<S sid="198" ssid="30">Lexical form and part of-speech tag of the argument node.LEFTWORD, LEFTPOS, RIGHTWORD, RIGHTPOS.</S>
			<S sid="199" ssid="31">Form/part-of-speech tag of the left most/rightmost dependent of the argument.</S>
			<S sid="200" ssid="32">LEFTSIBLINGWORD, LEFTSIBLINGPOS.</S>
			<S sid="201" ssid="33">Form/part-of-speech tag of the left sibling of the argument.</S>
			<S sid="202" ssid="34">PREDPOS.</S>
			<S sid="203" ssid="35">Part-of-speech tag of the predicate.RELPATH.</S>
			<S sid="204" ssid="36">A representation of the complex gram matical relation between the predicate and theargument.</S>
			<S sid="205" ssid="37">It consists of the sequence of de pendency relation labels and link directions in the path between predicate and argument, e.g. IM?OPRD?OBJ?.</S>
			<S sid="206" ssid="38">VERBCHAINHASSUBJ.</S>
			<S sid="207" ssid="39">Binary feature that is set to true if the predicate verb chain has a subject.</S>
			<S sid="208" ssid="40">The purpose of this feature is to resolve verb coordination ambiguity as in Figure 3.</S>
			<S sid="209" ssid="41">CONTROLLERHASOBJ.</S>
			<S sid="210" ssid="42">Binary feature that is true if the link between the predicate verb chain andits parent is OPRD, and the parent has an ob ject.</S>
			<S sid="211" ssid="43">This feature is meant to resolve control ambiguity as in Figure 4.FUNCTION.</S>
			<S sid="212" ssid="44">The grammatical function of the argument node.</S>
			<S sid="213" ssid="45">For direct dependents of the predi cate, this is identical to the RELPATH.</S>
			<S sid="214" ssid="46">I SBJ eat drinkyouand COORD SBJCONJ ROOT SBJ COORD ROOT drinkandeatI CONJ Figure 3: Coordination ambiguity: The subject I is in an ambiguous position with respect to drink.</S>
			<S sid="215" ssid="47">I to IMSBJ want sleephim OBJOPRD ROOT IM sleepI SBJ want ROOT to OPRDFigure 4: Subject/object control ambiguity: I is in an am biguous position with respect to sleep.</S>
			<S sid="216" ssid="48">77</S>
	</SECTION>
</PAPER>
