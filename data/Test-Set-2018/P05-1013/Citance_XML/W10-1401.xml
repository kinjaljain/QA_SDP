<PAPER>
	<S sid="0">Statistical Parsing of Morphologically Rich Languages (SPMRL) What How and Whither</S><ABSTRACT>
		<S sid="1" ssid="1">The term Morphologically Rich Languages(MRLs) refers to languages in which signif icant information concerning syntactic units and relations is expressed at word-level.</S>
		<S sid="2" ssid="2">Thereis ample evidence that the application of read ily available statistical parsing models to suchlanguages is susceptible to serious performance degradation.</S>
		<S sid="3" ssid="3">The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associ ated with parsing MRLs cut across languagesand parsing frameworks.</S>
		<S sid="4" ssid="4">In this paper we re view the current state-of-affairs with respectto parsing MRLs and point out central challenges.</S>
		<S sid="5" ssid="5">We synthesize the contributions of re searchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages.</S>
		<S sid="6" ssid="6">The overarching analysis suggests itself as a source of directions for future investigations.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="7" ssid="7">The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers.</S>
			<S sid="8" ssid="8">The devel opment of successful statistical parsing models for English focused on the Wall Street Journal PennTreebank (PTB, (Marcus et al, 1993)) as the primary, and sometimes only, resource.</S>
			<S sid="9" ssid="9">Since the ini tial release of the Penn Treebank (PTB Marcus et al.</S>
			<S sid="10" ssid="10">(1993)), many different constituent-based parsingmodels have been developed in the context of pars ing English (e.g.</S>
			<S sid="11" ssid="11">(Magerman, 1995; Collins, 1997;Charniak, 2000; Chiang, 2000; Bod, 2003; Char niak and Johnson, 2005; Petrov et al, 2006; Huang, 2008; Finkel et al, 2008; Carreras et al, 2008)).</S>
			<S sid="12" ssid="12">At their time, each of these models improved the state-of-the-art, bringing parsing performance on thestandard test set of the Wall-Street-Journal to a performance ceiling of 92% F1-score using the PARS EVAL evaluation metrics (Black et al, 1991).</S>
			<S sid="13" ssid="13">Someof these parsers have been adapted to other lan guage/treebank pairs, but many of these adaptations have been shown to be considerably less successful.</S>
			<S sid="14" ssid="14">Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks?</S>
			<S sid="15" ssid="15">annotationschemes, and inadequacy of the widely used PARS EVAL evaluation metrics.</S>
			<S sid="16" ssid="16">None of these aspects in isolation can account for the systematic performancedeterioration, but observed from a wider, cross linguistic perspective, a picture begins to emerge ? that the morphologically rich nature of some of the languages makes them inherently more susceptible to such performance degradation.</S>
			<S sid="17" ssid="17">Linguistic factors associated with MRLs, such as a large inventory of word-forms, higher degrees of word order freedom,and the use of morphological information in indi cating syntactic relations, makes them substantially harder to parse with models and techniques that have been developed with English data in mind.</S>
			<S sid="18" ssid="18">1In addition to these technical and linguistic factors, the prominence of English parsing in the litera ture reduces the visibility of research aiming to solveproblems particular to MRLs.</S>
			<S sid="19" ssid="19">The lack of stream lined communication among researchers working on different MRLs often leads to a reinventing thewheel syndrome.</S>
			<S sid="20" ssid="20">To circumvent this, the first work shop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2010) offers a platform for this growing community to share their views of the different problems and oftentimes similar solutions.</S>
			<S sid="21" ssid="21">We identify three main types of challenges, eachof which raises many questions.</S>
			<S sid="22" ssid="22">Many of the ques tions are yet to be conclusively answered.</S>
			<S sid="23" ssid="23">The first type of challenges has to do with the architecturalsetup of parsing MRLs: What is the nature of the in put?</S>
			<S sid="24" ssid="24">Can words be represented abstractly to reflect shared morphological aspects?</S>
			<S sid="25" ssid="25">How can we cope with morphological segmentation errors propagated through the pipeline?</S>
			<S sid="26" ssid="26">The second type concerns the representation of morphological information insidethe articulated syntactic model: Should morpholog ical information be encoded at the level of PoS tags?</S>
			<S sid="27" ssid="27">On dependency relations?</S>
			<S sid="28" ssid="28">On top of non-terminals symbols?</S>
			<S sid="29" ssid="29">How should the integrated representations be learned and used?</S>
			<S sid="30" ssid="30">A final genuine challengehas to do with sound estimation for lexical probabil ities: Given the finite, and often rather small, set ofdata, and the large number of morphological analy ses licensed by rich inflectional systems, how can we analyze words unseen in the training data?</S>
			<S sid="31" ssid="31">Many of the challenges reported here are mostly irrelevant when parsing Section 23 of the PTB butthey are of primordial importance in other tasks, in cluding out-of-domain parsing, statistical machine translation, and parsing resource-poor languages.</S>
			<S sid="32" ssid="32">By synthesizing the contributions to the workshop and bringing it to the forefront, we hope to advance the state of the art of statistical parsing in general.</S>
			<S sid="33" ssid="33">In this paper we therefore take the opportunity to analyze the knowledge that has been acquired inthe different investigations for the purpose of iden tifying main bottlenecks and pointing out promising research directions.</S>
			<S sid="34" ssid="34">In section 2, we define MRLs and identify syntactic characteristics associated with them.</S>
			<S sid="35" ssid="35">We then discuss work on parsing MRLs in both the dependency-based and constituency-basedsetup.</S>
			<S sid="36" ssid="36">In section 3, we review the types of challenges associated with parsing MRLs across frame works.</S>
			<S sid="37" ssid="37">In section 4, we focus on the contributions to the SPMRL workshop and identify recurring trends in the empirical results and conceptual solutions.</S>
			<S sid="38" ssid="38">In section 5, we analyze the emerging picture from a bird?s eye view, and conclude that many challenges could be more faithfully addressed in the context of parsing morphologically ambiguous input.</S>
	</SECTION>
	<SECTION title="Background. " number="2">
			<S sid="39" ssid="1">2.1 What are MRLs?.</S>
			<S sid="40" ssid="2">The term Morphologically Rich Languages (MRLs) is used in the CL/NLP literature to refer to languages in which substantial grammatical information, i.e., information concerning the arrangement of words into syntactic units or cues to syntactic relations, is expressed at word level.</S>
			<S sid="41" ssid="3">The common linguistic and typological wisdom is that ?morphology competes with syntax?</S>
			<S sid="42" ssid="4">(Bresnan, 2001).</S>
			<S sid="43" ssid="5">In effect, this means that rich morphology goes hand in hand with a host of nonconfigurational syntactic phenomena of the kind discussed by Hale(1983).</S>
			<S sid="44" ssid="6">Because information about the relations be tween syntactic elements is indicated in the form of words, these words can freely change their positionsin the sentence.</S>
			<S sid="45" ssid="7">This is referred to as free word order (Mithun, 1992).</S>
			<S sid="46" ssid="8">Information about the group ing of elements together can further be expressed by reference to their morphological form.</S>
			<S sid="47" ssid="9">Such logicalgroupings of disparate elements are often called dis continuous constituents.</S>
			<S sid="48" ssid="10">In dependency structures, such discontinuities impose nonprojectivity.</S>
			<S sid="49" ssid="11">Finally,rich morphological information is found in abun dance in conjunction with so-called pro-drop or zeroanaphora.</S>
			<S sid="50" ssid="12">In such cases, rich morphological information in the head (or co-head) of the clause of ten makes it possible to omit an overt subject which would be semantically impoverished.</S>
			<S sid="51" ssid="13">English, the most heavily studied language within the CL/NLP community, is not an MRL.</S>
			<S sid="52" ssid="14">Eventhough a handful of syntactic features (such as per son and number) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases.</S>
			<S sid="53" ssid="15">German, an Indo-European language closely related to English, already exhibits some of the properties that make 2 parsing MRLs problematic.</S>
			<S sid="54" ssid="16">The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering.</S>
			<S sid="55" ssid="17">2.2 Parsing MRLs.</S>
			<S sid="56" ssid="18">Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002).</S>
			<S sid="57" ssid="19">For Czech, the adaptation by Collins et al (1999) culminated in an 80 F1-score.</S>
			<S sid="58" ssid="20">German has become almost an archetype of the problems caused by MRLs; even though Germanhas a moderately rich morphology and a moder ately free word order, parsing results are far from those for English (see (Ku?bler, 2008) and references therein).</S>
			<S sid="59" ssid="21">Dubey (2005) showed that, for German parsing, adding case and morphology informationtogether with smoothed markovization and an ade quate unknown-word model is more important than lexicalization (Dubey and Keller, 2003).</S>
			<S sid="60" ssid="22">For Modern Hebrew, Tsarfaty and Sima?an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003).</S>
			<S sid="61" ssid="23">Results for parsingModern Standard Arabic using Bikel?s implemen tation on gold-standard tagging and segmentationhave not improved substantially since the initial re lease of the treebank (Maamouri et al, 2004; Kulick et al, 2006; Maamouri et al, 2008).For Italian, Corazza et al (2004) used the Stan ford parser and Bikel?s parser emulation of Collins?</S>
			<S sid="62" ssid="24">model 2 (Collins, 1997) on the ISST treebank, andobtained significantly lower results compared to English.</S>
			<S sid="63" ssid="25">It is notable that these models were applied without adding morphological signatures, using gold lemmas instead.</S>
			<S sid="64" ssid="26">Corazza et al (2004) further tried different refinements including parent an notation and horizontal markovization, but none of them obtained the desired improvement.For French, Crabbe?</S>
			<S sid="65" ssid="27">and Candito (2008) and Seddah et al (2010) show that, given a corpus compara ble in size and properties (i.e. the number of tokens and grammar size), the performance level, both forCharniak?s parser (Charniak, 2000) and the Berkeley parser (Petrov et al, 2006) was higher for parsing the PTB than it was for French.</S>
			<S sid="66" ssid="28">The split-mergesmooth implementation of (Petrov et al, 2006) consistently outperform various lexicalized and unlexi calized models for French (Seddah et al, 2009) and for many other languages (Petrov and Klein, 2007).</S>
			<S sid="67" ssid="29">In this respect, (Petrov et al, 2006) is considered MRL-friendly, due to its language agnostic design.</S>
			<S sid="68" ssid="30">The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on theposition of words and the internal grouping of surface chunks (Mel?c?uk, 1988).</S>
			<S sid="69" ssid="31">It is an entirely differ ent question, however, whether dependency parsers are in fact better suited for parsing such languages.The CoNLL shared tasks on multilingual depen dency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al, 2007a) demonstrated that dependency parsing for MRLs is quite challenging.</S>
			<S sid="70" ssid="32">While dependency parsers are adaptable to manylanguages, as reflected in the multiplicity of the lan guages covered,1 the analysis by Nivre et al (2007b) shows that the best result was obtained for English,followed by Catalan, and that the most difficult lan guages to parse were Arabic, Basque, and Greek.</S>
			<S sid="71" ssid="33">Nivre et al (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse.</S>
			<S sid="72" ssid="34">This was shown to be the case for both MaltParser (Nivre et al, 2007c) and MST (McDonald et al, 2005), two of the best performing parsers on the whole.Annotation and evaluation matter: An emerg ing question is therefore whether models that havebeen so successful in parsing English are necessar ily appropriate for parsing MRLs ? but associatedwith this question are important questions concern ing the annotation scheme of the related treebanks.</S>
			<S sid="73" ssid="35">Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB.</S>
			<S sid="74" ssid="36">1The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.</S>
			<S sid="75" ssid="37">3 For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks?</S>
			<S sid="76" ssid="38">native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997).</S>
			<S sid="77" ssid="39">For German, alanguage with four different treebanks and two sub stantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank.</S>
			<S sid="78" ssid="40">Dubey and Keller (2003), for example, showedthat a simple PCFG parser outperformed an emula tion of Collins?</S>
			<S sid="79" ssid="41">model 1 on NEGRA.</S>
			<S sid="80" ssid="42">They showedthat using sister-head dependencies instead of head head dependencies improved parsing performance, and hypothesized that it is due to the flatness ofphrasal annotation.</S>
			<S sid="81" ssid="43">Ku?bler et al (2006) showed con siderably lower PARSEVAL scores on NEGRA (Skutet al, 1998) relative to the more hierarchically structured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy pothesizing that this is due to annotation differences.</S>
			<S sid="82" ssid="44">Related to such comparisons is the question of the relevance of the PARSEVAL metrics for evaluatingparsing results across languages and treebanks.</S>
			<S sid="83" ssid="45">Rehbein and van Genabith (2007) showed that PARS EVAL measures are sensitive to annotation scheme particularities (e.g. the internal node ratio).</S>
			<S sid="84" ssid="46">It wasfurther shown that different metrics (i.e. the Leaf ancestor path (Sampson and Babarczy, 2003) and dependency based ones in (Lin, 1995)) can lead to different performance ranking.</S>
			<S sid="85" ssid="47">This was confirmed also for French by Seddah et al (2009).</S>
			<S sid="86" ssid="48">The questions of how to annotate treebanks for MRLs and how to evaluate the performance of thedifferent parsers on these different treebanks is cru cial.</S>
			<S sid="87" ssid="49">For the MRL parsing community to be able to assess the difficulty of improving parsing results for French, German, Arabic, Korean, Basque, Hindi orHebrew, we ought to first address fundamental ques tions including: Is the treebank sufficiently large to allow for proper grammar induction?</S>
			<S sid="88" ssid="50">Does the annotation scheme fit the language characteristics?</S>
			<S sid="89" ssid="51">Does the use of PTB annotation variants for otherlanguages influence parsing results?</S>
			<S sid="90" ssid="52">Does the space delimited tokenization allow for phrase boundary detection?</S>
			<S sid="91" ssid="53">Do the results for a specific approach generalize to more than one language?</S>
	</SECTION>
	<SECTION title="Primary Research Questions. " number="3">
			<S sid="92" ssid="1">It is firmly established in theoretical linguistics thatmorphology and syntax closely interact through pat terns of case marking, agreement, clitics and varioustypes of compounds.</S>
			<S sid="93" ssid="2">Because of such close interac tions, we expect morphological cues to help parsingperformance.</S>
			<S sid="94" ssid="3">But in practice, when trying to incorporate morphological information into parsing mod els, three types of challenges present themselves: Architecture and Setup: When attempting to parse complex word-forms that encapsulate bothlexical and functional information, important architectural questions emerge, namely, what is the na ture of the input that is given to the parsing system?</S>
			<S sid="95" ssid="4">Does the system attempt to parse sequences of words or does it aim to assign structures to sequences of morphological segments?</S>
			<S sid="96" ssid="5">If the former is the case,how can we represent words abstractly so as to re flect shared morphological aspects between them?</S>
			<S sid="97" ssid="6">If the latter is the case, how can we arrive at a good enough morphological segmentation for the purpose of statistical parsing, given raw input texts?When working with morphologically rich lan guages such as Hebrew or Arabic, affixes may have syntactically independent functions.</S>
			<S sid="98" ssid="7">Many parsingmodels assume segmentation of the syntactically in dependent parts, such as prepositions or pronominalclitics, prior to parsing.</S>
			<S sid="99" ssid="8">But morphological segmen tation requires disambiguation which is non-trivial,due to case syncretism and high morphological am biguity exhibited by rich inflectional systems.</S>
			<S sid="100" ssid="9">The question is then when should we disambiguate the morphological analyses of input forms?</S>
			<S sid="101" ssid="10">Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming thatthe input to our system reflects morphological information, one way or another, which types of morpho 2Most studies on parsing MRLs nowadays assume the goldstandard segmentation and disambiguated morphological infor mation as input.</S>
			<S sid="102" ssid="11">This is the case, for instance, for the Arabicparsing at CoNLL 2007 (Nivre et al, 2007a).</S>
			<S sid="103" ssid="12">This practice de ludes the community as to the validity of the parsing results reported for MRLs in shared tasks.</S>
			<S sid="104" ssid="13">Goldberg et al (2009), for instance, show a gap of up to 6pt F1-score between performanceon gold standard segmentation vs. raw text.</S>
			<S sid="105" ssid="14">One way to overcome this is to devise joint morphological and syntactic disam biguation frameworks (cf.</S>
			<S sid="106" ssid="15">(Goldberg and Tsarfaty, 2008)).</S>
			<S sid="107" ssid="16">4 logical information should we include in the parsingmodel?</S>
			<S sid="108" ssid="17">Inflectional and/or derivational?</S>
			<S sid="109" ssid="18">Case infor mation and/or agreement features?</S>
			<S sid="110" ssid="19">How can valency requirements reflected in derivational morphology affect the overall syntactic structure?</S>
			<S sid="111" ssid="20">In tandem withthe decision concerning the morphological information to include, we face genuine challenges concerning how to represent such information in the syntactic model, be it constituency-based or dependencybased.</S>
			<S sid="112" ssid="21">Should we encode morphological information at the level of PoS tags and/or on top of syn tactic elements?</S>
			<S sid="113" ssid="22">Should we decorate non-terminals nodes and/or dependency arcs or both?</S>
			<S sid="114" ssid="23">Incorporating morphology in the statistical model is often even more challenging than the sum ofthese bare decisions, because of the nonconfigu rational structures (free word order, discontinuous constituents) for rich markings are crucial (Hale,1983).</S>
			<S sid="115" ssid="24">The parsing models designed for English of ten focus on learning rigid word order, and they do not take morphological information into account (cf.</S>
			<S sid="116" ssid="25">developing parsers for German (Dubey and Keller,2003; Ku?bler et al, 2006)).</S>
			<S sid="117" ssid="26">The more complex ques tion is therefore: what type of parsing model should we use for parsing MRLs?</S>
			<S sid="118" ssid="27">shall we use a general purpose implementation and attempt to amend it?</S>
			<S sid="119" ssid="28">how?</S>
			<S sid="120" ssid="29">or perhaps we should devise a new model fromfirst principles, to address nonconfigurational phenomena effectively?</S>
			<S sid="121" ssid="30">using what form of representa tion?</S>
			<S sid="122" ssid="31">is it possible to find a single model that can effectively cope with different kinds of languages?Estimation and Smoothing: Compared to En glish, MRLs tend to have a greater number of word forms and higher out-of-vocabulary (OOV) rates, due to the many feature combinations licensed bythe inflectional system.</S>
			<S sid="123" ssid="32">A typical problem associ ated with parsing MRLs is substantial lexical data sparseness due to high morphological variation in surface forms.</S>
			<S sid="124" ssid="33">The question is therefore, given our finite, and often fairly small, annotated sets of data,how can we guess the morphological analyses, in cluding the PoS tag assignment and various features,of an OOV word?</S>
			<S sid="125" ssid="34">How can we learn the probabil ities of such assignments?</S>
			<S sid="126" ssid="35">In a more general setup, this problem is akin to handling out-of-vocabularyor rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance Constituency-Based Dependency-Based Arabic (Attia et al, 2010) (Marton et al, 2010)?</S>
			<S sid="127" ssid="36">Basque - (Bengoetxea and Gojenola, 2010)English (Attia et al, 2010) French (Attia et al, 2010) (Seddah et al, 2010)(Candito and Seddah, 2010)?</S>
			<S sid="128" ssid="37">German (Maier, 2010) Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?</S>
			<S sid="129" ssid="38">Hindi - (Ambati et al, 2010a)?</S>
			<S sid="130" ssid="39">(Ambati et al, 2010b)Korean (Chung et al, 2010) Table 1: An overview of SPMRL contributions.</S>
			<S sid="131" ssid="40">report results also for non-gold standard input)ment (also explored for English and other morpho logically impoverished languages).So, in fact, incorporating morphological informa tion inside the syntactic model for the purpose of statistical parsing is anything but trivial.</S>
			<S sid="132" ssid="41">In the next section we review the various approaches taken inthe individual contributions of the SPMRL work shop for addressing such challenges.</S>
	</SECTION>
	<SECTION title="Parsing MRLs: Recurring Trends. " number="4">
			<S sid="133" ssid="1">The first workshop on parsing MRLs features 11 contributions for a variety of languages with a range of different parsing frameworks.</S>
			<S sid="134" ssid="2">Table 1 lists the individual contributions within a cross-language cross-framework grid.</S>
			<S sid="135" ssid="3">In this section, we focus on trends that occur among the different contributions.This may be a biased view since some of the prob lems that exist for parsing MRLs may have not been at all present, but it is a synopsis of where we stand with respect to problems that are being addressed.</S>
			<S sid="136" ssid="4">4.1 Architecture and Setup: Gold vs. Predicted.</S>
			<S sid="137" ssid="5">Morphological InformationWhile morphological information can be very informative for syntactic analysis, morphological anal ysis of surface forms is ambiguous in many ways.In German, for instance, case syncretism (i.e. a sin gle surface form corresponding to different cases) ispervasive, and in Hebrew and Arabic, the lack of vo calization patterns in written texts leads to multiplemorphological analyses for each space-delimited token.</S>
			<S sid="138" ssid="6">In real world situations, gold morphological information is not available prior to parsing.</S>
			<S sid="139" ssid="7">Can pars ing systems make effective use of morphology even when gold morphological information is absent?</S>
			<S sid="140" ssid="8">5Several papers address this challenge by present ing results for both the gold and the automaticallypredicted PoS and morphological information (Am bati et al, 2010a; Marton et al, 2010; Goldberg andElhadad, 2010; Seddah et al, 2010).</S>
			<S sid="141" ssid="9">Not very surprisingly, all evaluated systems show a drop in pars ing accuracy in the non-gold settings.An interesting trend is that in many cases, us ing noisy morphological information is worse thannot using any at all.</S>
			<S sid="142" ssid="10">For Arabic Dependency pars ing, using predicted CASE causes a substantial drop in accuracy while it greatly improves performance in the gold setting (Marton et al, 2010).</S>
			<S sid="143" ssid="11">For Hindi Dependency Parsing, using chunk-internalcues (i.e. marking non-recursive phrases) is benefi cial when gold chunk-boundaries are available, but suboptimal when they are automatically predicted (Ambati et al, 2010a).</S>
			<S sid="144" ssid="12">For Hebrew DependencyParsing with the MST parser, using gold morpholog ical features shows no benefit over not using them, while using automatically predicted morphological features causes a big drop in accuracy compared to not using them (Goldberg and Elhadad, 2010).</S>
			<S sid="145" ssid="13">For French Constituency Parsing, Seddah et al (2010) and Candito and Seddah (2010) show that while gold information for the part-of-speech and lemmaof each word form results in a significant improve ment, the gain is low when switching to predicted information.</S>
			<S sid="146" ssid="14">Reassuringly, Ambati et al (2010a), Marton et al (2010), and Goldberg and Elhadad(2010) demonstrate that some morphological infor mation can indeed be beneficial for parsing even in the automatic setting.</S>
			<S sid="147" ssid="15">Ensuring that this is indeed so, appears to be in turn linked to the question of how morphology is represented and incorporated in the parsing model.</S>
			<S sid="148" ssid="16">The same effect in a different guise appears in the contribution of Chung et al (2010) concerningparsing Korean.</S>
			<S sid="149" ssid="17">Chung et al (2010) show a significant improvement in parsing accuracy when in cluding traces of null anaphors (a.k.a. pro-drop) in the input to the parser.</S>
			<S sid="150" ssid="18">Just like overt morphology,traces and null elements encapsulate functional in formation about relational entities in the sentence (the subject, the object, etc.), and including them at the input level provides helpful disambiguating cuesfor the overall structure that represents such rela tions.</S>
			<S sid="151" ssid="19">However, assuming that such traces are givenprior to parsing is, for all practical purposes, infeasible.</S>
			<S sid="152" ssid="20">This leads to an interesting question: will iden tifying such functional elements (marked as traces,overt morphology, etc) during parsing, while com plicating that task itself, be on the whole justified?</S>
			<S sid="153" ssid="21">Closely linked to the inclusion of morphological information in the input is the choice of PoS tag setto use.</S>
			<S sid="154" ssid="22">The generally accepted view is that finegrained PoS tags are morphologically more informa tive but may be harder to statistically learn and parsewith, in particular in the non-gold scenario.</S>
			<S sid="155" ssid="23">Mar ton et al (2010) demonstrate that a fine-grained tag set provides the best results for Arabic dependency parsing when gold tags are known, while a much smaller tag set is preferred in the automatic setting.</S>
			<S sid="156" ssid="24">4.2 Representation and Modeling:.</S>
			<S sid="157" ssid="25">Incorporating Morphological Information Many of the studies presented here explore the useof feature representation of morphological informa tion for the purpose of syntactic parsing (Ambati et al., 2010a; Ambati et al, 2010b; Bengoetxea andGojenola, 2010; Goldberg and Elhadad, 2010; Mar ton et al, 2010; Tsarfaty and Sima?an, 2010).</S>
			<S sid="158" ssid="26">Clear trends among the contributions emerge concerningthe kind of morphological information that helps sta tistical parsing.</S>
			<S sid="159" ssid="27">Morphological CASE is shown to be beneficial across the board.</S>
			<S sid="160" ssid="28">It is shown to help for parsing Basque, Hebrew, Hindi and to some extent Arabic.3 Morphological DEFINITENESS and STATEare beneficial for Hebrew and Arabic when explic itly represented in the model.</S>
			<S sid="161" ssid="29">STATE, ASPECT and MOOD are beneficial for Hindi, but only marginallybeneficial for Arabic.</S>
			<S sid="162" ssid="30">CASE and SUBORDINATION TYPE are the most beneficial features for Basque transition-based dependency parsing.</S>
			<S sid="163" ssid="31">A closer view into the results mentioned in the previous paragraph suggests that, beyond the kind of information that is being used, the way in which morphological information is represented and used by the model has substantial ramification as towhether or not it leads to performance improvements.</S>
			<S sid="164" ssid="32">The so-called ?agreement features?</S>
			<S sid="165" ssid="33">GEN DER, NUMBER, PERSON, provide for an interesting case study in this respect.</S>
			<S sid="166" ssid="34">When included directly as3For Arabic, CASE is useful when gold morphology infor mation is available, but substantially hurt results when it is not.</S>
			<S sid="167" ssid="35">6machine learning features, agreement features ben efit dependency parsing for Arabic (Marton et al, 2010), but not Hindi (dependency) (Ambati et al, 2010a; Ambati et al, 2010b) or Hebrew (Goldberg and Elhadad, 2010).</S>
			<S sid="168" ssid="36">When represented as simplesplits of non-terminal symbols, agreement information does not help constituency-based parsing per formance for Hebrew (Tsarfaty and Sima?an, 2010).However, when agreement patterns are directly represented on dependency arcs, they contribute an improvement for Hebrew dependency parsing (Goldberg and Elhadad, 2010).</S>
			<S sid="169" ssid="37">When agreement is encoded at the realization level inside a Relational Realizational model (Tsarfaty and Sima?an, 2008), agreement features improve the state-of-the-art for Hebrew parsing (Tsarfaty and Sima?an, 2010).</S>
			<S sid="170" ssid="38">One of the advantages of the latter study is that morphological information which is expressed at thelevel of words gets interpreted elsewhere, on func tional elements higher up the constituency tree.</S>
			<S sid="171" ssid="39">In dependency parsing, similar cases may arise, thatis, morphological information might not be as use ful on the form on which it is expressed, but would be more useful at a different position where it could influence the correct attachment of the main verb to other elements.</S>
			<S sid="172" ssid="40">Interesting patterns of that sort occur in Basque, where the SUBORDINATIONTYPE morpheme attaches to the auxiliary verb, though it mainly influences attachments to the main verb.</S>
			<S sid="173" ssid="41">Bengoetxea and Gojenola (2010) attempted twodifferent ways to address this, one using a trans formation segmenting the relevant morpheme and attaching it to the main verb instead, and another by propagating the morpheme along arcs, through a ?stacking?</S>
			<S sid="174" ssid="42">process, to where it is relevant.</S>
			<S sid="175" ssid="43">Both ways led to performance improvements.</S>
			<S sid="176" ssid="44">The idea of a segmentation transformation imposes non-trivial pre-processing, but it may be that automatically learning the propagation of morphological features is a promising direction for future investigation.Another, albeit indirect, way to include morpho logical information in the parsing model is using so-called latent information or some mechanism of clustering.</S>
			<S sid="177" ssid="45">The general idea is the following:when morphological information is added to stan dard terminal or non-terminal symbols, it imposesrestrictions on the distribution of these no-longerequivalent elements.</S>
			<S sid="178" ssid="46">Learning latent informa tion does not represent morphological informationdirectly, but presumably, the distributional restric tions can be automatically learned along with the splits of labels symbols in models such as (Petrov et al, 2006).</S>
			<S sid="179" ssid="47">For Korean (Chung et al, 2010),latent information contributes significant improve ments.</S>
			<S sid="180" ssid="48">One can further do the opposite, namely,merging terminals symbols for the purpose of ob taining an abstraction over morphological features.When such clustering uses a morphological signature of some sort, it is shown to significantly improve constituency-based parsing for French (Can dito and Seddah, 2010).</S>
			<S sid="181" ssid="49">4.3 Representation and Modeling: Free Word.</S>
			<S sid="182" ssid="50">Order and Flexible Constituency Structure Off-the-shelf parsing tools are found in abundance for English.</S>
			<S sid="183" ssid="51">One problematic aspect of using themto parse MRLs lies in the fact that these tools fo cus on the statistical modeling of configurational information.</S>
			<S sid="184" ssid="52">These models often condition on the position of words relative to one another (e.g. intransition-based dependency parsing) or on the dis tance between words inside constituents (e.g. in Head-Driven parsing).</S>
			<S sid="185" ssid="53">Many of the contributions tothe workshop show that working around existing im plementations may be insufficient, and we may have to come up with more radical solutions.</S>
			<S sid="186" ssid="54">Several studies present results that support the conjecture that when free word-order is explicitly taken into account, morphological information is more likely to contribute to parsing accuracy.</S>
			<S sid="187" ssid="55">The Relational-Realizational model used in (Tsarfatyand Sima?an, 2010) allows for reordering of constituents at a configuration layer, which is indepen dent of the realization patterns learned from the data(vis-a`-vis case marking and agreement).</S>
			<S sid="188" ssid="56">The easy first algorithm of (Goldberg and Elhadad, 2010) which allows for significant flexibility in the order ofattachment, allows the model to benefit from agree ment patterns over dependency arcs that are easier to detect and attach first.</S>
			<S sid="189" ssid="57">The use of larger subtrees in (Chung et al, 2010) for parsing Korean, within aBayesian framework, allows the model to learn dis tributions that take more elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance.</S>
			<S sid="190" ssid="58">7 In addition to free word order, MRLs show higher degree of freedom in extraposition.</S>
			<S sid="191" ssid="59">Both of these phenomena can result in discontinuous structures.In constituency-based treebanks, this is either an notated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German Tu?Ba-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models.</S>
			<S sid="192" ssid="60">Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency.</S>
			<S sid="193" ssid="61">Dependency representation uses non-projective dependencies to reflect discontinuities, which isproblematic to parse with models that assume pro jectivity.</S>
			<S sid="194" ssid="62">Different ways have been proposed to dealwith non-projectivity (Nivre and Nilsson, 2005; Mc Donald et al, 2005; McDonald and Pereira, 2006; Nivre, 2009).</S>
			<S sid="195" ssid="63">Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of(Nivre and Nilsson, 2005) improves accuracy for de pendency parsing of Basque.</S>
			<S sid="196" ssid="64">Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too.</S>
			<S sid="197" ssid="65">4.4 Estimation and Smoothing: Coping with.</S>
			<S sid="198" ssid="66">Lexical Sparsity Morphological word form variation augments thevocabulary size and thus worsens the problem of lexical data sparseness.</S>
			<S sid="199" ssid="67">Words occurring with medium frequency receive less reliable estimates, and the number of rare/unknown words is increased.</S>
			<S sid="200" ssid="68">One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflectstheir shared morphological and morphosyntactic as pects.</S>
			<S sid="201" ssid="69">This was done, for instance, in previous work on parsing German.</S>
			<S sid="202" ssid="70">Versley and Rehbein (2009) cluster words according to linear context features.</S>
			<S sid="203" ssid="71">These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes.</S>
			<S sid="204" ssid="72">The clusters are then integrated as features in a discriminative parsing model to cope with unknown words.</S>
			<S sid="205" ssid="73">Theirdiscriminative model thus obtains state-of-the-art re sults on parsing German.</S>
			<S sid="206" ssid="74">Several contribution address similar challenges.For constituency-based generative parsers, the sim ple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al,2010; Candito and Seddah, 2010).</S>
			<S sid="207" ssid="75">For French, replacing each word form by its predicted part-ofspeech and lemma pair results in a slight perfor mance improvement (Seddah et al, 2010).</S>
			<S sid="208" ssid="76">When words are clustered, even according to a very local linear-context similarity measure, measured over a large raw corpus, and when word clusters are used in place of word forms, the gain in performance is even higher (Candito and Seddah, 2010).</S>
			<S sid="209" ssid="77">In both cases, the technique provides more reliable estimates for in-vocabulary words, since a given lemma or cluster appear more frequently.</S>
			<S sid="210" ssid="78">It also increases the knownvocabulary.</S>
			<S sid="211" ssid="79">For instance, if a plural form is unseen in the training set but the corresponding singu lar form is known, then in a setting of using lemmas in terminal symbols, both forms are known.For dependency parsing, Marton et al (2010) investigates the use of morphological features that in volve some semantic abstraction over Arabic forms.The use of undiacritized lemmas is shown to im prove performance.</S>
			<S sid="212" ssid="80">Attia et al (2010) specificallyaddress the handling of unknown words in the latent variable parsing model.</S>
			<S sid="213" ssid="81">Here again, the technique that is investigated is to project unknown words to more general symbols using morphological clues.</S>
			<S sid="214" ssid="82">Astudy on three languages, English, French and Ara bic, shows that this method helps in all cases, but that the greatest improvement is obtained for Arabic, which has the richest morphology among three.</S>
	</SECTION>
	<SECTION title="Where we?re at. " number="5">
			<S sid="215" ssid="1">It is clear from the present overview that we are yet to obtain a complete understanding concerningwhich models effectively parse MRLs, how to an notate treebanks for MRLs and, importantly, howto evaluate parsing performance across types of lan guages and treebanks.</S>
			<S sid="216" ssid="2">These foundational issues arecrucial for deriving more conclusive recommenda tions as to the kind of models and morphologicalfeatures that can lead to advancing the state-of-the art for parsing MRLs.</S>
			<S sid="217" ssid="3">One way to target such anunderstanding would be to encourage the investiga tion of particular tasks, individually or in the context 8of shared tasks, that are tailored to treat those prob lematic aspects of MRLs that we surveyed here.So far, constituency-based parsers have been as sessed based on their performance on the PTB (and to some extent, across German treebanks (Ku?bler, 2008)) whereas comparison across languages was rendered opaque due to data set differences andrepresentation idiosyncrasies.</S>
			<S sid="218" ssid="4">It would be interesting to investigate such a cross-linguistic compari son of parsers in the context of a shared task on constituency-based statistical parsing, in additional to dependency-based ones as reported in (Nivre et al., 2007a).</S>
			<S sid="219" ssid="5">Standardizing data sets for a large number of languages with different characteristics, would require us, as a community, to aim forconstituency-representation guidelines that can rep resent the shared aspects of structures in differentlanguages, while at the same time allowing differ ences between them to be reflected in the model.Furthermore, it would be a good idea to intro duce parsing tasks, for either constituent-based or dependency-based setups, which consider raw text as input, rather than morphologically segmentedand analyzed text.</S>
			<S sid="220" ssid="6">Addressing the parsing prob lem while facing the morphological disambiguationchallenge in its full-blown complexity would be il luminating and educating for at least two reasons: firstly, it would give us a better idea of what is thestate-of-the-art for parsing MRLs in realistic scenar ios.</S>
			<S sid="221" ssid="7">Secondly, it might lead to profound insightsabout the potentially successful ways to use mor phology inside a parser, which may differ from the insights concerning the use of morphology in theless realistic parsing scenarios, where gold morpho logical information is given.</S>
			<S sid="222" ssid="8">Finally, to be able to perceive where we stand with respect to parsing MRLs and how models fare against one another across languages, it would be crucial to arrive at evaluation metrics that captureinformation that is shared among the different representations, for instance, functional information concerning predicate-argument relations.</S>
			<S sid="223" ssid="9">Using the different kinds of measures in the context of crossframework tasks will help us understand the util ity of the different evaluation metrics that have been proposed and to arrive at a clearer picture of what itis that we wish to compare, and how we can faith fully do so across models, languages and treebanks.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="224" ssid="1">This paper presents the synthesis of 11 contributionsto the first workshop on statistical parsing for mor phologically rich languages.</S>
			<S sid="225" ssid="2">We have shown that architectural, representational, and estimation issuesassociated with parsing MRLs are found to be chal lenging across languages and parsing frameworks.</S>
			<S sid="226" ssid="3">The use of morphological information in the nongold-tagged input scenario is found to cause sub stantial differences in parsing performance, and inthe kind of morphological features that lead to per formance improvements.Whether or not morphological features help pars ing also depends on the kind of model in which they are embedded, and the different ways they aretreated within.</S>
			<S sid="227" ssid="4">Furthermore, sound statistical esti mation methods for morphologically rich, complexlexica, turn out to be crucial for obtaining good pars ing accuracy when using general-purpose models and algorithms.</S>
			<S sid="228" ssid="5">In the future we hope to gain better understanding of the common pitfalls in, and novel solutions for, parsing morphologically ambiguousinput, and to arrive at principled guidelines for selecting the model and features to include when pars ing different kinds of languages.</S>
			<S sid="229" ssid="6">Such insights may be gained, among other things, in the context of more morphologically-aware shared parsing tasks.</S>
			<S sid="230" ssid="7">Acknowledgements The program committee would like to thank NAACL for hosting the workshop and SIGPARSEfor their sponsorship.</S>
			<S sid="231" ssid="8">We further thank INRIA Al page team for their generous sponsorship.</S>
			<S sid="232" ssid="9">We are finally grateful to our reviewers and authors for their dedicated work and individual contributions.</S>
	</SECTION>
</PAPER>
