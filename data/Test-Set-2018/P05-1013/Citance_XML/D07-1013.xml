<PAPER>
	<S sid="0">Characterizing the Errors of Data-Driven Dependency Parsing Models</S><ABSTRACT>
		<S sid="1" ssid="1">We present a comparative error analysisof the two dominant approaches in datadriven dependency parsing: global, exhaus tive, graph-based models, and local, greedy, transition-based models.</S>
		<S sid="2" ssid="2">We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models.</S>
		<S sid="3" ssid="3">This analysisleads to new directions for parser develop ment.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">Syntactic dependency representations have a long history in descriptive and theoretical linguistics and many formal models have been advanced (Hudson, 1984; Mel?c?uk, 1988; Sgall et al, 1986; Maruyama,1990).</S>
			<S sid="5" ssid="5">A dependency graph of a sentence repre sents each word and its syntactic modifiers through labeled directed arcs, as shown in Figure 1, taken from the Prague Dependency Treebank (Bo?hmova?</S>
			<S sid="6" ssid="6">etal., 2003).</S>
			<S sid="7" ssid="7">A primary advantage of dependency rep resentations is that they have a natural mechanismfor representing discontinuous constructions, aris ing from long distance dependencies or free wordorder, through non-projective dependency arcs, ex emplified by the arc from jedna to Z in Figure 1.</S>
			<S sid="8" ssid="8">Syntactic dependency graphs have recentlygained a wide interest in the computational linguistics community and have been successfully em ployed for many problems ranging from machine translation (Ding and Palmer, 2004) to ontology Figure 1: Example dependency graph.</S>
			<S sid="9" ssid="9">construction (Snow et al, 2004).</S>
			<S sid="10" ssid="10">In this work wefocus on a common parsing paradigm called data driven dependency parsing.</S>
			<S sid="11" ssid="11">Unlike grammar-basedparsing, data-driven approaches learn to produce dependency graphs for sentences solely from an anno tated corpus.</S>
			<S sid="12" ssid="12">The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist.As evident from the CoNLL-X shared task on de pendency parsing (Buchholz and Marsi, 2006), there are currently two dominant models for data-driven dependency parsing.</S>
			<S sid="13" ssid="13">The first is what Buchholz andMarsi (2006) call the ?all-pairs?</S>
			<S sid="14" ssid="14">approach, where ev ery possible arc is considered in the construction ofthe optimal parse.</S>
			<S sid="15" ssid="15">The second is the ?stepwise?</S>
			<S sid="16" ssid="16">ap proach, where the optimal parse is built stepwise and where the subset of possible arcs considered depend on previous decisions.</S>
			<S sid="17" ssid="17">Theoretically, these models are extremely different.</S>
			<S sid="18" ssid="18">The all-pairs models are globally trained, use exact (or near exact) inference algorithms, and define features over a limited history of parsing decisions.</S>
			<S sid="19" ssid="19">The stepwise models use local training and greedy inference algorithms, but definefeatures over a rich history of parse decisions.</S>
			<S sid="20" ssid="20">How ever, both models obtain similar parsing accuracies 122 McDonald Nivre Arabic 66.91 66.71 Bulgarian 87.57 87.41 Chinese 85.90 86.92 Czech 80.18 78.42 Danish 84.79 84.77 Dutch 79.19 78.59 German 87.34 85.82 Japanese 90.71 91.65 Portuguese 86.82 87.60 Slovene 73.44 70.30 Spanish 82.25 81.29 Swedish 82.55 84.58 Turkish 63.19 65.68 Overall 80.83 80.75 Table 1: Labeled parsing accuracy for top scoring systems at CoNLL-X (Buchholz and Marsi, 2006).</S>
			<S sid="21" ssid="21">on a variety of languages, as seen in Table 1, which shows results for the two top performing systems in the CoNLL-X shared task, McDonald et al (2006) (?all-pairs?)</S>
			<S sid="22" ssid="22">and Nivre et al (2006) (?stepwise?).Despite the similar performance in terms of over all accuracy, there are indications that the two types of models exhibit different behaviour.</S>
			<S sid="23" ssid="23">For example, Sagae and Lavie (2006) displayed that combining the predictions of both parsing models can lead to significantly improved accuracies.</S>
			<S sid="24" ssid="24">In order to pave the way for new and better methods, a much more detailed error analysis is needed to understand the strengths and weaknesses of different approaches.</S>
			<S sid="25" ssid="25">In this work we set out to do just that, focusing on the two top performing systems from the CoNLL-X shared task as representatives of the two dominant models in data-driven dependency parsing.</S>
	</SECTION>
	<SECTION title="Two Models for Dependency Parsing. " number="2">
			<S sid="26" ssid="1">2.1 Preliminaries.</S>
			<S sid="27" ssid="2">Let L = {l1, . . .</S>
			<S sid="28" ssid="3">, l|L|} be a set of permissible arclabels.</S>
			<S sid="29" ssid="4">Let x = w0, w1, . . .</S>
			<S sid="30" ssid="5">, wn be an input sen tence wherew0=root.</S>
			<S sid="31" ssid="6">Formally, a dependency graph for an input sentence x is a labeled directed graph G = (V,A) consisting of a set of nodes V and a set of labeled directed arcs A ? V ? V ? L, i.e., if (i, j, l) ? A for i, j ? V and l ? L, then there is an arc from node i to node j with label l in the graph.</S>
			<S sid="32" ssid="7">A dependency graph G for sentence x must satisfy the following properties: 1.</S>
			<S sid="33" ssid="8">V = {0, 1, . . .</S>
			<S sid="34" ssid="9">, n} 2.</S>
			<S sid="35" ssid="10">If (i, j, l) ? A, then j 6= 0..</S>
			<S sid="36" ssid="11">l? ? L, (i?, j, l?)</S>
			<S sid="37" ssid="12">A. 4.</S>
			<S sid="38" ssid="13">For all j ? V ?{0}, there is a (possibly empty).</S>
			<S sid="39" ssid="14">sequence of nodes i1, . . .</S>
			<S sid="40" ssid="15">, im?V and labels l1, . . .</S>
			<S sid="41" ssid="16">, lm, l?L such that (0, i1, l1),(i1, i2, l2), . . .</S>
			<S sid="42" ssid="17">, (im, j, l)?A. The constraints state that the dependency graph spans the entire input (1); that the node 0 is a root (2); that each node has at most one incoming arc in the graph (3); and that the graph is connected through directed paths from the node 0 to every othernode in the graph (4).</S>
			<S sid="43" ssid="18">A dependency graph satisfy ing these constraints is a directed tree originating outof the root node 0.</S>
			<S sid="44" ssid="19">We say that an arc (i, j, l) is non projective if not all words k occurring between i andj in the linear order are dominated by i (where dom inance is the transitive closure of the arc relation).</S>
			<S sid="45" ssid="20">2.2 Global, Exhaustive, Graph-Based Parsing.</S>
			<S sid="46" ssid="21">For an input sentence, x = w0, w1, . . .</S>
			<S sid="47" ssid="22">, wn consider the dense graph Gx = (Vx, Ax) where: 1.</S>
			<S sid="48" ssid="23">Vx = {0, 1, . . .</S>
			<S sid="49" ssid="24">, n}.</S>
			<S sid="50" ssid="25">2.</S>
			<S sid="51" ssid="26">Ax = {(i, j, l) | ? i, j ? Vx and l ? L}.</S>
			<S sid="52" ssid="27">Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the sentence x. Since Gx contains all possible labeled arcs, theset D(Gx) must necessarily contain all valid depen dency graphs for x. Assume that there exists a dependency arc scoring function, s : V ? V ? L ? R. Furthermore, define the score of a graph as the sum of its arc scores, s(G = (V,A)) = ?</S>
			<S sid="53" ssid="28">(i,j,l)?A s(i, j, l) The score of a dependency arc, s(i, j, l) represents the likelihood of creating a dependency from wordwi to word wj with the label l. If the arc score func tion is known a priori, then the parsing problem can be stated as, 123 G = argmax G?D(Gx) s(G) = argmax G?D(Gx) ?</S>
			<S sid="54" ssid="29">(i,j,l)?A s(i, j, l) This problem is equivalent to finding the highestscoring directed spanning tree in the graph Gx origi nating out of the root node 0, which can be solved for both the labeled and unlabeled case in O(n2) time(McDonald et al, 2005b).</S>
			<S sid="55" ssid="30">In this approach, non projective arcs are produced naturally through the inference algorithm that searches over all possible directed trees, whether projective or not.</S>
			<S sid="56" ssid="31">The parsing models of McDonald work primarilyin this framework.</S>
			<S sid="57" ssid="32">To learn arc scores, these mod els use large-margin structured learning algorithms(McDonald et al, 2005a), which optimize the parameters of the model to maximize the score mar gin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set.</S>
			<S sid="58" ssid="33">The learning procedure is global since model parameters are set relative to the classificationof the entire dependency graph, and not just over single arc attachment decisions.</S>
			<S sid="59" ssid="34">The primary disadvantage of these models is that the feature representa tion is restricted to a limited number of graph arcs.</S>
			<S sid="60" ssid="35">This restriction is required so that both inference and learning are tractable.</S>
			<S sid="61" ssid="36">The specific model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs)and uses near exhaustive search for unlabeled pars ing coupled with a separate classifier to label each arc. We call this system MSTParser, which is also the name of the freely available implementation.1 2.3 Local, Greedy, Transition-Based Parsing.</S>
			<S sid="62" ssid="37">A transition system for dependency parsing defines 1.</S>
			<S sid="63" ssid="38">a set C of parser configurations, each of which defines a (partially built) dependency graph G 2.</S>
			<S sid="64" ssid="39">a set T of transitions, each a function t :C?C 3.</S>
			<S sid="65" ssid="40">for every sentence x = w0, w1, . . .</S>
			<S sid="66" ssid="41">, wn, (a) a unique initial configuration cx (b) a set Cx of terminal configurations 1http://mstparser.sourceforge.net A transition sequence Cx,m = (cx, c1, . . .</S>
			<S sid="67" ssid="42">, cm) for a sentence x is a sequence of configurations such thatcm ? Cx and, for every ci (ci 6= cx), there is a tran sition t ? T such that ci = t(ci?1).</S>
			<S sid="68" ssid="43">The dependency graph assigned to x byCx,m is the graphGm defined by the terminal configuration cm.Assume that there exists a transition scoring func tion, s : C ? T ? R. The score of a transitiont in a configuration c, s(c, t), represents the likeli hood of taking transition t out of configuration c. The parsing problem consists in finding a terminal configuration cm ? Cx, starting from the initial configuration cx and taking the optimal transition t?</S>
			<S sid="69" ssid="44">= argmaxt?T s(c, t) out of every configuration c. This can be seen as a greedy search for the optimal dependency graph, based on a sequence of locally optimal decisions in terms of the transition system.Many transition systems for data-driven depen dency parsing are inspired by shift-reduce parsing,where configurations contain a stack for storing par tially processed nodes.</S>
			<S sid="70" ssid="45">Transitions in such systemsadd arcs to the dependency graph and/or manipu late the stack.</S>
			<S sid="71" ssid="46">One example is the transition system defined by Nivre (2003), which parses a sentencex = w0, w1, . . .</S>
			<S sid="72" ssid="47">, wn in O(n) time, producing a pro jective dependency graph satisfying conditions 1?4 in section 2.1, possibly after adding arcs (0, i, lr) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers).Nivre and Nilsson (2005) showed how the restric tion to projective dependency graphs could be liftedby using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing.To learn transition scores, these systems use dis criminative learning methods, e.g., memory-based learning or support vector machines.</S>
			<S sid="73" ssid="48">The learning procedure is local since only single transitions are scored, not entire transition sequences.</S>
			<S sid="74" ssid="49">The primaryadvantage of these models is that features are not re stricted to a limited number of graph arcs but can take into account the entire dependency graph built so far.</S>
			<S sid="75" ssid="50">The main disadvantage is that the greedy parsing strategy may lead to error propagation.The specific model studied in this work is that pre sented by Nivre et al (2006), which uses labeledpseudo-projective parsing with support vector ma chines.</S>
			<S sid="76" ssid="51">We call this systemMaltParser, which is also 124 the name of the freely available implementation.2 2.4 Comparison.</S>
			<S sid="77" ssid="52">These models differ primarily with respect to three important properties.</S>
			<S sid="78" ssid="53">1.</S>
			<S sid="79" ssid="54">Inference: MaltParser uses a transition-based.</S>
			<S sid="80" ssid="55">inference algorithm that greedily chooses thebest parsing decision based on a trained clas sifier and current parser history.</S>
			<S sid="81" ssid="56">MSTParser instead uses near exhaustive search over a dense graphical representation of the sentence to find the dependency graph that maximizes the score.</S>
			<S sid="82" ssid="57">2.</S>
			<S sid="83" ssid="58">Training: MaltParser trains a model to make.</S>
			<S sid="84" ssid="59">a single classification decision (choose the nexttransition).</S>
			<S sid="85" ssid="60">MSTParser trains a model to maxi mize the global score of correct graphs.</S>
	</SECTION>
	<SECTION title="Feature Representation: MaltParser can in-. " number="3">
			<S sid="86" ssid="1">troduce a rich feature history based on previ ous parser decisions.</S>
			<S sid="87" ssid="2">MSTParser is forced to restrict the score of features to a single or pair of nearby parsing decisions in order to make exhaustive inference tractable.These differences highlight an inherent trade-off be tween exhaustive inference algorithms plus globallearning and expressiveness of feature representa tions.</S>
			<S sid="88" ssid="3">MSTParser favors the former at the expense of the latter and MaltParser the opposite.</S>
			<S sid="89" ssid="4">3 The CoNLL-X Shared Task.</S>
			<S sid="90" ssid="5">The CoNLL-X shared task (Buchholz and Marsi,2006) was a large-scale evaluation of data-driven dependency parsers, with data from 13 different lan guages and 19 participating systems.</S>
			<S sid="91" ssid="6">The official evaluation metric was the labeled attachment score(LAS), defined as the percentage of tokens, exclud ing punctuation, that are assigned both the correct head and the correct dependency label.3 The output of all systems that participated in theshared task are available for download and consti tute a rich resource for comparative error analysis.</S>
			<S sid="92" ssid="7">2http://w3.msi.vxu.se/users/nivre/research/MaltParser.html 3In addition, results were reported for unlabeled attachment score (UAS) (tokens with the correct head) and label accuracy (LA) (tokens with the correct label).The data used in the experiments below are the outputs of MSTParser and MaltParser for all 13 languages, together with the corresponding gold stan dard graphs used in the evaluation.</S>
			<S sid="93" ssid="8">We constructed the data by simply concatenating a system?s outputfor every language.</S>
			<S sid="94" ssid="9">This resulted in a single out put file for each system and a corresponding single gold standard file.</S>
			<S sid="95" ssid="10">This method is sound because the data sets for each language contain approximatelythe same number of tokens ? 5,000.</S>
			<S sid="96" ssid="11">Thus, evalu ating system performance over the aggregated filescan be roughly viewed as measuring system perfor mance through an equally weighted arithmetic mean over the languages.</S>
			<S sid="97" ssid="12">It could be argued that a language by languagecomparison would be more appropriate than com paring system performance across all languages.</S>
			<S sid="98" ssid="13">However, as table Table 1 shows, the difference in accuracy between the two systems is typically small for all languages, and only in a few cases is this difference significant.</S>
			<S sid="99" ssid="14">Furthermore, by aggregating over all languages we gain better statistical estimates of parser errors, since the data set for each individual language is very small.</S>
	</SECTION>
	<SECTION title="Error Analysis. " number="4">
			<S sid="100" ssid="1">The primary purpose of this study is to characterize the errors made by standard data-driven dependency parsing models.</S>
			<S sid="101" ssid="2">To that end, we present a large set ofexperiments that relate parsing errors to a set of linguistic and structural properties of the input and pre dicted/gold standard dependency graphs.</S>
			<S sid="102" ssid="3">We arguethat the results can be correlated to specific theoreti cal aspects of each model ? in particular the trade-off highlighted in Section 2.4.</S>
			<S sid="103" ssid="4">For simplicity, all experiments report labeledparsing accuracies.</S>
			<S sid="104" ssid="5">Identical experiments using unlabeled parsing accuracies did not reveal any addi tional information.</S>
			<S sid="105" ssid="6">Furthermore, all experiments are based on the data from all 13 languages together, as explained in section 3.</S>
			<S sid="106" ssid="7">4.1 Length Factors.</S>
			<S sid="107" ssid="8">It is well known that parsing systems tend to have lower accuracies for longer sentences.</S>
			<S sid="108" ssid="9">Figure 2 shows the accuracy of both parsing models relative to sentence length (in bins of size 10: 1?10, 11?20, 125 10 20 30 40 50 50+Sentence Length (bins of size 10) 0.7 0.72 0.74 0.76 0.78 0.8 0.82 0.84 Depe nden cy A ccur acy MSTParserMaltParser Figure 2: Accuracy relative to sentence length.etc.).</S>
			<S sid="109" ssid="10">System performance is almost indistinguish able.</S>
			<S sid="110" ssid="11">However, MaltParser tends to perform betteron shorter sentences, which require the greedy in ference algorithm to make less parsing decisions.</S>
			<S sid="111" ssid="12">As a result, the chance of error propagation is reduced significantly when parsing these sentences.</S>
			<S sid="112" ssid="13">The fact that MaltParser has a higher accuracy (rather than the same accuracy) when the likelihood of error propagation is reduced comes from its richer feature representation.</S>
			<S sid="113" ssid="14">Another interesting property is accuracy relative to dependency length.</S>
			<S sid="114" ssid="15">The length of a dependency from word wi to word wj is simply equal to |i? j|.</S>
			<S sid="115" ssid="16">Longer dependencies typically represent modifiers of the root or the main verb in a sentence.</S>
			<S sid="116" ssid="17">Shorter dependencies are often modifiers of nouns such as determiners or adjectives or pronouns modifyingtheir direct neighbours.</S>
			<S sid="117" ssid="18">Figure 3 measures the precision and recall for each system relative to dependency lengths in the predicted and gold standard dependency graphs.</S>
			<S sid="118" ssid="19">Precision represents the percent age of predicted arcs of length d that were correct.</S>
			<S sid="119" ssid="20">Recall measures the percentage of gold standard arcs of length d that were correctly predicted.</S>
			<S sid="120" ssid="21">Here we begin to see separation between the two systems.</S>
			<S sid="121" ssid="22">MSTParser is far more precise for longer dependency arcs, whereas MaltParser does better for shorter dependency arcs.</S>
			<S sid="122" ssid="23">This behaviour can be explained using the same reasoning as above: shorter arcs are created before longer arcs in the greedy parsing procedure of MaltParser and are lessprone to error propagation.</S>
			<S sid="123" ssid="24">Theoretically, MST Parser should not perform better or worse for edges of any length, which appears to be the case.</S>
			<S sid="124" ssid="25">There is still a slight degradation, but this can be attributed to long dependencies occurring more frequently in constructions with possible ambiguity.</S>
			<S sid="125" ssid="26">Note that even though the area under the curve is much larger for MSTParser, the number of dependency arcs with a length greater than ten is much smaller than the number with length less than ten, which is why the overall accuracy of each system is nearly identical.</S>
			<S sid="126" ssid="27">For all properties considered here, bin size generally shrinks in size as the value on the x-axis increases.</S>
			<S sid="127" ssid="28">4.2 Graph Factors.</S>
			<S sid="128" ssid="29">The structure of the predicted and gold standard de pendency graphs can also provide insight into thedifferences between each model.</S>
			<S sid="129" ssid="30">For example, mea suring accuracy for arcs relative to their distance to the artificial root node will detail errors at different levels of the dependency graph.</S>
			<S sid="130" ssid="31">For a given arc, wedefine this distance as the number of arcs in the re verse path from the modifier of the arc to the root.Figure 4 plots the precision and recall of each system for arcs of varying distance to the root.</S>
			<S sid="131" ssid="32">Preci sion is equal to the percentage of dependency arcs in the predicted graph that are at a distance of d and are correct.</S>
			<S sid="132" ssid="33">Recall is the percentage of dependency arcs in the gold standard graph that are at a distance of d and were predicted.</S>
			<S sid="133" ssid="34">Figure 4 clearly shows that for arcs close to theroot, MSTParser is much more precise than Malt Parser, and vice-versa for arcs further away from the root.</S>
			<S sid="134" ssid="35">This is probably the most compelling graph given in this study since it reveals a clear distinction: MSTParser?s precision degrades as the distance tothe root increases whereas MaltParser?s precision increases.</S>
			<S sid="135" ssid="36">The plots essentially run in opposite directions crossing near the middle.</S>
			<S sid="136" ssid="37">Dependency arcs fur ther away from the root are usually constructed earlyin the parsing algorithm of MaltParser.</S>
			<S sid="137" ssid="38">Again a re duced likelihood of error propagation coupled witha rich feature representation benefits that parser substantially.</S>
			<S sid="138" ssid="39">Furthermore, MaltParser tends to over predict root modifiers, because all words that the parser fails to attach as modifiers are automatically connected to the root, as explained in section 2.3.</S>
			<S sid="139" ssid="40">Hence, low precision for root modifiers (without a corresponding drop in recall) is an indication that the transition-based parser produces fragmented parses.</S>
			<S sid="140" ssid="41">The behaviour of MSTParser is a little trickier to explain.</S>
			<S sid="141" ssid="42">One would expect that its errors should be distributed evenly over the graph.</S>
			<S sid="142" ssid="43">For the most part this is true, with the exception of spikes at the ends 126 0 5 10 15 20 25 30Dependency Length 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Depe nden cy P recis ion MSTParserMaltParser 0 5 10 15 20 25 30Dependency Length 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Depe nden cy R ecall MSTParserMaltParser Figure 3: Dependency arc precision/recall relative to predicted/gold dependency length.of the plot.</S>
			<S sid="143" ssid="44">The high performance for root modifica tion (distance of 1) can be explained through the factthat this is typically a low entropy decision ? usu ally the parsing algorithm has to determine the main verb from a small set of possibilities.</S>
			<S sid="144" ssid="45">On the other end of the plot there is a sharp downwards spike for arcs of distance greater than 10.</S>
			<S sid="145" ssid="46">It turns out that MSTParser over-predicts arcs near the bottom of the graph.</S>
			<S sid="146" ssid="47">Whereas MaltParser pushes difficult parsing decisions higher in the graph, MSTParser appears to push these decisions lower.</S>
			<S sid="147" ssid="48">The next graph property we will examine aims to quantify the local neighbourhood of an arc within a dependency graph.</S>
			<S sid="148" ssid="49">Two dependency arcs, (i, j, l)and (i?, j?, l?)</S>
			<S sid="149" ssid="50">are classified as siblings if they repre sent syntactic modifications of the same word, i.e., i = i?.</S>
			<S sid="150" ssid="51">Figure 5 measures the precision and recall of each system relative to the number of predicted and gold standard siblings of each arc. There is not much to distinguish between the parsers on this metric.</S>
			<S sid="151" ssid="52">MSTParser is slightly more precise for arcsthat are predicted with more siblings, whereas Malt Parser has slightly higher recall on arcs that have more siblings in the gold standard tree.</S>
			<S sid="152" ssid="53">Arcs closer to the root tend to have more siblings, which ties this result to the previous ones.</S>
			<S sid="153" ssid="54">The final graph property we wish to look at is thedegree of non-projectivity.</S>
			<S sid="154" ssid="55">The degree of a depen dency arc from word w to word u is defined here as the number of words occurring between w and u that are not descendants ofw and modify a word that does not occur between w and u (Nivre, 2006).</S>
			<S sid="155" ssid="56">In the example from Figure 1, the arc from jedna to Z has a degree of one, and all other arcs have a degree of zero.</S>
			<S sid="156" ssid="57">Figure 6 plots dependency arc precision and recall relative to arc degree in predicted and gold standard dependency graphs.</S>
			<S sid="157" ssid="58">MSTParser is more precise when predicting arcs with high degree and MaltParser vice-versa.</S>
			<S sid="158" ssid="59">Again, this can be explained by the fact that there is a tight correlation between a high degree of non-projectivity, dependency length, distance to root and number of siblings.</S>
			<S sid="159" ssid="60">4.3 Linguistic Factors.</S>
			<S sid="160" ssid="61">It is important to relate each system?s accuracy to a set of linguistic categories, such as parts of speech and dependency types.</S>
			<S sid="161" ssid="62">Therefore, we have made an attempt to distinguish a few broad categories that are cross-linguistically identifiable, based on the available documentation of the treebanks used in the shared task.For parts of speech, we distinguish verbs (including both main verbs and auxiliaries), nouns (including proper names), pronouns (sometimes also including determiners), adjectives, adverbs, adposi tions (prepositions, postpositions), and conjunctions(both coordinating and subordinating).</S>
			<S sid="162" ssid="63">For depen dency types, we distinguish a general root category(for labels used on arcs from the artificial root, in cluding either a generic label or the label assigned to predicates of main clauses, which are normallyverbs), a subject category, an object category (in cluding both direct and indirect objects), and various categories related to coordination.</S>
			<S sid="163" ssid="64">Figure 7 shows the accuracy of the two parsers for different parts of speech.</S>
			<S sid="164" ssid="65">This figure measures labeled dependency accuracy relative to the part ofspeech of the modifier word in a dependency relation.</S>
			<S sid="165" ssid="66">We see that MaltParser has slightly better ac curacy for nouns and pronouns, while MSTParserdoes better on all other categories, in particular con junctions.</S>
			<S sid="166" ssid="67">This pattern is consistent with previous results insofar as verbs and conjunctions are often involved in dependencies closer to the root that span 127 2 4 6 8 10Distance to Root 0.74 0.76 0.78 0.8 0.82 0.84 0.86 0.88 0.9 Depe nden cy P recis ion MSTParserMaltParser 2 4 6 8 10Distance to Root 0.76 0.78 0.8 0.82 0.84 0.86 0.88 Depe nden cy R ecall MSTParserMaltParser Figure 4: Dependency arc precision/recall relative to predicted/gold distance to root.</S>
			<S sid="167" ssid="68">0 2 4 6 8 10+Number of Modifier Siblings 0.5 0.6 0.7 0.8 0.9 Depe nden cy P recis ion MSTParserMaltParser 0 2 4 6 8 10+Number of Modifier Siblings 0.5 0.6 0.7 0.8 0.9 Depe nden cy R ecall MSTParserMaltParser Figure 5: Dependency arc precision/recall relative to number of predicted/gold siblings.longer distances, while nouns and pronouns are typ ically attached to verbs and therefore occur lower inthe graph, with shorter distances.</S>
			<S sid="168" ssid="69">Empirically, ad verbs resemble verbs and conjunctions with respect to root distance but group with nouns and pronouns for dependency length, so the former appears to be more important.</S>
			<S sid="169" ssid="70">In addition, both conjunctions andadverbs tend to have a high number of siblings, mak ing the results consistent with the graph in Figure 5.</S>
			<S sid="170" ssid="71">Adpositions and especially adjectives constitute a puzzle, having both high average root distance and low average dependency length.</S>
			<S sid="171" ssid="72">Adpositions do tend to have a high number of siblings on average, which could explain MSTParser?s performance on that category.</S>
			<S sid="172" ssid="73">However, adjectives on average occur the furthest away from the root, have the shortest dependency length and the fewest siblings.</S>
			<S sid="173" ssid="74">As such, we do not have an explanation for this behaviour.</S>
			<S sid="174" ssid="75">In the top half of Figure 8, we consider precision and recall for dependents of the root node (mostly verbal predicates), and for subjects and objects.</S>
			<S sid="175" ssid="76">As already noted, MSTParser has considerably betterprecision (and slightly better recall) for the root category, but MaltParser has an advantage for the nominal categories, especially subjects.</S>
			<S sid="176" ssid="77">A possible explanation for the latter result, in addition to the length based and graph-based factors invoked before, is that 60.0%65.0%70.0% 75.0%80.0%85.0% 90.0%95.0% Verb Noun Pron Adj Adv Adpos ConjPart of Speech (POS)Labele d Attachment Score ( LAS) MSTParserMaltParser Figure 7: Accuracy for different parts of speech.MaltParser integrates labeling into the parsing pro cess, so that previously assigned dependency labels can be used as features, which may be important to disambiguate subjects and objects.Finally, in the bottom half of Figure 8, we dis play precision and recall for coordinate structures, divided into different groups depending on the typeof analysis adopted in a particular treebank.</S>
			<S sid="177" ssid="78">The category CCH (coordinating conjunction as head) con tains conjunctions analyzed as heads of coordinate structures, with a special dependency label that does not describe the function of the coordinate structure in the larger syntactic structure, a type of categoryfound in the so-called Prague style analysis of coor dination and used in the data sets for Arabic, Czech, 128 0 1 2 3 4 5 6 7+Non-Projective Arc Degree 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Depe nden cy P recis ion MSTParserMaltParser 0 1 2 3 4 5 6 7+Non-Projective Arc Degree 0.6 0.65 0.7 0.75 0.8 0.85 Depe nden cy R ecall MSTParserMaltParser Figure 6: Dependency arc precision/recall relative to predicted/gold degree of non-projectivity.</S>
			<S sid="178" ssid="79">65.0%70.0% 75.0%80.0% 85.0%90.0% 95.0% Root Subj ObjDependency Type (DEP)De pendency Precision MSTParserMaltParser 72.0%74.0%76.0% 78.0%80.0%82.0% 84.0%86.0%88.0% 90.0% Root Subj ObjDependency Type (DEP)D ependency Recall MSTParserMaltParser 0.0%10.0%20.0% 30.0%40.0%50.0% 60.0%70.0%80.0% 90.0% CCH CCD CJCC CJCJDependency Type (DEP)De pendency Precision MSTParserMaltParser 0.0%10.0%20.0% 30.0%40.0%50.0% 60.0%70.0%80.0% 90.0% CCH CCD CJCC CJCJDependency Tyle (DEP)D ependency Recall MSTParserMaltParser Figure 8: Precision/recall for different dependency types.and Slovene.</S>
			<S sid="179" ssid="80">The category CCD (coordinating con junction as dependent) instead denotes conjunctionsthat are attached as dependents of one of the conjuncts with a label that only marks them as conjunc tions, a type of category found in the data sets for Bulgarian, Danish, German, Portuguese, Swedishand Turkish.</S>
			<S sid="180" ssid="81">The two remaining categories con tain conjuncts that are assigned a dependency labelthat only marks them as conjuncts and that are attached either to the conjunction (CJCC) or to another conjunct (CJCJ).</S>
			<S sid="181" ssid="82">The former is found in Bulgarian, Danish, and German; the latter only in Por tuguese and Swedish.</S>
			<S sid="182" ssid="83">For most of the coordination categories there is little or no difference between the two parsers, but for CCH there is a difference in both precision and recall of almost 20 percentage points to MSTParser?s advantage.</S>
			<S sid="183" ssid="84">This can be explained by noting that, while the categories CCD, CJCC, andCJCJ denote relations that are internal to the coor dinate structure and therefore tend to be local, theCCH relations hold between the coordinate struc ture and its head, which is often a relation that spans over a greater distance and is nearer the root of the dependency graph.</S>
			<S sid="184" ssid="85">It is likely that the difference in accuracy for this type of dependency accounts for a large part of the difference in accuracy noted earlier for conjunctions as a part of speech.</S>
			<S sid="185" ssid="86">4.4 Discussion.</S>
			<S sid="186" ssid="87">The experiments from the previous section highlight the fundamental trade-off between global trainingand exhaustive inference on the one hand and ex pressive feature representations on the other.</S>
			<S sid="187" ssid="88">Errorpropagation is an issue for MaltParser, which typi 129cally performs worse on long sentences, long depen dency arcs and arcs higher in the graphs.</S>
			<S sid="188" ssid="89">But this is offset by the rich feature representation available tothese models that result in better decisions for fre quently occurring arc types like short dependencies or subjects and objects.</S>
			<S sid="189" ssid="90">The errors for MSTParser are spread a little more evenly.</S>
			<S sid="190" ssid="91">This is expected, as the inference algorithm and feature representation should not prefer one type of arc over another.</S>
			<S sid="191" ssid="92">What has been learned?</S>
			<S sid="192" ssid="93">It was already known that the two systems make different errors through the work of Sagae and Lavie (2006).</S>
			<S sid="193" ssid="94">However, in that work an arc-based voting scheme was used that took only limited account of the properties of the words connected by a dependency arc (more precisely, the overall accuracy of each parser for the part of speech of the dependent).</S>
			<S sid="194" ssid="95">The analysis in this work not onlyshows that the errors made by each system are dif ferent, but that they are different in a way that can be predicted and quantified.</S>
			<S sid="195" ssid="96">This is an important step in parser development.</S>
			<S sid="196" ssid="97">To get some upper bounds of the improvement that can be obtained by combining the strengths ofeach models, we have performed two oracle experi ments.</S>
			<S sid="197" ssid="98">Given the output of the two systems, we can envision an oracle that can optimally choose which single parse or combination of sub-parses to predict as a final parse.</S>
			<S sid="198" ssid="99">For the first experiment the oracleis provided with the single best parse from each sys tem, say G = (V,A) and G?</S>
			<S sid="199" ssid="100">= (V ?, A?).</S>
			<S sid="200" ssid="101">The oraclechooses a parse that has the highest number of cor rectly predicted labeled dependency attachments.</S>
			<S sid="201" ssid="102">In this situation, the oracle accuracy is 84.5%.</S>
			<S sid="202" ssid="103">In the second experiment the oracle chooses the tree thatmaximizes the number of correctly predicted depen dency attachments, subject to the restriction that the tree must only contain arcs from A ? A?.</S>
			<S sid="203" ssid="104">This can be computed by setting the weight of an arc to 1 if it is in the correct parse and in the set A ? A?.</S>
			<S sid="204" ssid="105">All other arc weights are set to negative infinity.</S>
			<S sid="205" ssid="106">One can then simply find the tree that has maximal sum of arc weights using directed spanning tree algorithms.This technique is similar to the parser voting methods used by Sagae and Lavie (2006).</S>
			<S sid="206" ssid="107">In this situa tion, the oracle accuracy is 86.9%.</S>
			<S sid="207" ssid="108">In both cases we see a clear increase in accuracy: 86.9% and 84.5% relative to 81% for the individual systems.</S>
			<S sid="208" ssid="109">This indicates that there is still potential for improvement, just by combining the two existing models.</S>
			<S sid="209" ssid="110">More interestingly, however, we can use the analysis to get ideas for new models.</S>
			<S sid="210" ssid="111">Below we sketch some possible new directions: 1.</S>
			<S sid="211" ssid="112">Ensemble systems: The error analysis pre-.</S>
			<S sid="212" ssid="113">sented in this paper could be used as inspirationfor more refined weighting schemes for ensem ble systems of the kind proposed by Sagae and Lavie (2006), making the weights depend on a range of linguistic and graph-based factors.</S>
			<S sid="213" ssid="114">2.</S>
			<S sid="214" ssid="115">Hybrid systems: Rather than using an ensem-.</S>
			<S sid="215" ssid="116">ble of several parsers, we may construct a sin gle system integrating the strengths of each parser described here.</S>
			<S sid="216" ssid="117">This could defer to a greedy inference strategy during the early stages of the parse in order to benefit from a rich feature representation, but then default to a global exhaustive model as the likelihood for error propagation increases.</S>
			<S sid="217" ssid="118">3.</S>
			<S sid="218" ssid="119">Novel approaches: The two approaches inves-.</S>
			<S sid="219" ssid="120">tigated are each based on a particular combina tion of training and inference methods.</S>
			<S sid="220" ssid="121">Wemay naturally ask what other combinations mayprove fruitful.</S>
			<S sid="221" ssid="122">For example, what about glob ally trained, greedy, transition-based models?</S>
			<S sid="222" ssid="123">This is essentially what Daume?</S>
			<S sid="223" ssid="124">III et al (2006) provide, in the form of a general search-basedstructured learning framework that can be directly applied to dependency parsing.</S>
			<S sid="224" ssid="125">The ad vantage of this method is that the learning can set model parameters relative to errors resulting directly from the search strategy ? such as errorpropagation due to greedy search.</S>
			<S sid="225" ssid="126">When combined with MaltParser?s rich feature representation, this could lead to significant improve ments in performance.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="5">
			<S sid="226" ssid="1">We have presented a thorough study of the dif ference in errors made between global exhaustivegraph-based parsing systems (MSTParser) and local greedy transition-based parsing systems (Malt Parser).</S>
			<S sid="227" ssid="2">We have shown that these differences can be quantified and tied to theoretical expectations of each model, which may provide insights leading to better models in the future.</S>
			<S sid="228" ssid="3">130</S>
	</SECTION>
</PAPER>
