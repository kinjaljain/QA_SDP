<PAPER>
  <S sid="0">Named Entity Recognition for Ukrainian: A Resource-Light Approach</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Kruislaan 419, 1098VA the katrenko@science.uva.nl Pieter HCSL, University of Kruislaan 419, 1098VA the pitera@science.uva.nl Abstract Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes.</S>
    <S sid="2" ssid="2">In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology.</S>
    <S sid="3" ssid="3">The approach we follow uses a restricted number of features.</S>
    <S sid="4" ssid="4">We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The information extraction task has proved to be difficult for a variety of domains (Riloff, 1995).</S>
    <S sid="6" ssid="2">The extracted information can further be used for question answering, information retrieval and other applications.</S>
    <S sid="7" ssid="3">Depending on the final purpose, the extracted information can be of different type, e.g., temporal events, locations, etc.</S>
    <S sid="8" ssid="4">The information corresponding to locations and names, is referred to as the information about named entities.</S>
    <S sid="9" ssid="5">Hence, named entity recognition constitutes a subtask of the information extraction in general.</S>
    <S sid="10" ssid="6">It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002).</S>
    <S sid="11" ssid="7">Named entity recognition for the languages with a rich morphology and a free word order is difficult because of several reasons.</S>
    <S sid="12" ssid="8">The entropy of texts in such languages is usually higher than the entropy of English texts.</S>
    <S sid="13" ssid="9">It is either needed to use such resources as morphological analyzers to reduce the data sparseness or to annotate a large amount of data in order to obtain a good performance.</S>
    <S sid="14" ssid="10">Luckily, the free word order is not crucial for the named entity recognition task as the local context of a named entity should be sufficient for its detection.</S>
    <S sid="15" ssid="11">Besides, a free word order usually implies a free order of constituents (such as noun phrases or verb phrases) rather than words as such.</S>
    <S sid="16" ssid="12">For instance, although (1)1 is grammatically correct and can occur in the data, it would be less frequent than (2).</S>
    <S sid="17" ssid="13">The first phrase exemplifies that an adjective is in a focus, whereas the second reflects the word order which is more likely to occur.</S>
    <S sid="18" ssid="14">In terms of named entities, an entity consisting of several words is also less likely to be split (consider, e.g., National saw she Bank where &#8217;National Bank&#8217; represents one named entity of type organization).</S>
    <S sid="19" ssid="15">In the newspaper corpus we annotated, we have observed no examples of split named entities.</S>
    <S sid="20" ssid="16">In this paper, we study different data representation and machine learning methods to extract the named entities from text.</S>
    <S sid="21" ssid="17">Our goal is two-fold.</S>
    <S sid="22" ssid="18">First, 'all examples in the paper are in Ukrainian, for convenience translated and sometimes transliterated Prague, June 2007. c&#65533;2007 Association for Computational Linguistics we explore the possibility of using patterns induced from the data gathered on the Web.</S>
    <S sid="23" ssid="19">We also consider Levenshtein distance to find the most similar instances in the test data given a training set.</S>
    <S sid="24" ssid="20">Besides, we study the impact of different feature sets on the resulting classification performance.</S>
    <S sid="25" ssid="21">We start with the short overview of the methods for NER proposed by the IE community.</S>
    <S sid="26" ssid="22">Afterwards, the experiments are described.</S>
    <S sid="27" ssid="23">We conclude with the outlook for the future work.</S>
  </SECTION>
  <SECTION title="2 Related work" number="2">
    <S sid="28" ssid="1">The existing NER systems use many sources in order to be able to extract NEs from the text data.</S>
    <S sid="29" ssid="2">Some of them rely on hand-written rules and precompiled lists of city names, person names and other NEs in a given language, while others explore methods to automatically extract NEs without prior knowledge.</S>
    <S sid="30" ssid="3">In the first case, the gazetteers will in most cases improve NER results (Carreras et al., 2002) but, unfortunately, they may not exist for a language one is working on.</S>
    <S sid="31" ssid="4">Hand-written rules can also cover more NEs but building such patterns will be a very time-consuming process.</S>
    <S sid="32" ssid="5">There have been many methods applied to NER, varying from the statistical to the memory-based approaches.</S>
    <S sid="33" ssid="6">Most work on NER has been focused on English but there are also approaches to other languages such as Spanish (Kozareva et al., 2005), German, or Dutch.</S>
    <S sid="34" ssid="7">In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002).</S>
    <S sid="35" ssid="8">While analyzing the results of these shared tasks, it can be concluded that the selected features are of a great importance.</S>
    <S sid="36" ssid="9">In our view, they can be categorized in two types, i.e. contextual and orthographic 2.</S>
    <S sid="37" ssid="10">The first type includes words surrounding a given word while the other contains such features as capitalized letters, digits contained within the word, etc.</S>
    <S sid="38" ssid="11">Both types of features contribute to the information extraction task.</S>
    <S sid="39" ssid="12">Nevertheless, orthographic features can already be language-specific.</S>
    <S sid="40" ssid="13">For instance, capitalization is certainly very important for such languages as English or Dutch but it might be less useful for German.</S>
    <S sid="41" ssid="14">2Sometimes, these types of features are referred to as wordexternal and word-internal (Klein et al., 2003) The feature set of some NER methods (Wu, 2002) also includes part-of-speech information and/or word prefixes and suffixes.</S>
    <S sid="42" ssid="15">Although this information (and especially lemmas) is very useful for the languages with rich morphology, it presupposes the existence of POS taggers for a given language.</S>
    <S sid="43" ssid="16">Another conclusion which can be drawn relates to the machine learning approaches.</S>
    <S sid="44" ssid="17">The best results have been received by applying ensemble methods (Wu, 2002; Florian, 2002; Carreras et al., 2002).</S>
    <S sid="45" ssid="18">A very interesting work on named entity recognition task was reported by Collins et al. (1999) who used only few named entities to bootstrap more.</S>
    <S sid="46" ssid="19">The other approach proposed recently makes use of the data extracted from the Web (Talukdar et al., 2006).</S>
    <S sid="47" ssid="20">By restricting themselves to the fixed context of the extracted named entities and by employing grammar inference techniques, the authors filter out the most useful patterns.</S>
    <S sid="48" ssid="21">As they show, by applying such approach precision can already be largely boosted.</S>
    <S sid="49" ssid="22">Pastra et al. (2002) focused on the applicability of already existing resources in one language to another.</S>
    <S sid="50" ssid="23">Their case study was based on English and Romanian, where a system, originally developed for NER in English was adapted to Romanian.</S>
    <S sid="51" ssid="24">Their results suggest that such adaptation is easier than developing a named entity recognition system for Romanian from scratch.</S>
    <S sid="52" ssid="25">However, the authors also mention that not all phenomena in Romanian have been taken into account which resulted in low recall.</S>
  </SECTION>
  <SECTION title="3 Methodology" number="3">
    <S sid="53" ssid="1">Ukrainian belongs to the languages where the named entities are usually capitalized, which makes their detection relatively easy.</S>
    <S sid="54" ssid="2">In this paper we focus on using minimal information about the language in combination with the patterns learnt from the Web data, features extracted from the corpus and Levenshtein similarity measure.</S>
    <S sid="55" ssid="3">Our hypothesis behind all three components is the following.</S>
    <S sid="56" ssid="4">We expect orthographic features be useful for a named entity detection but not sufficient for its classification.</S>
    <S sid="57" ssid="5">Contextual information may already help but as we do not intend to use lemmas but words instead, it will likely not boost recall of the named entity recongnition.</S>
    <S sid="58" ssid="6">To be able to detect more named enities in the text, we propose to use patterns collected from the Web and Levenshtein similarity measure.</S>
    <S sid="59" ssid="7">Patterns from the Web should provide more contextual information than can be found in a corpus.</S>
    <S sid="60" ssid="8">In addition, a similarity measure gives us an opportunity to detect the named entities which have the same stem.</S>
    <S sid="61" ssid="9">The latter is especially useful when the same entity was mentioned in the training set as well as in the test data but its flections differ.</S>
    <S sid="62" ssid="10">The intention of our study is, therefore, to start with a standard set of features (contextual and orthographic) as used for the many languages in the past and to add some means which would account for the fact that Ukrainian is a highly-inflected language.</S>
    <S sid="63" ssid="11">First, we consider the features which can be easily extracted given the data, such as contextual and orthographic ones as described below in Table 1.</S>
    <S sid="64" ssid="12">For each word in a corpus its context (2 tokens to left and to the right) and its orthographic features are extracted.</S>
    <S sid="65" ssid="13">Orthographic features are binary features which, for instance, indicate whether a word is capitalized (1 or 0), etc.</S>
    <S sid="66" ssid="14">We have selected the following machine learning methods: k-nearest neighbor (knn) and voting and stacking as the ensemble methods which have been successfully applied to the named entity recognition task in the past.</S>
    <S sid="67" ssid="15">To overcome data sparseness and to increase recall, we make use of two techniques.</S>
    <S sid="68" ssid="16">First, we apply the patterns extracted from the Web.</S>
    <S sid="69" ssid="17">If we wish to collect patterns for a certain category C of the named entities (e.g.</S>
    <S sid="70" ssid="18">), we first collect all named entities that fall into it.</S>
    <S sid="71" ssid="19">Then, for each X E C, we use X as a query term for Google (for this purpose we used the Google API).</S>
    <S sid="72" ssid="20">The queries we constructed were mainly based on the locations, such as etc.</S>
    <S sid="73" ssid="21">For each of these words we created queries by declining them (as there are 7 cases in Ukrainian language which causes the high variability).</S>
    <S sid="74" ssid="22">Consequently, we get many snippets where X occurs.</S>
    <S sid="75" ssid="23">To extract patterns from snippets, we fix a context and use 2 words to the left and to the right of X as in the classification approach above.</S>
    <S sid="76" ssid="24">The patterns which only consist of a named entity, closed-class words (e.g., prepositions, conjunctions, etc.) and punctuation are removed as such that do not provide enough evidence to classify an instance.</S>
    <S sid="77" ssid="25">Intuitively, if there are many patterns acquired from the large collection of data on the Web, they must be sufficient (in some sense even redundant) to recognize named entities in a text.</S>
    <S sid="78" ssid="26">For instance, such pattern as was located in X in English can correspond to three patterns in Ukrainian was located (fem., sing.) in X, was located (masc., sing.) in X, was located (neut., sing.) in X.</S>
    <S sid="79" ssid="27">Even though these patterns could be embraced in one, we are rather interested in collecting all possible patterns avoiding this way stemming and morphological analysis.</S>
    <S sid="80" ssid="28">As in Talukdar&#8217;s approach (Talukdar et al., 2006), we expect patterns to provide high precision.</S>
    <S sid="81" ssid="29">We are, however, concerned about the size of Ukrainian Web which is much smaller than English part of the Web.</S>
    <S sid="82" ssid="30">As a consequence, it is not clear whether recall can be improved much by using the Web data.</S>
    <S sid="83" ssid="31">Yet another approach to address rich morphology of Ukrainian, is to carry out a matching of probable named entities in a test set against a list of named entities in a training set.</S>
    <S sid="84" ssid="32">It can be done by using string edit distances, such as Levenshtein.</S>
    <S sid="85" ssid="33">Levenshtein (or edit) distance of two strings, x and y is measured as the minimal number of insertions, deletions, or substitutions to transform one string into the other.</S>
    <S sid="86" ssid="34">Levenshtein distance has become popular in the natural language processing field and was used for the variety of tasks (e.g., semantic role labeling ).</S>
    <S sid="87" ssid="35">In the definition above, d(xi, yj) is a cost of substituting one symbol in x by a symbol from y.</S>
    <S sid="88" ssid="36">The insertion and deletion costs are equal to 1.</S>
    <S sid="89" ssid="37">Let A be a candidate named entity and L a list of all named entities found in the training set.</S>
    <S sid="90" ssid="38">By computing the Levenshtein distance between A and each element from L, the nearest neighbor to A will be a NE with the lowest Levenshtein score.</S>
    <S sid="91" ssid="39">It might, however, happen that there are no named entities in a training set that correspond to the candidate in a test set.</S>
    <S sid="92" ssid="40">Consider, for instance the Levenshtein distance of two words (George) and (besides) which is equal to 2.</S>
    <S sid="93" ssid="41">Even though the distance is low, we do not wish to classify as a named entity whose type is PERSON because it is simply a preposition.</S>
    <S sid="94" ssid="42">The problem we described can be solved in several ways.</S>
    <S sid="95" ssid="43">On the one hand, it is possible to use a list of stop words with most frequent prepositions, conjunctions and pronouns listed.</S>
    <S sid="96" ssid="44">On the other hand, we can also set a threshold for the Levenshtein distance.</S>
    <S sid="97" ssid="45">In the experiments we present below, we avoid setting threshold by using a simple heuristics.</S>
    <S sid="98" ssid="46">We align the first letters of A with its nearest neighbor.</S>
    <S sid="99" ssid="47">If they do not match (as in example above), we conclude that no variants of A belong to the training set.</S>
  </SECTION>
  <SECTION title="4 Experiments and Evaluation" number="4">
    <S sid="100" ssid="1">We have conducted three types of experiments using different feature sets, patterns extracted from the Web and Levenshtein distance.</S>
    <S sid="101" ssid="2">We expect that both types of experiments can shed a light on usefulness of the features that we defined for NER on Ukrainian data.</S>
    <S sid="102" ssid="3">Initially, several articles of the newspaper Mirror Weekly (year 2005)3 were annotated.</S>
    <S sid="103" ssid="4">During the annotating process we considered the following named instances: PERSON (person names), LOC (location), ORG (organization).In total, there were 10,000 tokens annotated, 514 of which are named entities.</S>
    <S sid="104" ssid="5">All named entities have been annotated according to the IOB annotation scheme (Ramshaw and Marcus, 1995).</S>
    <S sid="105" ssid="6">The annotated corpus can Scan be found at http://www.zn.kiev.ua be downloaded from http://www.science. uva.nl/&#732;katrenko/Corpus The corpus was split into training and test sets of 6,606 and 3,397 tokens, respectively.</S>
    <S sid="106" ssid="7">The corpus is relatively small but we hope to study whether such features as orthographic are sufficient for the NER task alone or it is needed to add more sources to approach this task.</S>
    <S sid="107" ssid="8">The results of our experiments on classification of named entities are provided in Table 2.</S>
    <S sid="108" ssid="9">Baseline B1 was defined by the most frequent tag in the data (ORG).</S>
    <S sid="109" ssid="10">Similarly to Conll shared task (Tjong Kim Sang, 2002), we also calculated a baseline by tagging all named entities which occurred in the training set (B2).</S>
    <S sid="110" ssid="11">Although there are many names of organizations detected, there are only 1,92% of person names recognized.</S>
    <S sid="111" ssid="12">Since we are interested in how much each type of the feature sets contributes to the classification accuracy, we have conducted experiments on contextual features only, on orthographic features only (model M2&#8722;knn in Table 2) and on the combinaortho tions of both (model M2&#8722;knn in Table 2).</S>
    <S sid="112" ssid="13">When ortho+cont used alone, contextual features do not provide a high performance.</S>
    <S sid="113" ssid="14">However, their combination with the orthographic features already results in a higher precision (at expense of recall) and in a higher Fscore.</S>
    <S sid="114" ssid="15">It is worth noting that all results given in Table 2 were obtained either by using memorybased learning (in particular, k-nearest neighbor as in M2&#8722;kon and in M2&#8722;knnortho+cont) or by ensemble orthmethods (as in MVoting and MStacking ) The ortho+cont ortho+cont latter option was particularly interesting to explore because it proved to provide accurate results for the named entity recognition task in the past.</S>
    <S sid="115" ssid="16">The results in Table 2 also seem to support a claim that the ensemble methods perform better.</S>
    <S sid="116" ssid="17">It can be seen when comparing M2&#65533;knn ortho+cont, MV oting ortho+cont and MStacking ortho+cont.</S>
    <S sid="117" ssid="18">Despite of using the same feature sets, Voting (based on Naive Bayes, decision trees and 2knn) and Stacking (2-knn as a meta-learner applied to Naive Bayes and decision tree learner) both provide higher precision but lower recall.</S>
    <S sid="118" ssid="19">By using x2 test on the training set, we determined which attributes are the most informative for the classification task.</S>
    <S sid="119" ssid="20">The most informative turned out to be a word itself followed by the surrounding context (one token to the right and to the left).</S>
    <S sid="120" ssid="21">The least informative feature is NUM, apparently because there have been not many named entities containing digits.</S>
    <S sid="121" ssid="22">As a next step, we employed the patterns extracted from the Web data.</S>
    <S sid="122" ssid="23">Some of the patterns accompanied with the translation and information on case are given in Table 3.</S>
    <S sid="123" ssid="24">It can be noticed that not all of the patterns are accurate.</S>
    <S sid="124" ssid="25">For instance, a pattern together with a city mayor LOC can also be used to extract a name of a mayor (hence, PERSON) and not a location (LOC).</S>
    <S sid="125" ssid="26">Patterns containing prepositions (so, mostly patterns containing a named entity in locative case) &#8217;in&#8217;, &#8217;with&#8217;, &#8217;nearby&#8217; are usually more accurate but they still require additional context (as a word &#8217;town&#8217; in in a little town LOC).</S>
    <S sid="126" ssid="27">The results we obtained by employing such patterns did not significantly change the overall performance (Table 2, model MV oting ) However, ortho+cont+pat the performance on some categories such as ORG or LOC (Table 5 and Table 6, model ALL+P) was positively affected in terms of F-score.</S>
    <S sid="127" ssid="28">Finally, we compare all capitalized words in a test set against the named entities found in the training data.</S>
    <S sid="128" ssid="29">The first 6 examples in Table 4 show the same nouns but in different cases.</S>
    <S sid="129" ssid="30">The distance in each case is equal 1.</S>
    <S sid="130" ssid="31">Since we did not carry out the morphological analysis of the corpus, many such occurrences of the named entities in the test data were found given the information from the training set using orthographic and contextual features only (as they do not match exactly).</S>
    <S sid="131" ssid="32">However, Levenshtein distance helps to identify the variants of the same named entity.</S>
    <S sid="132" ssid="33">The results of applying Levenshtein distance (together with the patterns and Voting model on all features) for each category are given in Table 5 and Table 6 (model ALL+P+L).</S>
    <S sid="133" ssid="34">LOC and ORG are two categories whose performance is greatly improved by using Levenshtein distance.</S>
    <S sid="134" ssid="35">In case of PERSON category, recall gets slightly higher, whereas precision does not change much.</S>
    <S sid="135" ssid="36">The last three examples in Table 4 are very interesting.</S>
    <S sid="136" ssid="37">They show that sometimes the nearest neighbor of the candidates for NEs in the test data is a named entity of the same category but it cannot be found by aligning.</S>
    <S sid="137" ssid="38">Having noticed this, we decided to exclude aligning step and to consider a nearest neighbor of every capitalized token in the test set.</S>
    <S sid="138" ssid="39">Although we extracted few novel person names and locations, performance in terms of precision dropped significantly.</S>
    <S sid="139" ssid="40">The very last example in Table 4 demonstrates a case when applying Levenshtein measure fails.</S>
    <S sid="140" ssid="41">In this case is of type ORG (a political party) and are people who belong to the party.</S>
    <S sid="141" ssid="42">Given the nearest neighbor and the successful alignment, it is predicted that belongs to the category ORG but it is not true.</S>
    <S sid="142" ssid="43">In the other example involving the same entity is correctly classified as ORG (it is the same named entity as in the training data but in dative case). instrumental It can be concluded that, in general, Levenshtein distance helps to identify many named entities which were either misclassified or not detected at all.</S>
    <S sid="143" ssid="44">However, it is sometimes unable to distinguish between the variant of the same named entity and a true negative.</S>
    <S sid="144" ssid="45">Additional constraints such as the upper threshold of the Levenshtein distance might solve this problem.</S>
    <S sid="145" ssid="46">As can be seen from Table 2, the best overall performance is achieved by combining contextual and orthographic features together with the patterns extracted from the Web and entities classified by employing the Levenshtein distance.</S>
  </SECTION>
  <SECTION title="5 Conclusions and Future work" number="5">
    <S sid="146" ssid="1">In this paper, we focused on standard features used for the named entity recognition on the newswire data which have been used on many languages.</S>
    <S sid="147" ssid="2">To improve the results that we get by employing orthographic and contextual features, we add patterns extracted from the Web and use a similarity measure to find the named entities similar to the NEs in the training set.</S>
    <S sid="148" ssid="3">The results we received are, in general, lower than the performance of NER systems in other languages but higher than both baselines.</S>
    <S sid="149" ssid="4">The former might be explained by the size of the corpus we use and by the characteristics of the language.</S>
    <S sid="150" ssid="5">As Ukrainian language is a language with a rich morphology, there are several directions we would like to explore in the future.</S>
    <S sid="151" ssid="6">From the language-oriented perspective, it would be useful to determine to which extent stemming and morphological analysis would boost performance.</S>
    <S sid="152" ssid="7">The other problem which we have not considered up to now is the ambiguity of some named entities.</S>
    <S sid="153" ssid="8">For example, a word &#8217;Ukraine&#8217; can belong to the category LOC as well as to the category ORG (as it is a part of a complex named entity).</S>
    <S sid="154" ssid="9">In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999).</S>
  </SECTION>
</PAPER>
