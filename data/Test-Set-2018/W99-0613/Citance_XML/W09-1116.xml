<PAPER>
  <S sid="0">Glen Glenda or Glendale: Unsupervised and Semi-supervised Learning of English Noun Gender</S>
  <ABSTRACT>
    <S sid="1" ssid="1">pronouns like reflect the gender and number of the entities to which they refer.</S>
    <S sid="2" ssid="2">Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender.</S>
    <S sid="3" ssid="3">Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems.</S>
    <S sid="4" ssid="4">Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class.</S>
    <S sid="5" ssid="5">While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method.</S>
    <S sid="6" ssid="6">Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model.</S>
    <S sid="7" ssid="7">Our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features.</S>
    <S sid="8" ssid="8">By leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classification.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="9" ssid="1">Pronoun resolution is the process of determining which preceding nouns are referred to by a particular pronoun in text.</S>
    <S sid="10" ssid="2">Consider the sentence: (1) Glen told Glenda that she was wrong about Glendale.</S>
    <S sid="11" ssid="3">A pronoun resolution system should determine that the pronoun she refers to the noun Glenda.</S>
    <S sid="12" ssid="4">Pronoun resolution is challenging because it requires a lot of world knowledge (general knowledge of word types).</S>
    <S sid="13" ssid="5">If she is replaced with the pronoun he in (1), Glen becomes the antecedent.</S>
    <S sid="14" ssid="6">Pronoun resolution systems need the knowledge of noun gender that advises that Glen is usually masculine (and thus referred to by he) while Glenda is feminine.</S>
    <S sid="15" ssid="7">English third-person pronouns are grouped in four gender/number categories: masculine (he, his, him, himself), feminine (she, her, herself), neutral (it, its, itself), and plural (they, their, them, themselves).</S>
    <S sid="16" ssid="8">We broadly refer to these gender and number classes simply as gender.</S>
    <S sid="17" ssid="9">The objective of our work is to correctly assign gender to English noun tokens, in context; to determine which class of pronoun will refer to a given noun.</S>
    <S sid="18" ssid="10">One successful approach to this problem is to build a statistical gender model from a noun&#8217;s association with pronouns in text.</S>
    <S sid="19" ssid="11">For example, Ge et al. (1998) learn Ford has a 94% chance of being neutral, based on its frequent co-occurrence with neutral pronouns in text.</S>
    <S sid="20" ssid="12">Such estimates are noisy but useful.</S>
    <S sid="21" ssid="13">Both Ge et al. (1998) and Bergsma and Lin (2006) show that learned gender is the most important feature in their pronoun resolution systems.</S>
    <S sid="22" ssid="14">English differs from other languages like French and German in that gender is not an inherent grammatical property of an English noun, but rather a property of a real-world entity that is being referred to.</S>
    <S sid="23" ssid="15">A common noun like lawyer can be (semantically) masculine in one document and feminine in another.</S>
    <S sid="24" ssid="16">While previous statistical gender models learn gender for noun types only, we use document context to correctly determine the current gender class of noun tokens, making dynamic decisions on common nouns like lawyer and ambiguous names like Ford.</S>
    <S sid="25" ssid="17">Furthermore, if a noun type has not yet been observed (an unknown word), previous approaches cannot estimate the gender.</S>
    <S sid="26" ssid="18">Our system, on the other hand, is able to correctly determine that unknown words corroborators and propeller-heads are plural, while Pope Formosus is masculine, using learned contextual and morphological cues.</S>
    <S sid="27" ssid="19">Our approach is based on the key observation that while gender information from noun-pronoun cooccurrence provides imperfect noun coverage, it can nevertheless provide rich and accurate training data for a large-scale discriminative classifier.</S>
    <S sid="28" ssid="20">The classifier leverages a wide variety of noun properties to generalize from the automatically-labeled examples.</S>
    <S sid="29" ssid="21">The steps in our approach are: This algorithm achieves significantly better performance than the existing state-of-the-art statistical gender classifier, while requiring no manuallylabeled examples to train.</S>
    <S sid="30" ssid="22">Furthermore, by training on a small number of manually-labeled examples, we can combine the predictions of this system with the counts from the original gender database.</S>
    <S sid="31" ssid="23">This semi-supervised extension achieves 95.5% accuracy on final unseen test data, an impressive 50% reduction in error over previous work.</S>
  </SECTION>
  <SECTION title="2 Path-based Statistical Noun Gender" number="2">
    <S sid="32" ssid="1">Seed (noun,gender) examples can be extracted reliably and automatically from raw text, providing the training data for our discriminative classifier.</S>
    <S sid="33" ssid="2">We call these examples pseudo-seeds because they are created fully automatically, unlike the small set of manually-created seeds used to initialize other bootstrapping approaches (cf. the bootstrapping approaches discussed in Section 6).</S>
    <S sid="34" ssid="3">We adopt a statistical approach to acquire the pseudo-seed (noun,gender) pairs.</S>
    <S sid="35" ssid="4">All previous statistical approaches rely on a similar observation: if a noun like Glen is often referred to by masculine pronouns, like he or his, then Glen is likely a masculine noun.</S>
    <S sid="36" ssid="5">But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics.</S>
    <S sid="37" ssid="6">Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009).</S>
    <S sid="38" ssid="7">In statistical approaches, the more frequent the noun, the more accurate the assignment of gender.</S>
    <S sid="39" ssid="8">We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer.</S>
    <S sid="40" ssid="9">To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns connected with these paths in the corpus.</S>
    <S sid="41" ssid="10">In their database, each noun is listed with its corresponding masculine, feminine, neutral, and plural pronoun co-occurrence counts, e.g.</S>
    <S sid="42" ssid="11">: glen 555 42 32 34 glenda 8 102 0 11 glendale 24 2 167 18 glendalians 0 0 0 1 glenn 3182 207 95 54 glenna 0 6 0 0 This sample of the gender data shows that the noun glenda, for example, occurs 8 times with masculine pronouns, 102 times with feminine pronouns, 0 times with neutral pronouns, and 11 times with plural pronouns; 84% of the time glenda co-occurs with a feminine pronoun.</S>
    <S sid="43" ssid="12">Note that all nouns in the data have been converted to lower-case.2 There are gender counts for 3.1 million English nouns in the online database.</S>
    <S sid="44" ssid="13">These counts form the basis for the state-of-the-art gender classifier.</S>
    <S sid="45" ssid="14">We can either take the most-frequent pronoun-gender (MFPG) as the class (e.g. feminine for glenda), or we can supply the logarithm of the counts as features in a 4-way multi-class classifier.</S>
    <S sid="46" ssid="15">We implement the latter approach as a comparison system and refer to it as PATHGENDER in our experiments.</S>
    <S sid="47" ssid="16">In our approach, rather than using these counts directly, we process the database to automatically extract a high-coverage but also high-quality set of pseudo-seed (noun,gender) pairs.</S>
    <S sid="48" ssid="17">First, we filter nouns that occur less than fifty times and whose MFPG accounts for less than 85% of counts.</S>
    <S sid="49" ssid="18">Next, we note that the most reliable nouns should occur relatively often in a coreferent path.</S>
    <S sid="50" ssid="19">For example, note that importance occurs twice as often on the web as Clinton, but has twenty-four times less counts in the gender database.</S>
    <S sid="51" ssid="20">This is because importance is unlikely to be a pronoun&#8217;s antecedent.</S>
    <S sid="52" ssid="21">We plan to investigate this idea further in future work as a possible filter on antecedent candidates for pronoun resolution.</S>
    <S sid="53" ssid="22">For the present work, simply note that a high ratio of database-count to webcount provides a good indication of the reliability of a noun&#8217;s gender counts, and thus we filter nouns that have such ratios below a threshold.3 After this filtering, we have about 45 thousand nouns to which we automatically assign gender according to their MFPG.</S>
    <S sid="54" ssid="23">These (noun,gender) pairs provide the seed examples for the training process described in the 2Statistical approaches can adapt to the idiosyncrasies of the particular text domain.</S>
    <S sid="55" ssid="24">In the news text from which this data was generated, for example, both the word ships and specific instances of ships (the USS Cole, the Titanic, etc.) are neutral.</S>
    <S sid="56" ssid="25">In Wikipedia, on the other hand, feminine pronouns are often used for ships.</S>
    <S sid="57" ssid="26">Such differences can be learned automatically.</S>
    <S sid="58" ssid="27">3We roughly tuned all the thresholds to obtain the highest number of seeds such that almost all of them looked correct (e.g.</S>
    <S sid="59" ssid="28">Figure 1).</S>
    <S sid="60" ssid="29">Further work is needed to determine whether a different precision/recall tradeoff can improve performance.</S>
    <S sid="61" ssid="30">.</S>
    <S sid="62" ssid="31">.</S>
    <S sid="63" ssid="32">. stefanie steffi graf steinem stella mccartney stellar jayne stepdaughter stephanie stephanie herseth stephanie white stepmother stewardess following section.</S>
    <S sid="64" ssid="33">Figure 1 provides a portion of the ordered feminine seed nouns that we extracted.</S>
  </SECTION>
  <SECTION title="3 Discriminative Learning of Gender" number="3">
    <S sid="65" ssid="1">Once we have extracted a number of pseudo-seed (noun,gender) pairs, we use them to automaticallylabel nouns (in context) in raw text.</S>
    <S sid="66" ssid="2">The autolabeled examples provide training data for discriminative learning of noun gender.</S>
    <S sid="67" ssid="3">Since the training pairs are acquired from a sparse and imperfect model of gender, what can we gain by training over them?</S>
    <S sid="68" ssid="4">We can regard the Bergsma and Lin (2006) approach and our discriminative system as two orthogonal views of gender, in a co-training sense (Blum and Mitchell, 1998).</S>
    <S sid="69" ssid="5">Some nouns can be accurately labeled by nounpronoun co-occurrence (a view based on pronoun co-occurrence), and these examples can be used to deduce other gender-indicating regularities (a view based on other features, described below).</S>
    <S sid="70" ssid="6">We presently explain how examples are extracted using our pseudo-seed pairs, turned into autolabeled feature vectors, and then used to train a supervised classifier.</S>
    <S sid="71" ssid="7">Our example-extraction module processes a large collection of documents (roughly a million documents in our experiments).</S>
    <S sid="72" ssid="8">For each document, we extract all the nouns, including context words within &#177;5 tokens of each noun.</S>
    <S sid="73" ssid="9">We then group the nouns by their (lower-case) string.</S>
    <S sid="74" ssid="10">If a group&#8217;s noun-string is in our set of seed (noun,gender) pairs, we assign the corresponding gender to be the class of the group.</S>
    <S sid="75" ssid="11">Otherwise, we discard the group.</S>
    <S sid="76" ssid="12">To prevent frequent nouns from dominating our training data, we only keep the first 200 groups corresponding to each noun string.</S>
    <S sid="77" ssid="13">Figure 2 gives an example training noun group with some (selected) context sentences.</S>
    <S sid="78" ssid="14">At test time, all nouns in the test documents are converted to this format for further processing.</S>
    <S sid="79" ssid="15">We group nouns because there is a strong tendency for nouns to have only one sense (and hence gender) per discourse.</S>
    <S sid="80" ssid="16">We extract contexts because nearby words provide good clues about which gender is being used.</S>
    <S sid="81" ssid="17">The notion that nouns have only one sense per discourse/collocation was also exploited by Yarowsky (1995) in his seminal work on bootstrapping for word sense disambiguation.</S>
    <S sid="82" ssid="18">Once the training instances are extracted, they are converted to labeled feature vectors for supervised learning.</S>
    <S sid="83" ssid="19">The automatically-determined gender provides the class label (e.g., masculine for the group in Figure 2).</S>
    <S sid="84" ssid="20">The features identify properties of the noun and its context that potentially correlate with a particular gender category.</S>
    <S sid="85" ssid="21">We divide the features into two sets: those that depend on the contexts within the document (Context features: features of the tokens in the document), and those that depend on the noun string only (Type features).</S>
    <S sid="86" ssid="22">In both cases we induce the feature space from the training examples, keeping only those features that occur more than 5 times.</S>
    <S sid="87" ssid="23">The first set of features represent the contexts of the word, using all the contexts in the noun group.</S>
    <S sid="88" ssid="24">To illustrate the potential utility of the context information, consider the context sentences for the masculine noun in Figure 2.</S>
    <S sid="89" ssid="25">Even if these snippets were all the information we were given, it would be easy to guess the gender of the noun.</S>
    <S sid="90" ssid="26">We use binary attribute-value features to flag, for any of the contexts, the presence of all words at context positions &#177;1, &#177;2, etc.</S>
    <S sid="91" ssid="27">(sometimes called collocation features (Golding and Roth, 1999)).</S>
    <S sid="92" ssid="28">For example, feature 255920 flags that the word two-tothe-right of the noun is he.</S>
    <S sid="93" ssid="29">We also provide features for the presence of all words anywhere within &#177;5 tokens of the noun (sometimes called context words).</S>
    <S sid="94" ssid="30">We also parse the sentence and provide a feature for the noun&#8217;s parent (and relationship with the parent) in the parse tree.</S>
    <S sid="95" ssid="31">For example, the instance in Figure 2 has features downloaded(subject), says(subject), etc.</S>
    <S sid="96" ssid="32">Since plural nouns should be governed by plural verbs, this feature is likely to be especially helpful for number classification.</S>
    <S sid="97" ssid="33">The next group of features represent morphological properties of the noun.</S>
    <S sid="98" ssid="34">Binary features flag the presence of all prefixes and suffixes of one-to-four characters.</S>
    <S sid="99" ssid="35">For multi-token nouns, we have features for the first and last token in the noun.</S>
    <S sid="100" ssid="36">Thus we hope to learn that Bob begins masculine nouns while inc. ends neutral ones.</S>
    <S sid="101" ssid="37">Finally, we have features that indicate if the noun or parts of the noun occur on various lists.</S>
    <S sid="102" ssid="38">Indicator features specify if any token occurs on in-house lists of given names, family names, cities, provinces, countries, corporations, languages, etc.</S>
    <S sid="103" ssid="39">A feature also indicates if a token is a corporate designation (like inc. or ltd.) or a human one (like Mr. or Sheik).</S>
    <S sid="104" ssid="40">We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).4 This data provides counts for pairs such as (Zhang Qiyue, spokeswoman) and (Thorvald Stoltenberg, mediator).</S>
    <S sid="105" ssid="41">We have features for all concepts (like spokeswoman and mediator) and therefore learn their association with each gender.</S>
    <S sid="106" ssid="42">Once all the feature vectors have been extracted, they are passed to a supervised machine learning algorithm.</S>
    <S sid="107" ssid="43">We train and classify using a multi-class linear-kernel Support Vector Machine (SVM) (Crammer and Singer, 2001).</S>
    <S sid="108" ssid="44">SVMs are maximum-margin classifiers that achieve good performance on a range of tasks.</S>
    <S sid="109" ssid="45">At test time, nouns in test documents are processed exactly as the training instances described above, converting them to feature vectors.</S>
    <S sid="110" ssid="46">The test vectors are classified by the SVM, providing gender classes for all the nouns in the test document.</S>
    <S sid="111" ssid="47">Since all training examples are labeled automatically (auto-trained), we denote systems using this approach as -AUTO.</S>
    <S sid="112" ssid="48">Although a good gender classifier can be learned from the automatically-labeled examples alone, we can also use a small quantity of gold-standard labeled examples to achieve better performance.</S>
    <S sid="113" ssid="49">Combining information from our two sets of labeled data is akin to a domain adaptation problem.</S>
    <S sid="114" ssid="50">The gold-standard data can be regarded as high-quality in-domain data, and the automaticallylabeled examples can be regarded as the weaker, but larger, out-of-domain evidence.</S>
    <S sid="115" ssid="51">There is a simple but effective method for combining information from two domains using predictions as features.</S>
    <S sid="116" ssid="52">We train a classifier on the full set of automatically-labeled data (as described in Section 3.3), and then use this classifier&#8217;s predictions as features in a separate classifier, which is trained on the gold-standard data.</S>
    <S sid="117" ssid="53">This is like the competitive Feats domain-adaptation system in Daum&#180;e III and Marcu (2006).</S>
    <S sid="118" ssid="54">For our particular SVM classifier (Section 4.1), predictions take the form of four numerical scores corresponding to the four different genders.</S>
    <S sid="119" ssid="55">Our gold-standard classifier has features for these four predictions plus features for the original path-based gender counts (Section 2).5 Since this approach uses both automatically-labeled and gold-standard data in a semi-supervised learning framework, we denote systems using this approach as -SEMI.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="120" ssid="1">We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data.</S>
    <S sid="121" ssid="2">We process this data as described in Section 3, making feature vectors from the first 4 million noun groups.</S>
    <S sid="122" ssid="3">We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVM struct software package (Tsochantaridis et al., 2004).</S>
    <S sid="123" ssid="4">To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004).</S>
    <S sid="124" ssid="5">In each document, we first group all nouns with a common lower-case string (exactly as done for our example extraction (Section 3.1)).</S>
    <S sid="125" ssid="6">Next, for each group we determine if a third-person pronoun refers to any noun in that group.</S>
    <S sid="126" ssid="7">If so, we label all nouns in the group with the gender of the referring pronoun.</S>
    <S sid="127" ssid="8">For example, if the pronoun he refers to a noun Brown, then all instances of Brown in the document are labeled as masculine.</S>
    <S sid="128" ssid="9">We extract the genders for 2794 nouns in the ANC training set (in 798 noun groups) and 2596 nouns in the ANC test set (in 642 groups).</S>
    <S sid="129" ssid="10">We apply this method to other annotated corpora (including MUC corpora) to create a development set.</S>
    <S sid="130" ssid="11">The gold standard ANC training set is used to set the weights on the counts in the PATHGENDER classifiers, and to train the semi-supervised approaches.</S>
    <S sid="131" ssid="12">We also use an SVM to learn these weights.</S>
    <S sid="132" ssid="13">We use the development set to tune the SVM&#8217;s regularization parameter, both for systems trained on automatically-generated data, and for systems trained on gold-standard data.</S>
    <S sid="133" ssid="14">We also optimize each automatically-trained system on the development set when we include this system&#8217;s predictions as features in the semi-supervised extension.</S>
    <S sid="134" ssid="15">We evaluate and state performance for all approaches on the final unseen ANC test set.</S>
    <S sid="135" ssid="16">The primary purpose of our experiments is to determine if we can improve on the existing state-ofthe-art in gender classification (path-based gender counts).</S>
    <S sid="136" ssid="17">We test systems both trained purely on automatically-labeled data (Section 3.3), and those that leverage some gold-standard annotations in a semi-supervised setting (Section 3.4).</S>
    <S sid="137" ssid="18">Another purpose of our experiments is to investigate the relative value of our context-based features and type-based features.</S>
    <S sid="138" ssid="19">We accomplish these objectives by implementing and evaluating the following systems: A classifier with the four path-based gender counts as features (Section 2).</S>
  </SECTION>
  <SECTION title="2." number="5">
    <S sid="139" ssid="1">A method of back-off to help classify unseen nouns: For multi-token nouns (like Bob Johnson), we also include the four gender counts aggregated over all nouns sharing the first token (Bob .</S>
    <S sid="140" ssid="2">*), and the four gender counts over all nouns sharing the last token (.</S>
    <S sid="141" ssid="3">* Johnson).</S>
    <S sid="142" ssid="4">Semi-sup. combination of the PATHGENDER+ features and the TYPE-AUTO predictions.</S>
    <S sid="143" ssid="5">8.</S>
    <S sid="144" ssid="6">FULL-SEMI: Semi-sup. combination of the PATHGENDER+ features and the FULL-AUTO predictions.</S>
    <S sid="145" ssid="7">We evaluate using accuracy: the percentage of labeled nouns that are correctly assigned a gender class.</S>
    <S sid="146" ssid="8">As a baseline, note that always choosing neutral achieves 38.1% accuracy on our test data.</S>
  </SECTION>
  <SECTION title="5 Results and Discussion" number="6">
    <S sid="147" ssid="1">Table 1 provides our experimental results.</S>
    <S sid="148" ssid="2">The original gender counts already do an excellent job classifying the nouns; PATHGENDER achieves 91.0% accuracy by looking for exact noun matches.</S>
    <S sid="149" ssid="3">Our simple method of using back-off counts for the first and last token, PATHGENDER+, achieves 92.1%.</S>
    <S sid="150" ssid="4">While PATHGENDER+ uses gold standard data to determine optimum weights on the twelve counts, FULL-AUTO achieves 92.6% accuracy using no gold standard training data.</S>
    <S sid="151" ssid="5">This confirms that our algorithm, using no manually-labeled training data, can produce a competitive gender classifier.</S>
    <S sid="152" ssid="6">Both PATHGENDER and PATHGENDER+ do poorly on the noun types that have low counts in the gender database, achieving only 63% and 66% on nouns with less than ten counts.</S>
    <S sid="153" ssid="7">On these same nouns, FULL-AUTO achieves 88% performance, demonstrating the robustness of the learned classifier on the most difficult examples for previous approaches (FULL-SEMI achieves 94% on these nouns).</S>
    <S sid="154" ssid="8">If we break down the contribution of the two feature types in FULL-AUTO, we find that we achieve 89.1% accuracy by only using type features, while we achieve 79.1% with only context features.</S>
    <S sid="155" ssid="9">While not as high as the type-based accuracy, it is impressive that almost four out of five nouns can be classified correctly based purely on the document context, using no information about the noun itself.</S>
    <S sid="156" ssid="10">This is information that has not previously been systematically exploited in gender classification models.</S>
    <S sid="157" ssid="11">We examine the relationship between training data size and accuracy by plotting a (logarithmicscale) learning curve for FULL-AUTO (Figure 3).</S>
    <S sid="158" ssid="12">Although using four million noun groups originally seemed sufficient, performance appears to still be increasing.</S>
    <S sid="159" ssid="13">Since more training data can be generated automatically, it appears we have not yet reached the full power of the FULL-AUTO system.</S>
    <S sid="160" ssid="14">Of course, even with orders of magnitude more data, the system does not appear destined to reach the performance obtained through other means described below.</S>
    <S sid="161" ssid="15">We achieve even higher accuracy when the output of the -AUTO systems are combined with the original gender counts (the semi-supervised extension).</S>
    <S sid="162" ssid="16">The relative value of the context and type-based features is now reversed: using only context-based features (CONTEXT-SEMI) achieves 92.4%, while using only type-based features (TYPE-SEMI) achieves 91.3%.</S>
    <S sid="163" ssid="17">This is because much of the type information is already implicit in the PATHGENDER counts.</S>
    <S sid="164" ssid="18">The TYPE-AUTO predictions contribute little information, only fragmenting the data and leading to over-training and lower accuracy.</S>
    <S sid="165" ssid="19">On the other hand, the CONTEXT-AUTO predictions improve accuracy, as these scores provide orthogonal and hence helpful information for the semi-supervised classifier.</S>
    <S sid="166" ssid="20">Combining FULL-AUTO with our enhanced path gender counts, PATHGENDER+, results in the overall best performance, 95.5% for FULL-SEMI, significantly better than PATHGENDER+ alone.7 This is a 50% error reduction over the PATHGENDER system, strongly confirming the benefit of our semisupervised approach.</S>
    <S sid="167" ssid="21">To illustrate the importance of the unlabeled data, we created a system that uses all features, including the PATHGENDER+ counts, and trained this system using only the gold standard training data.</S>
    <S sid="168" ssid="22">This system was unable to leverage the extra features to improve performance; its accuracy was 92.0%, roughly equal to PATHGENDER+ alone.</S>
    <S sid="169" ssid="23">While SVMs work well with high-dimensional data, they simply cannot exploit features that do not occur in the training set.</S>
    <S sid="170" ssid="24">We can improve performance further by doing some simple coreference before assigning gender.</S>
    <S sid="171" ssid="25">Currently, we only group nouns with the same string, and then decide gender collectively for the group.</S>
    <S sid="172" ssid="26">There are a few cases, however, where an ambiguous surname, such as Willey, can only be classified correctly if we link the surname to an earlier instance of the full name, e.g.</S>
    <S sid="173" ssid="27">Katherine Willey.</S>
    <S sid="174" ssid="28">We thus added the following simple post-processing rule: If a noun is classified as masculine or feminine (like the ambiguous Willey), and it was observed earlier as the last part of a larger noun, then re-assign the gender to masculine or feminine if one of these is the most common path-gender count for the larger noun.</S>
    <S sid="175" ssid="29">We back off to counts for the first name (e.g.</S>
    <S sid="176" ssid="30">Kathleen .</S>
    <S sid="177" ssid="31">*) if the full name is unobserved.</S>
    <S sid="178" ssid="32">This enhancement improved the PATHGENDER and PATHGENDER+ systems to 93.3% and 94.3%, respectively, while raising the accuracy of our FULL-SEMI system to 96.7%.</S>
    <S sid="179" ssid="33">This demonstrates that the surname-matching post-processor is a simple but worthwhile extension to a gender predictor.8 The remaining errors represent a number of challenging cases: United States, group, and public labeled as plural but classified as neutral; spectator classified as neutral, etc.</S>
    <S sid="180" ssid="34">Some of these may yield to more sophisticated joint classification of coreference and gender, perhaps along the lines of work in named-entity classification (Bunescu and Mooney, 2004) or anaphoricity (Denis and Baldridge, 2007).</S>
    <S sid="181" ssid="35">While gender has been shown to be the key feature for statistical pronoun resolution (Ge et al., 1998; Bergsma and Lin, 2006), it remains to be seen whether the exceptional accuracy obtained here will translate into improvements in resolution performance.</S>
    <S sid="182" ssid="36">However, given the clear utility of gender in coreference, substantial error reductions in gender assignment will likely be a helpful contribution.</S>
  </SECTION>
  <SECTION title="6 Related Work" number="7">
    <S sid="183" ssid="1">Most coreference and pronoun resolution papers mention that they use gender information, but few explain how it is acquired.</S>
    <S sid="184" ssid="2">Kennedy and Boguraev (1996) use gender information produced by their enhanced part-of-speech tagger.</S>
    <S sid="185" ssid="3">Gender mistakes account for 35% of their system&#8217;s errors.</S>
    <S sid="186" ssid="4">Gender is less crucial in some genres, like computer manuals; most nouns are either neutral or plural and gender can be determined accurately based solely on morphological information (Lappin and Leass, 1994).</S>
    <S sid="187" ssid="5">A number of researchers (Evans and Or&#728;asan, 2000; Soon et al., 2001; Harabagiu et al., 2001) use WordNet classes to infer gender knowledge.</S>
    <S sid="188" ssid="6">Unfortunately, manually-constructed databases like WordNet suffer from both low coverage and rare senses.</S>
    <S sid="189" ssid="7">Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she.</S>
    <S sid="190" ssid="8">In addition to using WordNet classes, Soon et al. (2001) assign gender if the noun has a gendered designator (like Mr. or Mrs.) or if the first token is present on a list of common human first names.</S>
    <S sid="191" ssid="9">Note that we incorporate such contextual and categorical information (among many other information sources) automatically in our discriminative classifier, while they manually specify a few high-precision rules for particular gender cues.</S>
    <S sid="192" ssid="10">Ge et al. (1998) pioneered the statistical approach to gender determination.</S>
    <S sid="193" ssid="11">Like others, they consider gender and number separately, only learning statistical gender for the masculine, feminine, and neutral classes.</S>
    <S sid="194" ssid="12">While gender and number can be handled together for pronoun resolution, it might be useful to learn them separately for other applications.</S>
    <S sid="195" ssid="13">Other statistical approaches to English noun gender are discussed in Section 2.</S>
    <S sid="196" ssid="14">In languages with &#8216;grammatical&#8217; gender and plentiful gold standard data, gender can be tagged along with other word properties using standard supervised tagging techniques (Haji&#711;c and Hladk&#180;a, 1997).</S>
    <S sid="197" ssid="15">While our approach is the first to exploit a dual or orthogonal representation of English noun gender, a bootstrapping approach has been applied to determining grammatical gender in other languages by Cucerzan and Yarowsky (2003).</S>
    <S sid="198" ssid="16">In their work, the two orthogonal views are: 1) the context of the noun, and 2) the noun&#8217;s morphological properties.</S>
    <S sid="199" ssid="17">Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender.</S>
    <S sid="200" ssid="18">We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive.</S>
    <S sid="201" ssid="19">Bootstrapping is also used in general information extraction.</S>
    <S sid="202" ssid="20">Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns.</S>
    <S sid="203" ssid="21">Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition.</S>
    <S sid="204" ssid="22">Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1.</S>
    <S sid="205" ssid="23">While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically.</S>
    <S sid="206" ssid="24">Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="8">
    <S sid="207" ssid="1">We have shown how noun-pronoun co-occurrence counts can be used to automatically annotate the gender of millions of nouns in unlabeled text.</S>
    <S sid="208" ssid="2">Training from these examples produced a classifier that clearly exceeds the state-of-the-art in gender classification.</S>
    <S sid="209" ssid="3">We incorporated thousands of useful but previously unexplored indicators of noun gender as features in our classifier.</S>
    <S sid="210" ssid="4">By combining the predictions of this classifier with the original gender counts, we were able to produce a gender predictor that achieves 95.5% classification accuracy on 2596 test nouns, a 50% reduction in error over the current state-of-the-art.</S>
    <S sid="211" ssid="5">A further name-matching post-processor reduced error even further, resulting in 96.7% accuracy on the test data.</S>
    <S sid="212" ssid="6">Our final system is the broadest and most accurate gender model yet created, and should be of value to many pronoun and coreference resolution systems.</S>
  </SECTION>
</PAPER>
