<PAPER>
  <S sid="0">Is the End of Supervised Parsing in Sight?</S>
  <ABSTRACT>
    <S sid="1" ssid="1">How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?</S>
    <S sid="2" ssid="2">We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees.</S>
    <S sid="3" ssid="3">We train both on Penn&#8217;s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set.</S>
    <S sid="4" ssid="4">While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl.</S>
    <S sid="5" ssid="5">We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">A major challenge in natural language parsing is the unsupervised induction of syntactic structure.</S>
    <S sid="7" ssid="2">While most parsing methods are currently supervised or semi-supervised (McClosky et al. 2006; Henderson 2004; Steedman et al.</S>
    <S sid="8" ssid="3">2003), they depend on hand-annotated data which are difficult to come by and which exist only for a few languages.</S>
    <S sid="9" ssid="4">Unsupervised parsing methods are becoming increasingly important since they operate with raw, unlabeled data of which unlimited quantities are available.</S>
    <S sid="10" ssid="5">There has been a resurgence of interest in unsupervised parsing during the last few years.</S>
    <S sid="11" ssid="6">Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings &#8804; 10 words (WSJ10) using a constituentcontext model called CCM.</S>
    <S sid="12" ssid="7">Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10.</S>
    <S sid="13" ssid="8">While Klein and Manning&#8217;s approach may be described as an &#8220;all-substrings&#8221; approach to unsupervised parsing, an even richer model consists of an &#8220;all-subtrees&#8221; approach to unsupervised parsing, called U-DOP (Bod 2006).</S>
    <S sid="14" ssid="9">U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as &#8220;UML-DOP&#8221;).</S>
    <S sid="15" ssid="10">The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings.</S>
    <S sid="16" ssid="11">Discontiguous context is important not only for learning structural dependencies but also for learning a variety of noncontiguous constructions such as nearest ... to... or take ... by surprise.</S>
    <S sid="17" ssid="12">Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004).</S>
    <S sid="18" ssid="13">Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included.</S>
    <S sid="19" ssid="14">In this paper we will also test an alternative model for unsupervised all-subtrees parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Sima&#8217;an (2005), and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus.</S>
    <S sid="20" ssid="15">While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006).</S>
    <S sid="21" ssid="16">We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995), showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing.</S>
    <S sid="22" ssid="17">Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition.</S>
    <S sid="23" ssid="18">We show that U-DOP* outperforms the supervised DOP model if tested on the German-English Europarl corpus in a syntax-based MT system.</S>
    <S sid="24" ssid="19">In the following, we first explain the DOP* estimator and discuss how it can be extended to unsupervised parsing.</S>
    <S sid="25" ssid="20">In section 3, we discuss how a PCFG reduction for supervised DOP can be applied to packed parse forests.</S>
    <S sid="26" ssid="21">In section 4, we will go into an experimental evaluation of UDOP* on annotated corpora, while in section 5 we will evaluate U-DOP* on unlabeled corpora in an MT application.</S>
  </SECTION>
  <SECTION title="2 From DOP* to U-DOP*" number="2">
    <S sid="27" ssid="1">DOP* is a modification of the DOP model in Bod (1998) that results in a statistically consistent estimator and in an efficient training procedure (Zollmann and Sima&#8217;an 2005).</S>
    <S sid="28" ssid="2">DOP* uses the allsubtrees idea from DOP: given a treebank, take all subtrees, regardless of size, to form a stochastic tree-substitution grammar (STSG).</S>
    <S sid="29" ssid="3">Since a parse tree of a sentence may be generated by several (leftmost) derivations, the probability of a tree is the sum of the probabilities of the derivations producing that tree.</S>
    <S sid="30" ssid="4">The probability of a derivation is the product of the subtree probabilities.</S>
    <S sid="31" ssid="5">The original DOP model in Bod (1998) takes the occurrence frequencies of the subtrees in the trees normalized by their root frequencies as subtree parameters.</S>
    <S sid="32" ssid="6">While efficient algorithms have been developed for this DOP model by converting it into a PCFG reduction (Goodman 2003), DOP&#8217;s estimator was shown to be inconsistent by Johnson (2002).</S>
    <S sid="33" ssid="7">That is, even with unlimited training data, DOP's estimator is not guaranteed to converge to the correct distribution.</S>
    <S sid="34" ssid="8">Zollmann and Sima&#8217;an (2005) developed a statistically consistent estimator for DOP which is based on the assumption that maximizing the joint probability of the parses in a treebank can be approximated by maximizing the joint probability of their shortest derivations (i.e. the derivations consisting of the fewest subtrees).</S>
    <S sid="35" ssid="9">This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well.</S>
    <S sid="36" ssid="10">On the basis of this shortest-derivation assumption, Zollmann and Sima&#8217;an come up with a model that uses held-out estimation: the training corpus is randomly split into two parts proportional to a fixed ratio: an extraction corpus EC and a held-out corpus HC.</S>
    <S sid="37" ssid="11">Applied to DOP, held-out estimation would mean to extract fragments from the trees in EC and to assign their weights such that the likelihood of HC is maximized.</S>
    <S sid="38" ssid="12">If we combine their estimation method with Goodman&#8217;s reduction of DOP, Zollman and Sima&#8217;an&#8217;s procedure operates as follows: Zollmann and Sima&#8217;an show that the resulting estimator is consistent.</S>
    <S sid="39" ssid="13">But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003).</S>
    <S sid="40" ssid="14">Moreover, DOP*&#8217;s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in Bod (2006) is particularly time consuming and can only operate by randomly sampling trees.</S>
    <S sid="41" ssid="15">Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing.</S>
    <S sid="42" ssid="16">We will use the same allsubtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP*based estimator.</S>
    <S sid="43" ssid="17">The resulting model, which we will call U-DOP*, roughly operates as follows: We will use this U-DOP* model to investigate our main research question: how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?</S>
  </SECTION>
  <SECTION title="3 Converting shared parse forests into PCFG reductions" number="3">
    <S sid="44" ssid="1">The main computational problem is how to deal with the immense number of subtrees in U-DOP*.</S>
    <S sid="45" ssid="2">There exists already an efficient supervised algorithm that parses a sentence by means of all subtrees from a treebank.</S>
    <S sid="46" ssid="3">This algorithm was extensively described in Goodman (2003) and converts a DOP-based STSG into a compact PCFG reduction that generates eight rules for each node in the treebank.</S>
    <S sid="47" ssid="4">The reduction is based on the following idea: every node in every treebank tree is assigned a unique number which is called its address.</S>
    <S sid="48" ssid="5">The notation A@k denotes the node at address k where A is the nonterminal labeling that node.</S>
    <S sid="49" ssid="6">A new nonterminal is created for each node in the training data.</S>
    <S sid="50" ssid="7">This nonterminal is called Ak.</S>
    <S sid="51" ssid="8">Let aj represent the number of subtrees headed by the node A@j, and let a represent the number of subtrees headed by nodes with nonterminal A, that is a = Ej aj.</S>
    <S sid="52" ssid="9">Then there is a PCFG with the following property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation.</S>
    <S sid="53" ssid="10">For example, for a node (A@j (B@k, C@l)), the following eight PCFG rules in figure 1 are generated, where the number following a rule is its weight.</S>
    <S sid="54" ssid="11">By simple induction it can be shown that this construction produces PCFG derivations isomorphic to DOP derivations (Goodman 2003: 130-133).</S>
    <S sid="55" ssid="12">The PCFG reduction is linear in the number of nodes in the corpus.</S>
    <S sid="56" ssid="13">While Goodman&#8217;s reduction method was developed for supervised DOP where each training sentence is annotated with exactly one tree, the method can be generalized to a corpus where each sentence is annotated with all possible binary trees (labeled with the generalized category X), as long as we represent these trees by a shared parse forest.</S>
    <S sid="57" ssid="14">A shared parse forest can be obtained by adding pointers from each node in the chart (or tabular diagram) to the nodes that caused it to be placed in the chart.</S>
    <S sid="58" ssid="15">Such a forest can be represented in cubic space and time (see Billot and Lang 1989).</S>
    <S sid="59" ssid="16">Then, instead of assigning a unique address to each node in each tree, as done by the PCFG reduction for supervised DOP, we now assign a unique address to each node in each parse forest for each sentence.</S>
    <S sid="60" ssid="17">However, the same node may be part of more than one tree.</S>
    <S sid="61" ssid="18">A shared parse forest is an AND-OR graph where AND-nodes correspond to the usual parse tree nodes, while OR-nodes correspond to distinct subtrees occurring in the same context.</S>
    <S sid="62" ssid="19">The total number of nodes is cubic in sentence length n. This means that there are O(n3) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied.</S>
    <S sid="63" ssid="20">This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work.</S>
    <S sid="64" ssid="21">Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2, where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X.</S>
    <S sid="65" ssid="22">The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003).</S>
    <S sid="66" ssid="23">Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG.</S>
    <S sid="67" ssid="24">The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Sima&#8217;an 2005, theorem 5.2).</S>
    <S sid="68" ssid="25">This means that UDOP* results in an STSG which is much more succinct than previous DOP-based STSGs.</S>
    <S sid="69" ssid="26">Moreover, as in Bod (1998, 2000), we use an extension of Good-Turing to smooth the subtrees and to deal with &#8216;unknown&#8217; subtrees.</S>
    <S sid="70" ssid="27">Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006).</S>
    <S sid="71" ssid="28">This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters.</S>
    <S sid="72" ssid="29">Both U-DOP*&#8217;s and UML-DOP&#8217;s estimators are known to be statistically consistent.</S>
    <S sid="73" ssid="30">But while U-DOP*&#8217;s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UMLDOP involves iterative training of the parameters.</S>
    <S sid="74" ssid="31">Once we have extracted the STSG, we compute the most probable parse for new sentences by Viterbi n-best, summing up the probabilities of derivations resulting in the same tree (the exact computation of the most probable parse is NP hard &#8211; see Sima&#8217;an 1996).</S>
    <S sid="75" ssid="32">We have incorporated the technique by Huang and Chiang (2005) into our implementation which allows for efficient Viterbi n-best parsing.</S>
  </SECTION>
  <SECTION title="4 Evaluation on hand-annotated corpora" number="4">
    <S sid="76" ssid="1">To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn&#8217;s WSJ10 which contains 7422 sentences &lt; 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences &lt; 10 words after removing punctuation.</S>
    <S sid="77" ssid="2">As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings.</S>
    <S sid="78" ssid="3">The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g.</S>
    <S sid="79" ssid="4">Sch&#252;tze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5).</S>
    <S sid="80" ssid="5">Each corpus was divided into 10 training/test set splits of 90%/10% (n-fold testing), and each training set was randomly divided into two equal parts, that serve as EC and HC and vice versa.</S>
    <S sid="81" ssid="6">We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Klein and Manning (2002, 2004).</S>
    <S sid="82" ssid="7">The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR).</S>
    <S sid="83" ssid="8">All trees in the test set were binarized beforehand, in the same way as in Bod (2006).</S>
    <S sid="84" ssid="9">For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations.</S>
    <S sid="85" ssid="10">The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted.</S>
    <S sid="86" ssid="11">We used the technique in Bod (1998, 2000) to include &#8216;unknown&#8217; subtrees.</S>
    <S sid="87" ssid="12">Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM.</S>
    <S sid="88" ssid="13">It should be kept in mind that an exact comparison can only be made between U-DOP* and UMLDOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora.</S>
    <S sid="89" ssid="14">Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing.</S>
    <S sid="90" ssid="15">As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC.</S>
    <S sid="91" ssid="16">Table 2 shows the size of the grammar (number of rules or subtrees) of the two models for resp.</S>
    <S sid="92" ssid="17">Penn WSJ10, the entire Penn WSJ and the first 2 million sentences from the NANC (North American News Text) corpus which contains a total of approximately 24 million sentences from different news sources. for WSJ10 (7,7K sentences), WSJ (50K sentences) and the first 2,000K sentences from NANC.</S>
    <S sid="93" ssid="18">Note that while U-DOP* is about 2 orders of magnitudes smaller than UML-DOP for the WSJ10, it is almost 3 orders of magnitudes smaller for the first 2 million sentences of the NANC corpus.</S>
    <S sid="94" ssid="19">Thus even if U-DOP* does not give the highest f-score in table 1, it is more apt to be trained on larger data sets.</S>
    <S sid="95" ssid="20">In fact, a well-known advantage of unsupervised methods over supervised methods is the availability of almost unlimited amounts of text.</S>
    <S sid="96" ssid="21">Table 2 indicates that U-DOP*&#8217;s grammar is still of manageable size even for text corpora that are (almost) two orders of magnitude larger than Penn&#8217;s WSJ.</S>
    <S sid="97" ssid="22">The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn&#8217;s WSJ and has been previously used by McClosky et al. (2006) in improving a supervised parser by selftraining.</S>
    <S sid="98" ssid="23">In our experiments below we will start by mixing subsets from the NANC&#8217;s WSJ data with Penn&#8217;s WSJ data.</S>
    <S sid="99" ssid="24">Next, we will do the same with 2 million sentences from the LA Times in the NANC corpus, and finally we will mix all data together for inducing a U-DOP* model.</S>
    <S sid="100" ssid="25">From Penn&#8217;s WSJ, we only use sections 2 to 21 for training (just as in supervised parsing) and section 23 (&#8804;100 words) for testing, so as to compare our unsupervised results with some binarized supervised parsers.</S>
    <S sid="101" ssid="26">The NANC data was first split into sentences by means of a simple discriminitive model.</S>
    <S sid="102" ssid="27">It was next p-o-s tagged with the the TnT tagger (Brants 2000) which was trained on the Penn Treebank such that the same tag set was used.</S>
    <S sid="103" ssid="28">Next, we added subsets of increasing size from the NANC p-o-s strings to the 40,000 Penn WSJ p-o-s strings.</S>
    <S sid="104" ssid="29">Each time the resulting corpus was split into two halfs and the shortest derivations were computed for one half by using the PCFGreduction from the other half and vice versa.</S>
    <S sid="105" ssid="30">The resulting trees were used for extracting an STSG which in turn was used to parse section 23 of Penn&#8217;s WSJ.</S>
    <S sid="106" ssid="31">Table 3 shows the results.</S>
    <S sid="107" ssid="32">Table 3 indicates that there is a monotonous increase in f-score on the WSJ test set if NANC text is added to our training data in both cases, independent of whether the sentences come from the WSJ domain or the LA Times domain.</S>
    <S sid="108" ssid="33">Although the effect of adding LA Times data is weaker than adding WSJ data, it is noteworthy that the unsupervised induction of trees from the LA Times domain still improves the f-score even if the test data are from a different domain.</S>
    <S sid="109" ssid="34">We also investigated the effect of adding the LA Times data to the total mix of Penn&#8217;s WSJ and NANC&#8217;s WSJ.</S>
    <S sid="110" ssid="35">Table 4 shows the results of this experiment, where the baseline of 0 sentences thus starts with the 2,040k sentences from the combined Penn-NANC WSJ data.</S>
    <S sid="111" ssid="36">As seen in table 4, the f-score continues to increase even when adding LA Times data to the large combined set of Penn-NANC WSJ sentences.</S>
    <S sid="112" ssid="37">The highest f-score is obtained by adding 2,000k sentences, resulting in a total training set of 4,040k sentences.</S>
    <S sid="113" ssid="38">We believe that our result is quite promising for the future of unsupervised parsing.</S>
    <S sid="114" ssid="39">In putting our best f-score in table 4 into perspective, it should be kept in mind that the gold standard trees from Penn-WSJ section 23 were binarized.</S>
    <S sid="115" ssid="40">It is well known that such a binarization has a negative effect on the f-score.</S>
    <S sid="116" ssid="41">Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences &lt; 40 words, while the binarized version achieves only 64.6% f-score.</S>
    <S sid="117" ssid="42">To compare UDOP*&#8217;s results against some supervised parsers, we additionally evaluated a PCFG treebank grammar and the supervised DOP* parser using the same test set.</S>
    <S sid="118" ssid="43">For these supervised parsers, we employed the standard training set, i.e.</S>
    <S sid="119" ssid="44">Penn&#8217;s WSJ sections 2-21, but only by taking the p-o-s strings as we did for our unsupervised U-DOP* model.</S>
    <S sid="120" ssid="45">Table 5 shows the results of this comparison.</S>
    <S sid="121" ssid="46">As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set.</S>
    <S sid="122" ssid="47">While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction.</S>
    <S sid="123" ssid="48">Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*.</S>
  </SECTION>
  <SECTION title="5 Evaluation on unlabeled corpora in a practical application" number="5">
    <S sid="124" ssid="1">Our experiments so far have shown that despite the addition of large amounts of unlabeled training data, U-DOP* is still outperformed by the supervised DOP* model when tested on handannotated corpora like the Penn Treebank.</S>
    <S sid="125" ssid="2">Yet it is well known that any evaluation on hand-annotated corpora unreasonably favors supervised parsers.</S>
    <S sid="126" ssid="3">There is thus a quest for designing an evaluation scheme that is independent of annotations.</S>
    <S sid="127" ssid="4">One way to go would be to compare supervised and unsupervised parsers as a syntax-based language model in a practical application such as machine translation (MT) or speech recognition.</S>
    <S sid="128" ssid="5">In Bod (2007), we compared U-DOP* and DOP* in a syntax-based MT system known as Data-Oriented Translation or DOT (Poutsma 2000; Groves et al. 2004).</S>
    <S sid="129" ssid="6">The DOT model starts with a bilingual treebank where each tree pair constitutes an example translation and where translationally equivalent constituents are linked.</S>
    <S sid="130" ssid="7">Similar to DOP, the DOT model uses all linked subtree pairs from the bilingual treebank to form an STSG of linked subtrees, which are used to compute the most probable translation of a target sentence given a source sentence (see Hearne and Way 2006).</S>
    <S sid="131" ssid="8">What we did in Bod (2007) is to let both DOP* and U-DOP* compute the best trees directly for the word strings in the German-English Europarl corpus (Koehn 2005), which contains about 750,000 sentence pairs.</S>
    <S sid="132" ssid="9">Differently from UDOP*, DOP* needed to be trained on annotated data, for which we used respectively the Negra and the Penn treebank.</S>
    <S sid="133" ssid="10">Of course, it is well-known that a supervised parser&#8217;s f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% fscore if tested on the Brown corpus.</S>
    <S sid="134" ssid="11">Yet, this score is still considerably higher than the accuracy obtained by the unsupervised U-DOP model, which achieves 67.6% unlabeled f-score on Brown sentences.</S>
    <S sid="135" ssid="12">Our main question of interest is in how far this difference in accuracy on hand-annotated corpora carries over when tested in the context of a concrete application like MT.</S>
    <S sid="136" ssid="13">This is not a trivial question, since U-DOP* learns &#8216;constituents&#8217; for word sequences such as Ich m&#246;chte (&#8220;I would like to&#8221;) and There are (Bod 2007), which are usually hand-annotated as non-constituents.</S>
    <S sid="137" ssid="14">While UDOP* is punished for this &#8216;incorrect&#8217; prediction if evaluated on the Penn Treebank, it may be rewarded for this prediction if evaluated in the context of machine translation using the Bleu score (Papineni et al. 2002).</S>
    <S sid="138" ssid="15">Thus similar to Chiang (2005), U-DOP can discover non-syntactic phrases, or simply &#8220;phrases&#8221;, which are typically neglected by linguistically syntax-based MT systems.</S>
    <S sid="139" ssid="16">At the same time, U-DOP* can also learn discontiguous constituents that are neglected by phrase-based MT systems (Koehn et al. 2003).</S>
    <S sid="140" ssid="17">In our experiments, we used both U-DOP* and DOP* to predict the best trees for the GermanEnglish Europarl corpus.</S>
    <S sid="141" ssid="18">Next, we assigned links between each two nodes in the respective trees for each sentence pair.</S>
    <S sid="142" ssid="19">For a 2,000 sentence test set from a different part of the Europarl corpus we computed the most probable target sentence (using Viterbi n best).</S>
    <S sid="143" ssid="20">The Bleu score was used to measure translation accuracy, calculated by the NIST script with its default settings.</S>
    <S sid="144" ssid="21">As a baseline we compared our results with the publicly available phrase-based system Pharaoh (Koehn et al. 2003), using the default feature set.</S>
    <S sid="145" ssid="22">Table 6 shows for each system the Bleu score together with a description of the productive units.</S>
    <S sid="146" ssid="23">&#8216;U-DOT&#8217; refers to &#8216;Unsupervised DOT&#8217; based on U-DOP*, while DOT is based on DOP*.</S>
    <S sid="147" ssid="24">The table shows that the unsupervised U-DOT model outperforms the supervised DOT model with 0.059.</S>
    <S sid="148" ssid="25">Using Zhang&#8217;s significance tester (Zhang et al. 2004), it turns out that this difference is statistically significant (p &lt; 0.001).</S>
    <S sid="149" ssid="26">Also the difference between U-DOT and the baseline Pharaoh is statistically significant (p &lt; 0.008).</S>
    <S sid="150" ssid="27">Thus even if supervised parsers like DOP* outperform unsupervised parsers like U-DOP* on hand-parsed data with &gt;10%, the same supervised parser is outperformed by the unsupervised parser if tested in an MT application.</S>
    <S sid="151" ssid="28">Evidently, U-DOP&#8217;s capacity to capture both constituents and phrases pays off in a concrete application and shows the shortcomings of models that only allow for either constituents (such as linguistically syntax-based MT) or phrases (such as phrase-based MT).</S>
    <S sid="152" ssid="29">In Bod (2007) we also show that U-DOT obtains virtually the same Bleu score as Pharaoh after eliminating subtrees with discontiguous yields.</S>
  </SECTION>
  <SECTION title="6 Conclusion: future of supervised parsing" number="6">
    <S sid="153" ssid="1">In this paper we have shown that the accuracy of unsupervised parsing under U-DOP* continues to grow when enlarging the training set with additional data.</S>
    <S sid="154" ssid="2">However, except for the simple treebank PCFG, U-DOP* scores worse than supervised parsers if evaluated on hand-annotated data.</S>
    <S sid="155" ssid="3">At the same time U-DOP* significantly outperforms the supervised DOP* if evaluated in a practical application like MT.</S>
    <S sid="156" ssid="4">We argued that this can be explained by the fact that U-DOP learns both constituents and (non-syntactic) phrases while supervised parsers learn constituents only.</S>
    <S sid="157" ssid="5">What should we learn from these results?</S>
    <S sid="158" ssid="6">We believe that parsing, when separated from a task-based application, is mainly an academic exercise.</S>
    <S sid="159" ssid="7">If we only want to mimick a treebank or implement a linguistically motivated grammar, then supervised, grammar-based parsers are preferred to unsupervised parsers.</S>
    <S sid="160" ssid="8">But if we want to improve a practical application with a syntaxbased language model, then an unsupervised parser like U-DOP* might be superior.</S>
    <S sid="161" ssid="9">The problem with most supervised (and semi-supervised) parsers is their rigid notion of constituent which excludes &#8216;constituents&#8217; like the German Ich m&#246;chte or the French Il y a.</S>
    <S sid="162" ssid="10">Instead, it has become increasingly clear that the notion of constituent is a fluid which may sometimes be in agreement with traditional syntax, but which may just as well be in opposition to it.</S>
    <S sid="163" ssid="11">Any sequence of words can be a unit of combination, including noncontiguous word sequences like closest X to Y.</S>
    <S sid="164" ssid="12">A parser which does not allow for this fluidity may be of limited use as a language model.</S>
    <S sid="165" ssid="13">Since supervised parsers seem to stick to categorical notions of constituent, we believe that in the field of syntax-based language models the end of supervised parsing has come in sight.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="166" ssid="1">Thanks to Willem Zuidema and three anonymous reviewers for useful comments and suggestions on the future of supervised parsing.</S>
  </SECTION>
</PAPER>
