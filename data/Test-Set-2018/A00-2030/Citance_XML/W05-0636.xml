<PAPER>
  <S sid="0">Joint Parsing And Semantic Role Labeling</S>
  <ABSTRACT>
    <S sid="1" ssid="1">A striking feature of human syntactic prois that it is that is, it seems to take into account semantic information from the discourse context and world knowledge.</S>
    <S sid="2" ssid="2">In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses.</S>
    <S sid="3" ssid="3">To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.</S>
    <S sid="4" ssid="4">Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Although much effort has gone into developing statistical parsing models and they have improved steadily over the years, in many applications that use parse trees errors made by the parser are a major source of errors in the final output.</S>
    <S sid="6" ssid="2">A promising approach to this problem is to perform both parsing and the higher-level task in a single, joint probabilistic model.</S>
    <S sid="7" ssid="3">This not only allows uncertainty about the parser output to be carried upward, such as through an k-best list, but also allows information from higher-level processing to improve parsing.</S>
    <S sid="8" ssid="4">For example, Miller et al. (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks.</S>
    <S sid="9" ssid="5">In particular, one suspects that attachment decisions, which are both notoriously hard and extremely important for semantic analysis, could benefit greatly from input from higher-level semantic analysis.</S>
    <S sid="10" ssid="6">The recent interest in semantic role labeling provides an opportunity to explore how higher-level semantic information can inform syntactic parsing.</S>
    <S sid="11" ssid="7">In previous work, it has been shown that SRL systems that use full parse information perform better than those that use shallow parse information, but that machine-generated parses still perform much worse than human-corrected gold parses.</S>
    <S sid="12" ssid="8">The goal of this investigation is to narrow the gap between SRL results from gold parses and from automatic parses.</S>
    <S sid="13" ssid="9">We aim to do this by jointly performing parsing and semantic role labeling in a single probabilistic model.</S>
    <S sid="14" ssid="10">In both parsing and SRL, stateof-the-art systems are probabilistic; therefore, their predictions can be combined in a principled way by multiplying probabilities.</S>
    <S sid="15" ssid="11">In this paper, we rerank the k-best parse trees from a probabilistic parser using an SRL system.</S>
    <S sid="16" ssid="12">We compare two reranking approaches, one that linearly weights the log probabilities, and the other that learns a reranker over parse trees and SRL frames in the manner of Collins (2000).</S>
    <S sid="17" ssid="13">Currently, neither method performs better than simply selecting the top predicted parse tree.</S>
    <S sid="18" ssid="14">We discuss some of the reasons for this; one reason being that the ranking over parse trees induced by the semantic role labeling score is unreliable, because the model is trained locally.</S>
  </SECTION>
  <SECTION title="2 Base SRL System" number="2">
    <S sid="19" ssid="1">Our approach to joint parsing and SRL begins with a base SRL system, which uses a standard architecture from the literature.</S>
    <S sid="20" ssid="2">Our base SRL system is a cascade of maximum-entropy classifiers which select the semantic argument label for each constituent of a full parse tree.</S>
    <S sid="21" ssid="3">As in other systems, we use three stages: pruning, identification, and classification.</S>
    <S sid="22" ssid="4">First, in pruning, we use a deterministic preprocessing procedure introduced by Xue and Palmer (2004) to prune many constituents which are almost certainly not arguments.</S>
    <S sid="23" ssid="5">Second, in identification, a binary MaxEnt classifier is used to prune remaining constituents which are predicted to be null with high probability.</S>
    <S sid="24" ssid="6">Finally, in classification, a multiclass MaxEnt classifier is used to predict the argument type of the remaining constituents.</S>
    <S sid="25" ssid="7">This classifer also has the option to output NULL.</S>
    <S sid="26" ssid="8">It can happen that the returned semantic arguments overlap, because the local classifiers take no global constraints into account.</S>
    <S sid="27" ssid="9">This is undesirable, because no overlaps occur in the gold semantic annotations.</S>
    <S sid="28" ssid="10">We resolve overlaps using a simple recursive algorithm.</S>
    <S sid="29" ssid="11">For each parent node that overlaps with one of its descendents, we check which predicted probability is greater: that the parent has its locally-predicted argument label and all its descendants are null, or that the descendants have their optimal labeling, and the parent is null.</S>
    <S sid="30" ssid="12">This algorithm returns the non-overlapping assignment with globally highest confidence.</S>
    <S sid="31" ssid="13">Overlaps are uncommon, however; they occurred only 68 times on the 1346 sentences in the development set.</S>
    <S sid="32" ssid="14">We train the classifiers on PropBank sections 02&#8211; 21.</S>
    <S sid="33" ssid="15">If a true semantic argument fails to match any bracketing in the parse tree, then it is ignored.</S>
    <S sid="34" ssid="16">Both the identification and classification models are trained using gold parse trees.</S>
    <S sid="35" ssid="17">All of our features are standard features for this task that have been used in previous work, and are listed in Tables 1 and 2.</S>
    <S sid="36" ssid="18">We use the maximum-entropy implementation in the Mallet toolkit (McCallum, 2002) with a Gaussian prior on parameters.</S>
  </SECTION>
  <SECTION title="3 Reranking Parse Trees Using SRL Information" number="3">
    <S sid="37" ssid="1">Here we give the general framework for the reranking methods that we present in the next section.</S>
    <S sid="38" ssid="2">We write a joint probability model over semantic frames F and parse trees t given a sentence x as where p(t|x) is given by a standard probabilistic parsing model, and p(F|t, x) is given by the baseline SRL model described previously. opment set by the type of parse trees used.</S>
    <S sid="39" ssid="3">In this paper, we choose (F*, t*) to approximately maximize the probability p(F, t|x) using a reranking approach.</S>
    <S sid="40" ssid="4">To do the reranking, we generate a list of k-best parse trees for a sentence, and for each predicted tree, we predict the best frame using the base SRL model.</S>
    <S sid="41" ssid="5">This results in a list {(F&#65533;, tz)} of parse tree / SRL frame pairs, from which the reranker chooses.</S>
    <S sid="42" ssid="6">Thus, our different reranking methods vary only in which parse tree is selected; given a parse tree, the frame is always chosen using the best prediction from the base model.</S>
    <S sid="43" ssid="7">The k-best list of parses is generated using Dan Bikel&#8217;s (2004) implementation of Michael Collins&#8217; parsing model.</S>
    <S sid="44" ssid="8">The parser is trained on sections 2&#8211; 21 of the WSJ Treebank, which does not overlap with the development or test sets.</S>
    <S sid="45" ssid="9">The k-best list is generated in Bikel&#8217;s implementation by essentially turning off dynamic programming and doing very aggressive beam search.</S>
    <S sid="46" ssid="10">We gather a maximum of 500 best parses, but the limit is not usually reached using feasible beam widths.</S>
    <S sid="47" ssid="11">The mean number of parses per sentence is 176.</S>
  </SECTION>
  <SECTION title="4 Results and Discussion" number="4">
    <S sid="48" ssid="1">In this section we present results on several reranking methods for joint parsing and semantic role labeling.</S>
    <S sid="49" ssid="2">Table 3 compares F1 on the development set of our different reranking methods.</S>
    <S sid="50" ssid="3">The first four rows in Table 3 are baseline systems.</S>
    <S sid="51" ssid="4">We present baselines using gold trees (row 1 in Table 3) and predicted trees (row 2).</S>
    <S sid="52" ssid="5">As shown in previous work, gold trees perform much better than predicted trees.</S>
    <S sid="53" ssid="6">We also report two cheating baselines to explore the maximum possible performance of a reranking system.</S>
    <S sid="54" ssid="7">First, we report SRL performance of ceiling parse trees (row 3), i.e., if the parse tree from the k-best list is chosen to be closest to the gold tree.</S>
    <S sid="55" ssid="8">This is the best expected performance of a parse reranking approach that maximizes parse F1.</S>
    <S sid="56" ssid="9">Second, we report SRL performance where the parse tree is selected to maximize SRL F1, computing using the gold frame (row 4).</S>
    <S sid="57" ssid="10">There is a significant gap both between parse-F1-reranked trees and SRL-F1-reranked trees, which shows promise for joint reranking.</S>
    <S sid="58" ssid="11">However, the gap between SRLF1-reranked trees and gold parse trees indicates that reranking of parse lists cannot by itself completely close the gap in SRL performance between gold and predicted parse trees.</S>
    <S sid="59" ssid="12">Equation 1 suggests a straightforward method for reranking: simply pick the parse tree from the k-best list that maximizes p(F, t|x), in other words, add the log probabilities from the parser and the base SRL system.</S>
    <S sid="60" ssid="13">More generally, we consider weighting the individual probabilities as Such a weighted combination is often used in the speech community to combine acoustic and language models.</S>
    <S sid="61" ssid="14">This reranking method performs poorly, however.</S>
    <S sid="62" ssid="15">No choice of &#945; performs better than &#945; = 1, i.e., choosing the 1-best predicted parse tree.</S>
    <S sid="63" ssid="16">Indeed, the more weight given to the SRL score, the worse the combined system performs.</S>
    <S sid="64" ssid="17">The problem is that often a bad parse tree has many nodes which are obviously not constituents: thus p(F It, x) for such a bad tree is very high, and therefore not reliable.</S>
    <S sid="65" ssid="18">As more weight is given to the SRL score, the unlabeled recall drops, from 55% when &#945; = 0 to 71% when &#945; = 1.</S>
    <S sid="66" ssid="19">Most of the decrease in F1 is due to the drop in unlabeled recall.</S>
    <S sid="67" ssid="20">One potential solution to this problem is to add features of the entire frame, for example, to vote against predicted frames that are missing key arguments.</S>
    <S sid="68" ssid="21">But such features depend globally on the entire frame, and cannot be represented by local classifiers.</S>
    <S sid="69" ssid="22">One way to train these global features is to learn a linear classifier that selects a parse / frame pair from the ranked list, in the manner of Collins (2000).</S>
    <S sid="70" ssid="23">Reranking has previously been applied to semantic role labeling by Toutanova et al. (2005), from which we use several features.</S>
    <S sid="71" ssid="24">The difference between this paper and Toutanova et al. is that instead of reranking k-best SRL frames of a single parse tree, we are reranking 1-best SRL frames from the k-best parse trees.</S>
    <S sid="72" ssid="25">Because of the the computational expense of training on k-best parse tree lists for each of 30,000 sentences, we train the reranker only on sections 15&#8211; 18 of the Treebank (the same subset used in previous CoNLL competitions).</S>
    <S sid="73" ssid="26">We train the reranker using LogLoss, rather than the boosting loss used by Collins.</S>
    <S sid="74" ssid="27">We also restrict the reranker to consider only the top 25 parse trees.</S>
    <S sid="75" ssid="28">This globally-trained reranker uses all of the features from the local model, and the following global features: (a) sequence features, i.e., the linear sequence of argument labels in the sentence (e.g.</S>
    <S sid="76" ssid="29">A0_V_A1), (b) the log probability of the parse tree, (c) has-arg features, that is, for each argument type a binary feature indicating whether it appears in the frame, (d) the conjunction of the predicate and hasarg feature, and (e) the number of nodes in the tree classified as each argument type.</S>
    <S sid="77" ssid="30">The results of this system on the development set are given in Table 3 (row 6).</S>
    <S sid="78" ssid="31">Although this performs better than the score combination method, it is still no better than simply taking the 1-best parse tree.</S>
    <S sid="79" ssid="32">This may be due to the limited training set we used in the reranking model.</S>
    <S sid="80" ssid="33">A base SRL model trained only on sections 15&#8211;18 has 61.26 F1, so in comparison, reranking provides a modest improvement.</S>
    <S sid="81" ssid="34">This system is the one that we submitted as our official submission.</S>
    <S sid="82" ssid="35">The results on the test sets are given in Table 4.</S>
  </SECTION>
  <SECTION title="5 Summing over parse trees" number="5">
    <S sid="83" ssid="1">that maximizes the posterior probability: p(FIx) _ Et p(Fjt, x)p(tjx).</S>
    <S sid="84" ssid="2">That is, we should be summing over the parse trees instead of maximizing over them.</S>
    <S sid="85" ssid="3">The practical advantage of this approach is that even if one seemingly-good parse tree does not have a constituent for a semantic argument, many other parse trees in the k-best list might, and all are considered when computing F*.</S>
    <S sid="86" ssid="4">Also, no single parse tree need have constituents for all of F*; because it sums over all parse trees, it can mix and match constituents between different trees.</S>
    <S sid="87" ssid="5">The optimal frame F* can be computed by an O(N3) parsing algorithm if appropriate independence assumptions are made on p(Flx).</S>
    <S sid="88" ssid="6">This requires designing an SRL model that is independent of the bracketing derived from any particular parse tree.</S>
    <S sid="89" ssid="7">Initial experiments performed poorly because the marginal model p(Flx) was inadequate.</S>
    <S sid="90" ssid="8">Detailed exploration is left for future work.</S>
  </SECTION>
  <SECTION title="6 Conclusion and Related Work" number="6">
    <S sid="91" ssid="1">In this paper, we have considered several methods for reranking parse trees using information from semantic role labeling.</S>
    <S sid="92" ssid="2">So far, we have not been able to show improvement over selecting the 1-best parse tree.</S>
    <S sid="93" ssid="3">Gildea and Jurafsky (Gildea and Jurafsky, 2002) also report results on reranking parses using an SRL system, with negative results.</S>
    <S sid="94" ssid="4">In this paper, we confirm these results with a MaxEnt-trained SRL model, and we extend them to show that weighting the probabilities does not help either.</S>
    <S sid="95" ssid="5">Our results with Collins-style reranking are too preliminary to draw definite conclusions, but the potential improvement does not appear to be great.</S>
    <S sid="96" ssid="6">In future work, we will explore the max-sum approach, which has promise to avoid the pitfalls of max-max reranking approaches.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="97" ssid="1">This work was supported in part by the Center for Intelligent Information Retrieval, in part by National Science Foundation under NSF grants #IIS-0326249 ond #IIS-0427594, and in part by the Defense Advanced Research Projec ts Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract number NBCHD030010.</S>
    <S sid="98" ssid="2">Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.</S>
  </SECTION>
</PAPER>
