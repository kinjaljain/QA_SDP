<PAPER>
	<S sid="0">University_Of_Sheffield: Two Approaches to Semantic Text Similarity</S><ABSTRACT>
		<S sid="1" ssid="1">This paper describes the University of Sheffield?s submission to SemEval-2012 Task 6: Semantic Text Similarity.</S>
		<S sid="2" ssid="2">Two approaches were developed.</S>
		<S sid="3" ssid="3">The first is an unsupervised technique based on the widely used vector space model and information from WordNet.The second method relies on supervised ma chine learning and represents each sentence as a set of n-grams.</S>
		<S sid="4" ssid="4">This approach also makes use of information from WordNet.</S>
		<S sid="5" ssid="5">Resultsfrom the formal evaluation show that both approaches are useful for determining the simi larity in meaning between pairs of sentences with the best performance being obtained bythe supervised approach.</S>
		<S sid="6" ssid="6">Incorporating information from WordNet alo improves perfor mance for both approaches.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="7" ssid="7">This paper describes the University of Sheffield?s submission to SemEval-2012 Task 6: Semantic TextSimilarity (Agirre et al, 2012).</S>
			<S sid="8" ssid="8">The task is con cerned with determining the degree of semantic equivalence between a pair of sentences.</S>
			<S sid="9" ssid="9">Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (Seo and Croft, 2008; Bendersky and Croft, 2009), textual entailment (Szpektor et al, 2004; Zanzotto et al, 2009), paraphrase detection(Barzilay and Lee, 2003; Dolan et al, 2004), In formation Extraction/Question Answering (Lin andPantel, 2001; Stevenson and Greenwood, 2005), In formation Retrieval (Baeza-Yates and Ribeiro-Neto, 1999), short answer grading (Pulman and Sukkarieh,2005; Mohler and Mihalcea, 2009), recommenda tion (Tintarev and Masthoff, 2006) and evaluation (Papineni et al, 2002; Lin, 2004).</S>
			<S sid="10" ssid="10">Many of the previous approaches to measuring thesimilarity between texts have relied purely on lexi cal matching techniques, for example (Baeza-Yates and Ribeiro-Neto, 1999; Papineni et al, 2002; Lin, 2004).</S>
			<S sid="11" ssid="11">In these approaches the similarity of texts is computed as a function of the number of matchingtokens, or sequences of tokens, they contain.</S>
			<S sid="12" ssid="12">How ever, this approach fails to identify similarities when the same meaning is conveyed using synonymous terms or phrases (for example, ?The dog sat on the mat?</S>
			<S sid="13" ssid="13">and ?The hound sat on the mat?)</S>
			<S sid="14" ssid="14">or when the meanings of the texts are similar but not identical (for example, ?The cat sat on the mat?</S>
			<S sid="15" ssid="15">and ?A dog sat on the chair?).</S>
			<S sid="16" ssid="16">Significant amounts of previous work on textsimilarity have focussed on comparing the mean ings of texts longer than a single sentence, such asparagraphs or documents (Baeza-Yates and Ribeiro Neto, 1999; Seo and Croft, 2008; Bendersky and Croft, 2009).</S>
			<S sid="17" ssid="17">The size of these texts means that there is a reasonable amount of lexical items in each document that can be used to determine similarity and failing to identify connections between relatedterms may not be problematic.</S>
			<S sid="18" ssid="18">The situation is dif ferent for the problem of semantic text similarity where the texts are short (single sentences).</S>
			<S sid="19" ssid="19">There are fewer lexical items to match in this case, making it more important that connections between relatedterms are identified.</S>
			<S sid="20" ssid="20">One way in which this infor mation has been incorporated in NLP systems has 655been to make use of WordNet to provide informa tion about similarity between word meanings, andthis approach has been shown to be useful for com puting text similarity (Mihalcea and Corley, 2006; Mohler and Mihalcea, 2009).This paper describes two approaches to the se mantic text similarity problem that use WordNet (Miller et al, 1990) to provide information aboutrelations between word meanings.</S>
			<S sid="21" ssid="21">The two ap proaches are based on commonly used techniques for computing semantic similarity based on lexical matching.</S>
			<S sid="22" ssid="22">The first is unsupervised while the other requires annotated data to train a learning algorithm.Results of the SemEval evaluation show that the su pervised approach produces the best overall results and that using the information provided by WordNet leads to an improvement in performance.The remainder of this paper is organised as fol lows.</S>
			<S sid="23" ssid="23">The next section describes the two approaches for computing semantic similarity between pairs ofsentences that were developed.</S>
			<S sid="24" ssid="24">The system submitted for the task is described in Section 3 and its performance in the official evaluation in Section 4.</S>
			<S sid="25" ssid="25">Sec tion 5 contains the conclusions and suggestions for future work.</S>
	</SECTION>
	<SECTION title="Computing Semantic Text Similarity. " number="2">
			<S sid="26" ssid="1">Two approaches for computing semantic similar ity between sentences were developed.</S>
			<S sid="27" ssid="2">The first method, described in Section 2.1, is unsupervised.</S>
			<S sid="28" ssid="3">It uses an enhanced version of the vector space model by calculating the similarity between word senses,and then finding the distances between vectors con structed using these distances.</S>
			<S sid="29" ssid="4">The second method,described in Section 2.2, is based on supervised ma chine learning and compares sentences based on the overlap of the n-grams they contain.</S>
			<S sid="30" ssid="5">2.1 Vector Space Model.</S>
			<S sid="31" ssid="6">The first approach is inspired by the vector spacemodel (Salton et al, 1975) commonly used to compare texts in Information Retrieval and Natural Lan guage Processing (Baeza-Yates and Ribeiro-Neto, 1999; Manning and Schu?tze, 1999; Jurafsky and Martin, 2009).</S>
			<S sid="32" ssid="7">2.1.1 Creating vectors Each sentence is tokenised, stop words removed and the remaining words lemmatised using NLTK (Bird et al, 2009).</S>
			<S sid="33" ssid="8">(The WordPunctTokenizer and WordNetLemmatizer are applied.)</S>
			<S sid="34" ssid="9">Binary vectors are then created for each sentence.The similarity between sentences can be com puted by comparing these vectors using the cosine metric.</S>
			<S sid="35" ssid="10">However, this does not take account of words with similar meanings, such as ?dog?</S>
			<S sid="36" ssid="11">and ?hound?</S>
			<S sid="37" ssid="12">in the sentences ?The dog sat on the mat?</S>
			<S sid="38" ssid="13">and ?The hound sat on the mat?.</S>
			<S sid="39" ssid="14">To take accountof these similarities WordNet-based similarity mea sures are used (Patwardhan and Pedersen, 2006).</S>
			<S sid="40" ssid="15">Any terms that occur in only one of the sentences do not contribute to the similarity score since they will have a 0 value in the binary vector.</S>
			<S sid="41" ssid="16">Any wordswith a 0 value in one of the binary vectors are com pared with all of the words in the other sentence andthe similarity values computed.</S>
			<S sid="42" ssid="17">The highest similar ity value is selected and use to replace the 0 value in that vector, see Figure 1.</S>
			<S sid="43" ssid="18">(If the similarity score is below the set threshold of 0.5 then the similarityvalue is not used and in these cases the 0 value re mains unaltered.)</S>
			<S sid="44" ssid="19">This substitution of 0 values in the vectors ensures that similarity between words can betaken account of when computing sentence similar ity.</S>
			<S sid="45" ssid="20">Figure 1: Determining word similarity values for vectors Various techniques were explored for determiningthe similarity values between words.</S>
			<S sid="46" ssid="21">These are de scribed and evaluated in Section 2.1.3.</S>
			<S sid="47" ssid="22">2.1.2 Computing Sentence SimilarityThe similarity between two sentences is computed using the cosine metric.</S>
			<S sid="48" ssid="23">Since the cosine met ric is a distance measure, which returns a score of 0for identical vectors, its complement is used to pro 656 duce the similarity score.</S>
			<S sid="49" ssid="24">This score is multiplied by 5 in order to generate a score in the range required for the task.</S>
			<S sid="50" ssid="25">2.1.3 Computing Word Similarity The similarity values for the vectors are computedby first disambiguating each sentence and then ap plying a similarity measure.</S>
			<S sid="51" ssid="26">Various approaches for carrying out these tasks were explored.</S>
			<S sid="52" ssid="27">Word Sense Disambiguation Two simple and commonly used techniques for Word Sense Disambiguation were applied.</S>
			<S sid="53" ssid="28">Most Frequent Sense (MFS) simply selects the first sense in WordNet, i.e., the most common occurring sense for the word.</S>
			<S sid="54" ssid="29">This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004).</S>
			<S sid="55" ssid="30">Lesk (1986) chooses a synset by comparing itsdefinition against the sentence and select ing the one with the highest number of words in common.</S>
			<S sid="56" ssid="31">Similarity measures WordNet-based similarity measures have been found to perform well when used in combination with text similarity measures (Mihalcea and Corley, 2006) andseveral of these were compared.</S>
			<S sid="57" ssid="32">Implementa tions of these measures from the NLTK (Bird et al, 2009) were used.</S>
			<S sid="58" ssid="33">Path Distance uses the length of the shortest path between two senses to determine the similarity between them.</S>
			<S sid="59" ssid="34">Leacock and Chodorow (1998) expand upon the path distance similarity measure by scaling the path length by the maximum depth of the WordNet taxonomy.</S>
			<S sid="60" ssid="35">Resnik (1995) makes use of techniques fromInformation Theory.</S>
			<S sid="61" ssid="36">The measure of re latedness between two concepts is based on the Information Content of the Least Common Subsumer.Jiang and Conrath (1997) also uses the In formation Content of the two input synsets.</S>
			<S sid="62" ssid="37">Lin (1998) uses the same values as Jiang and Conrath (1997) but takes the ratio of the shared information content to that of the individual concepts.</S>
			<S sid="63" ssid="38">Results produced by the various combinations of word sense disambiguation strategy and similarity measures are shown in Table 1.</S>
			<S sid="64" ssid="39">This table shows the Pearson correlation of the system output with the gold standard over all of the SemEval training data.</S>
			<S sid="65" ssid="40">The row labelled ?Binary?</S>
			<S sid="66" ssid="41">shows the results using binary vectors which are not augmented with any similarity values.</S>
			<S sid="67" ssid="42">The remainder of the table shows the performance of each of the similarity measures when the senses are selected using the two word sense disambiguation algorithms.</S>
			<S sid="68" ssid="43">Metric MFS Lesk Binary 0.657 Path Distance 0.675 0.669 Leacock and Chodorow (1998) 0.087 0.138 Resnik (1995) 0.158 0.153 Jiang and Conrath (1997) 0.435 0.474 Lin (1998) 0.521 0.631Table 1: Performance of Vector Space Model us ing various disambiguation strategies and similarity measuresThe results in this table show that the only simi larity measure that leads to improvement above the baseline is the path measure.</S>
			<S sid="69" ssid="44">When this is applied there is a modest improvement over the baseline for each of the word sense disambiguation algorithms.</S>
			<S sid="70" ssid="45">However, all other similarity measures lead to a dropin performance.</S>
			<S sid="71" ssid="46">Overall there seems to be little dif ference between the performance of the two wordsense disambiguation algorithms.</S>
			<S sid="72" ssid="47">The best perfor mance is obtained using the paths distance and MFS disambiguation.</S>
			<S sid="73" ssid="48">Table 2 shows the results of the highest scoring method broken down by the individual corpora used for the evaluation.</S>
			<S sid="74" ssid="49">There is a wide range between the highest (0.726) and lowest (0.485) correlation scores with the best performance being obtained for theMSRvid corpus which contains short, simple sen tences.</S>
			<S sid="75" ssid="50">657 Metric Correlation MSRpar 0.591 MSRvid 0.726 SMTeuroparl 0.485Table 2: Correlation scores across individual cor pora using Path Distance and Most Frequent Sense.</S>
			<S sid="76" ssid="51">2.2 Supervised Machine Learning.</S>
			<S sid="77" ssid="52">For the second approach the sentences are represented as sets of n-grams of varying length, a com mon approach in text comparison applications which preserves some information about the structure of the document.</S>
			<S sid="78" ssid="53">However, like the standard vector space model (Section 2.1) this technique also fails to identify similarity between texts when an alternative choice of lexical item is used to express the same,or similar, meaning.</S>
			<S sid="79" ssid="54">To avoid this problem Word Net is used to generate sets of alternative n-grams.After the n-grams have been generated for each sen tence they are augmented with semantic alternatives created using WordNet (Section 2.2.1).</S>
			<S sid="80" ssid="55">The overlap scores between the n-grams from the two sentencesare used as features for a supervised learning algo rithm (Section 2.2.2).</S>
			<S sid="81" ssid="56">2.2.1 Generating n-grams Preprocessing is carried out using NLTK.</S>
			<S sid="82" ssid="57">Each sentence is tokenised, lemmatised and stop words removed.</S>
			<S sid="83" ssid="58">A set of n-grams are then extracted from each sentence.</S>
			<S sid="84" ssid="59">The set of n-grams for the sentence S is referred to as So.For every n-gram in So a list of alternative n grams is generated using WordNet.</S>
			<S sid="85" ssid="60">Each item inthe n-gram is considered in turn and checked to de termine whether it occurs in WordNet.</S>
			<S sid="86" ssid="61">If it does then a set of alternative lexical items is constructed by combining all terms that are found in all synsetscontaining that item as well as their immediate hy pernyms and hyponyms of the terms.</S>
			<S sid="87" ssid="62">An additionaln-gram is created for each item in this set of alternative lexical items by substituting each for the origi nal term.</S>
			<S sid="88" ssid="63">This set of expanded n-grams is referred to as Sa.</S>
			<S sid="89" ssid="64">2.2.2 Sentence ComparisonOverlap metrics to determine the similarity be tween the sets of n-grams are used to create features for the learning algorithm.</S>
			<S sid="90" ssid="65">For two sentences, S1 and S2, four sets of n-grams are compared: S1o,S2o, S1a and S2a (i.e., the n-grams extracted directly from sentences S1 and S2 as well as the mod ified versions created using WordNet).</S>
			<S sid="91" ssid="66">The n-grams that are generated using WordNet (Sa) are not as important as the original n-grams(So) for determining the similarity between sen tences and this is accounted for by generating three different scores reflecting the overlap between the two sets of n-grams for each sentence.</S>
			<S sid="92" ssid="67">These scores can be expressed using the following equations: |S1o ? S2o| ? |S1o| ? |S2o| (1) |(S1o ? S2a)?(S2o ? S1a)| ? |(S1o ? S2a)| ? |(S2o ? S1a)| (2) |S1a ? S2a| ? |S1a| ? |S2a| (3) Equation 1 is the cosine measure applied to the two sets of original n-grams, equation 2 comparesthe original n-grams in each sentence with the alternative n-grams in the other while equation 3 com pares the alternative n-grams with each other.Other features are used in addition to these sim ilarity scores: the mean length of S1 and S2, the difference between the lengths of S1 and S2 and the corpus label (indicating which part of the SemEval training data the sentence pair was drawn from).</S>
			<S sid="93" ssid="68">Wefound that these additional features substantially in crease the performance of our system, particularly the corpus label.</S>
	</SECTION>
	<SECTION title="University of Sheffield?s entry for Task 6. " number="3">
			<S sid="94" ssid="1">Our entry for this task consisted of three runs using the two approaches described in Section 2.</S>
			<S sid="95" ssid="2">Run 1: Vector Space Model (VS) The first runused the unsupervised vector space approach (Sec tion 2.1).</S>
			<S sid="96" ssid="3">Comparison of word sense disambiguation strategies and semantic similarity measures on thetraining data showed that the best results were ob tained using the Path Distance Measure combined 658with the Most Frequent Sense approach (see Ta bles 1 and 2) and these were used for the official run.</S>
			<S sid="97" ssid="4">Post evaluation analysis also showed that this strategy produced the best performance on the test data.</S>
			<S sid="98" ssid="5">Run 2: Machine Learning (NG) The second run used the supervised machine learning approach (Section 2.2.2).</S>
			<S sid="99" ssid="6">The various parameters used bythis approach were explored using 10-fold cross validation applied to the SemEval training data.</S>
			<S sid="100" ssid="7">Wevaried the lengths of the n-grams generated, exper imented with various pre-processing strategies and machine learning algorithms.</S>
			<S sid="101" ssid="8">The best performancewas obtained using short n-grams, unigrams and bigrams, and these were used for the official run.</S>
			<S sid="102" ssid="9">Including longer n-grams did not lead to any improvement in performance but created significant computational cost due to the number of alternative n grams that were created using WordNet.</S>
			<S sid="103" ssid="10">When the pre-processing strategies were compared it wasfound that the best performance was obtained by ap plying both stemming and stop word removal before creating n-grams and this approach was used in theofficial run.</S>
			<S sid="104" ssid="11">The Weka1 LinearRegression al gorithm was used for the official run and a singlemodel was created by training on all of the data pro vided for the task.</S>
			<S sid="105" ssid="12">Run 3: Hybrid (VS + NG) The third run is ahybrid combination of the two methods.</S>
			<S sid="106" ssid="13">The su pervised approach (NG) was used for the three data sets that had been made available in the training data (MSRpar, MSRvid and SMT-eur) while the vector space model (VS) was used for the other two datasets.</S>
			<S sid="107" ssid="14">This strategy was based on analysis of perfor mance of the two approaches on the training data.</S>
			<S sid="108" ssid="15">The NG approach was found to provide the best performance.</S>
			<S sid="109" ssid="16">However it was sensitive to the data set from which the training data was obtained from while VS, which does not require training data, is more robust.</S>
			<S sid="110" ssid="17">A diagram depicting the various components of the submitted entry is shown in Figure 2.</S>
	</SECTION>
	<SECTION title="Evaluation. " number="4">
			<S sid="111" ssid="1">The overall performance (ALLnrm) of NG, VG and the hybrid systems is significantly higher than the 1http://www.cs.waikato.ac.nz/ml/weka/ Figure 2: System Digram for entryofficial baseline (see Table 3).</S>
			<S sid="112" ssid="2">The table also in cludes separate results for each of the evaluation corpora (rows three to seven): the unsupervised VS model performance is significantly higher than the baseline (p-value of 0.06) over all corpus types, as is that of the hybrid model.</S>
			<S sid="113" ssid="3">However, the performance of the supervised NGmodel is below the baseline for the (unseen in training data) SMT-news corpus.</S>
			<S sid="114" ssid="4">Given a pair of sentences from an unknown source, the algorithm em ploys a model trained on all data combined (i.e., omits the corpus information), which may resemble the input (On-WN) or it may not (SMT-news).</S>
			<S sid="115" ssid="5">After stoplist removal, the average sentence length within MSRvid is 4.5, whereas it is 6.0 and 6.9 in MSRpar and SMT-eur respectively, and thusthe last two corpora are expected to form better train ing data for each other.</S>
			<S sid="116" ssid="6">The overall performance onthe MSRvid data is higher than for the other corpora, which may be due to the small number of adjectives and the simpler structure of the shorter sen tences within the corpus.</S>
			<S sid="117" ssid="7">The hybrid system, which selects the supervised system (NG)?s output when the test sentence pair is drawn from a corpus within the training data 659 Corpus Baseline Vector Space (VS) Machine Learning (NG) Hybrid (NG+VS) ALL .3110 .6054 .7241 .6485 ALLnrm .6732 .7946 .8169 .8238 MSRpar .4334 .5460 .5166 .5166 MSRvid .2996 .7241 .8187 .8187 SMT-eur .4542 .4858 .4859 .4859 On-WN .5864 .6676 .6390 .6676 SMT-news .3908 .4280 .2089 .4280 Table 3: Correlation scores from official SemEval results Rank (/89) Rank Ranknrm RankMean Baseline 87 85 70 Vector Space (VS) 48 44 29 Machine Learning (NG) 17 18 37 Hybrid 34 15 20 Table 4: Ranks from official SemEval results and selects the unsupervised system (VS)?s answerotherwise, outperforms both systems in combination.</S>
			<S sid="118" ssid="8">Contrary to expectations, the supervised sys tem did not always outperform VS on phrases basedon training data ? the performance of VS on MSR par, with its long and complex sentences, proved to be slightly higher than that of NG.</S>
			<S sid="119" ssid="9">However, the unsupervised system was clearly the correct choice when the source was unknown.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number="5">
			<S sid="120" ssid="1">Two approaches for computing semantic similaritybetween sentences were explored.</S>
			<S sid="121" ssid="2">The first, unsu pervised approach, uses a vector space model andcomputes similarity between sentences by comparing vectors while the second is supervised and rep resents the sentences as sets of n-grams.</S>
			<S sid="122" ssid="3">Both approaches used WordNet to provide information about similarity between lexical items.</S>
			<S sid="123" ssid="4">Results fromevaluation show that the supervised approach provides the best results on average but also that per formance of the unsupervised approach is better forsome data sets.</S>
			<S sid="124" ssid="5">The best overall results for the SemEval evaluation were obtained using a hybrid system that attempts to choose the most suitable ap proach for each data set.</S>
			<S sid="125" ssid="6">The results reported here show that the semantic text similarity task can be successfully approached using lexical overlap techniques augmented withlimited semantic information derived from Word Net.</S>
			<S sid="126" ssid="7">In future, we would like to explore whether performance can be improved by applying deeper analysis to provide information about the structure and semantics of the sentences being compared.</S>
			<S sid="127" ssid="8">For example, parsing the input sentences would provide more information about their structure than can be obtained by representing them as a bag of words orset of n-grams.</S>
			<S sid="128" ssid="9">We would also like to explore methods for improving performance of the n-gram over lap approach and making it more robust to different data sets.</S>
			<S sid="129" ssid="10">AcknowledgementsThis research has been supported by a Google Re search Award.</S>
	</SECTION>
</PAPER>
