<PAPER>
  <S sid="0">Translation-Based Projection for Multilingual Coreference Resolution</S>
  <ABSTRACT>
    <S sid="1" ssid="1">To build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques.</S>
    <S sid="2" ssid="2">However, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages.</S>
    <S sid="3" ssid="3">To alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution.</S>
    <S sid="4" ssid="4">Experimental results on two target languages demonstrate the promise of our approach.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Noun phrase (NP) coreference resolution is the task of determining which NPs (or mentions) refer to each real-world entity in a document.</S>
    <S sid="6" ssid="2">Recent years have witnessed a surge of interest in multilingual coreference resolution.</S>
    <S sid="7" ssid="3">For instance, the ACE 2004/2005 evaluations and SemEval-2010 Shared Task 1 have both involved coreference resolution in multiple languages.</S>
    <S sid="8" ssid="4">As evidenced by the participants in these evaluations, the most common approach to building a resolver for a new language is supervised, which involves training a resolver on coreference-annotated documents from the target language.</S>
    <S sid="9" ssid="5">Although supervised approaches work reasonably well, they present a challenge to deploying coreference technologies across a large number of natural languages.</S>
    <S sid="10" ssid="6">Specifically, for each new language of interest, one has to hire native speakers of the language to go through the labor-intensive, timeconsuming process of hand-annotating a potentially large number of documents with coreference annotation before a supervised resolver can be trained.</S>
    <S sid="11" ssid="7">One may argue that a potential solution to this corpus annotation bottleneck is to employ an unsupervised or heuristic approach to coreference resolution, especially in light of the fact that they have recently started to rival their supervised counterparts.</S>
    <S sid="12" ssid="8">However, by adopting these approaches, we are simply replacing the corpus annotation bottleneck by another, possibly equally serious, bottleneck, the knowledge acquisition bottleneck.</S>
    <S sid="13" ssid="9">Specifically, in these approaches, one has to employ knowledge of the target language to design coreference rules (e.g., Mitkov (1999), Poon and Domingos (2008), Raghunathan et al. (2010)) or sophisticated generative models (e.g., Haghighi and Klein (2007,2010), Ng (2008)) to combine the available knowledge sources.</S>
    <S sid="14" ssid="10">One could argue that designing coreference rules and generative models may not be as timeconsuming as annotating a large coreference corpus.</S>
    <S sid="15" ssid="11">This may be true for a well-studied language like English, where we can easily compose a rule that disallows coreference between two mentions if they disagree in number and gender, for instance.</S>
    <S sid="16" ssid="12">However, computing these features may not be as simple as we hope for a language like Chinese: the lack of morphology complicates the determination of number information, and the fact that most Chinese first names are used by both genders makes gender determination difficult.</S>
    <S sid="17" ssid="13">The difficulty in accurately computing features translates to difficulties in composing coreference rules: for example, the aforementioned rule involving gender and number agreement, as well as rules that implement traditional linguistic constraints on coreference, may no longer be accurate and desirable to have if the features involved cannot be accurately computed.</S>
    <S sid="18" ssid="14">Consequently, we believe that research in multilingual coreference resolution will continue to be dominated by supervised approaches.</S>
    <S sid="19" ssid="15">Given the high cost of annotating data with coreference chains, it is crucial to explore methods for obtaining annotated data in a cost-effective manner.</S>
    <S sid="20" ssid="16">Motivated in part by this observation, we examine one such method that has recently shown promise for a variety of NLP tasks, translation-based projection, which is composed of three steps.</S>
    <S sid="21" ssid="17">To coreference annotate a text in the target language, we (1) machine-translate it to a resource-rich language (henceforth the source language); (2) automatically produce the desired linguistic annotations (which in our case are coreference annotations) on the translated text using the linguistic tool developed for the source language (which in our case is a coreference resolver) ; and (3) project the annotations from the source language to the target language.</S>
    <S sid="22" ssid="18">Unlike supervised approaches, this projection approach does not require any coreference-annotated data from the target language.</S>
    <S sid="23" ssid="19">Equally importantly, unlike its unsupervised counterparts, this approach does not require that we have any linguistic knowledge of the target language.</S>
    <S sid="24" ssid="20">In fact, we have no knowledge of the target languages we employ in our evaluation.</S>
    <S sid="25" ssid="21">One of our goals is to examine the feasibility of building a coreference resolver for a language for which we have no coreference-annotated data and no linguistic knowledge of the language.</S>
    <S sid="26" ssid="22">Recall that we view projection as an approach for alleviating the corpus annotation bottleneck, not as a solution to the multilingual coreference resolution problem.</S>
    <S sid="27" ssid="23">In fact, though rarely emphasized in previous work on applying projection, we note that projection alone cannot be used to solve multilingual NLP problems, including coreference resolution.</S>
    <S sid="28" ssid="24">The reason is that every language has its own idiosyncrasies with respect to linguistic properties, and projection simply cannot produce annotations capturing those properties that are specific to the target language.</S>
    <S sid="29" ssid="25">Our goal in this paper is to explore the extent to which projection, which does not require that we have any knowledge of the target language, can push the limits of multilingual coreference resolution.</S>
    <S sid="30" ssid="26">If our results indicate that projection is a promising approach, then the automatic coreference annotations it produces can be used to augment the manual annotations that capture the properties specific to the target language, thus alleviating the corpus annotation bottleneck.</S>
  </SECTION>
  <SECTION title="2 Related Work on Projection" number="2">
    <S sid="31" ssid="1">The idea of projecting annotations from a resourcerich language to a resource-scarce language was originally proposed by Yarowsky and Ngai (2001) and subsequently developed by others (e.g., Resnik (2004), Hwa et al. (2005)).</S>
    <S sid="32" ssid="2">These projection algorithms assume as input a parallel corpus for the source language and the target language.</S>
    <S sid="33" ssid="3">Given the recent availability of machine translation (MT) services on the Web, researchers have focused more on translated-based projection rather than acquiring a parallel corpus themselves.</S>
    <S sid="34" ssid="4">MT-based projection has been applied to various NLP tasks, such as partof-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)).</S>
    <S sid="35" ssid="5">There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus.</S>
    <S sid="36" ssid="6">Specifically, Harabagiu and Maiorano (2000) create an EnglishRomanian corpus by manually translating the MUC6 corpus into Romanian and manually project the English annotations to Romanian.</S>
    <S sid="37" ssid="7">On the other hand, Postolache et al. (2006) apply a word alignment algorithm to project the hand-annotated English coreference chains and then manually fix the projection errors on the Romanian side.</S>
    <S sid="38" ssid="8">Hence, their goal is different from ours in at least two respects.</S>
    <S sid="39" ssid="9">First, while they employ significant knowledge of the target language to create a clean coreference corpus, we examine the quality of coreferenceannotated data created via an entirely automatic process, determining quality by the performance of the resolver trained on the data.</S>
    <S sid="40" ssid="10">Second, unlike ours, neither of these attempts is at the level of defining a technology for projection annotations that can potentially be deployed across a large number of languages without coreference-annotated data.</S>
  </SECTION>
  <SECTION title="3 Translation-Based Projection" number="3">
    <S sid="41" ssid="1">Recall that our MT-based projection approach to coreference resolution is composed of three steps.</S>
    <S sid="42" ssid="2">Given a text in the target language, we (1) machinetranslate the text to the source language; (2) automatically produce coreference annotations on the translated text using a coreference resolver developed for the source language; and (3) project the annotations from the source language to the target language.</S>
    <S sid="43" ssid="3">In this section, we employ our approach in three settings, which differ in terms of the extent to which linguistic taggers (e.g., chunkers and named entity (NE) recognizers) for the target language are available.</S>
    <S sid="44" ssid="4">The goal is to examine whether these linguistic taggers can be profitably exploited to improve the performance of the projection approach.</S>
    <S sid="45" ssid="5">Below we assume that English and French are our source and target languages, respectively.</S>
    <S sid="46" ssid="6">In this setting, we assume that we do not have access to any French tagger that we can exploit to improve projection.</S>
    <S sid="47" ssid="7">Hence, all we can do is to employ the three steps involved in the projection approach as described at the beginning of this section to create coreference-annotated data for French.</S>
    <S sid="48" ssid="8">Specifically, we translate a French text to an English text using GoogleTranslate1, and create coreference chains for the translated English text using Reconcile2 (Stoyanov et al., 2010).</S>
    <S sid="49" ssid="9">To project mentions from English to French, we first align the English and French words in each pair of parallel sentences, and then project the English mentions onto the French text using the alignment.</S>
    <S sid="50" ssid="10">However, since the alignment is noisy, the French words to which the words in the English mention are aligned may not form a contiguous text span.</S>
    <S sid="51" ssid="11">To fix this problem, we follow Yarowsky and Ngai (2001) and use the smallest text span that covers all the aligned French words to create the French mention.3 We process the English mentions in the text in a left-to-right manner, as processing the mentions sequentially enables us to ensure that an English mention is not mapped to a French text span that has already been mapped to by a previously-processed English mention.4 To align English and French words, we trained a word alignment model using GIZA++5 (Och and Ney, 2000) on a parallel corpus comprising the English-French section of Europarl6 (Koehn, 2005) as well as all the French texts (and their translated English counterparts) for which we want to automatically create coreference chains.</S>
    <S sid="52" ssid="12">Following common practice, we stemmed the parallel corpus using the Porter stemmer (Porter, 1980) in order to reduce data sparseness.</S>
    <S sid="53" ssid="13">However, even with stemming, we found that many English words were not aligned to any French words by the resulting alignment model.</S>
    <S sid="54" ssid="14">This would prevent many English mentions from being projected to the French side, potentially harming the recall of the French coreference annotations.</S>
    <S sid="55" ssid="15">To improve alignment coverage, we retrained the alignment model by supplying GIZA++ with an English-French bilingual dictionary that we assembled using three online dictionary databases: OmegaWiki, Wiktionary, and Universal Dictionary.</S>
    <S sid="56" ssid="16">Furthermore, if a word w appears in both the English side and the French side in a pair of parallel sentences, we assume that it has the same orthographic form in both languages and hence we augment the bilingual dictionary with the entry (w, w).</S>
    <S sid="57" ssid="17">Note that the use of a supervised resolver like Reconcile does not render our approach supervised, since we can replace it with any resolver, be it supervised, heuristic, or unsupervised.</S>
    <S sid="58" ssid="18">In other words, we treat the resolver built for the source language as a black box that can produce coreference annotations.</S>
    <S sid="59" ssid="19">Next, we consider a comparatively less resourcescarce setting where a French mention extractor is available for identifying mentions in a French text7, and describe how we can modify the projection approach to exploit this French mention extractor.</S>
    <S sid="60" ssid="20">Given a French text we want to coreferenceannotate, we first translate it to English using GoogleTranslate and align the French and English words using a French-to-English word alignment algorithm.</S>
    <S sid="61" ssid="21">Next, we identify the mentions in the French text using the given mention extractor, and project them onto the English text using the NP projection algorithm described in Setting 1.</S>
    <S sid="62" ssid="22">Finally, we run Reconcile on the resulting English mentions to generate coreference chains for the translated text, and project these chains back to the French text.</S>
    <S sid="63" ssid="23">As explained before, the performance of this method is sensitive to the accuracy of the NP projection algorithm in recovering the English mentions, which in turn depends on the accuracy of the word alignment algorithm.</S>
    <S sid="64" ssid="24">To make this method more robust to noisy word alignment, we make a modification to it.</S>
    <S sid="65" ssid="25">Rather than running Reconcile on the mentions produced by the NP projection algorithm, we use Reconcile to identify the mentions directly from the translated text.</S>
    <S sid="66" ssid="26">After that, we create a mapping between the English mentions produced by the NP projection algorithm and those produced by Reconcile using a small set of heuristics.</S>
    <S sid="67" ssid="27">Specifically, let MP be the set of mentions identified by the NP projection algorithm and MR be the set of mentions identified by Reconcile.</S>
    <S sid="68" ssid="28">For each mention mP in MP, we map it to a mention in MR that shares the same right boundary.</S>
    <S sid="69" ssid="29">If this fails, we map it to a mention that covers its entire text span.</S>
    <S sid="70" ssid="30">If this fails again, we map it to a mention that has a partial overlap with it.</S>
    <S sid="71" ssid="31">If this still fails, we assume that mP is not found by Reconcile and simply add mP to MR. As before, we process the mentions in MP in a left-to-right manner in order to ensure that no two mentions in MP are mapped to the same Reconcile mention.</S>
    <S sid="72" ssid="32">Finally, we discard all mentions in MR that are not mapped by any mention in MP, and present MR to Reconcile for coreference resolution.</S>
    <S sid="73" ssid="33">Since we now have a 1-to-1 mapping between the Reconcile mentions and the French mentions, projecting the coreference results back to French is trivial.</S>
    <S sid="74" ssid="34">It may not be immediately clear why the exploitation of the mention extractor in this setting may yield better coreference annotations than those produced in Setting 1.</S>
    <S sid="75" ssid="35">To see the reason, recall that one source of errors inherent in a projection approach is word alignment errors.</S>
    <S sid="76" ssid="36">In Setting 1, when we tried to project English mentions to the French text, word alignment errors would adversely affect the ability of the NP projection algorithm to correctly define the boundaries of the French mentions.</S>
    <S sid="77" ssid="37">Since coreference performance depends crucially on the ability to correctly identify mentions (Stoyanov et al., 2009), the presence of word alignment errors implies that the resulting French coreference annotations could score poorly even if the English coreference annotations produced by Reconcile were of high quality.</S>
    <S sid="78" ssid="38">In the current setting, on the other hand, we reduce the sensitivity of coreference performance to word alignment errors via the use of the French mention extractor to produce more accurate French mention boundaries.</S>
    <S sid="79" ssid="39">Finally, we consider a setting that is the least resource-scarce of the three.</S>
    <S sid="80" ssid="40">We assume that in addition to a French mention extractor, we have access to other French linguistic taggers (e.g., syntactic and semantic parsers) that will allow us to generate the linguistic features needed to train a French resolver on the projected coreference annotations.</S>
    <S sid="81" ssid="41">Specifically, assume that Test is a set of French texts we want to coreference-annotate, and Training is a set of French texts that is disjoint from Test but is drawn from the same domain as Test.8 To annotate the Test texts, we perform the following steps.</S>
    <S sid="82" ssid="42">First, we employ the French mention extractor in combination with the method described in Setting 2 to automatically coreference-annotate the Training texts.</S>
    <S sid="83" ssid="43">Next, motivated by Kobdani et al. (2011), we train a French coreference resolver on the automatically coreference-annotated training texts, using the features provided by the available linguistic taggers.</S>
    <S sid="84" ssid="44">Finally, we apply the resolver to generate coreference chains for each Test text.</S>
    <S sid="85" ssid="45">Two questions arise.</S>
    <S sid="86" ssid="46">First, is this method necessarily better than the one described in Setting 2?</S>
    <S sid="87" ssid="47">We hypothesize that the answer is affirmative: not only can this method exploit the knowledge about the target language provided by the additional linguistic taggers, but the resulting coreference resolver may allow us to generalize from the (noisily labeled) data and make this method more robust to the noise inherent in the projected coreference annotations than the previously-described methods.</S>
    <S sid="88" ssid="48">Second, is this method necessarily better than projection via a parallel corpus?</S>
    <S sid="89" ssid="49">Like the first question, this is also an empirical question.</S>
    <S sid="90" ssid="50">Nevertheless, one reason why this method is intuitively better is that it ensures that the training and test documents are drawn from the same domain.</S>
    <S sid="91" ssid="51">On the other hand, when projecting annotations via a parallel corpus, we may encounter a domain mismatch problem if the parallel corpus and the test documents come from different domains, and the coreference resolver may not work well if it is trained and tested on different domains.</S>
  </SECTION>
  <SECTION title="4 Coreference Resolution System" number="4">
    <S sid="92" ssid="1">To train the coreference resolver employed in Setting 3 in the previous section, we need to derive linguistic features from the documents in the target language.</S>
    <S sid="93" ssid="2">In our experiments, we employ the coreference data sets produced as part of the SemEval2010 shared task on Coreference Resolution in Multiple Languages.</S>
    <S sid="94" ssid="3">The shared task organizers have made publicly available six data sets that correspond to six European languages.</S>
    <S sid="95" ssid="4">Each data set comprises not only training and test documents that are coreference-annotated, but also a number of word-based linguistic features from which we derive mention-based linguistic features for training a resolver.</S>
    <S sid="96" ssid="5">In this section, we will describe how this resolver is trained and then applied to generate coreference chains for unseen documents.</S>
    <S sid="97" ssid="6">Training the coreference classifier.</S>
    <S sid="98" ssid="7">As our coreference model, we train a mention-pair model, which is a classifier that determines whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002)).9 Each instance i(mj, mk) corresponds to mj (a candidate antecedent) and mk (the mention to be resolved), and is represented by a set of 23 features shown in Table 1.</S>
    <S sid="99" ssid="8">As we can see, each feature is either relational, capturing the relation between mj and mk, or non-relational, capturing the linguistic property of mk.</S>
    <S sid="100" ssid="9">The possible values of a relational feature (except LEXICAL) are C (compatible), I (incompatible), and NA (the comparison 9Note that any supervised coreference model can be used, such as an entity-mention model (e.g., Luo et al. (2004), Yang et al.</S>
    <S sid="101" ssid="10">(2008)) or a ranking model (e.g., Denis and Baldridge (2008), Rahman and Ng (2009)). cannot be made due to missing data).</S>
    <S sid="102" ssid="11">For a nonrelational feature, we refer the reader to the data sets for the list of possible values.10 We follow Soon et al.&#8217;s (2001) method for creating training instances.</S>
    <S sid="103" ssid="12">Specifically, we create (1) a positive instance for each anaphoric mention mk and its closest antecedent mj; and (2) a negative instance for mk paired with each of the intervening mentions, mj+1, mj+2, ... , mk&#8722;1.</S>
    <S sid="104" ssid="13">The classification associated with a training instance is either positive or negative, depending on whether the two mentions are coreferent in the associated text.</S>
    <S sid="105" ssid="14">To train the classifier, we use SVMUght (Joachims, 1999).</S>
    <S sid="106" ssid="15">Applying the classifier to a test text.</S>
    <S sid="107" ssid="16">After training, the classifier is used to identify an antecedent for a mention in a test text.</S>
    <S sid="108" ssid="17">Specifically, each mention, mk, is compared to each preceding mention, mj, from right to left, and mj is selected as the antecedent of mk if the pair is classified as coreferent.</S>
    <S sid="109" ssid="18">The process terminates as soon as an antecedent is found for mk or the beginning of the text is reached.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="110" ssid="1">We evaluate our MT-based projection approach for each of the three settings described in Section 3.</S>
    <S sid="111" ssid="2">Data sets.</S>
    <S sid="112" ssid="3">We use the Spanish and Italian data sets from the SemEval-2010 shared task on Coreference Resolution in Multiple Languages.11 Each data set is composed of a training set and a test set.</S>
    <S sid="113" ssid="4">Statistics of these data sets are shown in Table 2.</S>
    <S sid="114" ssid="5">Scoring programs.</S>
    <S sid="115" ssid="6">To score the output of a coreference resolver, we employ four scoring programs, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), &#966;3-CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011), which were downloaded from the shared task website (see Footnote 10).</S>
    <S sid="116" ssid="7">Gold-standard versus regular settings.</S>
    <S sid="117" ssid="8">The format of each data set follows that of a typical CoNLL shared task data set.</S>
    <S sid="118" ssid="9">In other words, each row corresponds to a word in a document; moreover, all but the last column contain the linguistic features computed for the words, and the last column stores the coreference information.</S>
    <S sid="119" ssid="10">Some of the features were computed via automatic means, but some were extracted from human annotations.</S>
    <S sid="120" ssid="11">Given this distinction, the shared task organizers defined two evaluation settings: in the regular setting, only the columns that were computed automatically can be used to derive coreference features for classifier training, and results should be reported on system mentions; on the other hand, in the gold-standard setting, only the columns that were extracted from human annotations can be used to derive coreference features, and results should be reported on true mentions.</S>
    <S sid="121" ssid="12">We will present results corresponding to both settings.</S>
    <S sid="122" ssid="13">Note that these two settings should not be confused with the three settings described in Section 3.</S>
    <S sid="123" ssid="14">Mention extraction.</S>
    <S sid="124" ssid="15">Recall that Settings 2 and 3 both assume the availability of a mention extractor for extracting mentions in the target language.</S>
    <S sid="125" ssid="16">In our experiments, we extract mentions using two methods.</S>
    <S sid="126" ssid="17">First, we assume the availability of an oracle mention extractor that will enable us to extract true mentions (i.e., gold-standard mentions) directly from the test texts.</S>
    <S sid="127" ssid="18">Second, we employ simple heuristics to automatically extract system mentions.</S>
    <S sid="128" ssid="19">Since coreference performance is sensitive to the accuracy of mention extraction (Stoyanov et al., 2009), we experiment with several heuristic methods for extracting system mentions for both Spanish and Italian.</S>
    <S sid="129" ssid="20">According to our cross-validation experiments on the training data, the best heuristic for extracting Spanish mentions is different from that for extracting Italian mentions.</S>
    <S sid="130" ssid="21">Specifically, for Spanish, the best heuristic method operates as follows.</S>
    <S sid="131" ssid="22">First, it extracts all the syntactic heads (i.e., the word tokens whose gold dependency labels are SUBJ, PRED, or GMOD).</S>
    <S sid="132" ssid="23">Second, for each syntactic head, it identifies the smallest text span containing the head and all of its dependents, and creates a mention from this text span.</S>
    <S sid="133" ssid="24">For Italian, on the other hand, the best heuristic simply involves creating one mention for each gold NE.</S>
    <S sid="134" ssid="25">The reason why this simple heuristic works well is that most of the Italian mentions are NEs, owing to the fact that abstract NPs and pronouns are also annotated as NEs in the Italian data set.</S>
    <S sid="135" ssid="26">When evaluated on the test set, the heuristic-based mention extractor achieves F-scores of 80.2 (78.4 recall, 82.1 precision) for Spanish and 92.3 (85.9 recall, 99.6 precision) for Italian.</S>
    <S sid="136" ssid="27">Our supervised systems.</S>
    <S sid="137" ssid="28">While our MT-based projection approach is unsupervised (i.e., it does not rely on any coreference annotations from the target language), it would be informative to see the performance of the supervised resolvers, since their performance can be viewed as a crude upper bound on the performance of our unsupervised systems.</S>
    <S sid="138" ssid="29">Specifically, we train a mention-pair model on the training set using the 23 features shown in Table 1 and SVMUght as the underlying learning algorithm12, and apply the resulting model in combination with Soon et al.&#8217;s clustering algorithm (see Section 4) to generate coreference chains for the test texts.</S>
    <S sid="139" ssid="30">Results on the test sets, reported in terms of recall (R), precision (P), and F-score (F) computed by the four coreference scorers, are shown in the first two rows of Table 3 (Spanish) and Table 4 (Italian).</S>
    <S sid="140" ssid="31">For convenience, we summarize a system&#8217;s performance using a single number, which is shown in the last column (Average) and is obtained by taking a simple average of the F-scores of the four scorers.</S>
    <S sid="141" ssid="32">More specifically, row 1, which is marked with a &#8217;G&#8217;, and row 2, which is marked with a &#8217;R&#8217;, show the results obtained under the gold-standard setting and the regular setting, respectively.</S>
    <S sid="142" ssid="33">As we can see, under the gold-standard setting, the supervised resolver achieves an average F-score of 66.1 (Spanish) and 65.9 (Italian).</S>
    <S sid="143" ssid="34">Not surprisingly, under the regular setting, its average F-score drops statistically significantly13 to 54.6 (Spanish) and 63.4 (Italian).14 Best systems in the shared task.</S>
    <S sid="144" ssid="35">To determine whether the upper bounds established by our supervised systems are reasonable, we show the results of the best-performing resolvers participating in the shared task for both languages under the goldstandard and regular settings in rows 3 and 4 of Tables 3 and 4.</S>
    <S sid="145" ssid="36">Since none of the participating systems achieved the best score over all four scorers, we report the performance of the system that has the highest average F-score.</S>
    <S sid="146" ssid="37">According to the shared task website, TANL-1 (Attardi et al., 2010) achieved the best average F-score in the regular setting for Spanish, whereas SUCRE (Kobdani and Sch&#168;utze, 2010) outperformed others in the remaining settings.</S>
    <S sid="147" ssid="38">Comparing these best shared task results with our supervised results in rows 1 and 2, we see that our average F-score for Spanish/Gold is worse than its shared task counterpart by 0.7 points, but otherwise our system outperforms in other settings w.r.t. average F-score, specifically by 5.0 points for Spanish/Regular (due to a better MUC F-score), by 3.4&#8211; 4.7 points for Italian (due to better CEAF, B3, and BLANC scores).</S>
    <S sid="148" ssid="39">Overall, these results suggest that the scores achieved by our systems are at least as competitive as the best shared task scores.</S>
    <S sid="149" ssid="40">Next, we evaluate our projection algorithm.</S>
    <S sid="150" ssid="41">Setting 1.</S>
    <S sid="151" ssid="42">Results of our approach, when applied in Setting 1, are shown in row 5 of Tables 3 and 4.</S>
    <S sid="152" ssid="43">Given that it has to operate under the severe condition where no linguistic taggers are available for the target language, it is perhaps not surprising to see that its performance is significantly worse than that of its supervised counterparts.</S>
    <S sid="153" ssid="44">Setting 2.</S>
    <S sid="154" ssid="45">Recall that this setting is less resourcescarce than Setting 1 in that a mention extractor for the target language is available.</S>
    <S sid="155" ssid="46">Results of our algorithm, when operating under Setting 2 using true mentions and system mentions, are shown in rows 6 and 7 of Tables 3 and 4, respectively.</S>
    <S sid="156" ssid="47">In comparison to the results for Setting 1, we see that the F-scores obtained under Setting 2 increase significantly, regardless of (1) the scoring programs and (2) whether true mentions or system mentions are used.</S>
    <S sid="157" ssid="48">These results provide evidence for our earlier hypothesis that our projection algorithm can profitably exploit the linguistic knowledge about the target language that is available to it.</S>
    <S sid="158" ssid="49">In particular, the mention extractor helps make our approach less sensitive to word alignment and NP projection errors.</S>
    <S sid="159" ssid="50">In comparison to our supervised results in rows 1 and 2, our algorithm still lags behind by about 8&#8211;10 points in average F-score.</S>
    <S sid="160" ssid="51">However, this should not be surprising, since our algorithm is unsupervised.</S>
    <S sid="161" ssid="52">Looking closer at the results, we can see that the performance lag by our approach can be attributed to its lower recall: in general, the lag in MUC recall appears to be more acute than that in B3 and CEAF recall.</S>
    <S sid="162" ssid="53">Since MUC only scores non-singleton clusters wheres B3 and CEAF score both singleton and non-singleton clusters, these results suggest that our approach is better at identifying singleton clusters than recovering coreference links.</S>
    <S sid="163" ssid="54">Setting 3.</S>
    <S sid="164" ssid="55">Finally, we evaluate our approach in a setting where it has access to all the information available to our supervised resolvers, except for the gold-standard coreference annotations on the training sets.</S>
    <S sid="165" ssid="56">Specifically, our approach uses projected coreference annotations to train a resolver on the training texts, whereas the supervised resolvers do so using gold-standard annotations.</S>
    <S sid="166" ssid="57">Comparing Settings 2 and 3 with respect to true mentions (rows 6 and 8 of Tables 3 and 4), we see mixed results.</S>
    <S sid="167" ssid="58">According to MUC and BLANC, the resolvers in Setting 3 are significantly better than those in Setting 2 for both languages.</S>
    <S sid="168" ssid="59">According to B3, the resolvers in Setting 2 are significantly better than those in Setting 3 for both languages.</S>
    <S sid="169" ssid="60">According to CEAF, the Spanish resolvers in Setting 3 are significantly better than their counterparts in Setting 2, but the opposite is true for the Italian resolvers.</S>
    <S sid="170" ssid="61">To understand these somewhat contradictory performance trends, let us first note that the dramatic increase in the MUC F-score can be attributed to large gains in MUC recall.</S>
    <S sid="171" ssid="62">This suggests that the classifiers being trained in Setting 3 have enabled the discovery of additional coreference links.</S>
    <S sid="172" ssid="63">In other words, there are benefits to be obtained just by learning over noisy coreference annotations, a result that we believe is quite interesting.</S>
    <S sid="173" ssid="64">However, not all of these newly discovered coreference links are correct.</S>
    <S sid="174" ssid="65">The fact that some scoring programs (e.g., B) are more sensitive to spurious coreference links than the others (e.g., MUC) explains these mixed results.</S>
    <S sid="175" ssid="66">Nevertheless, according to average F-score, the resolvers in Setting 3 perform significantly better than those in Setting 2 for both languages: F-score increases by 5.3 points for Spanish and 0.7 points for Italian.</S>
    <S sid="176" ssid="67">Similar trends can be observed when comparing the two settings w.r.t. system mentions (rows 7 and 9 of Tables 3 and 4): F-score increases by 4.9 points for Spanish and 2.8 points for Italian.</S>
    <S sid="177" ssid="68">While our Setting 3 results still underperform the supervised results in rows 1 and 2, we can see that they achieve 93&#8211;94% of the average F-scores of the supervised Spanish resolvers and 89&#8211;91% of the average F-scores of the supervised Italian resolvers.</S>
    <S sid="178" ssid="69">Importantly, recall that our approach achieves this level of performance without relying on any goldstandard coreference annotations in Spanish and Italian, and we believe that these results demonstrate the promise of our MT-based projection approach.</S>
    <S sid="179" ssid="70">Since these results suggest that our approach cannot be successfully applied without MT services, a parallel corpus for learning a word alignment model, and a mention extractor for the target language, a natural question is: to what extent do these requirements limit the applicability of our approach?</S>
    <S sid="180" ssid="71">While it is the case that our approach cannot be applied to a truly resource-scarce language, it can be applied to the numerous Indian and East European languages for which the aforementioned requirements are satisfied but coreference-annotated data is not readily available.</S>
  </SECTION>
  <SECTION title="6 Conclusions and Future Work" number="6">
    <S sid="181" ssid="1">We explored the under-investigated yet challenging task of performing coreference resolution for a language for which we have no coreference-annotated data and no linguistic knowledge of the language.</S>
    <S sid="182" ssid="2">Our translation-based projection approach has the flexibility to exploit any available knowledge about the target language.</S>
    <S sid="183" ssid="3">In experiments with Spanish and Italian, we obtained promising results: our approach achieved around 90% of the performance of a supervised resolver when only a mention extractor for the target language was available.</S>
    <S sid="184" ssid="4">We believe that this approach has the potential to allow coreference technologies to be deployed across a larger number of languages than is currently possible, and that this is just the beginning of a new line of work.</S>
    <S sid="185" ssid="5">To gain additional insights into our approach, we plan to pursue several directions.</S>
    <S sid="186" ssid="6">First, we will isolate the impact of each factor that adversely affects its performance, including errors in projection, translation, and coreference resolution in the resource-rich language.</S>
    <S sid="187" ssid="7">Second, we will perform an empirical comparison of two approaches to projecting coreference annotations, our translation-based approach and Camargo de Souza and Orasan&#8217;s (2011) approach, where annotations are projected via a parallel corpus.</S>
    <S sid="188" ssid="8">Third, rather than translate from the target to the source language, we will examine whether it is better to translate all the coreference-annotated data available in the source language to the target language, and train a coreference model for the target language on the translated data.</S>
    <S sid="189" ssid="9">Fourth, since the success of our projection approach depends heavily on the accuracies of machine translation as well as coreference resolution in the source language, we will determine whether their accuracies can be improved via an ensemble approach, where we employ multiple MT engines and multiple coreference resolvers.</S>
    <S sid="190" ssid="10">Finally, we plan to employ our approach to alleviate the corpus-annotation bottleneck, specifically by using the annotated data it produces to augment the manual coreference annotations that capture the specific properties of the target language.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="191" ssid="1">We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft of the paper.</S>
    <S sid="192" ssid="2">This work was supported in part by NSF Grants IIS-0812261 and IIS-1147644.</S>
    <S sid="193" ssid="3">Any opinions, findings, or conclusions expressed in this paper are those of the authors and do not necessarily reflect the views or official policies of NSF.</S>
  </SECTION>
</PAPER>
