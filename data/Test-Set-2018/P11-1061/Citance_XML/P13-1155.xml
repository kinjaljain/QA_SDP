<PAPER>
  <S sid="0">Social Text Normalization using Contextual Graph Random Walks</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text.</S>
    <S sid="2" ssid="2">The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text.</S>
    <S sid="3" ssid="3">The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus.</S>
    <S sid="4" ssid="4">We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4).</S>
    <S sid="5" ssid="5">When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%.</S>
    <S sid="6" ssid="6">The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Social Media text is usually very noisy and contains a lot of typos, ad-hoc abbreviations, phonetic substitutions, customized abbreviations and slang language.</S>
    <S sid="8" ssid="2">The social media text is evolving with new entities, words and expressions.</S>
    <S sid="9" ssid="3">Natural language processing and understanding systems such as Machine Translation, Information Extraction and Text-to-Speech are usually trained and optimized for clean data; therefore such systems would face a challenging problem with social media text.</S>
    <S sid="10" ssid="4">Various social media genres developed distinct characteristics.</S>
    <S sid="11" ssid="5">For example, SMS developed a nature of shortening messages to avoid multiple keystrokes.</S>
    <S sid="12" ssid="6">On the other hand, Facebook and instant messaging developed another genre where more emotional expressions and different abbreviations are very common.</S>
    <S sid="13" ssid="7">Somewhere in between, Twitter&#8217;s statuses come with some brevity similar to SMS along with the social aspect of Facebook.</S>
    <S sid="14" ssid="8">On the same time, various social media genres share many characteristics and typo styles.</S>
    <S sid="15" ssid="9">For example, repeating letters or punctuation for emphasizing and emotional expression such as &#8221;&#8216;goooood morniiing&#8221;&#8217;.</S>
    <S sid="16" ssid="10">Using phonetic spelling in a generalized way or to reflect a local accent; such as &#8221;&#8216;wuz up bro&#8221;&#8217; (what is up brother).</S>
    <S sid="17" ssid="11">Eliminating vowels such as &#8221;&#8216;cm to c my luv&#8221;&#8217;.</S>
    <S sid="18" ssid="12">Substituting numbers for letters such as &#8221;&#8216;4get&#8221;&#8217; (forget) , &#8221;&#8216;2morrow&#8221;&#8217; (tomorrow), and &#8221;&#8216;b4&#8221;&#8217; (before).</S>
    <S sid="19" ssid="13">Substituting phonetically similar letters such as &#8221;&#8216;phone&#8221;&#8217; (fon).</S>
    <S sid="20" ssid="14">Slang abbreviations which usually abbreviates multi-word expression such as &#8221;&#8216;LMS&#8221;&#8217; (like my status) , &#8221;&#8216;idk&#8221;&#8217; (i do not know), &#8221;&#8216;rofl&#8221;&#8217; (rolling on floor laughing).</S>
    <S sid="21" ssid="15">While social media genres share many characteristics, they have significant differences as well.</S>
    <S sid="22" ssid="16">It is crucial to have a solution for text normalization that can adapt to such variations automatically.</S>
    <S sid="23" ssid="17">We propose a text normalization approach using an unsupervised method to induce normalization equivalences from noisy data which can adapt to any genre of social media.</S>
    <S sid="24" ssid="18">In this paper, we focus on providing a solution for social media text normalization as a preprocessing step for NLP applications.</S>
    <S sid="25" ssid="19">However, this is a challenging problem for several reasons.</S>
    <S sid="26" ssid="20">First, it is not straightforward to define the Out-ofVocabulary (OOV) words.</S>
    <S sid="27" ssid="21">Traditionally, an OOV word is defined as a word that does not exist in the vocabulary of a given system.</S>
    <S sid="28" ssid="22">However, this definition is not adequate for the social media text which has a very dynamic nature.</S>
    <S sid="29" ssid="23">Many words and named entities that do not exist in a given vocabulary should not be considered for normalization.</S>
    <S sid="30" ssid="24">Second, same OOV word may have many appropriate normalization depending on the context and on the domain.</S>
    <S sid="31" ssid="25">Third, text normalization as a preprocessing step should have very high precision; in other words, it should provide conservative and confident normalization and not overcorrect.</S>
    <S sid="32" ssid="26">Moreover, the text normalization should have high recall, as well, to have a good impact on the NLP applications.</S>
    <S sid="33" ssid="27">In this paper, we introduce a social media text normalization system which addresses the challenges mentioned above.</S>
    <S sid="34" ssid="28">The proposed system is based on constructing a lattice from possible normalization candidates and finding the best normalization sequence according to an n-gram language model using a Viterbi decoder.</S>
    <S sid="35" ssid="29">We propose an unsupervised approach to learn the normalization candidates from unlabeled text data.</S>
    <S sid="36" ssid="30">The proposed approach uses Random Walks on a contextual similarity graph constructed form n-gram sequences on large unlabeled text corpus.</S>
    <S sid="37" ssid="31">The proposed approach is very scalable, accurate and adaptive to any domain and language.</S>
    <S sid="38" ssid="32">We evaluate the approach on the normalization task as well as machine translation task.</S>
    <S sid="39" ssid="33">The rest of this paper is organized as follows: Section(2) discusses the related work, Section(3) introduces the text normalization system and the baseline candidate generators, Section(4) introduces the proposed graph-based lexicon induction approach, Section(5) discusses the experiments and output analysis, and finally Section(6) concludes and discusses future work.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="40" ssid="1">Early work handled the text normalization problem as a noisy channel model where the normalized words go through a noisy channel to produce the noisy text.</S>
    <S sid="41" ssid="2">(Brill and Moore, 2000) introduced an approach for modeling the spelling errors as a noisy channel model based on string to string edits.</S>
    <S sid="42" ssid="3">Using this model gives significant performance improvements compared to previously proposed models.</S>
    <S sid="43" ssid="4">(Toutanova and Moore, 2002) improved the string to string edits model by modeling pronunciation similarities between words.</S>
    <S sid="44" ssid="5">(Choudhury et al., 2007) introduced a supervised HMM channel model for text normalization which has been expanded by (Cook and Stevenson, 2009) to introduce unsupervised noisy channel model using probabilistic models for common abbreviation and various spelling errors types.</S>
    <S sid="45" ssid="6">Some researchers used Statistical Machine Translation approach for text normalization; formalizing the problem as a translation from the noisy forms to the normalized forms.</S>
    <S sid="46" ssid="7">(Aw et al., 2006) proposed an approach for normalizing Short Messaging Service (SMS) texts by translating it into normalized forms using Phrase-based SMT techniques on character level.</S>
    <S sid="47" ssid="8">The main drawback of these approaches is that the noisy channel model cannot accurately represent the errors types without contextual information.</S>
    <S sid="48" ssid="9">More recent approaches tried to handle the text normalization problem using normalization lexicons which map the noisy form of the word to a normalized form.</S>
    <S sid="49" ssid="10">For example, (Han et al., 2011) proposed an approach using a classifier to identify the noisy words candidate for normalization; then using some rules to generate lexical variants and a small normalization lexicon.</S>
    <S sid="50" ssid="11">(Gouws et al., 2011) proposed an approach using an impoverished normalization lexicon based on string and distributional similarity along with a dictionary lookup approach to detect noisy words.</S>
    <S sid="51" ssid="12">More recently, (Han et al., 2012) introduced a similar approach by generating a normalization lexicon based on distributional similarity and string similarity.</S>
    <S sid="52" ssid="13">This approach uses pairwise similarity where any two words that share the same context are considered as normalization equivalences.</S>
    <S sid="53" ssid="14">The pairwise approach has a number of limitations.</S>
    <S sid="54" ssid="15">First, it does not take into account the relative frequencies of the normalization equivalences that might share different contexts.</S>
    <S sid="55" ssid="16">Therefore, the selection of the normalization equivalences is performed on pairwise basis only and is not optimized over the whole data.</S>
    <S sid="56" ssid="17">Secondly, the normalization equivalences must appear in the exact same context to be considered as a normalization candidate.</S>
    <S sid="57" ssid="18">These limitations affect the accuracy and the coverage of the produced lexicon.</S>
    <S sid="58" ssid="19">Our approach also adopts a lexicon based approach for text normalization, we construct a lattice from possible normalization candidates and find the best normalization sequence according to an n-gram language model using a Viterbi decoder.</S>
    <S sid="59" ssid="20">The normalization lexicon is acquired from unlabeled data using random walks on a contextual similarity graph constructed form n-gram sequences on large unlabeled text corpus.</S>
    <S sid="60" ssid="21">Our approach has some similarities with (Han et al., 2012) since both approaches utilize a normalization lexicon acquired form unlabeled data using distributional and string similarities.</S>
    <S sid="61" ssid="22">However, our approach is significantly different since we acquire the lexicon using random walks on a contextual similarity graph which has a number of advantages over the pairwise similarity approach used in (Han et al., 2012).</S>
    <S sid="62" ssid="23">Namely, the acquired normalization equivalence are optimized globally over the whole data, the rare equivalences are not considered as good candidates unless there is a strong statistical evidence across the data, and finally the normalization equivalences may not share the same context.</S>
    <S sid="63" ssid="24">Those are clear advantages over the pairwise similarity approach and result in a lexicon with higher accuracy as well as wider coverage.</S>
    <S sid="64" ssid="25">Those advantages will be clearer when we describe the proposed approach in details and during evaluation and comparison to the pairwise approach.</S>
  </SECTION>
  <SECTION title="3 Text Normalization System" number="3">
    <S sid="65" ssid="1">In this paper, we handle text normalization as a lattice scoring approach, where the translation is performed from noisy text as the source side to the normalized text as the target side.</S>
    <S sid="66" ssid="2">Unlike conventional MT systems, the translation table is not learned from parallel aligned data; instead it is modeled by the graph-based approach of lexicon generation as we will describe later.</S>
    <S sid="67" ssid="3">We construct a lattice from possible normalization candidates and find the best normalization sequence according to an n-gram language model using a Viterbi decoder.</S>
    <S sid="68" ssid="4">In this paper, we restrict the normalization lexicon to one-to-one word mappings, we do not consider multi words mapping for the lexicon induction.</S>
    <S sid="69" ssid="5">To identify OOV candidates for normalization; we restrict proposing normalization candidates to the words that we have in our induced normalization lexicon only.</S>
    <S sid="70" ssid="6">This way, the system would provide more confident and conservative normalization.</S>
    <S sid="71" ssid="7">We move the problem of identifying OOV words to training time; at training time we use soft criteria to identify OOV words.</S>
    <S sid="72" ssid="8">We experimented with two normalization candidate generators as baseline systems.</S>
    <S sid="73" ssid="9">The first is a dictionary based spelling correction similar to Aspell1.</S>
    <S sid="74" ssid="10">In this experiment we used the spell checker lhttp://aspell.net/ to generate all possible candidates for OOV words and then applied the Viterbi decoder on the constructed lattice to score the best correction candidates using a language model.</S>
    <S sid="75" ssid="11">Our second candidates generator is based on a trie approximate string matching with K errors similar to the approach proposed in (Chang et al., 2010), where K errors can be caused by substitution, insertion, or deletion operations.</S>
    <S sid="76" ssid="12">In our implementation, we customized the errors operations to accommodate the nature of the social media text.</S>
    <S sid="77" ssid="13">Such as lengthening, letter substitution, letter-number substitution and phonetic substitution.</S>
    <S sid="78" ssid="14">This approach overcomes the main problem of the dictionary-based approach which is providing inappropriate normalization candidates to the errors styles in the social media text.</S>
    <S sid="79" ssid="15">As we will show in the experiments in Section(5), dictionary-based normalization methods proved to be inadequate for social media domain normalization for many reasons.</S>
    <S sid="80" ssid="16">First, they provide generic corrections which are inappropriate for social media text.</S>
    <S sid="81" ssid="17">Second, they usually provide corrections with the minimal edit distance for any word or named entity regardless of the nature of the words.</S>
    <S sid="82" ssid="18">Finally, the previous approaches do not take into account the dynamics of the social media text where new terms can be introduced on a daily basis.</S>
  </SECTION>
  <SECTION title="4 Normalization Lexicons using Graph-based Random Walks" number="4">
    <S sid="83" ssid="1">The main motivation of this approach is that normalization equivalences share similar context; which we call contextual similarity.</S>
    <S sid="84" ssid="2">For instance, assume 5-gram sequences of words, two words may be normalization equivalences if their n-gram context shares the same two words on the left and the same two words on the right.</S>
    <S sid="85" ssid="3">In other words, they are sharing a wild card pattern such as (word 1 word 2 * word 4 word 5).</S>
    <S sid="86" ssid="4">This contextual similarity can be represented as a bipartite graph with the first partite representing the words and the second partite representing the n-gram contexts that may be shared by words.</S>
    <S sid="87" ssid="5">A word node can be either normalized word or noisy word.</S>
    <S sid="88" ssid="6">Identifying if a word is normalized or noisy (candidate for normalization) is crucial since this decision limits the candidate noisy words to be normalized.</S>
    <S sid="89" ssid="7">We adopted a soft criteria for idennodes represent contexts, gray right nodes represent the noisy words and white right nodes represent the normalized words.</S>
    <S sid="90" ssid="8">Edge weight is the co-occurrence count of a word and its context. tifying noisy words.</S>
    <S sid="91" ssid="9">A vocabulary is constructed from a large clean corpus.</S>
    <S sid="92" ssid="10">Any word that does not appear in this vocabulary more than a predefined threshold (i.e.</S>
    <S sid="93" ssid="11">10 times) is considered as a candidate for normalization (noisy word).</S>
    <S sid="94" ssid="12">Figure(1) shows a sample of the bipartite graph G(W, C, E), where noisy words are shown as gray nodes.</S>
    <S sid="95" ssid="13">The bipartite graph, G(W, C, E), is composed of W which includes all nodes representing normalized words and noisy words, C which includes all nodes representing shared context, and finally E which represents the edges of the graph connecting word nodes and context nodes.</S>
    <S sid="96" ssid="14">The weight on the edge is simply the number of occurrences of a given word in a context.</S>
    <S sid="97" ssid="15">While constructing the graph, we identify if a node represents a noisy word (N) (called source node) or a normalized word (M) (called absorbing node).</S>
    <S sid="98" ssid="16">The bipartite graph is constructed using the procedure in Algorithm(4.1).</S>
    <S sid="99" ssid="17">Our proposed approach uses Markov Random Walks on the bipartite graph in Figure(1) as defined in (Norris, 1997).</S>
    <S sid="100" ssid="18">The main objective is to identify pairs of noisy and normalized words that can be considered as normalization equivalences.</S>
    <S sid="101" ssid="19">In principal, this is similar to using random walks for semi-supervised label propagation which has been introduced in (Szummer and Jaakkola, 2002) and then used in many other applications.</S>
    <S sid="102" ssid="20">For example, (Hughes and Ramage, 2007) used random walks on Wordnet graph to measure lexical semantic relatedness between words.</S>
    <S sid="103" ssid="21">(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages.</S>
    <S sid="104" ssid="22">(Minkov and Cohen, 2012) introduced a path constrained graph walk algorithm given a small number of labeled examples to assess nodes relatedness in the graph.</S>
    <S sid="105" ssid="23">In this paper, we apply the label propagation approach to the text normalization problem.</S>
    <S sid="106" ssid="24">Consider a random walk on the bipartite graph G(W, C, E) starting at a noisy word (source node) and ending at a normalized word (absorbing node).</S>
    <S sid="107" ssid="25">The walker starts from any source node Ni belonging to the noisy words then move to any other connected node Mj with probability Pij.</S>
    <S sid="108" ssid="26">The transition between each pair of nodes is defined by a transition probability Pij which represents the normalized probability of the cooccurrence counts of the word and the corresponding context.</S>
    <S sid="109" ssid="27">Though the counts are symmetric, the probability is not symmetric.</S>
    <S sid="110" ssid="28">This is due to the probability normalization which is done according to the nodes connectivity.</S>
    <S sid="111" ssid="29">Therefore, the transition probability between any two nodes i, j is defined as: For any non-connected pair of nodes, Pij =0.</S>
    <S sid="112" ssid="30">It is worth noting that due to the bipartite graph representation; any word node, either noisy (source) or normalized (absorbing), is only connected to context nodes and not directly connected to any other word node.</S>
    <S sid="113" ssid="31">The algorithm repeats independent random walks for K times where the walks traverse the graph randomly according to the transition probability distribution in Eqn(1); each walk starts from the source noisy node and ends at an absorbing normalized node, or consumes the maximum number of steps without hitting an absorbing node.</S>
    <S sid="114" ssid="32">For any random walk the number of steps taken to traverse between any two nodes is called the hitting time (Norris, 1997).</S>
    <S sid="115" ssid="33">Therefore, the hitting time between a noisy and a normalized pair of nodes (n, m) with a walk r is h,(n, m).</S>
    <S sid="116" ssid="34">We define the cost between the two nodes as the average hitting time H(n, m) of all walks that connect those two nodes: Consider the bipartite graph in Figure(1), assume a random walk starting at the source node representing the noisy word &#8221;tkin&#8221; then moves to the context node C1 then to the absorbing node representing the normalized word &#8221;taking&#8221;.</S>
    <S sid="117" ssid="35">This random walk will associate &#8221;tkin&#8221; with &#8221;taking&#8221; with a walk of two steps (hits).</S>
    <S sid="118" ssid="36">Another random walk that can connect the two words is [&#8221;tkin&#8221; -+ C4 -+ &#8221;takin&#8221; -+ C1 -+ &#8221;taking&#8221;], which has 4 steps (hits).</S>
    <S sid="119" ssid="37">In this case, the cost of this pair of nodes is the average number of hits connecting them which is 3.</S>
    <S sid="120" ssid="38">It is worth noting that the random walks are selected according to the transition probability in Eqn(1); therefore, the more probable paths will be picked more frequently.</S>
    <S sid="121" ssid="39">The same pair of nodes can be connected with many walks of various steps (hits), and the same noisy word can be connected to many other normalized words.</S>
    <S sid="122" ssid="40">We define the contextual similarity probability of a normalization equivalence pair n, m as L(n, m).</S>
    <S sid="123" ssid="41">Which is the relative frequency of the average hitting of those two nodes, H(n, m), and all other normalized nodes linked to that noisy word.</S>
    <S sid="124" ssid="42">Thus L(n, m), is calculated as: Furthermore, we add another similarity cost between a noisy word and a normalized word based on the lexical similarity cost, SimCost(n, m), which we will describe in the next section.</S>
    <S sid="125" ssid="43">The final cost associated with a pair is: We used uniform interpolation, both A1 and A2 equals 1.</S>
    <S sid="126" ssid="44">The final Lexicon is constructed using those entries and if needed we prune the list to take top N according to the cost above.</S>
    <S sid="127" ssid="45">The algorithm is outlined in 4.2.</S>
    <S sid="128" ssid="46">We use a similarity function proposed in (Contractor et al., 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999).</S>
    <S sid="129" ssid="47">This cost function is defined as the ratio of LCSR and Edit distance between two strings as follows: We have modified the Edit Distance calculation ED(n,m) to be more adequate for social media text.</S>
    <S sid="130" ssid="48">The edit distance is calculated between the consonant skeleton of the two words; by removing all vowels, we used Editex edit distance as proposed in (Zobel and Philip, 1996), repetition is reduced to a single letter before calculating the edit distance, and numbers in the middle of words are substituted by their equivalent letters.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="131" ssid="1">We collected large amount of social media data to generate the normalization lexicon using the random walk approach.</S>
    <S sid="132" ssid="2">The data consists of 73 million Twitter statuses.</S>
    <S sid="133" ssid="3">All tweets were collected from March/April 2012 using the Twitter Streaming APIs2.</S>
    <S sid="134" ssid="4">We augmented this data with 50 million sentences of clean data from English LDC Gigaword corpus 3.</S>
    <S sid="135" ssid="5">We combined both data, noisy and clean, together to induce the normalization dictionary from them.</S>
    <S sid="136" ssid="6">While the Gigaword clean data was used to train the language model to score the normalized lattice.</S>
    <S sid="137" ssid="7">We constructed a test set of 1000 sentences of social media which had been corrected by a native human annotator, the main guidelines were to normalize noisy words to its corresponding clean words in a consistent way according to the evidences in the context.</S>
    <S sid="138" ssid="8">We will refer to this test set as SM-Test.</S>
    <S sid="139" ssid="9">Furthermore, we developed a test set for evaluating the effect of the normalization system when used as a preprocessing step for Machine translation.</S>
    <S sid="140" ssid="10">The machine translation test set is composed of 500 sentences of social media English text translated to normalized Spanish text by a bi-lingual translator.</S>
    <S sid="141" ssid="11">We extracted 5-gram sequences from the combined noisy and clean data; then we limited the space of noisy 5-gram sequences to those which contain only one noisy word as the center word and all other words, representing the context, are not noisy.</S>
    <S sid="142" ssid="12">As we mentioned before, we identify whether the word is noisy or not by looking up a vocabulary list constructed from clean data.</S>
    <S sid="143" ssid="13">In these experiments, the vocabulary is constructed from the Language Model data (50M sentences of the English Gigaword corpus).</S>
    <S sid="144" ssid="14">Any word that appears less than 10 times in this vocabulary is considered noisy and candidate for normalization during the lexicon induction process.</S>
    <S sid="145" ssid="15">It is worth noting that our notion of noisy word does not mean it is an OOV that has to be corrected; instead it indicates that it is candidate for correction but may be opted not to be normalized if there is no confident normalization for it.</S>
    <S sid="146" ssid="16">This helps to maintain the approach as a high precision text normalization system which is highly preferable as an NLP preprocessing step.</S>
    <S sid="147" ssid="17">We constructed a lattice using normalization candidates and score the best Viterbi path with 5gram language model.</S>
    <S sid="148" ssid="18">We experimented with two candidate generators as baseline systems, namely the dictionary-based spelling correction and the trie approximate match with K errors; where K=3.</S>
    <S sid="149" ssid="19">For both candidate generators the cost function for a given candidate is calculated using the lexical similarity cost in Eqn(5).</S>
    <S sid="150" ssid="20">We compared those approaches with our newly proposed unsupervised normalization lexicon induction; for this case the cost for a candidate is the combined cost of the contextual similarity probability and the lexical similarity cost as defined in Eqn(4).</S>
    <S sid="151" ssid="21">We examine the effect of data size and the steps of the random walks on the accuracy and the coverage of the induced dictionary.</S>
    <S sid="152" ssid="22">We constructed the bipartite graph with the ngram sequences as described in Algorithm 4.1.</S>
    <S sid="153" ssid="23">Then the Random Walks Algorithm in 4.2 is applied with 100 walks.</S>
    <S sid="154" ssid="24">The total number of word nodes is about 7M nodes and the total number of context nodes is about 480M nodes.</S>
    <S sid="155" ssid="25">We used MapReduce framework to implement the proposed technique to handle such large graph.</S>
    <S sid="156" ssid="26">We experimented with the maximum number of random walk steps of 2, 4 and 6; and with different portions of the data as well.</S>
    <S sid="157" ssid="27">Finally, we pruned the lexicon to keep the top 5 candidates per noisy word.</S>
    <S sid="158" ssid="28">Table(1) shows the resulting lexicons from different experiments.</S>
    <S sid="159" ssid="29">As shown in Table(1), we experimented with different data sizes and steps of the random walks.</S>
    <S sid="160" ssid="30">The more data we have the larger the lexicon we get.</S>
    <S sid="161" ssid="31">Also larger steps increase the induced lexicon size.</S>
    <S sid="162" ssid="32">A random walk step size of 2 means that the noisy/normalized pair shares the same context; while a step size of 4 or more means that they may not share the same context.</S>
    <S sid="163" ssid="33">Next, we will examine the effect of lexicon size on the normalization task.</S>
    <S sid="164" ssid="34">We experimented different candidate generators and compared it to the unsupervised lexicon approach.</S>
    <S sid="165" ssid="35">Table(2) shows the precision and recall on a the SM-Test set.</S>
    <S sid="166" ssid="36">In Table(2), the first baseline is using a dictionary based spell checker; which gets low precision and very low recall.</S>
    <S sid="167" ssid="37">Similarly the trie approximate string match is doing a similar job with better recall though the precision is worst.</S>
    <S sid="168" ssid="38">Both of the baseline approaches are inadequate for social media text since both will try to correct any word that is similar to a word in the dictionary.</S>
    <S sid="169" ssid="39">The Trie approximate match is doing better job on the recall since the approximate match is based on phonetic and lexical similarities.</S>
    <S sid="170" ssid="40">On the other hand, the induced normalization lexicon approach is doing much better even with a small amount of data as we can see with system RW1 which uses Lex1 generated from 20M sentences and has 123K lexicon entry.</S>
    <S sid="171" ssid="41">Increasing the amount of training data does impact the performance positively especially the recall.</S>
    <S sid="172" ssid="42">On the other hand, increasing the number of steps has a good impact on the recall as well; but with a considerable impact on the precision.</S>
    <S sid="173" ssid="43">It is clear that increasing the amount of data and keeping the steps limit at &#8221;&#8216;4&#8221;&#8217; gives better precision and coverage as well.</S>
    <S sid="174" ssid="44">This is a preferred setting since the main objective of this approach is to have better precision to serve as a reliable preprocessing step for Machine Translation and other NLP applications.</S>
    <S sid="175" ssid="45">We present experimental results to compare our proposed approach with (Han et al., 2012) which used pairwise contextual similarity to induce a normalization lexicon of 40K entries, we will refer to this lexicon as HB-Dict.</S>
    <S sid="176" ssid="46">We compare the performance of HB-Dict and our induced dictionary (system RW3).</S>
    <S sid="177" ssid="47">We evaluate both system on SMTest test set and on (Han et al., 2012) test set of 548 sentences which we call here HB-Test.</S>
    <S sid="178" ssid="48">As shown in Table(3), RW3 system significantly outperforms HB-Dict system with the lexicon from (Han et al., 2012) on both test sets for both precision and recall.</S>
    <S sid="179" ssid="49">The contextual graph random walks approach helps in providing high precision lexicon since the sampling nature of the approach helps in filtering out unreliable normalization equivalences.</S>
    <S sid="180" ssid="50">The random walks will traverse more frequent paths; which would lead to more probable normalization equivalence.</S>
    <S sid="181" ssid="51">On the other hand, the proposed approach provides high recall as well which is hard to achieve with higher precision.</S>
    <S sid="182" ssid="52">Since the proposed approach deploys random walks to sample paths that can traverse many steps, this relaxes the constraints that the normalization equivalences have to share the same context.</S>
    <S sid="183" ssid="53">Instead a noisy word may share a context with another noisy word which in turn shares a context with a clean equivalent normalization word.</S>
    <S sid="184" ssid="54">Therefore, we end up with a lexicon that have much higher recall than the pairwise similarity approach since it explores equivalences beyond the pairwise relation.</S>
    <S sid="185" ssid="55">Moreover, the random walk sampling emphasis the more frequent paths and hence provides high precision lexicon.</S>
    <S sid="186" ssid="56">Table(4) shows some examples of the induced normalization equivalences, the first part shows good examples where vowels are restored and phonetic similar words are matched.</S>
    <S sid="187" ssid="57">Remarkably the correction &#8221;&#8216;viewablity&#8221;&#8217; to &#8221;&#8216;visibility&#8221;&#8217; is interesting since the system picked the more frequent form.</S>
    <S sid="188" ssid="58">Moreover, the lexicon contains some entries with foreign language words normalized to its English translation.</S>
    <S sid="189" ssid="59">On the other hand, the lexicon has some bad normalization such as &#8221;&#8216;unrecycled &#8221;&#8217; which should be normalized to &#8221;&#8216;non recycled&#8221;&#8217; but since the system is limited to one word correction it did not get it.</S>
    <S sid="190" ssid="60">Another interesting bad normalization is &#8221;&#8216;tutting&#8221;&#8217; which is new type of The final evaluation of the text normalization system is an extrinsic evaluation where we evaluate the effect of the text normalization task on a social media text translating from English to Spanish using a large scale translation system trained on general domain data.</S>
    <S sid="191" ssid="61">The system is trained on English-Spanish parallel data from WMT 2012 evaluation 4.</S>
    <S sid="192" ssid="62">The data consists of about 5M parallel sentences on news, europal and UN data.</S>
    <S sid="193" ssid="63">The system is a state of the art phrase based system similar to Moses (Hoang et al., 2007).</S>
    <S sid="194" ssid="64">We used The BLEU score (Papineni et al., 2002) to evaluate the translation accuracy with and without the normalization.</S>
    <S sid="195" ssid="65">Table(6) shows the translation evaluation with different systems.</S>
    <S sid="196" ssid="66">The translation with normalization was improved by about 6% from 29.02 to 30.87 using RW3 as a preprocessing step.</S>
  </SECTION>
  <SECTION title="6 Conclusion and Future Work" number="6">
    <S sid="197" ssid="1">We introduced a social media text normalization system that can be deployed as a preprocessor for MT and various NLP applications to handle social media text.</S>
    <S sid="198" ssid="2">The proposed approach is very scalable, adaptive to any domain and language.</S>
    <S sid="199" ssid="3">We show that the proposed unsupervised approach provides a normalization system with very high precision and a reasonable recall.</S>
    <S sid="200" ssid="4">We compared the system with conventional correction approaches and with recent previous work; and we showed that it highly outperforms other systems.</S>
    <S sid="201" ssid="5">Finally, we have used the system as a preprocessing step for a machine translation system which improved the translation quality by 6%.</S>
    <S sid="202" ssid="6">As an extension to this work, we will extend the approach to handle many-to-many normalization pairs; also we plan to apply the approach to more languages.</S>
    <S sid="203" ssid="7">Furthermore, the approach can be easily extended to handle similar problems such as accent restoration and generic entity normalization. dancing and should not be corrected to &#8221;&#8216;tweeting&#8221;&#8217;.</S>
    <S sid="204" ssid="8">Table 5 lists a number of examples and their normalization using both Baseline1 and RW3.</S>
    <S sid="205" ssid="9">At the first example, RW3 got the correct normalization as &#8221;interesting&#8221; which apparently is not the one with the shortest edit distance, though it is the most frequent candidate at the generated lexicon.</S>
    <S sid="206" ssid="10">The baseline system did not get it right; it got a wrong normalization with shorter edit distance.</S>
    <S sid="207" ssid="11">Example(2) shows the same effect by getting &#8221;cuz&#8221; normalized to &#8221;because&#8221;.</S>
    <S sid="208" ssid="12">At Example(3), both the baseline and RW3 did not get the correct normalization of &#8221;yur&#8221; to &#8221;you are&#8221; which is currently a limitation in our system since we only allow one-to-one word mapping in the generated lexicons not one-to-many or many-tomany.</S>
    <S sid="209" ssid="13">At Example(4), RW3 did not normalize &#8221;dure&#8221; to &#8221;sure&#8221; ; however the baseline normalized it by mistake to &#8221;dare&#8221;.</S>
    <S sid="210" ssid="14">This shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for NLP applications.</S>
    <S sid="211" ssid="15">This limitation can be marginalized by providing more data for generating the lexicon.</S>
    <S sid="212" ssid="16">Finally, Example 4 shows also that the system normalize &#8221;gr8&#8221; which is mainly due to having a flexible similarity cost during the normalization lexicon construction.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="213" ssid="1">We would like to thank Lee Schwartz and Will Lewis for their help in constructing the test sets and in the error analysis.</S>
    <S sid="214" ssid="2">We would also like to thank the anonymous reviewers for their helpful and constructive comments.</S>
  </SECTION>
</PAPER>
