<PAPER>
  <S sid="0">A Structured Distributional Semantic Model for Event Co-reference</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods.</S>
    <S sid="2" ssid="2">We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models.</S>
    <S sid="3" ssid="3">We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Distributional Semantic Models (DSM) are popular in computational semantics.</S>
    <S sid="5" ssid="2">DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood.</S>
    <S sid="6" ssid="3">They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Sch&#252;tze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007).</S>
    <S sid="7" ssid="4">A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words.</S>
    <S sid="8" ssid="5">Composing the distributions for &#8220;Lincoln&#8221;, &#8220;Booth&#8221;, and &#8220;killed&#8221; gives the same result regardless of whether the input is &#8220;Booth killed Lincoln&#8221; or &#8220;Lincoln killed Booth&#8221;.</S>
    <S sid="9" ssid="6">But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power.</S>
    <S sid="10" ssid="7">Thus, to remedy the bag-of-words failing, we extend the generic DSM model to several relation-specific distributions over syntactic neighborhoods.</S>
    <S sid="11" ssid="8">In other words, one can think of the Structured DSM (SDSM) representation of a word/phrase as several vectors defined over the same vocabulary, each vector representing the word&#8217;s selectional preferences for its various syntactic arguments.</S>
    <S sid="12" ssid="9">We argue that this representation not only captures individual word semantics more effectively than the standard DSM, but is also better able to express the semantics of compositional units.</S>
    <S sid="13" ssid="10">We prove this on the task of judging event coreference.</S>
    <S sid="14" ssid="11">Experimental results indicate that our model achieves greater predictive accuracy on the task than models that employ weaker forms of composition, as well as a baseline that relies on stateof-the-art window based word embeddings.</S>
    <S sid="15" ssid="12">This suggests that our formalism holds the potential of greater expressive power in problems that involve underlying semantic compositionality.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="16" ssid="1">Next, we relate and contrast our work to prior research in the fields of Distributional Vector Space Models, Semantic Compositionality and Event Co-reference Resolution.</S>
    <S sid="17" ssid="2">The underlying idea that &#8220;a word is characterized by the company it keeps&#8221; was expressed by Firth Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467&#8211;473, Sofia, Bulgaria, August 4-9 2013. c&#65533;2013 Association for Computational Linguistics (1957).</S>
    <S sid="18" ssid="3">Several works have defined approaches to modelling context-word distributions anchored on a target word, topic, or sentence position.</S>
    <S sid="19" ssid="4">Collectively these approaches are called Distributional Semantic Models (DSMs).</S>
    <S sid="20" ssid="5">While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators such as negation.</S>
    <S sid="21" ssid="6">In order to model a stronger form of semantics, there has been a recent surge in studies that phrase the problem of DSM compositionality as one of vector composition.</S>
    <S sid="22" ssid="7">These techniques derive the meaning of the combination of two words a and b by a single vector c = f(a, b).</S>
    <S sid="23" ssid="8">Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition.</S>
    <S sid="24" ssid="9">While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors.</S>
    <S sid="25" ssid="10">To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework.</S>
    <S sid="26" ssid="11">Dinu and Lapata (2010) and S&#233;aghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model.</S>
    <S sid="27" ssid="12">Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al.</S>
    <S sid="28" ssid="13">(2012) and Collobert et al. (2011) have been proposed.</S>
    <S sid="29" ssid="14">However, these models do not efficiently account for structure.</S>
    <S sid="30" ssid="15">Pantel and Lin (2000) and Erk and Pad&#243; (2008) attempt to include syntactic context in distributional models.</S>
    <S sid="31" ssid="16">A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors.</S>
    <S sid="32" ssid="17">But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems.</S>
    <S sid="33" ssid="18">Furthermore, we also include structure into our method of composition.</S>
    <S sid="34" ssid="19">Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010).</S>
    <S sid="35" ssid="20">However, these methods do not explicitly model composition.</S>
    <S sid="36" ssid="21">While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution.</S>
    <S sid="37" ssid="22">Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments.</S>
    <S sid="38" ssid="23">We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs.</S>
  </SECTION>
  <SECTION title="3 Structured Distributional Semantics" number="3">
    <S sid="39" ssid="1">In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)).</S>
    <S sid="40" ssid="2">The word distributions drawn from the context defined by a set of relations anchored on the target word (or phrase) form a set of vectors, namely a matrix for the target word.</S>
    <S sid="41" ssid="3">One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary.</S>
    <S sid="42" ssid="4">The cells store word counts (or PMI scores, or other measures of word association).</S>
    <S sid="43" ssid="5">Note that collapsing the rows of the matrix provides the standard dependency based distributional representation.</S>
    <S sid="44" ssid="6">To build a lexicon of SDSM matrices for a given vocabulary we first construct a proposition knowledge base (the PropStore) created by parsing the Simple English Wikipedia.</S>
    <S sid="45" ssid="7">Dependency arcs are stored as 3-tuples of the form (w1, r, w2), denoting an occurrence of words w1, word w2 related by r. We also store sentence indices for triples as this allows us to achieve an intuitive technique to achieve compositionality.</S>
    <S sid="46" ssid="8">In addition to the words&#8217; surface-forms, the PropStore also stores their POS tags, lemmas, and Wordnet supersenses.</S>
    <S sid="47" ssid="9">This helps to generalize our representation when surface-form distributions are sparse.</S>
    <S sid="48" ssid="10">The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word.</S>
    <S sid="49" ssid="11">In the example in Figure 1, the query (SST(W1) = verb.consumption, ?, dobj) i.e.</S>
    <S sid="50" ssid="12">&#8220;what is consumed&#8221; might return expectations [pasta:1, spaghetti:1, mice:1 ... ].</S>
    <S sid="51" ssid="13">Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas using Wordnet Fellbaum (1998).</S>
    <S sid="52" ssid="14">For representing intermediate multi-word phrases, we extend the above word-relation matrix symbolism in a bottom-up fashion using the PropStore.</S>
    <S sid="53" ssid="15">The combination hinges on the intuition that when lexical units combine to form a larger syntactically connected phrase, the representation of the phrase is given by its own distributional neighborhood within the embedded parse tree.</S>
    <S sid="54" ssid="16">The distributional neighborhood of the net phrase can be computed using the PropStore given syntactic relations anchored on its parts.</S>
    <S sid="55" ssid="17">For the example in Figure 1, we can compose SST(w1) = Noun.person and Lemma(W1) = eat appearing together with a nsubj relation to obtain expectations around &#8220;people eat&#8221; yielding [pasta:1, spaghetti:1 ... ] for the object relation, [room:2, restaurant:1 ...] for the location relation, etc.</S>
    <S sid="56" ssid="18">Larger phrasal queries can be built to answer queries like &#8220;What do people in China eat with?&#8221;, &#8220;What do cows do?&#8221;, etc.</S>
    <S sid="57" ssid="19">All of this helps us to account for both relation r and knowledge K obtained from the PropStore within the compositional framework c = f(a, b, r, K).</S>
    <S sid="58" ssid="20">The general outline to obtain a composition of two words is given in Algorithm 1, which returns the distributional expectation around the composed unit.</S>
    <S sid="59" ssid="21">Note that the entire algorithm can conveniently be written in the form of database queries to our PropStore.</S>
    <S sid="60" ssid="22">For the example &#8220;noun.person nsubj eat&#8221;, steps (1) and (2) involve querying the PropStore for the individual tokens, noun.person and eat.</S>
    <S sid="61" ssid="23">Let the resulting matrices be M1 and M2, respectively.</S>
    <S sid="62" ssid="24">In step (3), SentIDs (sentences where the two words appear with the specified relation) are obtained by taking the intersection between the nsubj component vectors of the two matrices M1 and M2.</S>
    <S sid="63" ssid="25">In step (4), the entries of the original matrices M1 and M2 are intersected with this list of common SentIDs.</S>
    <S sid="64" ssid="26">Finally, the resulting matrix for the composition of the two words is simply the union of all the relationwise intersected sentence IDs.</S>
    <S sid="65" ssid="27">Intuitively, through this procedure, we have computed the expectation around the words w1 and w2 when they are connected by the relation &#8220;r&#8221;.</S>
    <S sid="66" ssid="28">Similar to the two-word composition process, given a parse subtree T of a phrase, we obtain its matrix representation of empirical counts over word-relation contexts (described in Algorithm 2).</S>
    <S sid="67" ssid="29">Let the E = {e1 ... en} be the set of edges in T, ei = (wi1, ri, wi2)bi = 1... n. The phrase representations becomes sparser as phrase length increases.</S>
    <S sid="68" ssid="30">For this study, we restrict phrasal query length to a maximum of three words.</S>
    <S sid="69" ssid="31">Given the SDSM formulation and assuming no sparsity constraints, it is possible to calculate SDSM matrices for composed concepts.</S>
    <S sid="70" ssid="32">However, are these correct?</S>
    <S sid="71" ssid="33">Intuitively, if they truly capture semantics, the two SDSM matrix representations for &#8220;Booth assassinated Lincoln&#8221; and &#8220;Booth shot Lincoln with a gun&amp;quot; should be (almost) the same.</S>
    <S sid="72" ssid="34">To test this hypothesis we turn to the task of predicting whether two event mentions are coreferent or not, even if their surface forms differ.</S>
    <S sid="73" ssid="35">It may be noted that this task is different from the task of full event coreference and hence is not directly comparable to previous experimental results in the literature.</S>
    <S sid="74" ssid="36">Two mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same.</S>
    <S sid="75" ssid="37">Given the non-compositional nature of determining equality of locations and times, we represent each event mention by a triple E = (e, a, p) for the event, agent, and patient.</S>
    <S sid="76" ssid="38">In our corpus, most event mentions are verbs.</S>
    <S sid="77" ssid="39">However, when nominalized events are encountered, we replace them by their verbal forms.</S>
    <S sid="78" ssid="40">We use SRL Collobert et al. (2011) to determine the agent and patient arguments of an event mention.</S>
    <S sid="79" ssid="41">When SRL fails to determine either role, its empirical substitutes are obtained by querying the PropStore for the most likely word expectations for the role.</S>
    <S sid="80" ssid="42">It may be noted that the SDSM representation relies on syntactic dependancy relations.</S>
    <S sid="81" ssid="43">Hence, to bridge the gap between these relations and the composition of semantic role participants of event mentions we empirically determine those syntactic relations which most strongly co-occur with the semantic relations connecting events, agents and patients.</S>
    <S sid="82" ssid="44">The triple (e, a, p) is thus the composition of the triples (a, relationsetagent, e) and (p, relationsetpatient, e), and hence a complex object.</S>
    <S sid="83" ssid="45">To determine equality of this complex composed representation we generate three levels of progressively simplified event constituents for comparison: Level 1: Full Composition: To judge coreference between events E1 and E2, we compute pairwise similarities Sim(M1full,M2full), Sim(M1part:EA, M2part:EA), etc., for each level of the composed triple representation.</S>
    <S sid="84" ssid="46">Furthermore, we vary the computation of similarity by considering different levels of granularity (lemma, SST), various choices of distance metric (Euclidean, Cityblock, Cosine), and score normalization techniques (Row-wise, Full, Column-collapsed).</S>
    <S sid="85" ssid="47">This results in 159 similaritybased features for every pair of events, which are used to train a classifier to decide conference.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="86" ssid="1">We evaluate our method on two datasets and compare it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition.</S>
    <S sid="87" ssid="2">IC Event Coreference Corpus: The dataset (Hovy et al., 2013), drawn from 100 news articles about violent events, contains manually created annotations for 2214 pairs of co-referent and noncoreferent events each.</S>
    <S sid="88" ssid="3">Where available, events&#8217; semantic role-fillers for agent and patient are annotated as well.</S>
    <S sid="89" ssid="4">When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments.</S>
    <S sid="90" ssid="5">EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents from Google News is clustered into 45 topics, with event coreference chains annotated over each topic.</S>
    <S sid="91" ssid="6">The event mentions are enriched with semantic roles to obtain the canonical event structure described above.</S>
    <S sid="92" ssid="7">Positive instances are obtained by taking pairwise event mentions within each chain, and negative instances are generated from pairwise event mentions across chains, but within the same topic.</S>
    <S sid="93" ssid="8">This results in 11039 positive instances and 33459 negative instances.</S>
    <S sid="94" ssid="9">To establish the efficacy of our model, we compare SDSM against a purely window-based baseline (DSM) trained on the same corpus.</S>
    <S sid="95" ssid="10">In our experiments we set a window size of seven words.</S>
    <S sid="96" ssid="11">We also compare SDSM against the window-based embeddings trained using a recursive neural network (SENNA) (Collobert et al., 2011) on both datsets.</S>
    <S sid="97" ssid="12">SENNA embeddings are state-of-the-art for many NLP tasks.</S>
    <S sid="98" ssid="13">The second baseline uses SENNA to generate level 3 similarity features for events&#8217; individual words (agent, patient and action).</S>
    <S sid="99" ssid="14">As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition.</S>
    <S sid="100" ssid="15">We extend it to our matrix representation and build two baselines AVC (element-wise addition) and MVC (element-wise multiplication).</S>
    <S sid="101" ssid="16">Among common classifiers, decision-trees (J48) yielded best results in our experiments.</S>
    <S sid="102" ssid="17">Table 1 summarizes our results on both datasets.</S>
    <S sid="103" ssid="18">The results reveal that the SDSM model consistently outperforms DSM, SENNA embeddings, and the MVC and AVC models, both in terms of F-1 score and accuracy.</S>
    <S sid="104" ssid="19">The IC corpus comprises of domain specific texts, resulting in high lexical overlap between event mentions.</S>
    <S sid="105" ssid="20">Hence, the scores on the IC corpus are consistently higher than those on the ECB corpus.</S>
    <S sid="106" ssid="21">The improvements over DSM and SENNA embeddings, support our hypothesis that syntax lends greater expressive power to distributional semantics in compositional configurations.</S>
    <S sid="107" ssid="22">Furthermore, the increase in predictive accuracy over MVC and AVC shows that our formulation of composition of two words based on the relation binding them yields a stronger form of compositionality than simple additive and multiplicative models.</S>
    <S sid="108" ssid="23">Next, we perform an ablation study to determine the most predictive features for the task of event coreferentiality.</S>
    <S sid="109" ssid="24">The forward selection procedure reveals that the most informative attributes are the level 2 compositional features involving the agent and the action, as well as their individual level 3 features.</S>
    <S sid="110" ssid="25">This corresponds to the intuition that the agent and the action are the principal determiners for identifying events.</S>
    <S sid="111" ssid="26">Features involving the patient and level 1 features are least useful.</S>
    <S sid="112" ssid="27">This is probably because features involving full composition are sparse, and not as likely to provide statistically significant evidence.</S>
    <S sid="113" ssid="28">This may change as our PropStore grows in size.</S>
  </SECTION>
  <SECTION title="5 Conclusion and Future Work" number="5">
    <S sid="114" ssid="1">We outlined an approach that introduces structure into distributed semantic representations gives us an ability to compare the identity of two representations derived from supposedly semantically identical phrases with different surface realizations.</S>
    <S sid="115" ssid="2">We employed the task of event coreference to validate our representation and achieved significantly higher predictive accuracy than several baselines.</S>
    <S sid="116" ssid="3">In the future, we would like to extend our model to other semantic tasks such as paraphrase detection, lexical substitution and recognizing textual entailment.</S>
    <S sid="117" ssid="4">We would also like to replace our syntactic relations to semantic relations and explore various ways of dimensionality reduction to solve this problem.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="118" ssid="1">The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper.</S>
    <S sid="119" ssid="2">This work was supported in part by the following grants: NSF grant IIS-1143703, NSF award IIS1147810, DARPA grant FA87501220342.</S>
  </SECTION>
</PAPER>
