<PAPER>
  <S sid="0">Exemplar-Based Word-Space Model for Compositionality Detection: Shared Task System Description</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper, we highlight the problems of polysemy in word space models of compositionality detection.</S>
    <S sid="2" ssid="2">Most models represent each word as a single prototype-based vector without addressing polysemy.</S>
    <S sid="3" ssid="3">We propose an exemplar-based model which is designed to handle polysemy.</S>
    <S sid="4" ssid="4">This model is tested for compositionality detection and it is found to outperform existing prototype-based models.</S>
    <S sid="5" ssid="5">We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">In the field of computational semantics, to represent the meaning of a compound word, two mechanisms are commonly used.</S>
    <S sid="7" ssid="2">One is based on the distributional hypothesis (Harris, 1954) and the other is on the principle of semantic compositionality (Partee, 1995, p. 313).</S>
    <S sid="8" ssid="3">The distributional hypothesis (DH) states that words that occur in similar contexts tend to have similar meanings.</S>
    <S sid="9" ssid="4">Using this hypothesis, distributional models like the Word-space model (WSM, Sahlgren, 2006) represent a target word&#8217;s meaning as a context vector (location in space).</S>
    <S sid="10" ssid="5">The similarity between two meanings is the closeness (proximity) between the vectors.</S>
    <S sid="11" ssid="6">The context vector of a target word is built from its distributional behaviour observed in a corpus.</S>
    <S sid="12" ssid="7">Similarly, the context vector of a compound word can be built by treating the compound as a single word.</S>
    <S sid="13" ssid="8">We refer to such a vector as a DH-based vector.</S>
    <S sid="14" ssid="9">The other mechanism is based on the principle of semantic compositionality (PSC) which states that the meaning of a compound word is a function of, and only of, the meaning of its parts and the way in which the parts are combined.</S>
    <S sid="15" ssid="10">If the meaning of a part is represented in a WSM using the distributional hypothesis, then the principle can be applied to compose the distributional behaviour of a compound word from its parts without actually using the corpus instances of the compound.</S>
    <S sid="16" ssid="11">We refer to this as a PSC-based vector.</S>
    <S sid="17" ssid="12">So a PSC-based is composed of component DH-based vectors.</S>
    <S sid="18" ssid="13">Both of these two mechanisms are capable of determining the meaning vector of a compound word.</S>
    <S sid="19" ssid="14">For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same.</S>
    <S sid="20" ssid="15">However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001).</S>
    <S sid="21" ssid="16">The DH-based and PSC-based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds.</S>
    <S sid="22" ssid="17">Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances.</S>
    <S sid="23" ssid="18">Such a representation is called the prototype-based modelling (Murphy, 2002).</S>
    <S sid="24" ssid="19">These prototype-based vectors do not distinguish the instances according to the senses of a target word.</S>
    <S sid="25" ssid="20">Since most compounds are less ambiguous than single words, there is less need for distinguishing instances in a DH-based prototype vector of a compound and we do not address that here but leave ambiguity of compounds for future work.</S>
    <S sid="26" ssid="21">However the constituent words of the compound are more ambiguous.</S>
    <S sid="27" ssid="22">When DH-based vectors of the constituent words are used for composing the PSCbased vector of the compound, the resulting vector may contain instances, and therefore contexts, that are not relevant for the given compound.</S>
    <S sid="28" ssid="23">These noisy contexts effect the similarity between the PSCbased vector and the DH-based vector of the compound.</S>
    <S sid="29" ssid="24">Basing compositionality judgements on a such a noisy similarity value is no longer reliable.</S>
    <S sid="30" ssid="25">In this paper, we address this problem of polysemy of constituent words of a compound using an exemplar-based modelling (Smith and Medin, 1981).</S>
    <S sid="31" ssid="26">In exemplar-based modelling of WSM (Erk and Pad&#180;o, 2010), each word is represented by all its corpus instances (exemplars) without merging them into a single vector.</S>
    <S sid="32" ssid="27">Depending upon the purpose, only relevant exemplars of the target word are activated and then these are merged to form a refined prototype-vector which is less-noisy compared to the original prototype-vector.</S>
    <S sid="33" ssid="28">Exemplar-based models are more powerful than prototype-based ones because they retain specific instance information.</S>
    <S sid="34" ssid="29">We have evaluated our models on the validation data released in the shared task (Biemann and Giesbrecht, 2011).</S>
    <S sid="35" ssid="30">Based on the validation results, we have chosen three systems for public evaluation and participated in the shared task (Biemann and Giesbrecht, 2011).</S>
  </SECTION>
  <SECTION title="2 Word Space Model" number="2">
    <S sid="36" ssid="1">In this section, construction of WSM for all our experiments is described.</S>
    <S sid="37" ssid="2">We use Sketch Engine1 (Kilgarriff et al., 2004) to retrieve all the exemplars for a target word or a pattern using corpus query language.</S>
    <S sid="38" ssid="3">Let w1 w2 be a compound word with constituent words w1 and w2.</S>
    <S sid="39" ssid="4">Ew denotes the set of exemplars of w. Vw is the prototype vector of the word w, which is built by merging all the exemplars in Ew For the purposes of producing a PSC-based vector for a compound, a vector of a constituent word is built using only the exemplars which do not contain the compound.</S>
    <S sid="40" ssid="5">Note that the vectors are sensitive to a compound&#8217;s word-order since the exemplars of w1 w2 are not the same as w2 w1.</S>
    <S sid="41" ssid="6">We use other WSM settings following Mitchell and Lapata (2008).</S>
    <S sid="42" ssid="7">The dimensions of the WSM are the top 2000 content words in the given corpus (along with their coarse-grained part-of-speech information).</S>
    <S sid="43" ssid="8">Cosine similarity (sim) is used to measure the similarity between two vectors.</S>
    <S sid="44" ssid="9">Values at the specific positions in the vector representing context words are set to the ratio of the probability of the context word given the target word to the overall probability of the context word.</S>
    <S sid="45" ssid="10">The context window of a target word&#8217;s exemplar is the whole sentence of the target word excluding the target word.</S>
    <S sid="46" ssid="11">Our language of interest is English.</S>
    <S sid="47" ssid="12">We use the ukWaC corpus (Ferraresi et al., 2008) for producing out WSMs.</S>
  </SECTION>
  <SECTION title="3 Related Work" number="3">
    <S sid="48" ssid="1">As described in Section 1, most WSM models for compositionality detection measure the similarity between the true distributional vector Vw1w2 of the compound and the composed vector Vw1&#174;w2, where &#174; denotes a compositionality function.</S>
    <S sid="49" ssid="2">If the similarity is high, the compound is treated as compositional or else non-compositional.</S>
    <S sid="50" ssid="3">Giesbrecht (2009); Katz and Giesbrecht (2006); Schone and Jurafsky (2001) obtained the compositionality vector of w1 w2 using vector addition Vw1&#174;w2 = aVw1 + bVw2.</S>
    <S sid="51" ssid="4">In this approach, if sim(Vw1&#174;w21 Vw1w2) &gt; -y, the compound is classified as compositional, where -y is a threshold for deciding compositionality.</S>
    <S sid="52" ssid="5">Global values of a and b were chosen by optimizing the performance on the development set.</S>
    <S sid="53" ssid="6">It was found that no single threshold value -y held for all compounds.</S>
    <S sid="54" ssid="7">Changing the threshold alters performance arbitrarily.</S>
    <S sid="55" ssid="8">This might be due to the polysemous nature of the constituent words which makes the composed vector Vw1&#174;w2 filled with noisy contexts and thus making the judgement unpredictable.</S>
    <S sid="56" ssid="9">In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003).</S>
    <S sid="57" ssid="10">They also observe similar behaviour of the threshold -y.</S>
    <S sid="58" ssid="11">We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling.</S>
    <S sid="59" ssid="12">The above models use a simple addition based compositionality function.</S>
    <S sid="60" ssid="13">Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition.</S>
    <S sid="61" ssid="14">Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors.</S>
    <S sid="62" ssid="15">In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication.</S>
    <S sid="63" ssid="16">Bannard et al. (2003); McCarthy et al.</S>
    <S sid="64" ssid="17">(2003) observed that methods based on distributional similarities between a phrase and its constituent words help when determining the compositionality behaviour of phrases.</S>
    <S sid="65" ssid="18">We therefore also use evidence from the similarities between each constituent word and the compound.</S>
  </SECTION>
  <SECTION title="4 Our Approach: Exemplar-based Model" number="4">
    <S sid="66" ssid="1">Our approach works as follows.</S>
    <S sid="67" ssid="2">Firstly, given a compound w1 w2, we build its DH-based prototype vector Vw1w2 from all its exemplars Ew1w2.</S>
    <S sid="68" ssid="3">Secondly, we remove irrelevant exemplars in Ew1 and Ew2 of constituent words and build the refined prototype vectors Vwr1 and Vwr2 of the constituent words w1 and w2 respectively.</S>
    <S sid="69" ssid="4">These refined vectors are used to compose the PSC-based vectors 2 of the compound.</S>
    <S sid="70" ssid="5">Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built.</S>
    <S sid="71" ssid="6">This work does not relate to compositionality but to measuring semantic similarity of single words.</S>
    <S sid="72" ssid="7">As such, their clusters are not influenced by other words whereas in our approach for detecting compositionality, the other constituent word plays a major role.</S>
    <S sid="73" ssid="8">We use the compositionality functions, simple addition and simple multiplication to build Vwr1+wr2 and Vwr1xwr2 respectively.</S>
    <S sid="74" ssid="9">Based on the similarities sim(Vw1w2, Vwr1), sim(Vw1w2, Vwr2), sim(Vw1w2, Vwr1+wr2) and sim(Vw1w2, Vwr1xwr2), we decide if the compound is compositional or noncompositional.</S>
    <S sid="75" ssid="10">These steps are described in a little more detail below.</S>
    <S sid="76" ssid="11">We aim to remove irrelevant exemplars of one constituent word with the help of the other constituent word&#8217;s distributional behaviour.</S>
    <S sid="77" ssid="12">For example, let us take the compound traffic light.</S>
    <S sid="78" ssid="13">Light occurs in many contexts such as quantum theory, optics, lamps and spiritual theory.</S>
    <S sid="79" ssid="14">In ukWaC, light has 316,126 instances.</S>
    <S sid="80" ssid="15">Not all these exemplars are relevant to compose the PSC-based vector of traffic light.</S>
    <S sid="81" ssid="16">These irrelevant exemplars increases the semantic differences between traffic light and light and thus increase the differences between Vtraffic&#174;light and Vtraffic light. sim(Vlight, Vtraffic light) is found to be 0.27.</S>
    <S sid="82" ssid="17">Our intuition and motivation for exemplar removal is that it is beneficiary to choose only the exemplars of light which share similar contexts of traffic since traffic light should have contexts similar to both traffic and light if it is compositional.</S>
    <S sid="83" ssid="18">We rank each exemplar of light based on common co-occurrences of traffic and also words which are distributionally similar to traffic.</S>
    <S sid="84" ssid="19">Co-occurrences of traffic are the context words which frequently occur with traffic, e.g. car, road etc.</S>
    <S sid="85" ssid="20">Using these, the exemplar from a sentence such as &#8220;Cameras capture cars running red lights ...&#8221; will be ranked higher than one which does not have contexts related to traffic.</S>
    <S sid="86" ssid="21">The distributionally similar words to traffic are the words (like synonyms, antonyms) which are similar to traffic in that they occur in similar contexts, e.g. transport, flow etc.</S>
    <S sid="87" ssid="22">Using these distributionally similar words helps reduce the impact of data sparseness and helps prioritise contexts of traffic which are semantically related.</S>
    <S sid="88" ssid="23">We use Sketch Engine to compute the scores of a word observed in a given corpus.</S>
    <S sid="89" ssid="24">Sketch Engine scores the cooccurrences (collocations) using logDice motivated by (Curran, 2003) and distributionally related words using (Rychl&#180;y and Kilgarriff, 2007; Lexical Computing Ltd., 2007).</S>
    <S sid="90" ssid="25">For a given word, both of these scores are normalised in the range (0,1) All the exemplars of light are ranked based on the co-occurrences of these collocations and distributionally related words of traffic using where straffic exemplar E w.r.t. traffic, c for context word in the exemplar E, xE c is the coordinate value (contextual score) of the context word c in the exemplar E and ytraffic c is the score of the context word c w.r.t. traffic.</S>
    <S sid="91" ssid="26">A refined prototype vector of light is then built by merging the top n exemplars of light where Etrafhfitc are the set of exemplars of light lig ranked using co-occurrence information from the other constituent word traffic. n is chosen such that sim(Vlightr, Vtraffic light) is maximised.</S>
    <S sid="92" ssid="27">This similarity is observed to be greatest using just 2286 (less than 1%) of the total exemplars of light.</S>
    <S sid="93" ssid="28">After exemplar removal, sim(Vlightr, Vtraffic light) increased to 0.47 from the initial value of 0.27.</S>
    <S sid="94" ssid="29">Though n is chosen by maximising similarity, which is not desirable for non-compositional compounds, the lack of similarity will give the strongest possible indication that a compound is not compositional.</S>
    <S sid="95" ssid="30">We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1xwr2.</S>
    <S sid="96" ssid="31">These are as described in (Mitchell and Lapata, 2008).</S>
    <S sid="97" ssid="32">In model addition, Vw1EDw2 = aVw1 + bVw2, all the previous approaches use static values of a and b.</S>
    <S sid="98" ssid="33">Instead, we use dynamic weights computed from the participating and b = 1&#8722;a.</S>
    <S sid="99" ssid="34">These weights differ from compound to compound.</S>
    <S sid="100" ssid="35">To judge if a compound is compositional or noncompositional, previous approaches (see Section 3) base their judgement on a single similarity value.</S>
    <S sid="101" ssid="36">As discussed, we base our judgement based on the collective evidences from all the similarity values using a linear equation of the form where the value of &#945; denotes the compositionality score.</S>
    <S sid="102" ssid="37">The range of &#945; is in between 0-100.</S>
    <S sid="103" ssid="38">If &#945; &lt; 34, the compound is treated as non-compositional, 34 &lt; &#945; &lt; 67 as medium compositional and &#945; &gt; 67 as highly compositional.</S>
    <S sid="104" ssid="39">The parameters ai&#8217;s are estimated using ordinary least square regression by training over the training data released in the shared task (Biemann and Giesbrecht, 2011).</S>
    <S sid="105" ssid="40">For the three categories &#8211; adjective-noun, verb-object and subject-verb &#8211; the parameters are estimated separately.</S>
    <S sid="106" ssid="41">Note that if a1 = a2 = a4 = 0, the model bases its judgement only on addition.</S>
    <S sid="107" ssid="42">Similarly if a1 = a2 = a3 = 0, the model bases its judgement only on multiplication.</S>
    <S sid="108" ssid="43">We also experimented with combinations such as &#945;(Vwr1, Vw2) and &#945;(Vw1, Vwr2) i.e. using refined vector for one of the constituent word and the unrefined prototype vector for the other constituent word.</S>
    <S sid="109" ssid="44">To participate in the shared task, we have selected the best performing model by evaluating the models on the validation data released in the shared task (Biemann and Giesbrecht, 2011).</S>
    <S sid="110" ssid="45">Table 1 displays the results on the validation data.</S>
    <S sid="111" ssid="46">The average point difference is calculated by taking the average of the difference in a model&#8217;s score &#945; and the gold score annotated by humans, over all compounds.</S>
    <S sid="112" ssid="47">Table 1 also displays the overall accuracy of coarse grained labels &#8211; low, medium and high.</S>
    <S sid="113" ssid="48">Best performance for verb(v)-object(o) compounds is found for the combination &#945;(Vvr, Vor) of Equation 3.</S>
    <S sid="114" ssid="49">For subject(s)-verb(v) compounds, it is for &#945;(Vsr, Vvr) and a3 = a4 = 0.</S>
    <S sid="115" ssid="50">For adjective(j)noun(n) compounds, it is &#945;(Vjr, Vn).</S>
    <S sid="116" ssid="51">We are not certain of the reason for this difference, perhaps there may be less ambiguity of words within specific grammatical relationships or it may be simply due to the actual compounds in those categories.</S>
    <S sid="117" ssid="52">We leave analysis of this for future work.</S>
    <S sid="118" ssid="53">We combined the outputs of these category-specific models to build the best model Exm-Best.</S>
    <S sid="119" ssid="54">For comparison, results of standard models prototype addition (Pro-Addn) and prototypemultiplication (Pro-Mult) are also displayed in Table 1.</S>
    <S sid="120" ssid="55">Pro-Addn can be represented as &#945;(Vw1, Vw2) with a1 = a2 = a4 = 0.</S>
    <S sid="121" ssid="56">Pro-Mult can be represented as &#945;(Vw1, Vw2) with a1 = a2 = a3 = 0.</S>
    <S sid="122" ssid="57">Pro-Best is the best performing model in prototype-based modelling.</S>
    <S sid="123" ssid="58">It is found to be &#945;(Vw1, Vw2).</S>
    <S sid="124" ssid="59">(Note: Depending upon the compound type, some of the ai&#8217;s in Pro-Best may be 0).</S>
    <S sid="125" ssid="60">Overall, exemplar-based modelling excelled in both the evaluations, average point difference and coarse-grained label accuracies.</S>
    <S sid="126" ssid="61">The systems ExmBest, Pro-Best and Exm &#945;(Vwi, Vw2) were submitted for the public evaluation in the shared task.</S>
    <S sid="127" ssid="62">All the model parameters were estimated by regression on the task&#8217;s training data separately for the 3 compound types as described in Section 4.3.</S>
    <S sid="128" ssid="63">We perform the regression separately for these classes to maximise performance.</S>
    <S sid="129" ssid="64">In the future, we will investigate whether these settings gave us better results on the test data compared to setting the values the same regardless of the category of compound.</S>
  </SECTION>
  <SECTION title="5 Shared Task Results" number="5">
    <S sid="130" ssid="1">Table 2 displays Spearman p and Kendalls z correlation scores of all the models.</S>
    <S sid="131" ssid="2">TotPrd stands for the total number of predictions.</S>
    <S sid="132" ssid="3">Rand-Base is the baseline system which randomly assigns a compositionality score for a compound.</S>
    <S sid="133" ssid="4">Our model ExmBest was the best performing system compared to all other systems in this evaluation criteria.</S>
    <S sid="134" ssid="5">SharedTaskNextBest is the next best performing system apart from our models.</S>
    <S sid="135" ssid="6">Due to lemmatization errors in the test data, our models could only predict judgements for 169 out of 174 compounds.</S>
    <S sid="136" ssid="7">Table 3 displays average point difference scores.</S>
    <S sid="137" ssid="8">Zero-Base is a baseline system which assigns a score of 50 to all compounds.</S>
    <S sid="138" ssid="9">SharedTaskBest is the overall best performing system.</S>
    <S sid="139" ssid="10">Exm-Best was ranked second best among all the systems.</S>
    <S sid="140" ssid="11">For ADJ-NN and V-SUBJ compounds, the best performing systems in the shared task are Pro-Best and Exm-Best respectively.</S>
    <S sid="141" ssid="12">Our models did less well on V-OBJ compounds and we will explore the reasons for this in future work.</S>
    <S sid="142" ssid="13">Table 4 displays coarse grained scores.</S>
    <S sid="143" ssid="14">As above, similar behaviour is observed for coarse grained accuracies.</S>
    <S sid="144" ssid="15">Most-Freq-Base is the baseline system which assigns the most frequent coarse-grained label for a compound based on its type (ADJ-NN, VSUBJ, V-OBJ) as observed in training data.</S>
    <S sid="145" ssid="16">MostFreq-Base outperforms all other systems.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="146" ssid="1">In this paper, we examined the effect of polysemy in word space models for compositionality detection.</S>
    <S sid="147" ssid="2">We showed exemplar-based WSM is effective in dealing with polysemy.</S>
    <S sid="148" ssid="3">Also, we use multiple evidences for compositionality detection rather than basing our judgement on a single evidence.</S>
    <S sid="149" ssid="4">Overall, performance of the Exemplar-based models of compositionality detection is found to be superior to prototype-based models.</S>
  </SECTION>
</PAPER>
