<PAPER>
  <S sid="0">Joint Decoding with Multiple Translation Models</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in phase.</S>
    <S sid="2" ssid="2">We instead propose a method that combines multiple translation models in one decoder.</S>
    <S sid="3" ssid="3">Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually.</S>
    <S sid="4" ssid="4">Therefore, one model can share translations and even derivations with other models.</S>
    <S sid="5" ssid="5">Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">System combination aims to find consensus translations among different machine translation systems.</S>
    <S sid="7" ssid="2">It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994).</S>
    <S sid="8" ssid="3">Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks.</S>
    <S sid="9" ssid="4">A confusion network consists of a sequence of sets of candidate words.</S>
    <S sid="10" ssid="5">Each candidate word is associated with a score.</S>
    <S sid="11" ssid="6">The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score.</S>
    <S sid="12" ssid="7">While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements.</S>
    <S sid="13" ssid="8">In this paper, we propose a framework for combining multiple translation models directly in decoding phase.</S>
    <S sid="14" ssid="9">1 Based on max-translation decoding and max-derivation decoding used in conventional individual decoders (Section 2), we go further to develop a joint decoder that integrates multiple models on a firm basis: to BLEU score for max-translation decoding (Section 4).</S>
    <S sid="15" ssid="10">We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset.</S>
    <S sid="16" ssid="11">Experimental results show that joint decoding with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5).</S>
  </SECTION>
  <SECTION title="2 Background" number="2">
    <S sid="17" ssid="1">Statistical machine translation is a decision problem where we need decide on the best of target sentence matching a source sentence.</S>
    <S sid="18" ssid="2">The process of searching for the best translation is conventionally called decoding, which usually involves sequences of decisions that translate a source sentence into a target sentence step by step.</S>
    <S sid="19" ssid="3">For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence &#8220;fabiao yanjiang&#8221; into an English sentence &#8220;give a talk&#8221;.</S>
    <S sid="20" ssid="4">Such sequence of decisions is called a derivation.</S>
    <S sid="21" ssid="5">In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases.</S>
    <S sid="22" ssid="6">In syntax-based models, decisions usually correspond to transduction rules.</S>
    <S sid="23" ssid="7">Often, there are many derivations that are distinct yet produce the same translation.</S>
    <S sid="24" ssid="8">Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly.</S>
    <S sid="25" ssid="9">Given a source sentence f, the probability of a target sentence e being its translation is the sum over all possible derivations: where A(e, f) is the set of all possible derivations that translate f into e and d is one such derivation.</S>
    <S sid="26" ssid="10">They use a log-linear model to define the conditional probability of a derivation d and corresponding translation e conditioned on a source sentence f: where hm is a feature function, Am is the associated feature weight, and Z(f) is a constant for normalization: A feature value is usually decomposed as the product of decision probabilities: 2 where d is a decision in the derivation d. Although originally proposed for supporting large sets of non-independent and overlapping features, the latent variable model is actually a more general form of conventional linear model (Och and Ney, 2002).</S>
    <S sid="27" ssid="11">Accordingly, decoding for the latent variable model can be formalized as where Z(f) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency.</S>
    <S sid="28" ssid="12">They search for the 1best derivation and take its target yield as the best translation: We refer to Eq.</S>
    <S sid="29" ssid="13">(5) as max-translation decoding and Eq.</S>
    <S sid="30" ssid="14">(6) as max-derivation decoding, which are first termed by Blunsom et al. (2008).</S>
    <S sid="31" ssid="15">By now, most current SMT systems, adopting either max-derivation decoding or max-translation decoding, have only used single models in decoding phase.</S>
    <S sid="32" ssid="16">We refer to them as individual decoders.</S>
    <S sid="33" ssid="17">In the following section, we will present a new method called joint decoding that includes multiple models in one decoder.</S>
  </SECTION>
  <SECTION title="3 Joint Decoding" number="3">
    <S sid="34" ssid="1">There are two major challenges for combining multiple models directly in decoding phase.</S>
    <S sid="35" ssid="2">First, they rely on different kinds of knowledge sources and thus need to collect different information during decoding.</S>
    <S sid="36" ssid="3">For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side.</S>
    <S sid="37" ssid="4">On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string.</S>
    <S sid="38" ssid="5">As a result, the hypothesis structures of the two models are fundamentally different.</S>
    <S sid="39" ssid="6">Second, translation models differ in decoding algorithms.</S>
    <S sid="40" ssid="7">Depending on the generating order of a target sentence, we distinguish between two major categories: left-to-right and bottom-up.</S>
    <S sid="41" ssid="8">Decoders that use rules with flat structures (e.g., phrase pairs) usually generate target sentences from left to right while those using rules with hierarchical structures (e.g., SCFG rules) often run in a bottom-up style.</S>
    <S sid="42" ssid="9">In response to the two challenges, we first argue that the search space of an arbitrary model can be structured as a translation hypergraph, which makes each model connectable to others (Section 3.1).</S>
    <S sid="43" ssid="10">Then, we show that a packed translation hypergraph that integrates the hypergraphs of individual models can be generated in a bottom-up topological order, either integrated at the translation level (Section 3.2) or the derivation level (Section 3.3).</S>
    <S sid="44" ssid="11">Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences.</S>
    <S sid="45" ssid="12">Therefore, we represent the search space of a translation model as a structure called translation hypergraph.</S>
    <S sid="46" ssid="13">Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model.</S>
    <S sid="47" ssid="14">A node in a hypergraph denotes a partial translation for a source substring, except for the starting node &#8220;S&#8221;.</S>
    <S sid="48" ssid="15">For example, given the example source sentence 0 fabiao 1 yanjiang 2 the node (&#8220;give talks&#8221;, [0, 2]) in Figure 2(a) denotes that &#8220;give talks&#8221; is one translation of the source string f21 = &#8220;fabiao yanjiang&#8221;.</S>
    <S sid="49" ssid="16">The hyperedges between nodes denote the decision steps that produce head nodes from tail nodes.</S>
    <S sid="50" ssid="17">For example, the incoming hyperedge of the node (&#8220;give talks&#8221;, [0, 2]) could correspond to an SCFG rule: Each hyperedge is associated with a number of weights, which are the feature values of the corresponding translation rules.</S>
    <S sid="51" ssid="18">A path of hyperedges constitutes a derivation. pergraph and decoding.</S>
    <S sid="52" ssid="19">More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple (V, E, R), where V is a set of nodes, E is a set of hyperedges, and R is a set of weights.</S>
    <S sid="53" ssid="20">For a given source sentence f = f'1 = f1 ... f,,,, each node v E V is in the form of (t, [i, j]), which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 ... fj).</S>
    <S sid="54" ssid="21">Each hyperedge e E E is a tuple e = (tails(e), head(e), w(e)), where head(e) E V is the consequent node in the deductive step, tails(e) E V &#8727; is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translation hypergraph is capable of characterizing the search space of an arbitrary translation model.</S>
    <S sid="55" ssid="22">Furthermore, it offers a graphic interpretation of decoding process.</S>
    <S sid="56" ssid="23">A node in a hypergraph denotes a translation, a hyperedge denotes a decision step, and a path of hyperedges denotes a derivation.</S>
    <S sid="57" ssid="24">A translation hypergraph is formally a semiring as the weight of a path is the product of hyperedge weights and the weight of a node is the sum of path weights.</S>
    <S sid="58" ssid="25">While max-derivation decoding only retains the single best path at each node, max-translation decoding sums up all incoming paths.</S>
    <S sid="59" ssid="26">Table 1 summarizes the relationship between translation hypergraph and decoding.</S>
    <S sid="60" ssid="27">The conventional interpretation of Eq.</S>
    <S sid="61" ssid="28">(1) is that the probability of a translation is the sum over all possible derivations coming from the same model.</S>
    <S sid="62" ssid="29">Alternatively, we interpret Eq.</S>
    <S sid="63" ssid="30">(1) as that the derivations could come from different models.3 This forms the theoretical basis of joint decoding.</S>
    <S sid="64" ssid="31">Although the information inside a derivation differs widely among translation models, the beginning and end points (i.e., f and e, respectively) must be identical.</S>
    <S sid="65" ssid="32">For example, a tree-to-string model first parses f to obtain a source tree T(f) and then transforms T(f) to the target sentence e. Conversely, a string-to-tree model first parses f into a target tree T(e) and then takes the surface string e as the translation.</S>
    <S sid="66" ssid="33">Despite different inside, their derivations must begin with f and end with e. This situation remains the same for derivations between a source substring fji and its partial translation t during joint decoding: where d might come from multiple models.</S>
    <S sid="67" ssid="34">In other words, derivations from multiple models could be brought together for computing the probability of one partial translation.</S>
    <S sid="68" ssid="35">Graphically speaking, joint decoding creates a packed translation hypergraph that combines individual hypergraphs by merging nodes that have identical translations.</S>
    <S sid="69" ssid="36">For example, Figure 2 (a) and (b) demonstrate two translation hypergraphs generated by two models respectively and Figure 2 (c) is the resulting packed hypergraph.</S>
    <S sid="70" ssid="37">The solid lines denote the hyperedges of the first model and the dashed lines denote those of the second model.</S>
    <S sid="71" ssid="38">The shaded nodes are shared by both models.</S>
    <S sid="72" ssid="39">Therefore, the two models are combined at the translation level.</S>
    <S sid="73" ssid="40">Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models.</S>
    <S sid="74" ssid="41">Now the question is how to decode with multiple models jointly in just one decoder.</S>
    <S sid="75" ssid="42">We believe that both left-to-right and bottom-up strategies can be used for joint decoding.</S>
    <S sid="76" ssid="43">Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle.</S>
    <S sid="77" ssid="44">Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs.</S>
    <S sid="78" ssid="45">They treat reordering of phrases as a binary classification problem.</S>
    <S sid="79" ssid="46">On the other hand, it is possible for syntax-based models to decode from left to right.</S>
    <S sid="80" ssid="47">Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation.</S>
    <S sid="81" ssid="48">Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience.</S>
    <S sid="82" ssid="49">Figure 3 demonstrates the search algorithm of our joint decoder.</S>
    <S sid="83" ssid="50">The input is a source language sentence f'1 , and a set of translation models M (line 1).</S>
    <S sid="84" ssid="51">After initializing the translation hypergraph G (line 2), the decoder runs in a bottomup style, adding nodes for each span [i, j] and for each model m. For each span [i, j] (lines 3-5), the procedure ADD(G, i, j, m) add nodes generated by the model m to the hypergraph G (line 6).</S>
    <S sid="85" ssid="52">Each model searches for partial translations independently: it uses its own knowledge sources and visits its own antecedent nodes, just running like a bottom-up individual decoder.</S>
    <S sid="86" ssid="53">After all models finishes adding nodes for span [i, j], the procedure PRUNE(G, i, j) merges identical nodes and removes less promising nodes to control the search space (line 8).</S>
    <S sid="87" ssid="54">The pruning strategy is similar to that of individual decoders, except that we require there must exist at least one node for each model to ensure further inference.</S>
    <S sid="88" ssid="55">Although translation-level combination will not offer new translations as compared to single models, it changes the way of selecting promising candidates in a combined search space and might potentially produce better translations than individual decoding.</S>
    <S sid="89" ssid="56">In translation-level combination, different models interact with each other only at the nodes.</S>
    <S sid="90" ssid="57">The derivations of one model are unaccessible to other models.</S>
    <S sid="91" ssid="58">However, if two models produce the same structures on the target side, it is possible to combine two models within one derivation, which we refer to as derivation-level combination.</S>
    <S sid="92" ssid="59">For example, although different on the source side, both hierarchical phrase-based and tree-tostring models produce strings of terminals and nonterminals on the target side.</S>
    <S sid="93" ssid="60">Figure 4 shows a derivation composed of both hierarchical phrase pairs and tree-to-string rules.</S>
    <S sid="94" ssid="61">Hierarchical phrase pairs are used for translating smaller units and tree-to-string rules for bigger ones.</S>
    <S sid="95" ssid="62">It is appealing to combine them in such a way because the hierarchical phrase-based model provides excellent rule coverage while the tree-to-string model offers linguistically motivated non-local reordering.</S>
    <S sid="96" ssid="63">Similarly, Blunsom and Osborne (2008) use both hierarchical phrase pairs and tree-to-string rules in decoding, where source parse trees serve as conditioning context rather than hard constraints.</S>
    <S sid="97" ssid="64">Depending on the target side output, we distinguish between string-targeted and tree-targeted models.</S>
    <S sid="98" ssid="65">String-targeted models include phrasebased, hierarchical phrase-based, and tree-tostring models.</S>
    <S sid="99" ssid="66">Tree-targeted models include string-to-tree and tree-to-tree models.</S>
    <S sid="100" ssid="67">All models can be combined at the translation level.</S>
    <S sid="101" ssid="68">Models that share with same target output structure can be further combined at the derivation level.</S>
    <S sid="102" ssid="69">The joint decoder usually runs as maxtranslation decoding because multiple derivations from various models are used.</S>
    <S sid="103" ssid="70">However, if all models involved belong to the same category, a joint decoder can also adopt the max-derivation fashion because all nodes and hyperedges are accessible now (Section 5.2).</S>
    <S sid="104" ssid="71">Allowing derivations for comprising rules from different models and integrating their strengths, derivation-level combination could hopefully produce new and better translations as compared with single models.</S>
  </SECTION>
  <SECTION title="4 Extended Minimum Error Rate Training" number="4">
    <S sid="105" ssid="1">Minimum error rate training (Och, 2003) is widely used to optimize feature weights for a linear model (Och and Ney, 2002).</S>
    <S sid="106" ssid="2">The key idea of MERT is to tune one feature weight to minimize error rate each time while keep others fixed.</S>
    <S sid="107" ssid="3">Therefore, each candidate translation can be represented as a line: where a is the feature value of current dimension, x is the feature weight being tuned, and b is the dotproduct of other dimensions.</S>
    <S sid="108" ssid="4">The intersection of two lines is where the candidate translation will change.</S>
    <S sid="109" ssid="5">Instead of computing all intersections, Och (2003) only computes critical intersections where highest-score translations will change.</S>
    <S sid="110" ssid="6">This method reduces the computational overhead significantly.</S>
    <S sid="111" ssid="7">Unfortunately, minimum error rate training cannot be directly used to optimize feature weights of max-translation decoding because Eq.</S>
    <S sid="112" ssid="8">(5) is not a linear model.</S>
    <S sid="113" ssid="9">However, if we also tune one dimension each time and keep other dimensions fixed, we obtain a monotonic curve as follows: where K is the number of derivations for a candidate translation, ak is the feature value of current dimension on the kth derivation and bk is the dotproduct of other dimensions on the kth derivation.</S>
    <S sid="114" ssid="10">If we restrict that ak is always non-negative, the curve shown in Eq.</S>
    <S sid="115" ssid="11">(9) will be a monotonically increasing function.</S>
    <S sid="116" ssid="12">Therefore, it is possible to extend the MERT algorithm to handle situations where multiple derivations are taken into account for decoding.</S>
    <S sid="117" ssid="13">The key difference is the calculation of critical intersections.</S>
    <S sid="118" ssid="14">The major challenge is that two curves might have multiple intersections while two lines have at most one intersection.</S>
    <S sid="119" ssid="15">Fortunately, as the curve is monotonically increasing, we need only to find the leftmost intersection of a curve with other curves that have greater values after the intersection as a candidate critical intersection.</S>
    <S sid="120" ssid="16">Figure 5 demonstrates three curves: t1, t2, and t3.</S>
    <S sid="121" ssid="17">Suppose that the left bound of x is 0, we compute the function values for t1, t2, and t3 at x = 0 and find that t3 has the greatest value.</S>
    <S sid="122" ssid="18">As a result, we choose x = 0 as the first critical intersection.</S>
    <S sid="123" ssid="19">Then, we compute the leftmost intersections of t3 with t1 and t2 and choose the intersection closest to x = 0, that is x1, as our new critical intersection.</S>
    <S sid="124" ssid="20">Similarly, we start from x1 and find x2 as the next critical intersection.</S>
    <S sid="125" ssid="21">This iteration continues until it reaches the right bound.</S>
    <S sid="126" ssid="22">The bold curve denotes the translations we will choose over different ranges.</S>
    <S sid="127" ssid="23">For example, we will always choose t2 for the range [x1, x2].</S>
    <S sid="128" ssid="24">To compute the leftmost intersection of two curves, we divide the range from current critical intersection to the right bound into many bins (i.e., smaller ranges) and search the bins one by one from left to right.</S>
    <S sid="129" ssid="25">We assume that there is at most one intersection in each bin.</S>
    <S sid="130" ssid="26">As a result, we can use the Bisection method for finding the intersection in each bin.</S>
    <S sid="131" ssid="27">The search process ends immediately once an intersection is found.</S>
    <S sid="132" ssid="28">We divide max-translation decoding into three phases: (1) build the translation hypergraphs, (2) generate n-best translations, and (3) generate n&#8242;best derivations.</S>
    <S sid="133" ssid="29">We apply Algorithm 3 of Huang and Chiang (2005) for n-best list generation.</S>
    <S sid="134" ssid="30">Extended MERT runs on n-best translations plus n&#8242;best derivations to optimize the feature weights.</S>
    <S sid="135" ssid="31">Note that feature weights of various models are tuned jointly in extended MERT.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="136" ssid="1">Our experiments were on Chinese-to-English translation.</S>
    <S sid="137" ssid="2">We used the FBIS corpus (6.9M + 8.9M words) as the training corpus.</S>
    <S sid="138" ssid="3">For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus.</S>
    <S sid="139" ssid="4">We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set.</S>
    <S sid="140" ssid="5">We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002).</S>
    <S sid="141" ssid="6">Our joint decoder included two models.</S>
    <S sid="142" ssid="7">The first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007).</S>
    <S sid="143" ssid="8">We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003).</S>
    <S sid="144" ssid="9">About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set.</S>
    <S sid="145" ssid="10">Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007).</S>
    <S sid="146" ssid="11">Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses.</S>
    <S sid="147" ssid="12">For 15,157 sentences we failed to obtain 1-best parses.</S>
    <S sid="148" ssid="13">Therefore, only 93.7% of the training corpus were used by the tree-to-string model.</S>
    <S sid="149" ssid="14">About 578K tree-to-string rules extracted from the training corpus were used on the test set.</S>
    <S sid="150" ssid="15">Table 2 shows the results of comparing individual decoding and joint decoding on the test set.</S>
    <S sid="151" ssid="16">With conventional max-derivation decoding, the hierarchical phrase-based model achieved a BLEU score of 30.11 on the test set, with an average decoding time of 40.53 seconds/sentence.</S>
    <S sid="152" ssid="17">We found that accounting for all possible derivations in maxtranslation decoding resulted in a small negative effect on BLEU score (from 30.11 to 29.82), even though the feature weights were tuned with respect to BLEU score.</S>
    <S sid="153" ssid="18">One possible reason is that we only used n-best derivations instead of all possible derivations for minimum error rate training.</S>
    <S sid="154" ssid="19">Max-derivation decoding with the tree-to-string model yielded much lower BLEU score (i.e., 27.23) than the hierarchical phrase-based model.</S>
    <S sid="155" ssid="20">One reason is that the tree-to-string model fails to capture a large amount of linguistically unmotivated mappings due to syntactic constraints.</S>
    <S sid="156" ssid="21">Another reason is that the tree-to-string model only used part of the training data because of parsing failure.</S>
    <S sid="157" ssid="22">Similarly, accounting for all possible derivations in max-translation decoding failed to bring benefits for the tree-to-string model (from 27.23 to 27.11).</S>
    <S sid="158" ssid="23">When combining the two models at the translation level, the joint decoder achieved a BLEU score of 30.79 that outperformed the best result (i.e., 30.11) of individual decoding significantly (p &lt; 0.05).</S>
    <S sid="159" ssid="24">This suggests that accounting for all possible derivations from multiple models will help discriminate among candidate translations.</S>
    <S sid="160" ssid="25">Figure 6 demonstrates the percentages of nodes shared by the two models over various span widths in packed translation hypergraphs during maxtranslation decoding.</S>
    <S sid="161" ssid="26">For one-word source strings, 89.33% nodes in the hypergrpah were shared by both models.</S>
    <S sid="162" ssid="27">With the increase of span width, the percentage decreased dramatically due to the diversity of the two models.</S>
    <S sid="163" ssid="28">However, there still exist nodes shared by two models even for source substrings that contain 33 words.</S>
    <S sid="164" ssid="29">When combining the two models at the derivation level using max-derivation decoding, the joint decoder achieved a BLEU score of 31.63 that outperformed the best result (i.e., 30.11) of individpercentage ual decoding significantly (p &lt; 0.01).</S>
    <S sid="165" ssid="30">This improvement resulted from the mixture of hierarchical phrase pairs and tree-to-string rules.</S>
    <S sid="166" ssid="31">To produce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from training data, 6,800 glue rules connecting partial translations monotonically, and 16,554 tree-to-string rules.</S>
    <S sid="167" ssid="32">While tree-to-string rules offer linguistically motivated non-local reordering during decoding, hierarchical phrase pairs ensure good rule coverage.</S>
    <S sid="168" ssid="33">Max-translation decoding still failed to surpass max-derivation decoding in this case.</S>
    <S sid="169" ssid="34">We re-implemented a state-of-the-art system combination method (Rosti et al., 2007).</S>
    <S sid="170" ssid="35">As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly lower than that of joint decoding.</S>
    <S sid="171" ssid="36">But this difference is not significant statistically.</S>
    <S sid="172" ssid="37">Table 4 shows the effects of individual training and joint training.</S>
    <S sid="173" ssid="38">By individual, we mean that the two models are trained independently.</S>
    <S sid="174" ssid="39">We concatenate and normalize their feature weights for the joint decoder.</S>
    <S sid="175" ssid="40">By joint, we mean that they are trained together by the extended MERT algorithm.</S>
    <S sid="176" ssid="41">We found that joint training outperformed individual training significantly for both max-derivation decoding and max-translation decoding.</S>
  </SECTION>
  <SECTION title="6 Related Work" number="6">
    <S sid="177" ssid="1">System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemblebased parsing (e.g., (Henderson and Brill, 1999)).</S>
    <S sid="178" ssid="2">In machine translation, confusion-network based combination techniques (e.g., (Rosti et al., 2007; He et al., 2008)) have achieved the state-of-theart performance in MT evaluations.</S>
    <S sid="179" ssid="3">From a different perspective, we try to combine different approaches directly in decoding phase by using hypergraphs.</S>
    <S sid="180" ssid="4">While system combination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information.</S>
    <S sid="181" ssid="5">Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly.</S>
    <S sid="182" ssid="6">They show that max-translation decoding outperforms max-derivation decoding for the latent variable model.</S>
    <S sid="183" ssid="7">While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.</S>
    <S sid="184" ssid="8">Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008).</S>
    <S sid="185" ssid="9">Both Mi et al. (2008) and Blunsom et al.</S>
    <S sid="186" ssid="10">(2008) use a translation hypergraph to represent search space.</S>
    <S sid="187" ssid="11">The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="7">
    <S sid="188" ssid="1">We have presented a framework for including multiple translation models in one decoder.</S>
    <S sid="189" ssid="2">Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even hyperedges.</S>
    <S sid="190" ssid="3">As our decoder accounts for multiple derivations, we extend the MERT algorithm to tune feature weights with respect to BLEU score for max-translation decoding.</S>
    <S sid="191" ssid="4">In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the latticebased MERT (Macherey et al., 2008).</S>
  </SECTION>
  <SECTION title="Acknowledgement" number="8">
    <S sid="192" ssid="1">The authors were supported by National Natural Science Foundation of China, Contracts 60873167 and 60736014, and 863 State Key Project No.</S>
    <S sid="193" ssid="2">2006AA010108.</S>
    <S sid="194" ssid="3">Part of this work was done while Yang Liu was visiting the SMT group led by Stephan Vogel at CMU.</S>
    <S sid="195" ssid="4">We thank the anonymous reviewers for their insightful comments.</S>
    <S sid="196" ssid="5">We are also grateful to Yajuan L&#168;u, Liang Huang, Nguyen Bach, Andreas Zollmann, Vamshi Ambati, and Kevin Gimpel for their helpful feedback.</S>
  </SECTION>
</PAPER>
