<PAPER>
  <S sid="0">Combining Constituent Parsers</S>
  <ABSTRACT>
    <S sid="1" ssid="1">the output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006).</S>
    <S sid="2" ssid="2">We propose three ways to improve upon existing methods for parser combination.</S>
    <S sid="3" ssid="3">First, we propose a method of parse hybridization that recomproductions of conthereby preserving the structure of the output of the individual parsers to a greater extent.</S>
    <S sid="4" ssid="4">Second, we propose an efficient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection.</S>
    <S sid="5" ssid="5">Third, we extend these parser combination from multiple outputs to muloutputs.</S>
    <S sid="6" ssid="6">We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Parse quality impacts the quality of downstream applications such as syntax-based machine translation (Quirk and Corston-Oliver, 2006).</S>
    <S sid="8" ssid="2">Combining the output of multiple parsers can boost the accuracy of such applications.</S>
    <S sid="9" ssid="3">Parses can be combined in two ways: parse selection (selecting the best parse from the output of the individual parsers) or parse hybridization (constructing the best parse by recombining sub-sentential components from the output of the individual parsers).</S>
  </SECTION>
  <SECTION title="1.1 Related Work" number="2">
    <S sid="10" ssid="1">(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined.</S>
    <S sid="11" ssid="2">(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents.</S>
    <S sid="12" ssid="3">In this work, we propose three ways to improve upon existing methods for parser combination.</S>
    <S sid="13" ssid="4">First, while constituent recombination (Henderson and Brill, 1999; Sagae and Lavie, 2006) gives a significant improvement in f-score, it tends to flatten the structure of the individual parses.</S>
    <S sid="14" ssid="5">To illustrate, Figures 1 and 2 contrast the output of the Charniak parser with the output of constituent recombination on a sentence from WSJ section 24.</S>
    <S sid="15" ssid="6">We recombine context-free productions instead of constituents, producing trees containing only context-free productions that have been seen in the individual parsers&#8217; output (Figure 3).</S>
    <S sid="16" ssid="7">Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Minimum Bayes Risk (MBR) framework.</S>
    <S sid="17" ssid="8">Third, we extend these parser combination methods from 1-best outputs to n-best outputs.</S>
    <S sid="18" ssid="9">We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus.</S>
  </SECTION>
  <SECTION title="2 Parse Selection" number="3">
    <S sid="19" ssid="1">In the MBR framework, although the true reference parse is unknown, we assume that the individual parsers&#8217; output forms a reasonable distribution over possible reference parses.</S>
    <S sid="20" ssid="2">We compute the expected f-score of each parse tree pi using this distribution: where f(pi, pj) is the f-score of parse pi with respect to parse pj and pr(pj) is the prior probability of parse pj.</S>
    <S sid="21" ssid="3">We estimate pr(pj) as follows: pr(pj) = pr(parserk) &#183; pr(pj|parserk), where parserk is the parser generating pj.</S>
    <S sid="22" ssid="4">We set pr(parserk) according to the proportion of sentences in the development set for which the 1-best output of parserk achieves the highest f-score of any individual parser, breaking ties randomly.</S>
    <S sid="23" ssid="5">When n = 1, pr(pj|parserk) = 1 for all pj; when n &gt; 1 we must estimate pr(pj|parserk), the distribution over parses in the n-best list output by any given parser.</S>
    <S sid="24" ssid="6">We estimate this distribution using the model score, or log probability, given by parserk to each entry pj in its n-best list: We tune &#945; on a development set to maximize fscore,1 and select the parse pi with highest expected f-score.</S>
    <S sid="25" ssid="7">Computing exact expected f-score requires O(m2) operations per sentence, where m is the number of parses being combined.</S>
    <S sid="26" ssid="8">We can compute an approximate expected f-score in O(m) time.</S>
    <S sid="27" ssid="9">To do so, we compute expected precision for all parses in O(m) time by associating with each unique constituent ci a list of parses in which it occurs, plus the total probability qi of those parses.</S>
    <S sid="28" ssid="10">For each parse p associated with ci, we increment the expected precision of that parse by qi/size(p).</S>
    <S sid="29" ssid="11">This computation yields the same result as the O(m2) algorithm.</S>
    <S sid="30" ssid="12">We carry out a similar operation for expected recall.</S>
    <S sid="31" ssid="13">We then compute the harmonic mean of expected precision and expected recall, which closely approximates the true expected f-score.</S>
  </SECTION>
  <SECTION title="3 Constituent Recombination" number="4">
    <S sid="32" ssid="1">(Henderson and Brill, 1999) convert each parse into constituents with syntactic labels and spans, and weight each constituent by summing pr(parserk) over all parsers k in whose output the constituent appears.</S>
    <S sid="33" ssid="2">They include all constituents with weight above a threshold t = m+1 2 , where m is the number of input parses, in the combined parse.</S>
    <S sid="34" ssid="3">(Sagae and Lavie, 2006) extend this method by tuning t on a development set to maximize fscore.2 They populate a chart with constituents whose weight meets the threshold, and use a CKYstyle parsing algorithm to find the heaviest tree, where the weight of a tree is the sum of its constituents&#8217; weights.</S>
    <S sid="35" ssid="4">Parsing is not constrained by a grammar; any context-free production is permitted.</S>
    <S sid="36" ssid="5">Thus, the combined parses may contain context-free productions not seen in the individual parsers&#8217; outputs.</S>
    <S sid="37" ssid="6">While this failure to preserve the structure of individual parses does not affect f-score, it may hinder downstream applications.</S>
    <S sid="38" ssid="7">To extend this method from 1-best to n-best lists, we weight each constituent by summing pr(parserk)&#183;pr(pj|parserk) overall parses pj generated by parserk in which the constituent appears.</S>
  </SECTION>
  <SECTION title="4 Context-Free Production Recombination" number="5">
    <S sid="39" ssid="1">To ensure that all context-free productions in the combined parses have been seen in the individual parsers&#8217; outputs, we recombine context-free productions rather than constituents.</S>
    <S sid="40" ssid="2">We convert each parse into context-free productions, labelling each constituent in the production with its span and syntactic category and weighting each production by summing pr(parserk) &#183; pr(pj|parserk) over all parses pj generated by parserk in which the production appears.</S>
    <S sid="41" ssid="3">We re-parse the sentence with these productions, returning the heaviest tree (where the weight of a tree is the sum of its context-free productions&#8217; weights).</S>
    <S sid="42" ssid="4">We optimize f-score by varying the tradeoff between precision and recall using a derivation length penalty, which we tune on a development set.3</S>
  </SECTION>
  <SECTION title="5 Experiments" number="6">
    <S sid="43" ssid="1">Table 1 illustrates the 5 parsers used in our combination experiments and the f-scores of their 1-best output on our data sets.</S>
    <S sid="44" ssid="2">We use the n-best output of the Berkeley, Charniak, and Soricut parsers, and the 1-best output of the Bikel and Stanford parsers.</S>
    <S sid="45" ssid="3">All parsers were trained on the standard WSJ training sections.</S>
    <S sid="46" ssid="4">We use two corpora: the WSJ (sections 24 and 23 are the development and test sets, respectively) and English text from the LDC2007T02 Chinese-English parallel corpus (the development and test sets contain 400 sentences each).</S>
  </SECTION>
  <SECTION title="6 Discussion &amp; Conclusion" number="7">
    <S sid="47" ssid="1">Results are shown in Tables 2, 3, and 4.</S>
    <S sid="48" ssid="2">On both test sets, constituent recombination achieves the best f-score (1.0 points on WSJ test and 2.3 points on Chinese-English test), followed by context-free production combination, then parse selection, though the differences in f-score among the combination methods are not statistically significant.</S>
    <S sid="49" ssid="3">Increasing the n-best list size from 1 to 10 improves parse selection and context-free production recombination, though further increasing n does not, in general, help.4 Chinese-English test set f-score gets a bigger boost from combination than WSJ test set f-score, perhaps because the best individual parser&#8217;s baseline f-score is lower on the out-of-domain data.</S>
    <S sid="50" ssid="4">We have presented an algorithm for parse hybridization by recombining context-free productions.</S>
    <S sid="51" ssid="5">While constituent recombination results in the highest f-score of the methods explored, contextfree production recombination produces trees which better preserve the syntactic structure of the individual parses.</S>
    <S sid="52" ssid="6">We have also presented an efficient linear-time algorithm for selecting the parse with maximum expected f-score.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="53" ssid="1">We thank Steven Abney, John Henderson, and Kenji Sagae for helpful discussions.</S>
    <S sid="54" ssid="2">This research was supported by DARPA (contract HR0011-06-C0022) and by NSF ITR (grant IIS-0428020).</S>
    <S sid="55" ssid="3">4These diminishing gains in f-score as n increases reflect the diminishing gains in f-score of the oracle parse produced by each individual parser as n increases.</S>
  </SECTION>
</PAPER>
