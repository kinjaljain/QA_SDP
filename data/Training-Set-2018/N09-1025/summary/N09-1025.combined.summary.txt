11,001 New Features for Statistical Machine Translation We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and+1.1 B, respectively. We analyze the impact of the new features and the performance of the learning algorithm. This paper talks about 11,001 New Features for Statistical Machine Translation. For this they use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and a syntax-based translation system. We add more than 250 features to improve a syntax-based MT system—already the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 Bleu. They also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bleu improvement. These features are discriminatively trained using MIRA, and led to significant improvements. The authors also provide a closer look at the results to see how the new features qualitatively improved translation quality. We add more than 250 features to improve a syntax- based MT system—already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations. 