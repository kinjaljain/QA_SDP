A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. The paper 'A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC ' by Shaojun Zhao and Daniel Gildea proposes a model that adds fertility feature to HMM(Hidden Markov Model).Their model is a combination of HMM and IBM model 4 for achieving less alignment error rate  than HMM and making it easier than IBM model 4.They use Gibbs sampling method than the neighborhood method used in IBM Model 4.They evaluate their model by computing the word alignment error rate (AER) as the word alignment evaluation criterion and machine translation quality.They have evaluated their fertility models on a Chinese-English corpus.The results from the experiment proves that the fertility hidden Markov model  runs faster and has lower AER than the HMM. Their model is thus much faster and easier to understand than IBM Model 4.While better word alignment results do not necessarily correspond to better translation quality,their translation results are comparable in translation quality to both the HMM and IBM Model 4. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Our model is a coherent generative model that combines the HMM and IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 911 October 2010. Qc 2010 Association for Computational Linguistics estimation. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (φI , φǫ, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. Our model has only one parameter for each target word, which can be learned more reliably. Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977). The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm. Instead, we do “batch learning”: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). 