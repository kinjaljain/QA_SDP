n<S sid = "0">Subjectivity Recognition onWord Senses via Semi-supervised Mincuts</S> We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Fangzhong Su and Katja Markert in their paper 'Subjectivity Recognition on Word Senses via Semi-supervised Mincuts' proposes the approach for supplementing WordNet entries with infor- mation on the subjectivity of its word senses.They propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.They  deal with classification at the word sense level,including subjectivity-ambiguous words.the main idea is binary classification with minimum cuts (Mincuts) in graphs which is based on the idea that similar items should be grouped in the same cut.The formulation of semi-supervised Mincut for sense subjectivity classification involves the steps:Selection of unlabeled data,Weighting of edges to the classification vertices and Assigning weights to WordNet relations.Their best result from Mincuts is 84.6%. We supplement WordNet entries with information on the subjectivity of its word senses. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework. Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories). We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. "<S sid ="72" ssid = "25">We define two vertices s (source) and t (sink),.</S> which correspond to the “subjective” and “objective” category, respectively. Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/˜cjlin/libsvm/. It includes 298 words with 703 objective and 358 subjective WordNet senses. Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2). 