Citance Number: 1 | Reference Article:  D08-1022.txt | Citing Article:  P09-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3.</S> | Reference Offset:  ['0','100'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 100 ssid = >Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1022.txt | Citing Article:  P13-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['32','129'] | Reference Text:  <S sid = 32 ssid = >Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).</S><S sid = 129 ssid = >We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1022.txt | Citing Article:  P12-3022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008).</S> | Reference Offset:  ['81','119'] | Reference Text:  <S sid = 81 ssid = >Inspired by the parsing literature on pruning (Charniak and Johnson, 2005; Huang, 2008) we penalize a rule r by the posterior probability of its tree fragment frag = lhs(r).</S><S sid = 119 ssid = >The final BLEU score results are shown in Table 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1022.txt | Citing Article:  P12-3022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['32','129'] | Reference Text:  <S sid = 32 ssid = >Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).</S><S sid = 129 ssid = >We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009).</S> | Reference Offset:  ['25','93'] | Reference Text:  <S sid = 25 ssid = >We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S><S sid = 93 ssid = >There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008).</S> | Reference Offset:  ['25','76'] | Reference Text:  <S sid = 25 ssid = >We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S><S sid = 76 ssid = >So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure.</S> | Reference Offset:  ['63','80'] | Reference Text:  <S sid = 63 ssid = >When we follow one of them to grow a fragment, there again will be multiple choices at each of its tail nodes.</S><S sid = 80 ssid = >In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences.</S> | Reference Offset:  ['102','117'] | Reference Text:  <S sid = 102 ssid = >To test the effect of forest-based rule extraction, we parse the training set into parse forests and use three levels of pruning thresholds: pe = 2, 5, 8.</S><S sid = 117 ssid = >To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest.</S> | Reference Offset:  ['50','117'] | Reference Text:  <S sid = 50 ssid = >Our experiments use composed rules.</S><S sid = 117 ssid = >To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008).</S> | Reference Offset:  ['25','76'] | Reference Text:  <S sid = 25 ssid = >We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S><S sid = 76 ssid = >So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input.</S> | Reference Offset:  ['0','89'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 89 ssid = >The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008).</S> | Reference Offset:  ['0','46'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 46 ssid = >Nodes with non-empty, contiguous, and faithful spans form the admissible set (shaded nodes in the figure), which serve as potential cut-points for rule extraction.3 With the admissible set computed, rule extraction is as simple as a depth-first traversal from the root: we “cut” the tree at all admissible nodes to form tree fragments and extract a rule for each fragment, with variables matching the admissible descendant nodes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest.</S> | Reference Offset:  ['98','100'] | Reference Text:  <S sid = 98 ssid = >We refer readers to Mi et al. (2008) for details of the decoding algorithm.</S><S sid = 100 ssid = >Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree.</S> | Reference Offset:  ['80','106'] | Reference Text:  <S sid = 80 ssid = >In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.</S><S sid = 106 ssid = >With pruning threshold pe = 8, forest-based extraction achieves a (case insensitive) BLEU score of 0.2533, which is an absolute improvement of 1.0% points over the 1-best baseline, and is statistically significant using the sign-test of Collins et al. (2005) (p < 0.01).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string.</S> | Reference Offset:  ['0','27'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 27 ssid = >The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008).</S> | Reference Offset:  ['24','120'] | Reference Text:  <S sid = 24 ssid = >Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models.</S><S sid = 120 ssid = >With both tree-based and forest-based decoding, rules extracted from forests significantly outperform those extracted from 1-best trees (p < 0.01).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mi and Huang (2008) extend the tree-based rule extraction.</S> | Reference Offset:  ['0','51'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 51 ssid = >We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest.</S> | Reference Offset:  ['0','59'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 59 ssid = >Like in tree-based extraction, we extract rules from a packed forest F in two steps: It turns out that the exact formulation developed for admissible set in the tree-based case can be applied to a forest without any change.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008).</S> | Reference Offset:  ['12','76'] | Reference Text:  <S sid = 12 ssid = >Typically, these models extract rules using parse trees from both or either side(s) of the bitext.</S><S sid = 76 ssid = >So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest.</S> | Reference Offset:  ['0','96'] | Reference Text:  <S sid = 0 ssid = >Forest-based Translation Rule Extraction</S><S sid = 96 ssid = >The conditional probability P(d  |T) decomposes into the product of rule probabilities: where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


