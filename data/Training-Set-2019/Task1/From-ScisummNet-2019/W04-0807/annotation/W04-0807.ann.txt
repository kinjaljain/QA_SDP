Citance Number: 1 | Reference Article:  W04-0807.txt | Citing Article:  W07-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007).</S> | Reference Offset:  ['5','27'] | Reference Text:  <S sid = 5 ssid = >This task is a follow-up to similar tasks organized during the SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Preiss and Yarowsky, 2001) evaluations.</S><S sid = 27 ssid = >Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-0807.txt | Citing Article:  W07-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances.</S> | Reference Offset:  ['22','24'] | Reference Text:  <S sid = 22 ssid = >Table 1 presents the number of words under each part of speech, and the average number of senses for each class.</S><S sid = 24 ssid = >Consequently, the training and test data sets made available for this task do not contain collocations as possible target words, but only single word units.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-0807.txt | Citing Article:  D09-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993).</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 16 ssid = >The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-0807.txt | Citing Article:  H05-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained.</S> | Reference Offset:  ['42','46'] | Reference Text:  <S sid = 42 ssid = >The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.</S><S sid = 46 ssid = >Precision and recall figures are provided for both fine grained and coarse grained scoring.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-0807.txt | Citing Article:  D07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)).</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 3 ssid = >We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-0807.txt | Citing Article:  W06-1649.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['27','56'] | Reference Text:  <S sid = 27 ssid = >Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.</S><S sid = 56 ssid = >We are particularly grateful to the National Science Foundation for their support under research grant IIS-0336793, and to the University of North Texas for a research grant that provided funding for contributor prizes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-0807.txt | Citing Article:  W06-1669.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method.</S> | Reference Offset:  ['7','42'] | Reference Text:  <S sid = 7 ssid = >2 Building a Sense Tagged Corpus with Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002) 1.</S><S sid = 42 ssid = >The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-0807.txt | Citing Article:  W07-2099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes.</S> | Reference Offset:  ['43','48'] | Reference Text:  <S sid = 43 ssid = >Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.</S><S sid = 48 ssid = >The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-0807.txt | Citing Article:  W06-3814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method.</S> | Reference Offset:  ['0','55'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 55 ssid = >We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-0807.txt | Citing Article:  W06-3814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do).</S> | Reference Offset:  ['42','48'] | Reference Text:  <S sid = 42 ssid = >The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.</S><S sid = 48 ssid = >The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-0807.txt | Citing Article:  W06-3814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems).</S> | Reference Offset:  ['42','48'] | Reference Text:  <S sid = 42 ssid = >The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.</S><S sid = 48 ssid = >The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-0807.txt | Citing Article:  P07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems).</S> | Reference Offset:  ['37','48'] | Reference Text:  <S sid = 37 ssid = >Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.</S><S sid = 48 ssid = >The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-0807.txt | Citing Article:  P07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004).</S> | Reference Offset:  ['4','36'] | Reference Text:  <S sid = 4 ssid = >The goal of this task was to create a framework for evaluation of systems that perform targeted Word Sense Disambiguation.</S><S sid = 36 ssid = >27 teams participated in this word sense disambiguation task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-0807.txt | Citing Article:  P06-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004).</S> | Reference Offset:  ['0','44'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 44 ssid = >The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-0807.txt | Citing Article:  P06-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns.</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 16 ssid = >The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-0807.txt | Citing Article:  W07-2092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length.</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 20 ssid = >The main reason motivating selection of a different sense inventory is the weak verb performance of systems participating in the English lexical sample in SENSEVAL-2, which may be due to the high number of senses defined for verbs in the WordNet sense inventory.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-0807.txt | Citing Article:  N10-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007).</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 16 ssid = >The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-0807.txt | Citing Article:  W06-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations.</S> | Reference Offset:  ['0','44'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 44 ssid = >The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-0807.txt | Citing Article:  W06-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives).</S> | Reference Offset:  ['0','44'] | Reference Text:  <S sid = 0 ssid = >The Senseval-3 English Lexical Sample Task</S><S sid = 44 ssid = >The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-0807.txt | Citing Article:  W06-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004).</S> | Reference Offset:  ['25','42'] | Reference Text:  <S sid = 25 ssid = >This is a somewhat different definition of the task as compared to previous similar evaluations; the difference may have an impact on the overall performance achieved by systems participating in the task.</S><S sid = 42 ssid = >The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.</S> | Discourse Facet:  NA | Annotator: Automatic


