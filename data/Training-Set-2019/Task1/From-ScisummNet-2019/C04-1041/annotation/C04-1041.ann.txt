Citance Number: 1 | Reference Article:  C04-1041.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.</S> | Reference Offset:  ['51','124'] | Reference Text:  <S sid = 51 ssid = >This simple approach has the advantage of being very efficient, and we findthat it is accurate enough to enable highly accu rate parsing.</S><S sid = 124 ssid = >In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1041.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parser used in this paper is described in Clark and Curran (2004b).</S> | Reference Offset:  ['27','66'] | Reference Text:  <S sid = 27 ssid = >Here we use the Maximum En tropy models described in Curran and Clark (2003).</S><S sid = 66 ssid = >The parser is described in detail in Clark and Curran (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1041.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories.</S> | Reference Offset:  ['17','27'] | Reference Text:  <S sid = 17 ssid = >To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis.</S><S sid = 27 ssid = >Here we use the Maximum En tropy models described in Curran and Clark (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1041.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG.</S> | Reference Offset:  ['69','78'] | Reference Text:  <S sid = 69 ssid = >Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG.</S><S sid = 78 ssid = >In Clark and Curran (2004) we describe a discrim inative method for estimating the parameters of a log-linear parsing model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1041.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word).</S> | Reference Offset:  ['47','49'] | Reference Text:  <S sid = 47 ssid = >Theper-word accuracy is between 91 and 92% on un seen data in CCGbank; however, Clark (2002) shows this is not high enough for integration into a parser since the large number of incorrect categories results in a significant loss in coverage.</S><S sid = 49 ssid = >For each word inthe sentence, the multi-tagger assigns all those cat ? CATS/ ACC SENT ACC SENT WORD ACC (POS) ACC 0.1 1.4 97.0 62.6 96.4 57.4 0.075 1.5 97.4 65.9 96.8 60.6 0.05 1.7 97.8 70.2 97.3 64.4 0.01 2.9 98.5 78.4 98.2 74.2 0.01k=100 3.5 98.9 83.6 98.6 78.9 0 21.9 99.1 84.8 99.0 83.0 Table 2: Supertagger accuracy on section 00 egories whose probability according to (1) is within some factor, ?, of the highest probability category for the word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1041.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery.</S> | Reference Offset:  ['8','76'] | Reference Text:  <S sid = 8 ssid = >Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003).Supertagging accuracy is relatively high for man ually constructed LTAGs (Bangalore and Joshi,1999).</S><S sid = 76 ssid = >Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1041.txt | Citing Article:  C10-2051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004).</S> | Reference Offset:  ['66','69'] | Reference Text:  <S sid = 66 ssid = >The parser is described in detail in Clark and Curran (2004).</S><S sid = 69 ssid = >Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data.</S> | Reference Offset:  ['76','124'] | Reference Text:  <S sid = 76 ssid = >Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures.</S><S sid = 124 ssid = >In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data.</S> | Reference Offset:  ['27','66'] | Reference Text:  <S sid = 27 ssid = >Here we use the Maximum En tropy models described in Curran and Clark (2003).</S><S sid = 66 ssid = >The parser is described in detail in Clark and Curran (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a).</S> | Reference Offset:  ['21','67'] | Reference Text:  <S sid = 21 ssid = >Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories to gether using CCG?s combinatory rules.1 The first stage can be accomplished by simply assigning to each word all categories from the word?s entry in the lexicon (Hockenmaier, 2003).</S><S sid = 67 ssid = >It takes POS tagged sentences as input with each word assigned a set of lexical categories.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training.</S> | Reference Offset:  ['1','12'] | Reference Text:  <S sid = 1 ssid = >This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.</S><S sid = 12 ssid = >Our wide-coverage CCG parser uses a log-linear model to select an analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model.</S> | Reference Offset:  ['69','70'] | Reference Text:  <S sid = 69 ssid = >Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG.</S><S sid = 70 ssid = >In this paper weuse the normal-form model, which defines proba bilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b).</S> | Reference Offset:  ['48','74'] | Reference Text:  <S sid = 48 ssid = >Clark (2002) shows how the models in (1) can be used to define a multi-tagger which can assign more than one category to a word.</S><S sid = 74 ssid = >For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002).</S> | Reference Offset:  ['77','86'] | Reference Text:  <S sid = 77 ssid = >3.1 Model Estimation.</S><S sid = 86 ssid = >is the log-likelihood of model ?, and G(?)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure.</S> | Reference Offset:  ['68','74'] | Reference Text:  <S sid = 68 ssid = >A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart.</S><S sid = 74 ssid = >For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies.</S> | Reference Offset:  ['72','83'] | Reference Text:  <S sid = 72 ssid = >The normal-form derivations in CCGbank provide the gold standard training data.</S><S sid = 83 ssid = >, S m, to gether with gold standard normal-form derivations, d1, . . .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b).</S> | Reference Offset:  ['30','66'] | Reference Text:  <S sid = 30 ssid = >2.1 The Lexical Category Set.</S><S sid = 66 ssid = >The parser is described in detail in Clark and Curran (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect.</S> | Reference Offset:  ['32','56'] | Reference Text:  <S sid = 32 ssid = >Following Clark (2002), we apply a fre quency cutoff to the training set, only using thosecategories which appear at least 10 times in sections 2-21.</S><S sid = 56 ssid = >A value of k = 20was used in this work, and sections 2-21 of CCG bank were used as training data.Table 2 gives the per-word accuracy (acc) on sec tion 00 for various values of ?, together with the average number of categories per word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes.</S> | Reference Offset:  ['30','44'] | Reference Text:  <S sid = 30 ssid = >2.1 The Lexical Category Set.</S><S sid = 44 ssid = >Features are defined for each word in the window and for the POS tag of each word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C04-1041.txt | Citing Article:  N06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.</S> | Reference Offset:  ['17','68'] | Reference Text:  <S sid = 17 ssid = >To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis.</S><S sid = 68 ssid = >A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart.</S> | Discourse Facet:  NA | Annotator: Automatic


