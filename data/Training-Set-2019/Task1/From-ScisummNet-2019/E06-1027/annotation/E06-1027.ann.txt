Citance Number: 1 | Reference Article:  E06-1027.txt | Citing Article:  W07-2022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The unigrams used for sentence/headline classification were learned from WordNet (Fellbaum, 1998) dictionary entries using the STEP system described in (Andreevskaia and Bergler, 2006b).</S> | Reference Offset:  ['10','38'] | Reference Text:  <S sid = 10 ssid = >The proposed approach based on the fuzzy logic(Zadeh, 1987) is used here to assign fuzzy sen timent tags to all words in WordNet (Fellbaum, 1998), that is it assigns sentiment tags and a degreeof centrality of the annotated words to the sentiment category.</S><S sid = 38 ssid = >WordNet Entries Word lists for sentiment tagging applications can be compiled using different methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1027.txt | Citing Article:  W07-2022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since sentiment-bearing words in English have different degree of centrality to the category of sentiment, we have constructed a measure of word centrality to the category of positive or negative sentiment described in our earlier work (Andreevskaiaand Bergler, 2006a).</S> | Reference Offset:  ['9','73'] | Reference Text:  <S sid = 9 ssid = >In this paper, we challenge the applicability of this assump tion to the semantic category of sentiment, whichconsists of positive, negative and neutral subcate gories, and present a dictionary-based Sentiment Tag Extraction Program (STEP) that we use to generate a fuzzy set of English sentiment-bearing words for the use in sentiment tagging systems 1.</S><S sid = 73 ssid = >centrality to the semantic category The approach to sentiment category as a fuzzyset ascribes the category of sentiment some spe cific structural properties.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1027.txt | Citing Article:  P14-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','154'] | Reference Text:  <S sid = 48 ssid = >At the second pass, the system goes through all WordNet glosses and identifies the entries that contain in their definitions the sentiment-bearing words from the extended seed list and adds these head words (or rather, lexemes) to the corresponding category ? positive, negative or neutral (the remainder).</S><S sid = 154 ssid = >Therefore, the disagreement between the annota tors does not necessarily reflect a quality problem in human annotation, but rather a structural property of the semantic category.This suggests that inter-annotator disagree ment rates can serve as an important source of empirical information about the structural properties of the semantic category and canhelp define and validate fuzzy sets of seman tic category members for a number of NLP tasks and applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1027.txt | Citing Article:  C10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At lexical level, Andreevskaia and Bergler (2006) exploit an algorithm for extracting sentiment-bearing adjectives from the WordNet based on fuzzy logic.</S> | Reference Offset:  ['10','129'] | Reference Text:  <S sid = 10 ssid = >The proposed approach based on the fuzzy logic(Zadeh, 1987) is used here to assign fuzzy sen timent tags to all words in WordNet (Fellbaum, 1998), that is it assigns sentiment tags and a degreeof centrality of the annotated words to the sentiment category.</S><S sid = 129 ssid = >The list of sentiment-bearing adjectives.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1027.txt | Citing Article:  N09-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use.</S> | Reference Offset:  ['39','75'] | Reference Text:  <S sid = 39 ssid = >Automatic methods of sentiment annotation at the word level can be grouped into two major categories: (1) corpus-based approaches and (2) dictionary-based3We use the term crisp set to refer to traditional, non fuzzy sets 210 approaches.</S><S sid = 75 ssid = >Second, the membership of these cen tral words in the category is less ambiguous than the membership of more peripheral words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1027.txt | Citing Article:  N09-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance.</S> | Reference Offset:  ['31','83'] | Reference Text:  <S sid = 31 ssid = >in importance of various sentiment markers have crystallized in two main approaches: automatic assignment of weights based on some statistical criterion ((Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002; Kim and Hovy, 2004), and others) or manual annotation (Subasic andHuettner, 2001).</S><S sid = 83 ssid = >With multiple bootstrapping runs on different seed 4It is consistent with the observation by Kim and Hovy (2004) who noticed that, when positives and neutrals were collapsed into the same category opposed to negatives, the agreement between human annotators rose by 12%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1027.txt | Citing Article:  C08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use.</S> | Reference Offset:  ['39','75'] | Reference Text:  <S sid = 39 ssid = >Automatic methods of sentiment annotation at the word level can be grouped into two major categories: (1) corpus-based approaches and (2) dictionary-based3We use the term crisp set to refer to traditional, non fuzzy sets 210 approaches.</S><S sid = 75 ssid = >Second, the membership of these cen tral words in the category is less ambiguous than the membership of more peripheral words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1027.txt | Citing Article:  N09-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al, 2006), sentiment classification of phrases (Wilson et al, 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a).</S> | Reference Offset:  ['41','96'] | Reference Text:  <S sid = 41 ssid = >The majority of dictionary-based approaches use WordNet information, especially, synsets and hierarchies, to acquire sentiment-marked words (Hu and Liu, 2004; Valitutti et al, 2004; Kim and Hovy, 2004) or to measure the similarity between candidate words and sentiment-bearing words such as good and bad (Kamps et al, 2004).In this paper, we propose an approach to sentiment annotation of WordNet entries that was implemented and tested in the Semantic Tag Extrac tion Program (STEP).</S><S sid = 96 ssid = >Figure 1: Accuracy of word sentiment tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1027.txt | Citing Article:  D10-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses</S><S sid = 15 ssid = >In this paper we approach the category of sentiment as one of such fuzzy categories wheresome words ? such as good, bad ? are very central, prototypical members, while other, less central words may be interpreted differently by differ ent people.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1027.txt | Citing Article:  P08-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags in Andreevskaia and Bergler (2006).</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses</S><S sid = 10 ssid = >The proposed approach based on the fuzzy logic(Zadeh, 1987) is used here to assign fuzzy sen timent tags to all words in WordNet (Fellbaum, 1998), that is it assigns sentiment tags and a degreeof centrality of the annotated words to the sentiment category.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1027.txt | Citing Article:  W11-1703.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','154'] | Reference Text:  <S sid = 48 ssid = >At the second pass, the system goes through all WordNet glosses and identifies the entries that contain in their definitions the sentiment-bearing words from the extended seed list and adds these head words (or rather, lexemes) to the corresponding category ? positive, negative or neutral (the remainder).</S><S sid = 154 ssid = >Therefore, the disagreement between the annota tors does not necessarily reflect a quality problem in human annotation, but rather a structural property of the semantic category.This suggests that inter-annotator disagree ment rates can serve as an important source of empirical information about the structural properties of the semantic category and canhelp define and validate fuzzy sets of seman tic category members for a number of NLP tasks and applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1027.txt | Citing Article:  P10-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A similar method is presented in (Andreevskaia and Bergler, 2006) where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds.</S> | Reference Offset:  ['0','46'] | Reference Text:  <S sid = 0 ssid = >Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses</S><S sid = 46 ssid = >This list is augmented during thefirst pass by adding synonyms, antonyms and hy ponyms of the seed words supplied in WordNet.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1027.txt | Citing Article:  P13-2090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','154'] | Reference Text:  <S sid = 48 ssid = >At the second pass, the system goes through all WordNet glosses and identifies the entries that contain in their definitions the sentiment-bearing words from the extended seed list and adds these head words (or rather, lexemes) to the corresponding category ? positive, negative or neutral (the remainder).</S><S sid = 154 ssid = >Therefore, the disagreement between the annota tors does not necessarily reflect a quality problem in human annotation, but rather a structural property of the semantic category.This suggests that inter-annotator disagree ment rates can serve as an important source of empirical information about the structural properties of the semantic category and canhelp define and validate fuzzy sets of seman tic category members for a number of NLP tasks and applications.</S> | Discourse Facet:  NA | Annotator: Automatic


