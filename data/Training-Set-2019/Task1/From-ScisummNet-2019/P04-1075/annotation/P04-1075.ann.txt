Citance Number: 1 | Reference Article:  P04-1075.txt | Citing Article:  W06-1660.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is only recently employed in NER (Shen et al, 2004).</S> | Reference Offset:  ['9','30'] | Reference Text:  <S sid = 9 ssid = >1999; Tang et al. 2002; Steedman et al.</S><S sid = 30 ssid = >Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-1075.txt | Citing Article:  D10-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This issue was previously addressed in Shen et al (2004) in the context of named-entity recognition, where they used a two-step procedure to first select the most informative and representative samples, followed by a diversity filter.</S> | Reference Offset:  ['0','107'] | Reference Text:  <S sid = 0 ssid = >Multi-Criteria-Based Active Learning For Named Entity Recognition</S><S sid = 107 ssid = >When selecting a machine-annotated named entity, we compare it with all previously selected named entities in the current batch.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-1075.txt | Citing Article:  D07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In a more recent study, Shen et al (2004) consider AL for entity recognition based on Support Vector Machines.</S> | Reference Offset:  ['24','30'] | Reference Text:  <S sid = 24 ssid = >We build our NER model using Support Vector Machines (SVM).</S><S sid = 30 ssid = >Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-1075.txt | Citing Article:  D07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >In SVM, only the support vectors are useful for the classification, which is different from statistical models.</S><S sid = 193 ssid = >Another interesting work is to study when to stop active learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-1075.txt | Citing Article:  W07-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given the variety of methods that are available for generating training data efficiently automatically using extant domain resources (Morgan et al, 2004) or semi-automatically (active learning approaches like Shen et al (2004) or systems using seed rules such as Mikheev et al.</S> | Reference Offset:  ['9','16'] | Reference Text:  <S sid = 9 ssid = >1999; Tang et al. 2002; Steedman et al.</S><S sid = 16 ssid = >Many existing work in the area focus on two approaches: certainty-based methods (Thompson et al. 1999; Tang et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-1075.txt | Citing Article:  D09-1158.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Active learning, which has been applied to the problem of NER in (Shen et al, 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive.</S> | Reference Offset:  ['30','50'] | Reference Text:  <S sid = 30 ssid = >Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.</S><S sid = 50 ssid = >The support vectors can later be used to classify the test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-1075.txt | Citing Article:  C08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Diversity measures as proposed by (Shen et al, 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER.</S> | Reference Offset:  ['41','181'] | Reference Text:  <S sid = 41 ssid = >In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.</S><S sid = 181 ssid = >On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-1075.txt | Citing Article:  W06-2209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al, 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data.</S> | Reference Offset:  ['26','185'] | Reference Text:  <S sid = 26 ssid = >The results in both MUC6 and GENIA show that the amount of the labeled training data can be reduced by at least 80% without degrading the quality of the named entity recognizer.</S><S sid = 185 ssid = >In this paper, we study the active learning in a more complex NLP task, named entity recognition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-1075.txt | Citing Article:  W06-3328.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to circumvent this obstacle several approaches have been presented, among them active learning (Shen et al, 2004) and rule-based systems encoding domain specific knowledge (Gaizauskas et al, 2003).</S> | Reference Offset:  ['9','16'] | Reference Text:  <S sid = 9 ssid = >1999; Tang et al. 2002; Steedman et al.</S><S sid = 16 ssid = >Many existing work in the area focus on two approaches: certainty-based methods (Thompson et al. 1999; Tang et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-1075.txt | Citing Article:  D10-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Shen et al (2004) combine multiple criteria to measure the informativeness, representativeness, and diversity of examples in active learning for named entity recognition.</S> | Reference Offset:  ['0','116'] | Reference Text:  <S sid = 0 ssid = >Multi-Criteria-Based Active Learning For Named Entity Recognition</S><S sid = 116 ssid = >In this section, we will study how to combine and strike a proper balance between these criteria, viz. informativeness, representativeness and diversity, to reach the maximum effectiveness on NER active learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-1075.txt | Citing Article:  D10-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, in order to avoid recursion and over-complexity, we employ a diversity-motivated intra-stratum sampling scheme (Shen et al, 2004), called KDN (K-diverse neighbors), which aims to maximize the training utility of all seeds from a stratum.</S> | Reference Offset:  ['79','92'] | Reference Text:  <S sid = 79 ssid = >We employ the dynamic time warping (DTW) algorithm (Rabiner et al. 1978) to find an optimal alignment between the words in the sequences which maximize the accumulated similarity degree between the sequences.</S><S sid = 92 ssid = >Diversity criterion is to maximize the training utility of a batch.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-1075.txt | Citing Article:  E06-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al, 2005a) used self-training and co-training to detect and classify named entities in news domain, (Shen et al, 2004) conducted experiments with multi-criteria-based active learning for biomedical NER.</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Multi-Criteria-Based Active Learning For Named Entity Recognition</S><S sid = 9 ssid = >1999; Tang et al. 2002; Steedman et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-1075.txt | Citing Article:  C08-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >In SVM, only the support vectors are useful for the classification, which is different from statistical models.</S><S sid = 193 ssid = >Another interesting work is to study when to stop active learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-1075.txt | Citing Article:  C08-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Shen et al (2004) proposed an approach to selecting examples based on informativeness, representativeness and diversity criteria.</S> | Reference Offset:  ['41','186'] | Reference Text:  <S sid = 41 ssid = >In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.</S><S sid = 186 ssid = >We propose a multi-criteria-based approach to select examples based on their informativeness, representativeness and diversity, which are incorporated all together by two strategies (local and global).</S> | Discourse Facet:  NA | Annotator: Automatic


