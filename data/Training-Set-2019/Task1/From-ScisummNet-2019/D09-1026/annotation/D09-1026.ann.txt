Citance Number: 1 | Reference Article:  D09-1026.txt | Citing Article:  N10-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).</S> | Reference Offset:  ['3','37'] | Reference Text:  <S sid = 3 ssid = >This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags.</S><S sid = 37 ssid = >Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D09-1026.txt | Citing Article:  P14-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','197'] | Reference Text:  <S sid = 73 ssid = >The derivation of the algorithm so far has focused on its relationship to LDA.</S><S sid = 197 ssid = >This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D09-1026.txt | Citing Article:  D12-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','197'] | Reference Text:  <S sid = 73 ssid = >The derivation of the algorithm so far has focused on its relationship to LDA.</S><S sid = 197 ssid = >This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D09-1026.txt | Citing Article:  D12-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Labeled LDA (LLDA) (Ramage et al 2009a) can be used to solve this problem.</S> | Reference Offset:  ['144','188'] | Reference Text:  <S sid = 144 ssid = >Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.</S><S sid = 188 ssid = >By themselves, most words used here have a higher probability in programming than in design.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D09-1026.txt | Citing Article:  P13-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Modeling Tweets in a Latent Space: Ramage et al (2010) also use hash tags to improve the latent representation of tweets in a LDA framework, Labeled-LDA (Ramage et al, 2009), treating each hashtag as a label.</S> | Reference Offset:  ['3','144'] | Reference Text:  <S sid = 3 ssid = >This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags.</S><S sid = 144 ssid = >Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D09-1026.txt | Citing Article:  W11-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA (LLDA) (Ramage et al, 2009) and supervised LDA (sLDA) (Blei and McAuliffe, 2008) are powerful generative models that capture the underlying semantics of texts.</S> | Reference Offset:  ['23','37'] | Reference Text:  <S sid = 23 ssid = >Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label.</S><S sid = 37 ssid = >Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D09-1026.txt | Citing Article:  P12-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','197'] | Reference Text:  <S sid = 73 ssid = >The derivation of the algorithm so far has focused on its relationship to LDA.</S><S sid = 197 ssid = >This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D09-1026.txt | Citing Article:  D11-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al, 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity's context across its mentions.</S> | Reference Offset:  ['13','144'] | Reference Text:  <S sid = 13 ssid = >One simple approach to these challenges can be found in models that explicitly address the credit attribution problem by associating individual words in a document with their most appropriate labels.</S><S sid = 144 ssid = >Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D09-1026.txt | Citing Article:  D11-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Distant Supervision with Topic Models: To model unlabeled entities and their possible types, we apply LabeledLDA (Ramage et al, 2009), constraining each entity's distribution over topics based on its set of possible types according to Freebase.</S> | Reference Offset:  ['26','38'] | Reference Text:  <S sid = 26 ssid = >A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.</S><S sid = 38 ssid = >Unlike LDA, L-LDA incorporates supervision by simply constraining the topic model to use only those topics that correspond to a document’s (observed) label set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D09-1026.txt | Citing Article:  P11-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In other models, this input is sometimes used to "fix," i.e. deterministically hold constant topic assignments (Ramage et al, 2009).</S> | Reference Offset:  ['26','47'] | Reference Text:  <S sid = 26 ssid = >A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.</S><S sid = 47 ssid = >Since the word-topic assignments zi (see step 9 in Table 1) are drawn from this distribution, this restriction ensures that all the topic assignments are limited to the document’s labels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D09-1026.txt | Citing Article:  E12-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To enable this, we first take class labeled data (doesn't need to be multi-class labeled data unlike (Ramage et al 2009)) and identify the discriminating features for each class.</S> | Reference Offset:  ['92','167'] | Reference Text:  <S sid = 92 ssid = >We will refer to this collection of data as the del.icio.us tag dataset.</S><S sid = 167 ssid = >It is worth noting that the Yahoo datasets are skewed by construction to contain many documents with highly overlapping content: because each collection is within the same super-class such as “Arts”, “Business”, etc., each sub-categories’ of the named Yahoo directory categories.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D09-1026.txt | Citing Article:  E12-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of these models, the most related one to SeededLDA is the Labeled LDA model (Ramage et al 2009).</S> | Reference Offset:  ['26','144'] | Reference Text:  <S sid = 26 ssid = >A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.</S><S sid = 144 ssid = >Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D09-1026.txt | Citing Article:  P11-2118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al, 2009a) or FixedhLDA (Reisinger and Pas? ca, 2009) where the set of topics associated with a document is known a priori.</S> | Reference Offset:  ['26','97'] | Reference Text:  <S sid = 26 ssid = >A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.</S><S sid = 97 ssid = >Labeled LDA’s topics are named by their associated tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D09-1026.txt | Citing Article:  P11-2118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','197'] | Reference Text:  <S sid = 73 ssid = >The derivation of the algorithm so far has focused on its relationship to LDA.</S><S sid = 197 ssid = >This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D09-1026.txt | Citing Article:  P13-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al, 2009), and sequential (Du et al, 2010) topic models.</S> | Reference Offset:  ['144','145'] | Reference Text:  <S sid = 144 ssid = >Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.</S><S sid = 145 ssid = >(2008)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D09-1026.txt | Citing Article:  P12-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc.</S> | Reference Offset:  ['26','144'] | Reference Text:  <S sid = 26 ssid = >A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.</S><S sid = 144 ssid = >Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D09-1026.txt | Citing Article:  D11-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al (2009)).</S> | Reference Offset:  ['97','100'] | Reference Text:  <S sid = 97 ssid = >Labeled LDA’s topics are named by their associated tag.</S><S sid = 100 ssid = >Figure 2 shows the top words associated with 20 topics learned by Labeled LDA and 20 topics learned by unsupervised LDA on the del.icio.us document collection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D09-1026.txt | Citing Article:  D11-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien et al, 2008).</S> | Reference Offset:  ['23','101'] | Reference Text:  <S sid = 23 ssid = >Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label.</S><S sid = 101 ssid = >Labeled LDA’s topics are directly named with the tag that corresponds to each topic, an improvement over standard practice of inferring the topic name by inspection (Mei et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D09-1026.txt | Citing Article:  D11-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','197'] | Reference Text:  <S sid = 73 ssid = >The derivation of the algorithm so far has focused on its relationship to LDA.</S><S sid = 197 ssid = >This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D09-1026.txt | Citing Article:  D11-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >L-LDA (Ramage et al, 2009) extends standard LDA to include supervision for specific target categories, yet in a different way: (i) The generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.</S> | Reference Offset:  ['43','62'] | Reference Text:  <S sid = 43 ssid = >The generative process for the algorithm is found in Table 1.</S><S sid = 62 ssid = >Although the equation above looks exactly the same as that of LDA, we have an important distinction in that, the target topic j is restricted to belong to the set of labels, i.e., j ∈ X(d).</S> | Discourse Facet:  NA | Annotator: Automatic


