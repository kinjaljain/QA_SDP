Citance Number: 1 | Reference Article:  W07-0718.txt | Citing Article:  W08-0312.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Callison-Burch et al, 2007) reported that the inter coder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation.</S> | Reference Offset:  ['145','146'] | Reference Text:  <S sid = 145 ssid = >The agreement on the other two types of manual evaluation that we introduced were considerably better.</S><S sid = 146 ssid = >The both the sentence and constituent ranking had moderate inter-annotator agreement and substantial intra-annotator agreement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W07-0718.txt | Citing Article:  W12-4204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al, 2007), the technique and its evaluation are still far from satisfactory.</S> | Reference Offset:  ['138','146'] | Reference Text:  <S sid = 138 ssid = >For intra-annotator agreement we did similarly, but gathered items that were annotated on multiple occasions by a single annotator.</S><S sid = 146 ssid = >The both the sentence and constituent ranking had moderate inter-annotator agreement and substantial intra-annotator agreement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W07-0718.txt | Citing Article:  W09-2301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Moses system with a 4-gram language model and a distance-6 lexical reordering model ("lex RO") scores similarly to state-of-the-art systems of this type on the test 2007 French English data (Callison-Burch et al, 2007).</S> | Reference Offset:  ['1','126'] | Reference Text:  <S sid = 1 ssid = >j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.</S><S sid = 126 ssid = >The following systems were the best performing for the different language pairs: SYSTRAN was ranked the highest in German-English, University of Catalonia was ranked the highest in Spanish-English, LIMSI-CNRS was ranked highest in French-English, and the University of Maryland and a commercial system were the highest for agreement for the different types of manual evaluation Czech-English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W07-0718.txt | Citing Article:  W08-0324.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This behavior appears to be consistent on the test 2007 and nc-test2007 data sets across systems (Callison-Burch et al, 2007).</S> | Reference Offset:  ['23','107'] | Reference Text:  <S sid = 23 ssid = >This year’s data included training and development sets for the News Commentary data, which was the surprise outof-domain test set last year.</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W07-0718.txt | Citing Article:  W12-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This evaluation was inspired by the sentence ranking evaluation in Callison-Burch et al (2007).</S> | Reference Offset:  ['79','107'] | Reference Text:  <S sid = 79 ssid = >The German source sentence is parsed, and various phrases are selected for evaluation.</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W07-0718.txt | Citing Article:  W09-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).</S> | Reference Offset:  ['1','9'] | Reference Text:  <S sid = 1 ssid = >j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.</S><S sid = 9 ssid = >Previous ACL Workshops on Machine Translation were more limited in scope (Koehn and Monz, 2005; Koehn and Monz, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W07-0718.txt | Citing Article:  D08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['57','183'] | Reference Text:  <S sid = 57 ssid = >A number of criteria could be adopted for choosing among different types of manual evaluation: the ease with which people are able to perform the task, their agreement with other annotators, their reliability when asked to repeat judgments, or the number of judgments which can be collected in a fixed time period.</S><S sid = 183 ssid = >Thanks to Brooke Cowan for parsing the Spanish test sentences, to Josh Albrecht for his script for normalizing fluency and adequacy on a per judge basis, and to Dan Melamed, Rebecca Hwa, Alon Lavie, Colin Bannard and Mirella Lapata for their advice about statistical tests.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W07-0718.txt | Citing Article:  W09-0809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch et al (2007) show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency.</S> | Reference Offset:  ['146','179'] | Reference Text:  <S sid = 146 ssid = >The both the sentence and constituent ranking had moderate inter-annotator agreement and substantial intra-annotator agreement.</S><S sid = 179 ssid = >On the other hand, comparing systems by ranking them manually (constituents or entire sentences), resulted in much higher inter-annotator agreement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W07-0718.txt | Citing Article:  I08-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the WMT 2007 shared task evaluation campaign (Callison-Burch et al, 2007) domain adaptation was a special challenge.</S> | Reference Offset:  ['6','22'] | Reference Text:  <S sid = 6 ssid = >This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation.</S><S sid = 22 ssid = >The data used in this year’s shared task was similar to the data used in last year’s shared task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W07-0718.txt | Citing Article:  P08-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al, 2007), a total of automatic MT evaluation metrics were evaluated for correlation with human judgement.</S> | Reference Offset:  ['4','107'] | Reference Text:  <S sid = 4 ssid = >We measured the correlation of automatic evaluation metrics with human judgments.</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W07-0718.txt | Citing Article:  P08-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, when evaluated on the datasets of the recent ACL 07 MT workshop (Callison-Burch et al, 2007).</S> | Reference Offset:  ['6','107'] | Reference Text:  <S sid = 6 ssid = >This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation.</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W07-0718.txt | Citing Article:  P08-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We gather the correlation results of these metrics from the workshop paper (Callison-Burch et al, 2007), and show in Table 1 the overall correlations of these metrics over the Europarl and News Commentary datasets.</S> | Reference Offset:  ['6','35'] | Reference Text:  <S sid = 6 ssid = >This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation.</S><S sid = 35 ssid = >The News Commentary test set differs from the Europarl data in various ways.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W07-0718.txt | Citing Article:  P11-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A complete description on WMT-07 evaluation campaign and dataset is available in Callison-Burch et al (2007).</S> | Reference Offset:  ['47','107'] | Reference Text:  <S sid = 47 ssid = >For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W07-0718.txt | Citing Article:  P11-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Different from Callison-Burch et al (2007), where Spear man's correlation coefficients were used, we use here Pearson's coefficients as, instead of focusing on ranking; this first evaluation exercise focuses on evaluating the significance and noisiness of the association, if any, between the automatic metrics and human-generated scores.</S> | Reference Offset:  ['4','159'] | Reference Text:  <S sid = 4 ssid = >We measured the correlation of automatic evaluation metrics with human judgments.</S><S sid = 159 ssid = >To measure the correlation of the automatic metrics with the human judgments of translation quality we used Spearman’s rank correlation coefficient p. We opted for Spearman rather than Pearson because it makes fewer assumptions about the data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W07-0718.txt | Citing Article:  P11-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Callison-Burch et al (2007) for details on the human evaluation task.</S> | Reference Offset:  ['15','107'] | Reference Text:  <S sid = 15 ssid = >Can we improve human evaluation?</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W07-0718.txt | Citing Article:  E12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['57','183'] | Reference Text:  <S sid = 57 ssid = >A number of criteria could be adopted for choosing among different types of manual evaluation: the ease with which people are able to perform the task, their agreement with other annotators, their reliability when asked to repeat judgments, or the number of judgments which can be collected in a fixed time period.</S><S sid = 183 ssid = >Thanks to Brooke Cowan for parsing the Spanish test sentences, to Josh Albrecht for his script for normalizing fluency and adequacy on a per judge basis, and to Dan Melamed, Rebecca Hwa, Alon Lavie, Colin Bannard and Mirella Lapata for their advice about statistical tests.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W07-0718.txt | Citing Article:  P11-4010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al (2007)).</S> | Reference Offset:  ['135','142'] | Reference Text:  <S sid = 135 ssid = >We define chance agreement for fluency and adequacy as 5, since they are based on five point scales, and for ranking as s since there are three possible out comes when ranking the output of a pair of systems: A > B, A = B, A < B.</S><S sid = 142 ssid = >The K values for fluency and adequacy should give us pause about using these metrics in the future.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W07-0718.txt | Citing Article:  P11-4010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison (Callison-Burch et al, 2007), and word alignment (Ahrenberg et al, 2003).</S> | Reference Offset:  ['84','107'] | Reference Text:  <S sid = 84 ssid = >The phrases in the translations were located using techniques from phrase-based statistical machine translation which extract phrase pairs from word alignments (Koehn et al., 2003; Och and Ney, 2004).</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W07-0718.txt | Citing Article:  P11-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The method of manually scoring the 11 submitted Chinese system translations of each segment is the same as that used in (Callison-Burch et al, 2007).</S> | Reference Offset:  ['60','107'] | Reference Text:  <S sid = 60 ssid = >We examined three different ways of manually evaluating machine translation quality: The most widely used methodology when manually evaluating MT is to assign values from two five point scales representing fluency and adequacy.</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W07-0718.txt | Citing Article:  N09-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative factors (Callison-Burch et al, 2007).</S> | Reference Offset:  ['15','107'] | Reference Text:  <S sid = 15 ssid = >Can we improve human evaluation?</S><S sid = 107 ssid = >However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


