Citance Number: 1 | Reference Article:  A00-2019.txt | Citing Article:  P06-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Chodorow and Leacock (2000) point out that the word concentrate is usually used as a noun in a general corpus whereas it is a verb 91% of the time in essays written by non-native learners of English.</S> | Reference Offset:  ['50','51'] | Reference Text:  <S sid = 50 ssid = >ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the singular determiner a preceding a singular noun is common in English but rare when the noun is specific corpora, we tried to minimize the mismatch between the domains of newspapers and TOEFL essays.</S><S sid = 51 ssid = >For example, in the newspaper domain, concentrate is usually used as a noun, as in orange juice concentrate but in TOEFL essays it is a verb 91% of the time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A00-2019.txt | Citing Article:  W07-1607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chodorow and Leacock (2000) try to identify errors on the basis of context, as we do here, and more specifically a 2 word window around the word of interest, from which they consider function words and POS tags.</S> | Reference Offset:  ['21','39'] | Reference Text:  <S sid = 21 ssid = >They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).</S><S sid = 39 ssid = >It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A00-2019.txt | Citing Article:  D07-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >N-gram-based approaches to the problem of error detection have been proposed and implemented in various forms by Atwell (1987), Bigert and Knutsson (2002), and Chodorow and Leacock (2000) amongst others.</S> | Reference Offset:  ['19','30'] | Reference Text:  <S sid = 19 ssid = >We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem.</S><S sid = 30 ssid = >Unfortunately, this approach was not effective for error detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A00-2019.txt | Citing Article:  D07-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chodorow and Leacock (2000) use a mutual information measure in addition to raw frequency of n grams.</S> | Reference Offset:  ['47','49'] | Reference Text:  <S sid = 47 ssid = >From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks).</S><S sid = 49 ssid = >Here we use this measure for the opposite purpose — to find combinations that occur less often than expected.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A00-2019.txt | Citing Article:  P11-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among unsupervised checkers, Chodorow and Leacock (2000) exploits negative evidence from edited textual corpora achieving high precision but low recall, while Tsao and Wible (2009) uses general corpus only.</S> | Reference Offset:  ['1','10'] | Reference Text:  <S sid = 1 ssid = >We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S><S sid = 10 ssid = >Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A00-2019.txt | Citing Article:  P11-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Chodorow and Leacock (2000) exploit bigrams and trigrams of function words and part-of-speech (PoS) tags, while Sun et al (2007) use labeled sequential patterns of function, time expression, and part-of-speech tags.</S> | Reference Offset:  ['39','79'] | Reference Text:  <S sid = 39 ssid = >It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994).</S><S sid = 79 ssid = >Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A00-2019.txt | Citing Article:  W11-1412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, unsupervised systems of (Chodorow and Leacock, 2000) and (Tsao and Wible, 2009) leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (Hermet et al, 2008) and (Gamon and Leacock, 2010) use Web as a corpus.</S> | Reference Offset:  ['58','62'] | Reference Text:  <S sid = 58 ssid = >ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.</S><S sid = 62 ssid = >In addition to bigram and trigram measures, ALEK compares the target word's part-ofspeech tag in the word-specific corpus and in the general corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A00-2019.txt | Citing Article:  C10-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason.</S> | Reference Offset:  ['21','94'] | Reference Text:  <S sid = 21 ssid = >They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).</S><S sid = 94 ssid = >Precision and recall for the three words are shown below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A00-2019.txt | Citing Article:  W11-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000).</S> | Reference Offset:  ['124','156'] | Reference Text:  <S sid = 124 ssid = >ALEK recognizes all of these types of errors.</S><S sid = 156 ssid = >Since these errors are not indicative of one's ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A00-2019.txt | Citing Article:  P11-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An example is the error detection method (Chodorow and Leacock, 2000), which identifies unnatural sequences of POSs as grammatical errors in the writing of learners.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >An Unsupervised Method For Detecting Grammatical Errors</S><S sid = 1 ssid = >We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our method outperforms Microsoft Word03 and ALEK (Chodorow and Leacock, 2000) from Educational Testing Service (ETS) in some cases.</S> | Reference Offset:  ['14','133'] | Reference Text:  <S sid = 14 ssid = >ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service.</S><S sid = 133 ssid = >In both cases, the system recognizes that the target is in the wrong syntactic environment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >An Unsupervised Method For Detecting Grammatical Errors</S><S sid = 1 ssid = >We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, we compared our technique with two other methods of checking errors, Microsoft Word03 and ALEK method (Chodorow and Leacock, 2000).</S> | Reference Offset:  ['0','161'] | Reference Text:  <S sid = 0 ssid = >An Unsupervised Method For Detecting Grammatical Errors</S><S sid = 161 ssid = >Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we compare our technique with the grammar checker of Microsoft Word03 and the ALEK (Chodorow and Leacock, 2000) method used by ETS.</S> | Reference Offset:  ['161','190'] | Reference Text:  <S sid = 161 ssid = >Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97.</S><S sid = 190 ssid = >However, its techniques could be incorporated into a grammar checker for native speakers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A00-2019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus.</S> | Reference Offset:  ['48','58'] | Reference Text:  <S sid = 48 ssid = >Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent.</S><S sid = 58 ssid = >ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A00-2019.txt | Citing Article:  W12-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The filter-based system combines unsupervised detection of a set of possible errors (Chodorow and Leacock, 2000) with hand-crafted filters designed to reduce this set to the largest subset of correctly flagged errors and the smallest possible number of false positives.</S> | Reference Offset:  ['66','115'] | Reference Text:  <S sid = 66 ssid = >To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times.</S><S sid = 115 ssid = >The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A00-2019.txt | Citing Article:  W12-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chodorow and Leacock (2000) found that low-frequency bigrams (sequences of two lexical categories with a negative log-likelihood) are quite reliable predictors of grammatical errors.</S> | Reference Offset:  ['65','81'] | Reference Text:  <S sid = 65 ssid = >The mutual information measures provide candidate errors, but this approach overgenerates — it finds rare, but still quite grammatical, sequences.</S><S sid = 81 ssid = >If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these ngrams.</S> | Discourse Facet:  NA | Annotator: Automatic


