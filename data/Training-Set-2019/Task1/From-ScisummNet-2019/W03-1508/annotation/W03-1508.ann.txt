Citance Number: 1 | Reference Article:  W03-1508.txt | Citing Article:  C04-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character.</S> | Reference Offset:  ['58','70'] | Reference Text:  <S sid = 58 ssid = >The language model required for translating pinyin sequences to Chinese characters is relatively straightforward.</S><S sid = 70 ssid = >The second translation system, for converting pinyin sequences to character sequences, has a one-toone mapping between symbols and therefore has no words with zero fertility.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-1508.txt | Citing Article:  P06-1142.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Virga and Khudanpur (2003) and Kuo et al (2005) adopted the noisy channel modeling framework.</S> | Reference Offset:  ['29','62'] | Reference Text:  <S sid = 29 ssid = >The IBM source-channel model for statistical machine translation (P. Brown et al., 1993) plays a central role in our system.</S><S sid = 62 ssid = >In the IBM model described earlier, these are the words which may be “deleted” by the noisy channel when transforming into .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-1508.txt | Citing Article:  N07-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003).</S> | Reference Offset:  ['93','94'] | Reference Text:  <S sid = 93 ssid = >The Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) is a research retrieval system developed at the Johns Hopkins University Applied Physics Laboratory.</S><S sid = 94 ssid = >The system was developed to investigate knowledgelight methods for linguistic processing in text retrieval.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-1508.txt | Citing Article:  N07-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003).</S> | Reference Offset:  ['107','137'] | Reference Text:  <S sid = 107 ssid = >A small improvement in mAP is obtained by the Haircut system with name transliteration over the system without name transliteration: the improvement from 0.501 to 0.515 is statistically significant at a -value of 0.084.</S><S sid = 137 ssid = >Note that significant improvements in transliteration performance result from this alternate method of data selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-1508.txt | Citing Article:  H05-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This result was comparable to other state-of-the-art statistical name transliteration systems (Virga and Khudanpur, 2003).</S> | Reference Offset:  ['76','109'] | Reference Text:  <S sid = 76 ssid = >We concede that this performance, while comparable to other systems, is not satisfactory and merits further investigation.</S><S sid = 109 ssid = >In any event, a need for improvement in transliteration is suggested by this result.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-1508.txt | Citing Article:  D09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['142','143'] | Reference Text:  <S sid = 142 ssid = >In a more intrinsic and direct evaluation, we have found ways to gainfully filter a large but noisy training corpus to augment the training data for our models and improve transliteration accuracy considerably beyond our starting point, e.g., to reduce Pin-yin error rates from 51.1% to 42.5%.</S><S sid = 143 ssid = >We expect to further refine the translation models in the future and apply them in other tasks such as text translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-1508.txt | Citing Article:  N09-3011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s (e, f)= Pr (f |e) Pr (e).</S> | Reference Offset:  ['97','128'] | Reference Text:  <S sid = 97 ssid = >Various smoothing methods have been proposed to combine the contributions for each term based on the document model and also a generic model of the language.</S><S sid = 128 ssid = >We estimated a pin-yin language model from the training portion above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-1508.txt | Citing Article:  P09-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003).</S> | Reference Offset:  ['16','65'] | Reference Text:  <S sid = 16 ssid = >Several techniques have been proposed in the recent past for name transliteration.</S><S sid = 65 ssid = >For the phoneme-to-GIF translation model, the “words” which need to be inserted in this manner are syllabic nuclei!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-1508.txt | Citing Article:  P04-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese characters, due to homophone confusion.</S> | Reference Offset:  ['58','79'] | Reference Text:  <S sid = 58 ssid = >The language model required for translating pinyin sequences to Chinese characters is relatively straightforward.</S><S sid = 79 ssid = >Note that while significantly lower error rates have been reported for converting pin-yin to characters in generic Chinese text, ours is a highly specialized subset of transliterated foreign names, where the choice between several characters sharing the same pin-yin symbol is somewhat arbitrary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-1508.txt | Citing Article:  P04-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits.</S> | Reference Offset:  ['73','105'] | Reference Text:  <S sid = 73 ssid = >The results are shown in Table 1, where pin-yin error rate is the edit distance between the “correct” pin-yin representation of the correct transliteration and the pin-yin sequence output by the system.</S><S sid = 105 ssid = >Our results and the corresponding results from Meng et al (2001) are reported in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W03-1508.txt | Citing Article:  P04-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The reference data are extracted from Table 1 and 3 of (Virga and Khudanpur 2003).</S> | Reference Offset:  ['123','131'] | Reference Text:  <S sid = 123 ssid = >We then aligned all the (nearly 1M) training “sentence” pairs with this translation model, and extracted roughly a third of the sentences with an alignment score above a certain tunable threshold ().</S><S sid = 131 ssid = >The result of this evaluation is reported in Table 3 against the line “Huge MT (Self),” where we also report the transliteration performance of the so-called Big MT system of Table 1 on this new test set.</S> | Discourse Facet:  NA | Annotator: Automatic


