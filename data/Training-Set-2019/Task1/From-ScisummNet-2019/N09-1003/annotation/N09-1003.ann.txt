Citance Number: 1 | Reference Article:  N09-1003.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets.</S> | Reference Offset:  ['31','137'] | Reference Text:  <S sid = 31 ssid = >More details of our algorithm can be found in (Agirre and Soroa, 2009).</S><S sid = 137 ssid = >Table 5 shows the results on the relatedness and similarity subsets of WordSim353 for the different methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N09-1003.txt | Citing Article:  W11-2503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts).</S> | Reference Offset:  ['103','107'] | Reference Text:  <S sid = 103 ssid = >Therefore, terms that are topically related can appear in the same textual passages and will get high values using this model.</S><S sid = 107 ssid = >Therefore, true synonyms and hyponyms/hyperonyms will receive high similarities, whereas terms related topically or based on any other semantic relation (e.g. movie and star) will have lower scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N09-1003.txt | Citing Article:  N10-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Spearman's rank correlation (œÅ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models.</S> | Reference Offset:  ['79','87'] | Reference Text:  <S sid = 79 ssid = >In order to calculate similarities in a cross-lingual setting, where some of the words are in a language l other than English, the following algorithm is used: models and distributional models.</S><S sid = 87 ssid = >The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N09-1003.txt | Citing Article:  N10-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009).</S> | Reference Offset:  ['31','87'] | Reference Text:  <S sid = 31 ssid = >More details of our algorithm can be found in (Agirre and Soroa, 2009).</S><S sid = 87 ssid = >The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N09-1003.txt | Citing Article:  P13-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006).</S> | Reference Offset:  ['0','191'] | Reference Text:  <S sid = 0 ssid = >A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches</S><S sid = 191 ssid = >This work pioneers cross-lingual extension and evaluation of both distributional and WordNet-based measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N09-1003.txt | Citing Article:  D11-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores.</S> | Reference Offset:  ['76','77'] | Reference Text:  <S sid = 76 ssid = >The final corpus remaining at the end of this process contains roughly 1.6 Terawords.</S><S sid = 77 ssid = >All calculations are done in parallel sharding by dimension, and it is possible to calculate all pairwise similarities of the words in the test sets very quickly on this corpus using the MapReduce infrastructure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N09-1003.txt | Citing Article:  D11-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The two Spanish WordNet versions are referred to as MCR16 and WN30g.</S><S sid = 193 ssid = >A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N09-1003.txt | Citing Article:  D11-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009).</S> | Reference Offset:  ['31','184'] | Reference Text:  <S sid = 31 ssid = >More details of our algorithm can be found in (Agirre and Soroa, 2009).</S><S sid = 184 ssid = >This paper has presented two state-of-the-art distributional and WordNet-based similarity measures, with a study of several parameters, including performance on similarity and relatedness data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N09-1003.txt | Citing Article:  C10-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006).</S> | Reference Offset:  ['7','160'] | Reference Text:  <S sid = 7 ssid = >The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007).</S><S sid = 160 ssid = >The results using 10-fold crossvalidation are shown in Table 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N09-1003.txt | Citing Article:  C10-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words.</S> | Reference Offset:  ['56','74'] | Reference Text:  <S sid = 56 ssid = >To calculate the similarity of two words w1 and w2, Ruiz-Casado et al. (2005) collect snippets containing w1 from a Web search engine, extract a context around it, replace it with w2 and check for the existence of that modified context in the Web.</S><S sid = 74 ssid = >We have used a corpus of four billion documents, crawled from the Web in August 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N09-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The two Spanish WordNet versions are referred to as MCR16 and WN30g.</S><S sid = 193 ssid = >A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N09-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The two Spanish WordNet versions are referred to as MCR16 and WN30g.</S><S sid = 193 ssid = >A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N09-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009).</S> | Reference Offset:  ['179','182'] | Reference Text:  <S sid = 179 ssid = >Our distributional methods also outperform all other corpus-based methods.</S><S sid = 182 ssid = >The only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, probably because of the dense, manually distilled knowledge contained in Wikipedia.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N09-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009).</S> | Reference Offset:  ['31','87'] | Reference Text:  <S sid = 31 ssid = >More details of our algorithm can be found in (Agirre and Soroa, 2009).</S><S sid = 87 ssid = >The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N09-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009).</S> | Reference Offset:  ['31','129'] | Reference Text:  <S sid = 31 ssid = >More details of our algorithm can be found in (Agirre and Soroa, 2009).</S><S sid = 129 ssid = >As the results in Section 4 show, different techniques are more appropriate to calculate either similarity or relatedness.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N09-1003.txt | Citing Article:  C10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them.</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches</S><S sid = 18 ssid = >Section 5 presents some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet similarity, the contrast between similarity and relatedness, and the combination of methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N09-1003.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs.</S> | Reference Offset:  ['0','180'] | Reference Text:  <S sid = 0 ssid = >A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches</S><S sid = 180 ssid = >The most similar approach to our distributional technique is Finkelstein et al. (2002), who combined distributional similarities from Web documents with a similarity from WordNet.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N09-1003.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.</S> | Reference Offset:  ['34','135'] | Reference Text:  <S sid = 34 ssid = >We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0.</S><S sid = 135 ssid = >This annotation was used to group the pairs in three categories: similar pairs (those classified as synonyms, antonyms, identical, or hyponym-hyperonym), related pairs (those classified as meronym-holonym, and pairs classified as none-of-the-above, with a human average similarity greater than 5), and unrelated pairs (those classified as none-of-the-above that had average similarity less than or equal to 5).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N09-1003.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The two Spanish WordNet versions are referred to as MCR16 and WN30g.</S><S sid = 193 ssid = >A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N09-1003.txt | Citing Article:  D10-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The two Spanish WordNet versions are referred to as MCR16 and WN30g.</S><S sid = 193 ssid = >A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5</S> | Discourse Facet:  NA | Annotator: Automatic


