Citance Number: 1 | Reference Article:  D10-1125.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010).</S> | Reference Offset:  ['0','202'] | Reference Text:  <S sid = 0 ssid = >Dual Decomposition for Parsing with Non-Projective Head Automata</S><S sid = 202 ssid = >Table 3 shows results for projective and non-projective parsing using the dual decomposition approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D10-1125.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.</S> | Reference Offset:  ['33','214'] | Reference Text:  <S sid = 33 ssid = >Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D10-1125.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010).</S> | Reference Offset:  ['28','33'] | Reference Text:  <S sid = 28 ssid = >Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).</S><S sid = 33 ssid = >Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D10-1125.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010).</S> | Reference Offset:  ['146','154'] | Reference Text:  <S sid = 146 ssid = >Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training.</S><S sid = 154 ssid = >In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D10-1125.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition.</S> | Reference Offset:  ['33','214'] | Reference Text:  <S sid = 33 ssid = >Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D10-1125.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).</S> | Reference Offset:  ['31','163'] | Reference Text:  <S sid = 31 ssid = >Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach.</S><S sid = 163 ssid = >Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D10-1125.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010).</S> | Reference Offset:  ['33','214'] | Reference Text:  <S sid = 33 ssid = >Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D10-1125.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence.</S> | Reference Offset:  ['173','214'] | Reference Text:  <S sid = 173 ssid = >We again recover exact solutions more often than the Martins et al. relaxations.</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D10-1125.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores.</S> | Reference Offset:  ['123','214'] | Reference Text:  <S sid = 123 ssid = >Second, we may have z∗↑(i, j) =� z∗(i, j) for some values of (i, j).</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D10-1125.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.</S><S sid = 228 ssid = >We have not tested other values for Q.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D10-1125.txt | Citing Article:  D12-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010).</S> | Reference Offset:  ['160','204'] | Reference Text:  <S sid = 160 ssid = >(We use the acronym UAS (unlabeled attachment score) for dependency accuracy.)</S><S sid = 204 ssid = >In contrast, there is little difference in accuracy between projective and nonprojective decoding on English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D10-1125.txt | Citing Article:  N12-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010).</S> | Reference Offset:  ['79','89'] | Reference Text:  <S sid = 79 ssid = >Thus, it is these constraints that complicate the optimization.</S><S sid = 89 ssid = >4 was the z = y constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D10-1125.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011).</S> | Reference Offset:  ['142','214'] | Reference Text:  <S sid = 142 ssid = >Our training approach is closely related to local training methods (Punyakanok et al., 2005).</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D10-1125.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t.</S> | Reference Offset:  ['214','218'] | Reference Text:  <S sid = 214 ssid = >Meshi et al. (2010).</S><S sid = 218 ssid = >Then define αk = S/(1 + 77k), where 77k is the number of times that L(u(k�)) > L(u(k'−1)) for k' < k. Hence the learning rate drops at a rate of 1/(1 + t), where t is the number of times that the dual increases from one iteration to the next.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D10-1125.txt | Citing Article:  N12-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.</S><S sid = 228 ssid = >We have not tested other values for Q.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D10-1125.txt | Citing Article:  P13-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work.</S> | Reference Offset:  ['206','214'] | Reference Text:  <S sid = 206 ssid = >There are a number of possible areas for future work.</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D10-1125.txt | Citing Article:  P13-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.</S><S sid = 228 ssid = >We have not tested other values for Q.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D10-1125.txt | Citing Article:  P13-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.).</S> | Reference Offset:  ['28','33'] | Reference Text:  <S sid = 28 ssid = >Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).</S><S sid = 33 ssid = >Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D10-1125.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006).</S> | Reference Offset:  ['163','214'] | Reference Text:  <S sid = 163 ssid = >Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008).</S><S sid = 214 ssid = >Meshi et al. (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D10-1125.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.</S><S sid = 228 ssid = >We have not tested other values for Q.</S> | Discourse Facet:  NA | Annotator: Automatic


