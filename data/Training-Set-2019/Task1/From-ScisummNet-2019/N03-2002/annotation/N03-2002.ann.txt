Citance Number: 1 | Reference Article:  N03-2002.txt | Citing Article:  N04-4034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Different back-off strategies, including different back-off paths as well as combination methods (Bilmes and Kirchhoff, 2003), were tried and here we present the best results.</S> | Reference Offset:  ['10','29'] | Reference Text:  <S sid = 10 ssid = >The field is both diverse and intricate (Rosenfeld, 2000; Chen and Goodman, 1998; Jelinek, 1997; Ney et al., 1994), with many different forms of LMs including maximumentropy, whole-sentence, adaptive and cache-based, to name a small few.</S><S sid = 29 ssid = >Many possible backoff paths could be taken.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N03-2002.txt | Citing Article:  N04-4034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare an optimized four-gram, a three gram baseline, and various numbers of cluster sizes using our MCMI method and generalized back off (Bilmes and Kirchhoff, 2003), which, (again) with 500 clusters, achieves an 8.9% relative improvement over the trigram.</S> | Reference Offset:  ['54','57'] | Reference Text:  <S sid = 54 ssid = >Results are given in Table 1 and show perplexity for: 1) the baseline 3-gram; 2) a FLM 3-gram using morphs and stems; 3) a GPB-FLM 3-gram using morphs, stems and backoff function g1; 4) the baseline 2-gram; 5) an FLM 2-gram using morphs; 6) an FLM 2-gram using morphs and stems; and 7) an GPB-FLM 2-gram using morphs and stems.</S><S sid = 57 ssid = >Also, it is possible to obtain a 2-gram with lower perplexity than the optimized baseline 3-gram.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N03-2002.txt | Citing Article:  P13-2037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity.</S> | Reference Offset:  ['0','57'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 57 ssid = >Also, it is possible to obtain a 2-gram with lower perplexity than the optimized baseline 3-gram.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N03-2002.txt | Citing Article:  P10-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Conditional probability distributions are represented as factored language models smoothed using Witten-Bell interpolated back off smoothing (Bilmes and Kirchhoff, 2003), according to the backoff graphs in Fig.</S> | Reference Offset:  ['0','11'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 11 ssid = >Many models are simply smoothed conditional probability distributions for a word given its preceding history, typically the two preceding words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N03-2002.txt | Citing Article:  W07-0735.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Class-based LMs (Brown et al, 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario.</S> | Reference Offset:  ['10','18'] | Reference Text:  <S sid = 10 ssid = >The field is both diverse and intricate (Rosenfeld, 2000; Chen and Goodman, 1998; Jelinek, 1997; Ney et al., 1994), with many different forms of LMs including maximumentropy, whole-sentence, adaptive and cache-based, to name a small few.</S><S sid = 18 ssid = >Clearly, a two-factor FLM generalizes standard class-based language models, where one factor is the word class and the other is words themselves.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N03-2002.txt | Citing Article:  N06-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003).</S> | Reference Offset:  ['0','38'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 38 ssid = >Standard backoff occurs with g(f, f1, f2) = pBO(f|f1), but the GPB procedures can be obtained by using different g-functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N03-2002.txt | Citing Article:  N06-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 1 ssid = >We introduce factored language models (FLMs) and generalized parallel backoff (GPB).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N03-2002.txt | Citing Article:  N06-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmesand Kirchhoff, 2003).</S> | Reference Offset:  ['10','42'] | Reference Text:  <S sid = 10 ssid = >The field is both diverse and intricate (Rosenfeld, 2000; Chen and Goodman, 1998; Jelinek, 1997; Ney et al., 1994), with many different forms of LMs including maximumentropy, whole-sentence, adaptive and cache-based, to name a small few.</S><S sid = 42 ssid = >During the recent 2002 JHU workshop (Kirchhoff et al., 2003), significant extensions were made to the SRI language modeling toolkit (Stolcke, 2002) to support arbitrary FLMs and GPB procedures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N03-2002.txt | Citing Article:  N06-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the SRILM toolkit with extensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding.</S> | Reference Offset:  ['42','72'] | Reference Text:  <S sid = 42 ssid = >During the recent 2002 JHU workshop (Kirchhoff et al., 2003), significant extensions were made to the SRI language modeling toolkit (Stolcke, 2002) to support arbitrary FLMs and GPB procedures.</S><S sid = 72 ssid = >Therefore, FLMs with GPB will be incorporated into GMTK (Bilmes, 2002), a general purpose graphical model toolkit for speech recognition and language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N03-2002.txt | Citing Article:  D09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors.</S> | Reference Offset:  ['2','15'] | Reference Text:  <S sid = 2 ssid = >An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.</S><S sid = 15 ssid = >In a factored language model, a word is viewed as a vector of k factors, so that wt â‰¡ {f1t , f2t , ... , fKt }.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N03-2002.txt | Citing Article:  N12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003).</S> | Reference Offset:  ['12','19'] | Reference Text:  <S sid = 12 ssid = >In this work, we introduce two new methods for language modeling: factored language model (FLM) and generalized parallel backoff (GPB).</S><S sid = 19 ssid = >An FLM is a model over factors, i.e., p(ft:K|ft1 F'1K that can be factored as a product of probabilities of the form p(f|f1, f2, ... , fN).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N03-2002.txt | Citing Article:  W05-0821.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework.</S> | Reference Offset:  ['59','64'] | Reference Text:  <S sid = 59 ssid = >Word and POS tag information (Tt) was extracted.</S><S sid = 64 ssid = >Model A uses the true by-hand tag information from the Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N03-2002.txt | Citing Article:  P11-2077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This work is related to several existing directions: generative factored language model, discriminative language models, online passive-aggressive learning and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003).</S> | Reference Offset:  ['0','12'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 12 ssid = >In this work, we introduce two new methods for language modeling: factored language model (FLM) and generalized parallel backoff (GPB).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N03-2002.txt | Citing Article:  N06-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A more powerful back off strategy is used in factored language models (FLMs) (Bilmes and Kirchhoff, 2003), which view a word as a vector of word features or factors.</S> | Reference Offset:  ['1','15'] | Reference Text:  <S sid = 1 ssid = >We introduce factored language models (FLMs) and generalized parallel backoff (GPB).</S><S sid = 15 ssid = >In a factored language model, a word is viewed as a vector of k factors, so that wt â‰¡ {f1t , f2t , ... , fKt }.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N03-2002.txt | Citing Article:  N10-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex back off mechanism (Bilmes and Kirchhoff, 2003).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 1 ssid = >We introduce factored language models (FLMs) and generalized parallel backoff (GPB).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N03-2002.txt | Citing Article:  W08-0510.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 1 ssid = >We introduce factored language models (FLMs) and generalized parallel backoff (GPB).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N03-2002.txt | Citing Article:  P05-3012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In COMIC, the OpenCC Grealiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multi modal co articulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module.</S> | Reference Offset:  ['0','43'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 43 ssid = >This uses a graphicalmodel like specification language, and where many different backoff functions (19 in total) were implemented.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N03-2002.txt | Citing Article:  W11-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 15 ssid = >In a factored language model, a word is viewed as a vector of k factors, so that wt â‰¡ {f1t , f2t , ... , fKt }.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N03-2002.txt | Citing Article:  W07-0702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel back off (Bilmesand Kirchhoff, 2003) which is used in factored language models.</S> | Reference Offset:  ['0','29'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 29 ssid = >Many possible backoff paths could be taken.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N03-2002.txt | Citing Article:  P07-2045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Factored Language Models And Generalized Parallel Backoff</S><S sid = 1 ssid = >We introduce factored language models (FLMs) and generalized parallel backoff (GPB).</S> | Discourse Facet:  NA | Annotator: Automatic


