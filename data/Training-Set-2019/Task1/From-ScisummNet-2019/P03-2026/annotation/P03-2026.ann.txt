Citance Number: 1 | Reference Article:  P03-2026.txt | Citing Article:  W07-1604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Izumi et al (2003) and (2004) used error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.</S> | Reference Offset:  ['0','26'] | Reference Text:  <S sid = 0 ssid = >Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = 26 ssid = >(e.g. article and tense errors) These are different from “error types” (omission or replacement).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-2026.txt | Citing Article:  C08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% in a Japanese learner corpus.</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = 18 ssid = >In this section, we would like to describe how we proceeded with error detection in the learner corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-2026.txt | Citing Article:  C08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Izumi et al., 2003) and (Izumi et al, 2004) used an ME approach to classify different grammatical errors in transcripts of Japanese interviews.</S> | Reference Offset:  ['22','26'] | Reference Text:  <S sid = 22 ssid = >We applied different methods to detecting these two kinds of errors.</S><S sid = 26 ssid = >(e.g. article and tense errors) These are different from “error types” (omission or replacement).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-2026.txt | Citing Article:  P08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, in the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = 1 ssid = >This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-2026.txt | Citing Article:  P08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A maximum entropy model, using lexical and POS features, is trained in (Izumi et al, 2003) to recognize a variety of errors.</S> | Reference Offset:  ['53','54'] | Reference Text:  <S sid = 53 ssid = >The Maximum Entropy (ME) model (Jaynes 1957) is a general technique that is used to estimate the probability distributions of data.</S><S sid = 54 ssid = >The over-riding principle in ME is that when nothing is known, the distribution should be as uniform as possible, i.e., maximum entropy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-2026.txt | Citing Article:  W10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Izumi et al (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers.</S> | Reference Offset:  ['31','78'] | Reference Text:  <S sid = 31 ssid = >Here, there is an article missing in front of “telephone”, so this can be considered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.).</S><S sid = 78 ssid = >We did this only for article errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-2026.txt | Citing Article:  N07-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al, 2003).</S> | Reference Offset:  ['13','14'] | Reference Text:  <S sid = 13 ssid = >We designed an original error tagset for learners’ grammatical and lexical errors, which were relatively easy to categorize.</S><S sid = 14 ssid = >Our error tags contained three pieces of information, i.e., the part of speech, the grammatical/lexical system and the corrected form.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-2026.txt | Citing Article:  N07-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The usage of articles has been found to be the most frequent error class in the JLE corpus (Izumi et al, 2003).</S> | Reference Offset:  ['15','79'] | Reference Text:  <S sid = 15 ssid = >We prepared special tags for some errors that cannot be categorized into any word class, such as the misordering of words.</S><S sid = 79 ssid = >We first examined what kind of errors had been made with articles and found that “a”, “an”, “the” and the absence of articles were often confused.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-2026.txt | Citing Article:  N07-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the future, we would like to search for more salient features through a careful study of non-native errors, using error-tagged corpora such as (Izumi et al., 2003).</S> | Reference Offset:  ['7','18'] | Reference Text:  <S sid = 7 ssid = >In this paper, we introduce a method of detecting learners’ errors, and we examine to what extent this could be accomplished using our learner corpus data including error tags that are labeled with the learners’ errors.</S><S sid = 18 ssid = >In this section, we would like to describe how we proceeded with error detection in the learner corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-2026.txt | Citing Article:  P11-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['31','86'] | Reference Text:  <S sid = 31 ssid = >Here, there is an article missing in front of “telephone”, so this can be considered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.).</S><S sid = 86 ssid = >By adding corrected sentences and artificially made errors, the precision rate rose to 80% while the recall rate remained the same.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-2026.txt | Citing Article:  P11-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We based our error annotation scheme on that used in the NICT JLE corpus (Izumi et al, 2003a), whose detailed description is readily available, for example, in Izumi et al (2005).</S> | Reference Offset:  ['17','84'] | Reference Text:  <S sid = 17 ssid = >The following example is a sentence with an error tag.</S><S sid = 84 ssid = >Since some more detailed context might be necessary to decide whether “a” or “the” must be used, the features we used here might be insufficient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-2026.txt | Citing Article:  P11-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['31','86'] | Reference Text:  <S sid = 31 ssid = >Here, there is an article missing in front of “telephone”, so this can be considered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.).</S><S sid = 86 ssid = >By adding corrected sentences and artificially made errors, the precision rate rose to 80% while the recall rate remained the same.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-2026.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The method (Izumi et al, 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs.</S> | Reference Offset:  ['42','82'] | Reference Text:  <S sid = 42 ssid = >As we did in detecting omission-type errors, if more than one error category was given, we use two methods of detection.</S><S sid = 82 ssid = >We obtained a better recall and precision rate for omission-type errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-2026.txt | Citing Article:  N10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Izumi et al (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus.</S> | Reference Offset:  ['0','53'] | Reference Text:  <S sid = 0 ssid = >Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = 53 ssid = >The Maximum Entropy (ME) model (Jaynes 1957) is a general technique that is used to estimate the probability distributions of data.</S> | Discourse Facet:  NA | Annotator: Automatic


