Citance Number: 1 | Reference Article:  P09-1088.txt | Citing Article:  N10-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blunsom et al (2009) describe a blocked sampler following John son et al (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O (|f |3|e|3) complexity.</S> | Reference Offset:  ['22','53'] | Reference Text:  <S sid = 22 ssid = >The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 2008)).</S><S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1088.txt | Citing Article:  N10-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','170'] | Reference Text:  <S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S><S sid = 170 ssid = >HR001106-2-001 (Dyer).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1088.txt | Citing Article:  N10-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al, 2009), in terms of mixing and translation quality.</S> | Reference Offset:  ['0','26'] | Reference Text:  <S sid = 0 ssid = >A Gibbs Sampler for Phrasal Synchronous Grammar Induction</S><S sid = 26 ssid = >We describe our Bayesian SCFG model in Section 4 and a Gibbs sampler to explore its posterior.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1088.txt | Citing Article:  N10-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate three initialisers: M4: the symmetrised output of GIZA++factorised into ITG form (as used in Blunsom et al (2009)).</S> | Reference Offset:  ['53','135'] | Reference Text:  <S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S><S sid = 135 ssid = >We evaluate the translation models using IBM BLEU (Papineni et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1088.txt | Citing Article:  P13-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Blunsom et al, 2009) we used a vague gamma prior (10? 4, 104), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3.</S> | Reference Offset:  ['106','108'] | Reference Text:  <S sid = 106 ssid = >Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx — Gamma(10−4,104).</S><S sid = 108 ssid = >We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx|d, α−) using the MetropolisHastings algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1088.txt | Citing Article:  W10-3813.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)).</S> | Reference Offset:  ['7','123'] | Reference Text:  <S sid = 7 ssid = >The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003).</S><S sid = 123 ssid = >We use the GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1088.txt | Citing Article:  W11-2167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009).</S> | Reference Offset:  ['22','53'] | Reference Text:  <S sid = 22 ssid = >The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., 2008)).</S><S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1088.txt | Citing Article:  W11-2167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We explore a subset of the space of rules being considered by (Blunsom et al, 2009) i.e., only those rules satisfying the word alignments and heuristically grown phrase alignments.</S> | Reference Offset:  ['38','145'] | Reference Text:  <S sid = 38 ssid = >Taking a different tack, DeNero et al. (2008) presented an interesting new model with inference courtesy of a Gibbs sampler, which was better able to explore the full space of phrase translations.</S><S sid = 145 ssid = >However these alignments are much more locally consistent, containing fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1088.txt | Citing Article:  P13-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this we adopt the approach of Blunsom et al (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision.</S> | Reference Offset:  ['136','140'] | Reference Text:  <S sid = 136 ssid = >Table 1 lists the statistics of the corpora used in these experiments.</S><S sid = 140 ssid = >Table 2 shows the results for the benchmark Moses and Hiero systems on this corpus using both the heuristic phrase estimation, and our proposed Bayesian SCFG model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1088.txt | Citing Article:  P13-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We apply the technique from Blunsom et al (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler.</S> | Reference Offset:  ['113','116'] | Reference Text:  <S sid = 113 ssid = >Recent work (Newman et al., 2007; Asuncion et al., 2008) suggests that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus.</S><S sid = 116 ssid = >However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1088.txt | Citing Article:  P13-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our ArEn training data comprises several LDCcorpora, using the same experimental setup as in Blunsom et al (2009a).</S> | Reference Offset:  ['53','135'] | Reference Text:  <S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S><S sid = 135 ssid = >We evaluate the translation models using IBM BLEU (Papineni et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1088.txt | Citing Article:  D11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use this method motivated by Gibbs Sampler (Blunsom et al, 2009) which has been used for efficiently learning rules.</S> | Reference Offset:  ['85','111'] | Reference Text:  <S sid = 85 ssid = >Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample.</S><S sid = 111 ssid = >While employing a collapsed Gibbs sampler allows us to efficiently perform inference over the massive space of possible grammars, it induces dependencies between all the sentences in the training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1088.txt | Citing Article:  W10-2915.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009).</S> | Reference Offset:  ['13','53'] | Reference Text:  <S sid = 13 ssid = >The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008).</S><S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1088.txt | Citing Article:  W10-2915.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our smoothing distribution of phrase pairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean, drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language, similar to (Blunsom et al, 2009).</S> | Reference Offset:  ['34','93'] | Reference Text:  <S sid = 34 ssid = >Various heuristics are used to combine source-to-target and target-to-source alignments, after which a further heuristic is used to read off phrase pairs which are ‘consistent’ with the alignment.</S><S sid = 93 ssid = >If the source position adjacent nodes in a ternary rule can be re-parented as a binary rule, or vice-versa. falls between two existing terminals whose target phrases are adjacent, then any new target segmentation within those target phrases can be sampled, including null alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1088.txt | Citing Article:  D10-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work, e.g. by Blunsom et al (2009) and Haghihi et al. (2009) just to name a few, show that alignment models that bear closer resemblance to state-of-the art translation model consistently yields not only a better alignment quality but also an improved translation quality.</S> | Reference Offset:  ['7','118'] | Reference Text:  <S sid = 7 ssid = >The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003).</S><S sid = 118 ssid = >Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1088.txt | Citing Article:  P11-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blunsom et al (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data.</S> | Reference Offset:  ['2','168'] | Reference Text:  <S sid = 2 ssid = >Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora.</S><S sid = 168 ssid = >In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1088.txt | Citing Article:  D11-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009).</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >A Gibbs Sampler for Phrasal Synchronous Grammar Induction</S><S sid = 2 ssid = >Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1088.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','170'] | Reference Text:  <S sid = 53 ssid = >Our Bayesian model of SCFG derivations resembles that of Blunsom et al. (2008).</S><S sid = 170 ssid = >HR001106-2-001 (Dyer).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1088.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al, 2009a).</S> | Reference Offset:  ['128','129'] | Reference Text:  <S sid = 128 ssid = >The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar.</S><S sid = 129 ssid = >This is achieved by using the factorisation algorithm of Zhang et al. (2008a) to first create initial trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1088.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For AREN experiments the language model is trained on English data as (Blunsom et al, 2009a), and for FA-EN and UR EN the English data are the target sides of the bilingual training data.</S> | Reference Offset:  ['132','147'] | Reference Text:  <S sid = 132 ssid = >However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data.</S><S sid = 147 ssid = >The Chinese-English training data consists of the FBIS corpus (LDC2003E14) and the first 100k sentences from the Sinorama corpus (LDC2005E47).</S> | Discourse Facet:  NA | Annotator: Automatic


