Citance Number: 1 | Reference Article:  P07-2045.txt | Citing Article:  D07-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007).</S> | Reference Offset:  ['6','57'] | Reference Text:  <S sid = 6 ssid = >3 Factored Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1.</S><S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-2045.txt | Citing Article:  P14-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality.</S> | Reference Offset:  ['63','75'] | Reference Text:  <S sid = 63 ssid = >Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006).</S><S sid = 75 ssid = >It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-2045.txt | Citing Article:  S12-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).</S> | Reference Offset:  ['42','121'] | Reference Text:  <S sid = 42 ssid = >Moses implements an efficient representation of the phrase translation table.</S><S sid = 121 ssid = >Moses implements an efficient representation of the phrase translation table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-2045.txt | Citing Article:  S12-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007).</S> | Reference Offset:  ['42','121'] | Reference Text:  <S sid = 42 ssid = >Moses implements an efficient representation of the phrase translation table.</S><S sid = 121 ssid = >Moses implements an efficient representation of the phrase translation table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-2045.txt | Citing Article:  W11-2143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007).</S> | Reference Offset:  ['56','135'] | Reference Text:  <S sid = 56 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-2045.txt | Citing Article:  P14-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007).</S> | Reference Offset:  ['75','135'] | Reference Text:  <S sid = 75 ssid = >It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-2045.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT.</S> | Reference Offset:  ['57','135'] | Reference Text:  <S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-2045.txt | Citing Article:  P14-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec.</S> | Reference Offset:  ['2','83'] | Reference Text:  <S sid = 2 ssid = >Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006).</S><S sid = 83 ssid = >Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-2045.txt | Citing Article:  D09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002).</S> | Reference Offset:  ['57','135'] | Reference Text:  <S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-2045.txt | Citing Article:  D09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders.</S> | Reference Offset:  ['57','64'] | Reference Text:  <S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S><S sid = 64 ssid = >It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-2045.txt | Citing Article:  D09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding.</S> | Reference Offset:  ['56','135'] | Reference Text:  <S sid = 56 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-2045.txt | Citing Article:  W12-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007).</S> | Reference Offset:  ['56','135'] | Reference Text:  <S sid = 56 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-2045.txt | Citing Article:  W12-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007).</S> | Reference Offset:  ['75','76'] | Reference Text:  <S sid = 75 ssid = >It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).</S><S sid = 76 ssid = >Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-2045.txt | Citing Article:  P11-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text.</S> | Reference Offset:  ['57','135'] | Reference Text:  <S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-2045.txt | Citing Article:  P11-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version.</S> | Reference Offset:  ['57','135'] | Reference Text:  <S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-2045.txt | Citing Article:  W12-3117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes.</S> | Reference Offset:  ['20','75'] | Reference Text:  <S sid = 20 ssid = >Every factor on the target language can have its own language model.</S><S sid = 75 ssid = >It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-2045.txt | Citing Article:  W11-2022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus.</S> | Reference Offset:  ['18','97'] | Reference Text:  <S sid = 18 ssid = >However, Moses can have ambiguous input in the form of confusion networks.</S><S sid = 97 ssid = >However, Moses can have ambiguous input in the form of confusion networks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-2045.txt | Citing Article:  W11-2129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders.</S> | Reference Offset:  ['57','135'] | Reference Text:  <S sid = 57 ssid = >Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-2045.txt | Citing Article:  W09-0429.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints.</S> | Reference Offset:  ['56','135'] | Reference Text:  <S sid = 56 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-2045.txt | Citing Article:  W12-3146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag.</S> | Reference Offset:  ['56','135'] | Reference Text:  <S sid = 56 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S><S sid = 135 ssid = >Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


