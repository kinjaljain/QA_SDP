Citance Number: 1 | Reference Article:  C02-1114.txt | Citing Article:  N03-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002).</S> | Reference Offset:  ['38','102'] | Reference Text:  <S sid = 38 ssid = >Riloff and Shepherd (1997) also give some credit for ?related words?</S><S sid = 102 ssid = >(The first 4 cat egories are also used by (Riloff and Shepherd, 1997) and (Roark and Charniak, 1998) and so were included for comparison.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C02-1114.txt | Citing Article:  P11-1148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (Veronis, 2004) or Pagerank (Agirre et al, 2006).</S> | Reference Offset:  ['46','139'] | Reference Text:  <S sid = 46 ssid = >Another way to obtain word-senses directly from corpora is to use clustering algorithms on feature-vectors (Lin, 1998; Schu?tze, 1998).Clustering techniques can also be used to discriminate between different senses of an ambiguous word.</S><S sid = 139 ssid = >Breadth of cover age does not in itself solve this problem: general lexical resources such as WordNet can provide too many senses many of which are rarely used in particular domains or corpora (Gale et al, 1992).The graph model presented in this paper suggests a new method for recognising relevant polysemy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C02-1114.txt | Citing Article:  D09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002).</S> | Reference Offset:  ['21','134'] | Reference Text:  <S sid = 21 ssid = >Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.</S><S sid = 134 ssid = >So far we have presented a graph model built upon noun co-occurrence which performs much better than previously reported methods at the task of automatic lexical acquisition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C02-1114.txt | Citing Article:  D09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >This algorithm has been built into an on-line demonstration where the user inputs a givenseed word and can then see the cluster of re lated words being gradually assembled.</S><S sid = 160 ssid = >2 1http://infomap.stanford.edu/graphs 2http://muchmore.dfki.deFigure 1: Automatically generated graph show ing the word apple and semantically related nouns</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C02-1114.txt | Citing Article:  P08-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes.</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >A Graph Model For Unsupervised Lexical Acquisition</S><S sid = 21 ssid = >Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is the same general observation exploited by (Widdows and Dorow, 2002), who try to find graph regions that are more connected internally than externally.</S> | Reference Offset:  ['141','144'] | Reference Text:  <S sid = 141 ssid = >Definition 2 (Bolloba?s, 1998, Ch 1 ?1) Let G = (V,E) be a graph, where V is the set of vertices (nodes) of G and E ? V ? V is the set of edges of G. ? Two nodes v1, vn are said to be connected if there exists a path {v1, v2, . . .</S><S sid = 144 ssid = >Definition 3 Let G be a graph of words closely related to a seed-word w, and let G \ w be the subgraph which results from the removal of the seed-node w. The connected components of the subgraph G \ w are the senses of the word w with respect to the graph G. As an illustrative example, consider the localgraph generated for the word apple (6).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The major guideline in this part of the evaluation was to compare our results with previous work having a similar goal (Widdows and Dorow, 2002).</S> | Reference Offset:  ['16','109'] | Reference Text:  <S sid = 16 ssid = >Section 2 reviews previous work on semanticsimilarity and lexical acquisition.</S><S sid = 109 ssid = >5.2 Results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >This algorithm has been built into an on-line demonstration where the user inputs a givenseed word and can then see the cluster of re lated words being gradually assembled.</S><S sid = 160 ssid = >2 1http://infomap.stanford.edu/graphs 2http://muchmore.dfki.deFigure 1: Automatically generated graph show ing the word apple and semantically related nouns</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The only difference from the (Widdows and Dorow, 2002) experiment is the usage of pairs rather than single words.</S> | Reference Offset:  ['2','3'] | Reference Text:  <S sid = 2 ssid = >The graph model is built by linking pairs of words which participate in particular syntacticrelationships.</S><S sid = 3 ssid = >We focus on the symmetric relationship between pairs of nouns which occur to gether in lists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This was not needed in (Widdows and Dorow, 2002) because they had directly accessed the word graph, which may be an advantage in some applications. The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small.</S> | Reference Offset:  ['26','92'] | Reference Text:  <S sid = 26 ssid = >For a given category, choose a small.</S><S sid = 92 ssid = >or ?WordNet category?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fortunately, the subject list is such that WordNet words (Widdows and Dorow, 2002) also reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones.</S> | Reference Offset:  ['36','109'] | Reference Text:  <S sid = 36 ssid = >used in these cases was based on co occurrence in lists.</S><S sid = 109 ssid = >5.2 Results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C02-1114.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This metric was reported to be 82% in (Widdows and Dorow, 2002).</S> | Reference Offset:  ['118','128'] | Reference Text:  <S sid = 118 ssid = >(Our results are also slightly better than those reported by Riloff and Jones (1999)).</S><S sid = 128 ssid = >The LSI similarity thesaurus obtained an accuracy of 31%, much less than the graph model?s 82%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C02-1114.txt | Citing Article:  W09-0805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node x to y if the corresponding content words appear in the pattern such that x precedes y.</S> | Reference Offset:  ['60','77'] | Reference Text:  <S sid = 60 ssid = >Our full graph contains many directed links between words of different parts of speech.</S><S sid = 77 ssid = >The best new node is taken to be the node b ? N(A)\A with the highest proportion of links to N(A).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C02-1114.txt | Citing Article:  W08-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition.</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >A Graph Model For Unsupervised Lexical Acquisition</S><S sid = 21 ssid = >Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C02-1114.txt | Citing Article:  W06-1664.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition.</S> | Reference Offset:  ['0','134'] | Reference Text:  <S sid = 0 ssid = >A Graph Model For Unsupervised Lexical Acquisition</S><S sid = 134 ssid = >So far we have presented a graph model built upon noun co-occurrence which performs much better than previously reported methods at the task of automatic lexical acquisition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C02-1114.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to identify symmetric patterns, for each pattern we define a pattern graph G (P), as proposed by (Widdows and Dorow, 2002).</S> | Reference Offset:  ['91','143'] | Reference Text:  <S sid = 91 ssid = >Define the ?WordNet class?</S><S sid = 143 ssid = >The equivalence classes of the graph G un der this relation are called the components of G. We are now in a position to define the senses of a word as represented by a particular graph.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C02-1114.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The concept discovery algorithmis essentially the same as used by (Davidov and Rappoport, 2006) and has some similarity with the one used by (Widdows and Dorow, 2002).</S> | Reference Offset:  ['36','119'] | Reference Text:  <S sid = 36 ssid = >used in these cases was based on co occurrence in lists.</S><S sid = 119 ssid = >Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C02-1114.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have also performed an indirect comparison to (Widdows and Dorow, 2002). While there is a significant number of other related studies on concept acquisition (see Section 2), Most are supervised and/or use language-specific tools.</S> | Reference Offset:  ['16','127'] | Reference Text:  <S sid = 16 ssid = >Section 2 reviews previous work on semanticsimilarity and lexical acquisition.</S><S sid = 127 ssid = >The same number of nouns was re trieved for each class using the graph model and LSI.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C02-1114.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002).</S> | Reference Offset:  ['57','119'] | Reference Text:  <S sid = 57 ssid = >Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998).</S><S sid = 119 ssid = >Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C02-1114.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This also allows indirect comparison to several other studies, thus (Widdows and Dorow, 2002) reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones.</S> | Reference Offset:  ['36','109'] | Reference Text:  <S sid = 36 ssid = >used in these cases was based on co occurrence in lists.</S><S sid = 109 ssid = >5.2 Results.</S> | Discourse Facet:  NA | Annotator: Automatic


