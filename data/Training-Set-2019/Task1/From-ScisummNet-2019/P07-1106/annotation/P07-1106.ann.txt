Citance Number: 1 | Reference Article:  P07-1106.txt | Citing Article:  P08-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S> | Reference Offset:  ['0','159'] | Reference Text:  <S sid = 0 ssid = >Chinese Segmentation with a Word-Based Perceptron Algorithm</S><S sid = 159 ssid = >In comparison, our system is based on a single perceptron model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1106.txt | Citing Article:  P14-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S><S sid = 172 ssid = >We thank the anonymous reviewers for their insightful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1106.txt | Citing Article:  D10-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Chinese Segmentation with a Word-Based Perceptron Algorithm</S><S sid = 1 ssid = >Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S><S sid = 172 ssid = >We thank the anonymous reviewers for their insightful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features.</S> | Reference Offset:  ['75','94'] | Reference Text:  <S sid = 75 ssid = >At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.</S><S sid = 94 ssid = >Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach.</S> | Reference Offset:  ['32','162'] | Reference Text:  <S sid = 32 ssid = >Furtherfor decoding. more, our approach provides an example of the poSeveral discriminatively trained models have re- tential of search-based discriminative training methcently been applied to the CWS problem.</S><S sid = 162 ssid = >We proposed a word-based CWS model using the discriminative perceptron learning algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.).</S> | Reference Offset:  ['136','169'] | Reference Text:  <S sid = 136 ssid = >Table 4 shows the accuracy with various features from the model removed.</S><S sid = 169 ssid = >Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model).</S> | Reference Offset:  ['155','169'] | Reference Text:  <S sid = 155 ssid = >We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.</S><S sid = 169 ssid = >Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1106.txt | Citing Article:  P12-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S><S sid = 172 ssid = >We thank the anonymous reviewers for their insightful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002).</S> | Reference Offset:  ['61','162'] | Reference Text:  <S sid = 61 ssid = >The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data.</S><S sid = 162 ssid = >We proposed a word-based CWS model using the discriminative perceptron learning algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model.</S> | Reference Offset:  ['98','159'] | Reference Text:  <S sid = 98 ssid = >One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.</S><S sid = 159 ssid = >In comparison, our system is based on a single perceptron model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset.</S> | Reference Offset:  ['105','107'] | Reference Text:  <S sid = 105 ssid = >The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff).</S><S sid = 107 ssid = >80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S><S sid = 172 ssid = >We thank the anonymous reviewers for their insightful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1106.txt | Citing Article:  C10-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference.</S> | Reference Offset:  ['31','82'] | Reference Text:  <S sid = 31 ssid = >These results demonstrate the imalgorithms such as the Viterbi algorithm can be used portance of word-based features for CWS.</S><S sid = 82 ssid = >Figure 2 gives the decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1106.txt | Citing Article:  C10-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is different from the experiments reported in (Zhang and Clark, 2007).</S> | Reference Offset:  ['104','167'] | Reference Text:  <S sid = 104 ssid = >Two sets of experiments were conducted.</S><S sid = 167 ssid = >Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1106.txt | Citing Article:  C10-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S><S sid = 172 ssid = >We thank the anonymous reviewers for their insightful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1106.txt | Citing Article:  P13-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used.</S> | Reference Offset:  ['47','82'] | Reference Text:  <S sid = 47 ssid = >Hence we are Boolean valued vectors containing the indicator use a beam-search decoder during training and test- features for one element in the sequence. ing; our idea is similar to that of Collins and Roark Denote the global feature vector for segmented (2004) who used a beam-search decoder as part of sentence y with 4b(y) E Rd, where d is the total a perceptron parsing model.</S><S sid = 82 ssid = >Figure 2 gives the decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1106.txt | Citing Article:  P13-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively.</S> | Reference Offset:  ['98','155'] | Reference Text:  <S sid = 98 ssid = >One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.</S><S sid = 155 ssid = >We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1106.txt | Citing Article:  P08-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).</S> | Reference Offset:  ['162','169'] | Reference Text:  <S sid = 162 ssid = >We proposed a word-based CWS model using the discriminative perceptron learning algorithm.</S><S sid = 169 ssid = >Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1106.txt | Citing Article:  P08-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Chinese Segmentation with a Word-Based Perceptron Algorithm</S><S sid = 15 ssid = >There is no fixed standard for Chinese word segmentation.</S> | Discourse Facet:  NA | Annotator: Automatic


