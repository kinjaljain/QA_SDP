Citance Number: 1 | Reference Article:  D07-1031.txt | Citing Article:  D10-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >[vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007).</S> | Reference Offset:  ['83','89'] | Reference Text:  <S sid = 83 ssid = >and Variational Bayes A Bayesian estimator combines a likelihood termP(x|?, ?) and a prior P(?, ?) to estimate the poste rior probability of a model or hidden state sequence.</S><S sid = 89 ssid = >Similarly, as ?y approaches zero the state-to-state transitions become sparser.There are two main techniques for Bayesian esti mation of such models: Markov Chain Monte Carlo(MCMC) and Variational Bayes (VB).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1031.txt | Citing Article:  D10-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009).</S> | Reference Offset:  ['127','142'] | Reference Text:  <S sid = 127 ssid = >However, in the ad hoc approach the expected count plus ??1 may be less than zero,resulting in a value of zero for the corresponding parameter (Johnson et al, 2007; Goldwater and Grif fiths, 2007).</S><S sid = 142 ssid = >(2002) and Teh et al (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1031.txt | Citing Article:  D10-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007).</S> | Reference Offset:  ['28','72'] | Reference Text:  <S sid = 28 ssid = >This section describes how we evaluate how well thesesequences of hidden states correspond to the gold standard POS tags for the training corpus (here, the PTB POS tags).</S><S sid = 72 ssid = >Wecan understand these results by comparing the dis tribution of words to hidden states to the distribution of words to POS tags in the gold-standard evaluation corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1031.txt | Citing Article:  D10-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These figures are the MCMC settings that provided the best results in Johnson (2007).</S> | Reference Offset:  ['90','136'] | Reference Text:  <S sid = 90 ssid = >MCMC en compasses a broad range of sampling techniques, including component-wise Gibbs sampling, which is the MCMC technique we used here (Robert and Casella, 2004; Bishop, 2006).</S><S sid = 136 ssid = >Interestingly, we obtained our best per formance on 1-to-1 accuracy when the Dirchlet prior?x = 0.1, a relatively large number, but best per formance on many-to-1 accuracy was achieved with a much lower value for the Dirichlet prior, namely ?x = 10?4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1031.txt | Citing Article:  D10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b).</S> | Reference Offset:  ['127','142'] | Reference Text:  <S sid = 127 ssid = >However, in the ad hoc approach the expected count plus ??1 may be less than zero,resulting in a value of zero for the corresponding parameter (Johnson et al, 2007; Goldwater and Grif fiths, 2007).</S><S sid = 142 ssid = >(2002) and Teh et al (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1031.txt | Citing Article:  P09-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed.</S> | Reference Offset:  ['2','9'] | Reference Text:  <S sid = 2 ssid = >We find that the HMMs es timated by EM generally assign a roughlyequal number of word tokens to each hid den state, while the empirical distribution of tokens to POS tags is highly skewed.</S><S sid = 9 ssid = >We suggest that onereason for the apparent failure of EM for POS tagging is that it tends to assign relatively equal numbers of tokens to each hidden state, while the em pirical distribution of POS tags is highly skewed, like many linguistic (and non-linguistic) phenomena(Mitzenmacher, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1031.txt | Citing Article:  D09-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags.</S> | Reference Offset:  ['30','35'] | Reference Text:  <S sid = 30 ssid = >We call this themany-to-1 accuracy of the hidden state sequence be cause several hidden states may map to the same POS tag (and some POS tags may not be mapped to by any hidden states at all).</S><S sid = 35 ssid = >This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted (note that if the number ofhidden states and POS tags differ, some will be unas signed).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1031.txt | Citing Article:  D09-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work.</S> | Reference Offset:  ['38','151'] | Reference Text:  <S sid = 38 ssid = >Goldwater and Griffiths (2007) propose using the Variation of Information (VI) metric described byMeila?</S><S sid = 151 ssid = >6 Conclusion and future work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1031.txt | Citing Article:  D09-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging.</S> | Reference Offset:  ['1','152'] | Reference Text:  <S sid = 1 ssid = >This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.</S><S sid = 152 ssid = >This paper studied why EM seems to do so badly in HMM estimation for unsupervised POS tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D07-1031.txt | Citing Article:  D09-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim.</S> | Reference Offset:  ['28','38'] | Reference Text:  <S sid = 28 ssid = >This section describes how we evaluate how well thesesequences of hidden states correspond to the gold standard POS tags for the training corpus (here, the PTB POS tags).</S><S sid = 38 ssid = >Goldwater and Griffiths (2007) propose using the Variation of Information (VI) metric described byMeila?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D07-1031.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own.</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >Why Doesn't EM Find Good HMM POS-Taggers?</S><S sid = 18 ssid = >Toutanova etal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D07-1031.txt | Citing Article:  P10-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages.</S> | Reference Offset:  ['116','154'] | Reference Text:  <S sid = 116 ssid = >Here m is the number of possible observations (i.e., the size of the vocabulary), s is the number of hidden states and I(?)</S><S sid = 154 ssid = >Then we noted the distribution of words to hidden states found by EM is relatively uniform, comparedto the distribution of words to POS tags in the eval uation corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D07-1031.txt | Citing Article:  P11-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007).</S> | Reference Offset:  ['11','36'] | Reference Text:  <S sid = 11 ssid = >In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?</S><S sid = 36 ssid = >We call the accuracy of the POS sequence obtained using this map its 1-to-1 accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D07-1031.txt | Citing Article:  P11-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007).</S> | Reference Offset:  ['25','33'] | Reference Text:  <S sid = 25 ssid = >(This ap proach cannot be used in an unsupervised setting since the empirical tag distribution is not available).</S><S sid = 33 ssid = >Cross-validation, i.e., identifying themany-to-1 mapping and evaluating on different subsets of the data, would answer many of these objections.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D07-1031.txt | Citing Article:  W11-0320.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007).</S> | Reference Offset:  ['161','162'] | Reference Text:  <S sid = 161 ssid = >Also, the Bayesian framework permits a wide variety of different priors besides Dirichlet priors explored here.</S><S sid = 162 ssid = >For example, it should be possible to encode linguistic knowledge such markedness preferences in a prior, and there are other linguistically uninformative priors, such the ?entropic priors?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D07-1031.txt | Citing Article:  P10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','165'] | Reference Text:  <S sid = 61 ssid = >Since we evaluatedthe accuracy of the estimated tags after each iteration, it was important that decoding be done effi ciently as well.</S><S sid = 165 ssid = >303</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D07-1031.txt | Citing Article:  W10-2925.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','165'] | Reference Text:  <S sid = 61 ssid = >Since we evaluatedthe accuracy of the estimated tags after each iteration, it was important that decoding be done effi ciently as well.</S><S sid = 165 ssid = >303</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D07-1031.txt | Citing Article:  W10-2925.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','165'] | Reference Text:  <S sid = 61 ssid = >Since we evaluatedthe accuracy of the estimated tags after each iteration, it was important that decoding be done effi ciently as well.</S><S sid = 165 ssid = >303</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D07-1031.txt | Citing Article:  N09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007).</S> | Reference Offset:  ['27','57'] | Reference Text:  <S sid = 27 ssid = >All of the experiments described below have the same basic structure: an estimator is used to infera bitag HMM from the unsupervised training cor pus (the words of Penn Treebank (PTB) Wall Street Journal corpus (Marcus et al, 1993)), and then the resulting model is used to label each word of that corpus with one of the HMM?s hidden states.</S><S sid = 57 ssid = >It?s well-known thataccuracy often decreases after the first few EM it erations (which we also observed); however in our experiments we found that performance improves again after 100 iterations and continues improving roughly monotonically.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D07-1031.txt | Citing Article:  C10-2159.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state.</S> | Reference Offset:  ['9','152'] | Reference Text:  <S sid = 9 ssid = >We suggest that onereason for the apparent failure of EM for POS tagging is that it tends to assign relatively equal numbers of tokens to each hidden state, while the em pirical distribution of POS tags is highly skewed, like many linguistic (and non-linguistic) phenomena(Mitzenmacher, 2003).</S><S sid = 152 ssid = >This paper studied why EM seems to do so badly in HMM estimation for unsupervised POS tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


