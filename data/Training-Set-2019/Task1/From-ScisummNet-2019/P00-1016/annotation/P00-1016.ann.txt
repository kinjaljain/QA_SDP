Citance Number: 1 | Reference Article:  P00-1016.txt | Citing Article:  P01-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing.</S> | Reference Offset:  ['33','76'] | Reference Text:  <S sid = 33 ssid = >Engelson & Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners.</S><S sid = 76 ssid = >The rule-writing experiments were conducted by a group of 17 advanced computer science students, using the identical test set as in the annotation experiments and the same initial 100 gold standard sentences for both initial bracketing standards guidance and rule-quality feedback throughout their work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P00-1016.txt | Citing Article:  P13-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective.</S> | Reference Offset:  ['3','46'] | Reference Text:  <S sid = 3 ssid = >Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.</S><S sid = 46 ssid = >Because our goal is to investigate the relative costs of rule writing versus annotation, it is essential that we use a realistic model of annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P00-1016.txt | Citing Article:  P10-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking.</S> | Reference Offset:  ['0','46'] | Reference Text:  <S sid = 0 ssid = >Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking</S><S sid = 46 ssid = >Because our goal is to investigate the relative costs of rule writing versus annotation, it is essential that we use a realistic model of annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P00-1016.txt | Citing Article:  D08-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning.</S> | Reference Offset:  ['1','13'] | Reference Text:  <S sid = 1 ssid = >This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.</S><S sid = 13 ssid = >This paper will conclude with a comprehensive cost model exposition and analysis, and an empirical study contrasting human rule-writing versus annotation-based learning approaches that are sensitive to these cost models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P00-1016.txt | Citing Article:  W05-0619.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000).</S> | Reference Offset:  ['29','72'] | Reference Text:  <S sid = 29 ssid = >On the experimental side, active learning has been applied to several different problems.</S><S sid = 72 ssid = >Table 1 presents a comparison of our rule format against that of Brill & Ngai's.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P00-1016.txt | Citing Article:  P08-2017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking</S><S sid = 14 ssid = >The domain in which our experiments are performed is base noun phrase chunking.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P00-1016.txt | Citing Article:  P08-2017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence.</S> | Reference Offset:  ['53','83'] | Reference Text:  <S sid = 53 ssid = >To acquaint the subjects with the Treebank conventions, they were first asked to spend some time in a feedback phase, where they would annotate up to 50 sentences (they were allowed to stop at any time) drawn from the initial 100 sentences in T. The sentences were annotated one at a time, and the Treebank annotation was shown to them after every sentence.</S><S sid = 83 ssid = >The x-axes show the time spent by each human subject (either annotating or writing rules) in minutes; the y-axes show the f-measure performance achieved by the systems built using the given level of supervision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P00-1016.txt | Citing Article:  P07-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking.</S> | Reference Offset:  ['41','102'] | Reference Text:  <S sid = 41 ssid = >To divide the corpus into the different subsets in Step 3, we tried using two approaches: bagging and n-fold partitioning.</S><S sid = 102 ssid = >The potential disadvantages of annotationbased system development for applications such as base NP chunking are limited.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P00-1016.txt | Citing Article:  D09-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000).</S> | Reference Offset:  ['25','56'] | Reference Text:  <S sid = 25 ssid = >The idea is that the more uncertain the example, the less well modeled this situation is, and therefore, the more useful it would be to have this example annotated.</S><S sid = 56 ssid = >The f-complement disagreement measure was used to select 50 sentences from the rest of Ramshaw & Marcus' training set and the annotator was instructed to annotate them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P00-1016.txt | Citing Article:  P08-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007).</S> | Reference Offset:  ['6','30'] | Reference Text:  <S sid = 6 ssid = >The construction of the Penn Treebank significantly improved performance for English systems dealing in the &quot;traditional&quot; NLP domains (eg parsing, part-of-speech tagging, etc).</S><S sid = 30 ssid = >Lewis & Gale (1994), Lewis & Catlett (1994) and Liere & Tadepalli (1997) all applied it to text categorization; Engelson & Dagan (1996) applied it to part-of-speech tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P00-1016.txt | Citing Article:  W09-1905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach.</S> | Reference Offset:  ['3','26'] | Reference Text:  <S sid = 3 ssid = >Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.</S><S sid = 26 ssid = >Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P00-1016.txt | Citing Article:  W07-1502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking</S><S sid = 14 ssid = >The domain in which our experiments are performed is base noun phrase chunking.</S> | Discourse Facet:  NA | Annotator: Automatic


