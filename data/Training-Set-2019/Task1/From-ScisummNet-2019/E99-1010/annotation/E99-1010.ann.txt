Citance Number: 1 | Reference Article:  E99-1010.txt | Citing Article:  P01-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class.</S> | Reference Offset:  ['38','72'] | Reference Text:  <S sid = 38 ssid = >We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).</S><S sid = 72 ssid = >The alignment templates are automatically trained using a parallel training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E99-1010.txt | Citing Article:  P12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the publicly available implementation MK CLS3 (Och, 1999) to train this model.</S> | Reference Offset:  ['68','76'] | Reference Text:  <S sid = 68 ssid = >The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.</S><S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E99-1010.txt | Citing Article:  W10-3812.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization.</S> | Reference Offset:  ['68','76'] | Reference Text:  <S sid = 68 ssid = >The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.</S><S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E99-1010.txt | Citing Article:  N07-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code.</S> | Reference Offset:  ['68','76'] | Reference Text:  <S sid = 68 ssid = >The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.</S><S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E99-1010.txt | Citing Article:  W06-3108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The word classes for the class-based features are trained using the mkcls tool (Och, 1999).</S> | Reference Offset:  ['35','72'] | Reference Text:  <S sid = 35 ssid = >For the first we use the class-based bigram probability from Eq.</S><S sid = 72 ssid = >The alignment templates are automatically trained using a parallel training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E99-1010.txt | Citing Article:  P11-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the publicly available implementation MKCLS (Och, 1999) to train this model.</S> | Reference Offset:  ['68','76'] | Reference Text:  <S sid = 68 ssid = >The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.</S><S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E99-1010.txt | Citing Article:  W07-0720.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments).</S> | Reference Offset:  ['82','91'] | Reference Text:  <S sid = 82 ssid = >The target language of our experiments is English.</S><S sid = 91 ssid = >For EuTRANs-I we used 60 classes and for EuTRANs-II we used 500 classes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E99-1010.txt | Citing Article:  W07-0720.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This feature implements a 5-gram language model of target statistical classes (Och, 1999).</S> | Reference Offset:  ['82','88'] | Reference Text:  <S sid = 82 ssid = >The target language of our experiments is English.</S><S sid = 88 ssid = >The classes BIL-2 are determined by first optimizing mono-lingually classes for the target language (English) and afterwards optimizing classes for the source language (Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E99-1010.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems.</S> | Reference Offset:  ['5','68'] | Reference Text:  <S sid = 5 ssid = >We will show that the usage of the bilingual word classes we get can improve statistical machine translation.</S><S sid = 68 ssid = >The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E99-1010.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a baseline we report the performance of mkcls (Och, 1999) on all test corpora.</S> | Reference Offset:  ['76','92'] | Reference Text:  <S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S><S sid = 92 ssid = >We chose the number of classes in such a way that the final performance of the translation system was optimal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E99-1010.txt | Citing Article:  W05-0804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model.</S> | Reference Offset:  ['71','109'] | Reference Text:  <S sid = 71 ssid = >The advantage of the alignment template approach against word-based statistical translation models is that word context and local re-orderings are explicitly taken into account.</S><S sid = 109 ssid = >The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E99-1010.txt | Citing Article:  W05-0804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach was shown to give the best results in (Och, 1999).</S> | Reference Offset:  ['70','102'] | Reference Text:  <S sid = 70 ssid = >Examples of alignment templates are shown in Figure 2.</S><S sid = 102 ssid = >The bilingual classes show better results than the monolingual classes MONO.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E99-1010.txt | Citing Article:  W05-0804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Practically, we can use word alignment as used in (Och, 1999).</S> | Reference Offset:  ['41','74'] | Reference Text:  <S sid = 41 ssid = >The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996).</S><S sid = 74 ssid = >The bilingual word classes are used to generalize the applicability of the alignment templates in search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E99-1010.txt | Citing Article:  W12-3144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes.</S> | Reference Offset:  ['21','76'] | Reference Text:  <S sid = 21 ssid = >Rewriting the corpus probability using classes we arrive at the following probability model p(wiv IC): In this model we have two types of probabilities: the transition probability p(C1C1) for class C given its predecessor class C' and the membership probability p(wIC) for word w given class C. To determine the optimal classes C for a given number of classes M we perform a maximumlikelihood approach: = arg mrc p(wiv IC) (2) We estimate the probabilities of Eq.</S><S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E99-1010.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.</S> | Reference Offset:  ['0','74'] | Reference Text:  <S sid = 0 ssid = >An Efficient Method For Determining Bilingual Word Classes</S><S sid = 74 ssid = >The bilingual word classes are used to generalize the applicability of the alignment templates in search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E99-1010.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011).</S> | Reference Offset:  ['38','41'] | Reference Text:  <S sid = 38 ssid = >We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).</S><S sid = 41 ssid = >The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E99-1010.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999).</S> | Reference Offset:  ['91','107'] | Reference Text:  <S sid = 91 ssid = >For EuTRANs-I we used 60 classes and for EuTRANs-II we used 500 classes.</S><S sid = 107 ssid = >By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E99-1010.txt | Citing Article:  P12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >An Efficient Method For Determining Bilingual Word Classes</S><S sid = 14 ssid = >The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E99-1010.txt | Citing Article:  P12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes.</S> | Reference Offset:  ['72','76'] | Reference Text:  <S sid = 72 ssid = >The alignment templates are automatically trained using a parallel training corpus.</S><S sid = 76 ssid = >More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E99-1010.txt | Citing Article:  W07-0721.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999).</S> | Reference Offset:  ['54','72'] | Reference Text:  <S sid = 54 ssid = >(6) and secondly we determine classes F optimizing the bilingual part (without changing 6): By using these two optimization processes we enforce that the classes E are mono-lingually 'good' classes and that the classes .7- correspond to 6.</S><S sid = 72 ssid = >The alignment templates are automatically trained using a parallel training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


