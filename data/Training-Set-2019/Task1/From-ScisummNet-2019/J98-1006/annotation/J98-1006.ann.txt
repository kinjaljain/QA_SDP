Citance Number: 1 | Reference Article:  J98-1006.txt | Citing Article:  E12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora.</S> | Reference Offset:  ['28','325'] | Reference Text:  <S sid = 28 ssid = >The Cognitive Science Laboratory at Princeton University, with support from NSF-ARPA, is producing textual corpora that can be used in developing and evaluating automatic methods for disambiguation.</S><S sid = 325 ssid = >Because the supply of manually tagged training data will always be limited, we propose a method to obtain training data automatically using commonly available materials: exploiting WordNet's lexical relations to harvest training examples from LDC corpora or even the World Wide Web.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J98-1006.txt | Citing Article:  C08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content.</S> | Reference Offset:  ['251','254'] | Reference Text:  <S sid = 251 ssid = >The similarity measures on WordNet found that sauerbraten was most similar to dinner in the training, and dumpling to bacon.</S><S sid = 254 ssid = >Augmentation of the local .context classifier with WordNet similarity measures showed a small but consistent improvement in the classifier's performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J98-1006.txt | Citing Article:  W06-1649.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995).</S> | Reference Offset:  ['125','133'] | Reference Text:  <S sid = 125 ssid = >Thus, Leacock, Towell, and Voorhees (1993) found that some senses of the noun line are not susceptible to disambiguation with topical context.</S><S sid = 133 ssid = >An estimate is possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J98-1006.txt | Citing Article:  P05-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998).</S> | Reference Offset:  ['96','188'] | Reference Text:  <S sid = 96 ssid = >Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj s,), including providing probabilities for cues that did not occur in the training.</S><S sid = 188 ssid = >The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J98-1006.txt | Citing Article:  C08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF.</S> | Reference Offset:  ['199','257'] | Reference Text:  <S sid = 199 ssid = >We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous.</S><S sid = 257 ssid = >A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J98-1006.txt | Citing Article:  W12-2429.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was first suggested by Leacock et al (1998).</S> | Reference Offset:  ['49','188'] | Reference Text:  <S sid = 49 ssid = >Leacock, Towell, and Voorhees (1996) showed that performance of the content vector topical classifier could be improved with the addition of local templates— specific word patterns that were recognized as being indicative of a particular sense— in an extension of an idea initially suggested by Weiss (1973).</S><S sid = 188 ssid = >The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J98-1006.txt | Citing Article:  W04-0854.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet.</S> | Reference Offset:  ['199','273'] | Reference Text:  <S sid = 199 ssid = >We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous.</S><S sid = 273 ssid = >Finally, we hope to avoid inclusion of spurious senses by using monosemous relatives.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J98-1006.txt | Citing Article:  W04-0854.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives.</S> | Reference Offset:  ['329','333'] | Reference Text:  <S sid = 329 ssid = >For a polysemous word, locate the monosemous relatives for each of its senses in WordNet and extract examples containing these relatives from a large corpus.</S><S sid = 333 ssid = >Since the frequencies of the monosemous relatives do not correlate with the frequencies of the senses, prior probabilities must be estimated for classifiers that use them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J98-1006.txt | Citing Article:  I08-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words.</S> | Reference Offset:  ['14','325'] | Reference Text:  <S sid = 14 ssid = >They range from dictionary-based approaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994).</S><S sid = 325 ssid = >Because the supply of manually tagged training data will always be limited, we propose a method to obtain training data automatically using commonly available materials: exploiting WordNet's lexical relations to harvest training examples from LDC corpora or even the World Wide Web.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J98-1006.txt | Citing Article:  W08-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF.</S> | Reference Offset:  ['199','257'] | Reference Text:  <S sid = 199 ssid = >We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous.</S><S sid = 257 ssid = >A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J98-1006.txt | Citing Article:  C08-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples.</S> | Reference Offset:  ['101','109'] | Reference Text:  <S sid = 101 ssid = >Positions j = —2, —1, 1,2 are used.</S><S sid = 109 ssid = >When it is configured for local information, cue types (2), (3), and (4) are used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J98-1006.txt | Citing Article:  P04-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words.</S> | Reference Offset:  ['130','188'] | Reference Text:  <S sid = 130 ssid = >This was done on the assumption that these words are not ambiguous.</S><S sid = 188 ssid = >The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J98-1006.txt | Citing Article:  W08-2114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy.</S> | Reference Offset:  ['88','233'] | Reference Text:  <S sid = 88 ssid = >This step normalizes across morphological variants without resorting to the more drastic measure of stemming.</S><S sid = 233 ssid = >The verbal hierarchy is based on troponymy, the is a manner of relation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J98-1006.txt | Citing Article:  W06-1621.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)).</S> | Reference Offset:  ['251','254'] | Reference Text:  <S sid = 251 ssid = >The similarity measures on WordNet found that sauerbraten was most similar to dinner in the training, and dumpling to bacon.</S><S sid = 254 ssid = >Augmentation of the local .context classifier with WordNet similarity measures showed a small but consistent improvement in the classifier's performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J98-1006.txt | Citing Article:  W04-0847.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.</S> | Reference Offset:  ['44','120'] | Reference Text:  <S sid = 44 ssid = >Leacock, Towel!, and Voorhees (1993) compared this Bayesian classifier with a content vector classifier as used in information retrieval and a neural network with backpropagation.</S><S sid = 120 ssid = >All classifiers performed best with at least 200 training examples per sense, but the learning curve tended to level off beyond a minimum 100 training examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J98-1006.txt | Citing Article:  N10-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes.</S> | Reference Offset:  ['256','257'] | Reference Text:  <S sid = 256 ssid = >Resnik (1992) uses an information-based measure, the most informative class, on the WordNet taxonomy.</S><S sid = 257 ssid = >A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J98-1006.txt | Citing Article:  S10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008).</S> | Reference Offset:  ['14','188'] | Reference Text:  <S sid = 14 ssid = >They range from dictionary-based approaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994).</S><S sid = 188 ssid = >The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J98-1006.txt | Citing Article:  W04-3204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical).</S> | Reference Offset:  ['279','329'] | Reference Text:  <S sid = 279 ssid = >The system first looks for the strongest or top-level relatives: for monosemous synonyms of the sense (e.g., tribunal) and for daughter collocations that contain the target word as the head (e.g., superior court) and tallies the number of examples in the corpus for each.</S><S sid = 329 ssid = >For a polysemous word, locate the monosemous relatives for each of its senses in WordNet and extract examples containing these relatives from a large corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J98-1006.txt | Citing Article:  W04-3204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented.</S> | Reference Offset:  ['305','326'] | Reference Text:  <S sid = 305 ssid = >Table 6 shows TLC's performance on the other eight words after training with monosemous relatives and testing on manually tagged examples.</S><S sid = 326 ssid = >We found this method to be effective, although not as effective as using manually tagged training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J98-1006.txt | Citing Article:  W04-3204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This method is inspired in (Leacock et al, 1998).</S> | Reference Offset:  ['188','203'] | Reference Text:  <S sid = 188 ssid = >The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).</S><S sid = 203 ssid = >Schulze has used the method to disambiguate pseudowords, homographs, and polysemous words.</S> | Discourse Facet:  NA | Annotator: Automatic


