Citance Number: 1 | Reference Article:  N06-1041.txt | Citing Article:  P07-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary.</S> | Reference Offset:  ['22','125'] | Reference Text:  <S sid = 22 ssid = >Our general approach is to use distributional similarity to link any given word to similar prototypes.</S><S sid = 125 ssid = >The best comparison is to Smith and Eisner (2005) who use a partial tagging dictionary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N06-1041.txt | Citing Article:  P14-2044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007).</S> | Reference Offset:  ['91','102'] | Reference Text:  <S sid = 91 ssid = >We followed the common approach in the literature, greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset.</S><S sid = 102 ssid = >We used the dot product between left singular vectors as a measure of distributional similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N06-1041.txt | Citing Article:  D09-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate.</S> | Reference Offset:  ['15','18'] | Reference Text:  <S sid = 15 ssid = >For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).</S><S sid = 18 ssid = >Second, it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N06-1041.txt | Citing Article:  D09-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)).</S> | Reference Offset:  ['16','167'] | Reference Text:  <S sid = 16 ssid = >This manner of specifying prior knowledge about the task has several advantages.</S><S sid = 167 ssid = >These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N06-1041.txt | Citing Article:  D09-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Prototype-Driven Learning For Sequence Models</S><S sid = 14 ssid = >In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N06-1041.txt | Citing Article:  D09-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3.</S> | Reference Offset:  ['109','137'] | Reference Text:  <S sid = 109 ssid = >4Details of distributional similarity features: To extract context vectors, we used a window of size 2 in either direction and use the first 250 singular vectors.</S><S sid = 137 ssid = >Nonetheless, the addition of distributional similarity features does reduce the error rate by 35% from BASE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N06-1041.txt | Citing Article:  D09-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples.</S> | Reference Offset:  ['143','157'] | Reference Text:  <S sid = 143 ssid = >On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)’s unsupervised HMM baseline.</S><S sid = 157 ssid = >Furthermore, our PROTO+SIM+BOUND model comes close to the supervised HMM accuracy of 74.4% reported in Grenager et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N06-1041.txt | Citing Article:  W10-4109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another interesting idea is to select some exemplars (Haghighi and Klein, 2006).</S> | Reference Offset:  ['52','63'] | Reference Text:  <S sid = 52 ssid = >As a result, learning an MRF is slightly harder than learning a CRF; we discuss this issue in section 4.4.</S><S sid = 63 ssid = >The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness (Radford, 1988).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N06-1041.txt | Citing Article:  D12-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting.</S> | Reference Offset:  ['98','113'] | Reference Text:  <S sid = 98 ssid = >It appears as though the prototype information is not spreading to non-prototype words.</S><S sid = 113 ssid = >However, rare words with a single prototype feature are almost always given that prototype’s label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N06-1041.txt | Citing Article:  N09-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features.</S> | Reference Offset:  ['22','111'] | Reference Text:  <S sid = 22 ssid = >Our general approach is to use distributional similarity to link any given word to similar prototypes.</S><S sid = 111 ssid = >We limited each word to have similarity features for its top 5 most similar prototypes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N06-1041.txt | Citing Article:  P07-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity.</S> | Reference Offset:  ['26','125'] | Reference Text:  <S sid = 26 ssid = >For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.</S><S sid = 125 ssid = >The best comparison is to Smith and Eisner (2005) who use a partial tagging dictionary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N06-1041.txt | Citing Article:  W11-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006).</S> | Reference Offset:  ['136','138'] | Reference Text:  <S sid = 136 ssid = >We believe the performance for Chinese POS tagging is not as high as English for two reasons: the general difficulty of Chinese POS tagging (Tseng et al., 2005) and the lack of a larger segmented corpus from which to build distributional models.</S><S sid = 138 ssid = >We tested our framework on the CLASSIFIEDS data described in Grenager et al. (2005) under conditions similar to POS tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N06-1041.txt | Citing Article:  W11-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006).</S> | Reference Offset:  ['13','44'] | Reference Text:  <S sid = 13 ssid = >In particular, we argue for a certain kind of semi-supervised learning, which we call prototype-driven learning.</S><S sid = 44 ssid = >The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N06-1041.txt | Citing Article:  I08-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners.</S> | Reference Offset:  ['36','72'] | Reference Text:  <S sid = 36 ssid = >Both of these works require specification of the legal tags for each word.</S><S sid = 72 ssid = >Since we have a log-linear formulation, we instead use a gradientbased search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N06-1041.txt | Citing Article:  I08-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly.</S> | Reference Offset:  ['26','42'] | Reference Text:  <S sid = 26 ssid = >For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.</S><S sid = 42 ssid = >Given enough refinement set of the CLASSIFIEDS data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N06-1041.txt | Citing Article:  I08-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006).</S> | Reference Offset:  ['14','113'] | Reference Text:  <S sid = 14 ssid = >In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.</S><S sid = 113 ssid = >However, rare words with a single prototype feature are almost always given that prototype’s label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N06-1041.txt | Citing Article:  W11-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features.</S> | Reference Offset:  ['86','132'] | Reference Text:  <S sid = 86 ssid = >We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in Smith and Eisner (2005): exact word type, character suffixes of length up to 3, initial-capital, contains-hyphen, and contains-digit.</S><S sid = 132 ssid = >We also tested our POS induction system on the Chinese POS data in the Chinese Treebank (Ircs, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N06-1041.txt | Citing Article:  W11-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs.</S> | Reference Offset:  ['5','15'] | Reference Text:  <S sid = 5 ssid = >For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.</S><S sid = 15 ssid = >For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N06-1041.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation.</S> | Reference Offset:  ['86','121'] | Reference Text:  <S sid = 86 ssid = >We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in Smith and Eisner (2005): exact word type, character suffixes of length up to 3, initial-capital, contains-hyphen, and contains-digit.</S><S sid = 121 ssid = >Of course in a realistic, iterative design setting, we could have altered the prototype list to include a contains-digit prototype for CD and corrected this confusion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N06-1041.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data.</S> | Reference Offset:  ['26','133'] | Reference Text:  <S sid = 26 ssid = >For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.</S><S sid = 133 ssid = >The model is wholly unmodified from the English version except that the suffix features are removed since, in Chinese, suffixes are not a reliable indicator of part-of-speech as in English (Tseng et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


