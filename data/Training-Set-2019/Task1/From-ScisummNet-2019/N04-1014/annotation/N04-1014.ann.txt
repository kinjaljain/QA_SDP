Citance Number: 1 | Reference Article:  N04-1014.txt | Citing Article:  P05-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Training Tree Transducers</S><S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1014.txt | Citing Article:  P14-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013).</S> | Reference Offset:  ['4','127'] | Reference Text:  <S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S><S sid = 127 ssid = >In this section, we implement the translation model of (Yamada and Knight, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1014.txt | Citing Article:  W06-3601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Training Tree Transducers</S><S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1014.txt | Citing Article:  W06-3601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs).</S> | Reference Offset:  ['113','127'] | Reference Text:  <S sid = 113 ssid = >We now turn to tree-to-string transducers (xRS).</S><S sid = 127 ssid = >In this section, we implement the translation model of (Yamada and Knight, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1014.txt | Citing Article:  C08-5001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)).</S> | Reference Offset:  ['32','119'] | Reference Text:  <S sid = 32 ssid = >However, we can simulate lookahead using states, as in these productions: By omitting rules like qpro NP → ..., we ensure that the entire production sequence will dead-end unless the first child of the input tree is in fact PRO.</S><S sid = 119 ssid = >Input: xR transducer X = (E, A, Q, Qd, R), observed weighted tree pairs T ∈ TE × TA × R+, normalization function Z({countr  |r ∈ R}, r′ ∈ R), minimum relative log-likelihood change for convergence ǫ ∈ R+, maximum number of iterations maxit ∈ N, and prior counts (for a so-called Dirichlet prior) {priorr  |r ∈ R} for smoothing each rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1014.txt | Citing Article:  W07-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Training Tree Transducers</S><S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1014.txt | Citing Article:  P08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation.</S> | Reference Offset:  ['4','113'] | Reference Text:  <S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S><S sid = 113 ssid = >We now turn to tree-to-string transducers (xRS).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1014.txt | Citing Article:  D09-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004).</S> | Reference Offset:  ['109','144'] | Reference Text:  <S sid = 109 ssid = >Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.</S><S sid = 144 ssid = >Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1014.txt | Citing Article:  P06-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)).</S> | Reference Offset:  ['3','113'] | Reference Text:  <S sid = 3 ssid = >The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.</S><S sid = 113 ssid = >We now turn to tree-to-string transducers (xRS).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1014.txt | Citing Article:  P06-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm.</S> | Reference Offset:  ['69','109'] | Reference Text:  <S sid = 69 ssid = >What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅.</S><S sid = 109 ssid = >Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1014.txt | Citing Article:  I08-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings.</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >Training Tree Transducers</S><S sid = 2 ssid = >Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1014.txt | Citing Article:  W10-2503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982).</S> | Reference Offset:  ['10','12'] | Reference Text:  <S sid = 10 ssid = >Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003).</S><S sid = 12 ssid = >(Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1014.txt | Citing Article:  W10-2503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example.</S> | Reference Offset:  ['26','116'] | Reference Text:  <S sid = 26 ssid = >One advantage of working with tree transducers is the large and useful body of literature about these automata; two excellent surveys are (Gécseg and Steinby, 1984) and (Comon et al., 1997).</S><S sid = 116 ssid = >We give an explicit tree-to-string transducer example in the next section.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1014.txt | Citing Article:  C10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006).</S> | Reference Offset:  ['116','129'] | Reference Text:  <S sid = 116 ssid = >We give an explicit tree-to-string transducer example in the next section.</S><S sid = 129 ssid = >We now build a trainable xRS tree-to-string transducer that embodies the same P(Japanese string  |English tree).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1014.txt | Citing Article:  C10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates.</S> | Reference Offset:  ['43','96'] | Reference Text:  <S sid = 43 ssid = >The inside and outside probabilities of this packed derivation structure are used to compute expected counts of the productions from the original, given transducer (Sections 6-7).</S><S sid = 96 ssid = >Given a wRTG G = (E, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside-outside algorithm for weighted context-free (string) grammars (Lari and Young,1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1014.txt | Citing Article:  P06-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Training Tree Transducers</S><S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1014.txt | Citing Article:  W06-1628.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Graehl and Knight (2004) describe methods for training tree transducers.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Training Tree Transducers</S><S sid = 4 ssid = >We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1014.txt | Citing Article:  D07-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such an algorithm is presented by Graehl and Knight (2004).</S> | Reference Offset:  ['127','144'] | Reference Text:  <S sid = 127 ssid = >In this section, we implement the translation model of (Yamada and Knight, 2001).</S><S sid = 144 ssid = >Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1014.txt | Citing Article:  D07-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends.</S> | Reference Offset:  ['104','112'] | Reference Text:  <S sid = 104 ssid = >If enumerating rules matching transducer input-patterns and output-subtrees has cost L (constant given a transducer), then DERIV has time complexity O(L · |Q |· |I |· |O |· |R|).</S><S sid = 112 ssid = >For a corpus of K examples with average input/output size M, an iteration takes (at worst) O(|Q |· |R |· K · M2) time—quadratic, like the forward-backward algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1014.txt | Citing Article:  D07-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)).</S> | Reference Offset:  ['109','144'] | Reference Text:  <S sid = 109 ssid = >Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.</S><S sid = 144 ssid = >Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.</S> | Discourse Facet:  NA | Annotator: Automatic


