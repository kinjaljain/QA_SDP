Citance Number: 1 | Reference Article:  W02-2018.txt | Citing Article:  C02-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002).</S> | Reference Offset:  ['4','13'] | Reference Text:  <S sid = 4 ssid = >In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.</S><S sid = 13 ssid = >Thus, highly efficient, accurate, scalable methods are required for estimating the parameters of practical models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-2018.txt | Citing Article:  S12-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model.</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >A Comparison Of Algorithms For Maximum Entropy Parameter Estimation</S><S sid = 18 ssid = >In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w ∈ W. A conditional maximum entropy model qθ(x|w) for p has the parametric form (Berger et al., 1996; Chi, 1998; where θ is a d-dimensional parameter vector and θT f (x) is the inner product of the parameter vector and a feature vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-2018.txt | Citing Article:  N07-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002).</S> | Reference Offset:  ['109','112'] | Reference Text:  <S sid = 109 ssid = >And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.</S><S sid = 112 ssid = >And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-2018.txt | Citing Article:  N04-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights.</S> | Reference Offset:  ['98','109'] | Reference Text:  <S sid = 98 ssid = >However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.</S><S sid = 109 ssid = >And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-2018.txt | Citing Article:  I08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['30','122'] | Reference Text:  <S sid = 30 ssid = >Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.</S><S sid = 122 ssid = >Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-2018.txt | Citing Article:  C10-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage.</S> | Reference Offset:  ['98','109'] | Reference Text:  <S sid = 98 ssid = >However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.</S><S sid = 109 ssid = >And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-2018.txt | Citing Article:  D10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['30','122'] | Reference Text:  <S sid = 30 ssid = >Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.</S><S sid = 122 ssid = >Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-2018.txt | Citing Article:  P06-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['30','122'] | Reference Text:  <S sid = 30 ssid = >Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.</S><S sid = 122 ssid = >Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-2018.txt | Citing Article:  C10-2163.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002).</S> | Reference Offset:  ['0','83'] | Reference Text:  <S sid = 0 ssid = >A Comparison Of Algorithms For Maximum Entropy Parameter Estimation</S><S sid = 83 ssid = >Thus, the use of such optimizations is independent of the choice of parameter estimation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-2018.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >because this method seems substantially faster than comparable methods (Malouf, 2002).</S> | Reference Offset:  ['25','109'] | Reference Text:  <S sid = 25 ssid = >While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates δ(k) at each search step differs substantially.</S><S sid = 109 ssid = >And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-2018.txt | Citing Article:  W06-1661.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior.</S> | Reference Offset:  ['55','109'] | Reference Text:  <S sid = 55 ssid = >For such cases, we can apply limited memory variable metric methods, which implicitly approximate the Hessian matrix in the vicinity of the current estimate of θ(k) using the previous m values of y(k) and δ(k).</S><S sid = 109 ssid = >And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-2018.txt | Citing Article:  P06-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process.</S> | Reference Offset:  ['7','115'] | Reference Text:  <S sid = 7 ssid = >In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999).</S><S sid = 115 ssid = >And, since the parameters of individual models can be estimated quite quickly, this will further open up the possibility for more sophisticated model and feature selection techniques which compare large numbers of alternative model specifications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-2018.txt | Citing Article:  E09-3005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002).</S> | Reference Offset:  ['83','98'] | Reference Text:  <S sid = 83 ssid = >Thus, the use of such optimizations is independent of the choice of parameter estimation method.</S><S sid = 98 ssid = >However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-2018.txt | Citing Article:  P07-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002).</S> | Reference Offset:  ['19','104'] | Reference Text:  <S sid = 19 ssid = >Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector θ which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: ratio of Ep[f] to Eq(k)[f], with the restriction that ∑j fj(x) = C for each event x in the training data (a condition which can be easily satisfied by the addition of a correction feature).</S><S sid = 104 ssid = >In this case, the training data is very sparse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-2018.txt | Citing Article:  W10-3007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For model estimation we use the TADM3 software (Malouf, 2002).</S> | Reference Offset:  ['83','89'] | Reference Text:  <S sid = 83 ssid = >Thus, the use of such optimizations is independent of the choice of parameter estimation method.</S><S sid = 89 ssid = >For each run, we report the KL divergence between the fitted model and the training data at convergence, the prediction accuracy of fitted model on a held-out test set (the fraction of contexts for which the event with the highest probability under the model also had the highest probability under the reference distribution), the number of iterations required, the number of log-likelihood and gradient evaluations required (algorithms which use a line search may require several function evaluations per iteration), and the total elapsed time (in seconds).2 There are a few things to observe about these results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-2018.txt | Citing Article:  E09-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002).</S> | Reference Offset:  ['4','14'] | Reference Text:  <S sid = 4 ssid = >In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.</S><S sid = 14 ssid = >In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-2018.txt | Citing Article:  W09-1204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For parameter estimation, we use the open source TADM system (Malouf, 2002).</S> | Reference Offset:  ['0','83'] | Reference Text:  <S sid = 0 ssid = >A Comparison Of Algorithms For Maximum Entropy Parameter Estimation</S><S sid = 83 ssid = >Thus, the use of such optimizations is independent of the choice of parameter estimation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-2018.txt | Citing Article:  W07-1204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally.</S> | Reference Offset:  ['72','112'] | Reference Text:  <S sid = 72 ssid = >For the other optimization techniques, we used TAO (the “Toolkit for Advanced Optimization”), a library layered on top of the foundation of PETSc for solving nonlinear optimization problems (Benson et al., 2002).</S><S sid = 112 ssid = >And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-2018.txt | Citing Article:  D08-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002).</S> | Reference Offset:  ['72','112'] | Reference Text:  <S sid = 72 ssid = >For the other optimization techniques, we used TAO (the “Toolkit for Advanced Optimization”), a library layered on top of the foundation of PETSc for solving nonlinear optimization problems (Benson et al., 2002).</S><S sid = 112 ssid = >And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-2018.txt | Citing Article:  W04-3223.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed.</S> | Reference Offset:  ['70','103'] | Reference Text:  <S sid = 70 ssid = >Then given a parameter vector θ, the unnormalized probabilities ˙q0 are the matrix-vector product: and the feature expectations are the transposed matrix-vector product: By expressing these computations as matrix-vector operations, we can take advantage of the high performance sparse matrix primitives of PETSc.</S><S sid = 103 ssid = >More dramatically, both iterative scaling methods perform very poorly on the ‘shallow’ dataset.</S> | Discourse Facet:  NA | Annotator: Automatic


