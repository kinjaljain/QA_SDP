Citance Number: 1 | Reference Article:  H94-1048.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, Ratnaparkhi et al (1994) conducted human experiments with a subset of their corpus.</S> | Reference Offset:  ['11','146'] | Reference Text:  <S sid = 11 ssid = >We also describe a search procedure for selecting a "good" subset of features from a much larger pool of features for PP-attachment.</S><S sid = 146 ssid = >Magerman, D., 1994.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  H94-1048.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model.</S> | Reference Offset:  ['70','138'] | Reference Text:  <S sid = 70 ssid = >Train Maximum Entropy Model, using features in .A4 5.</S><S sid = 138 ssid = >Wadsworth and Brooks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  H94-1048.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet.</S> | Reference Offset:  ['8','138'] | Reference Text:  <S sid = 8 ssid = >We present our methods in the context of prepositional phrase (PP) at- tachment.</S><S sid = 138 ssid = >Wadsworth and Brooks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  H94-1048.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The system achieved 89.0% on a similarly modified Ratnaparkhi et al (1994) dataset.</S> | Reference Offset:  ['92','146'] | Reference Text:  <S sid = 92 ssid = >The results in Table 2 are achieved in the neighborhood of about 200 features.</S><S sid = 146 ssid = >Magerman, D., 1994.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  H94-1048.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They used a version of the Ratnaparkhi et al (1994) dataset that had all words lemmatized and all digits replaced by @.</S> | Reference Offset:  ['79','146'] | Reference Text:  <S sid = 79 ssid = >The chosen feature is added to M and used in the ME Model.</S><S sid = 146 ssid = >Magerman, D., 1994.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  H94-1048.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Their final system had 85.0% precision and 91.8% recall on the Ratnaparkhi et al (1994) dataset.</S> | Reference Offset:  ['68','146'] | Reference Text:  <S sid = 68 ssid = >Add it to .A4 2With a certain f requency cut-off, usual ly 3 to 5 3 Also with a certain f requency cut-off 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 PERFORMANCE: Wall St. Journal io io .</S><S sid = 146 ssid = >Magerman, D., 1994.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  H94-1048.txt | Citing Article:  P06-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank (Marcus et al, 1993) and the prepositional phrase dataset first described in (Ratnaparkhi et al, 1994).</S> | Reference Offset:  ['140','147'] | Reference Text:  <S sid = 140 ssid = >Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. Class-based n-gram Models of Natural Language.</S><S sid = 147 ssid = >Natural Language Parsing as Statistical Pattern Recognition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  H94-1048.txt | Citing Article:  P06-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We did not use the PP data set described by (Ratnaparkhi et al, 1994) because we are using more context than the limited context available in that set (see below).</S> | Reference Offset:  ['2','8'] | Reference Text:  <S sid = 2 ssid = >Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs.</S><S sid = 8 ssid = >We present our methods in the context of prepositional phrase (PP) at- tachment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  H94-1048.txt | Citing Article:  D11-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For English, the average human performance on pp-attachment for the (v, n1, p, n2) problem formulation is just 88.2% when given only the four head-words, but increases to 93.2% when given the full sentence (Ratnaparkhi et al, 1994).</S> | Reference Offset:  ['100','108'] | Reference Text:  <S sid = 100 ssid = >In the first trial, they were given only the four head words to make the attachment decision, and in the next, they were given the headwords along with the sentence in which they occurred.</S><S sid = 108 ssid = >In this trial, the participants made attachment decisions given only the four head words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  H94-1048.txt | Citing Article:  W04-2410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by Ratnaparkhi et al (1994) and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources.</S> | Reference Offset:  ['77','87'] | Reference Text:  <S sid = 77 ssid = >Features redun- dmlt or correlated to those features already in .A.4 will produce 252 1 0.9 0.~ 0.7 0.6 0.5 ENTROPY: Wall St. Journal Training 0.4 20 A dO dO .</S><S sid = 87 ssid = >of Lancaster, and the Wall St. Journal Data, annotated by Univ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  H94-1048.txt | Citing Article:  W04-2410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ratnaparkhi et al (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieved 81.6% accuracy.</S> | Reference Offset:  ['69','78'] | Reference Text:  <S sid = 69 ssid = >20 1 O0 120 140 160 180 200 Figure 1" Performance of Maximum Entropy Model on Wall St. Journal Data 4.</S><S sid = 78 ssid = >100 120 140 160 180 200 Figure 2: Entropy of Maximum Entropy Model on Wall St. Journal Data a zero or negligible 6L, and will therefore be outranked by genuinely informative features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  H94-1048.txt | Citing Article:  W04-2410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The maximum entropy approach of Ratnaparkhiet al (1994) uses the mutual information clustering algorithm described in (Brown et al, 1992).</S> | Reference Offset:  ['43','50'] | Reference Text:  <S sid = 43 ssid = >we use a binary hierarchy of classes derived by mutual information clustering which we describe below.</S><S sid = 50 ssid = >Mutual Information Bits Mutual information clustering, as described in [10], creates a a class "tree" for a given vocab- ulary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  H94-1048.txt | Citing Article:  W04-2410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al (1994).</S> | Reference Offset:  ['77','87'] | Reference Text:  <S sid = 77 ssid = >Features redun- dmlt or correlated to those features already in .A.4 will produce 252 1 0.9 0.~ 0.7 0.6 0.5 ENTROPY: Wall St. Journal Training 0.4 20 A dO dO .</S><S sid = 87 ssid = >of Lancaster, and the Wall St. Journal Data, annotated by Univ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  H94-1048.txt | Citing Article:  P06-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).</S> | Reference Offset:  ['20','98'] | Reference Text:  <S sid = 20 ssid = >Maximum Entropy Modeling The Maximum Entropy model [1] produces aprobability dis- tribution for the PP-attachment decision using only informa- tion from the verb phrase in which the attachment occurs.</S><S sid = 98 ssid = >The ]VIE models are slightly better than the decision tree models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  H94-1048.txt | Citing Article:  H05-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ratnaparkhi et al (1994) created a benchmark dataset of 27,937 quadruples (v, n1, p, n2), extracted from the Wall Street Journal.</S> | Reference Offset:  ['40','87'] | Reference Text:  <S sid = 40 ssid = >Head Noun (N1) 3.</S><S sid = 87 ssid = >of Lancaster, and the Wall St. Journal Data, annotated by Univ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  H94-1048.txt | Citing Article:  P01-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has been used in a variety of difficult classification tasks such as part-of-speech tagging (Ratnaparkhi, 1996), prepositional phrase attachment (Ratnaparkhi et al, 1994) and named entity tagging (Borthwick et al, 1998), and achieves state of the art performance.</S> | Reference Offset:  ['8','137'] | Reference Text:  <S sid = 8 ssid = >We present our methods in the context of prepositional phrase (PP) at- tachment.</S><S sid = 137 ssid = >Classification and Regression Trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  H94-1048.txt | Citing Article:  W06-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A year later (Ratnaparkhi et al, 1994) published a supervised approach to the PP attachment problem.</S> | Reference Offset:  ['112','130'] | Reference Text:  <S sid = 112 ssid = >Currently, the use of the mutual information class bits gives us a few percentage points in performance, but the ME model should gain more from other word classing schemes which are better tuned to the PP-attachment problem.</S><S sid = 130 ssid = >Statistically- driven Computer Grammars of English: The IBM/Lancaster Approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  H94-1048.txt | Citing Article:  W06-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Ratnaparkhi et al, 1994) used 20,801 tuples for training and 3097 tuples for evaluation.</S> | Reference Offset:  ['77','79'] | Reference Text:  <S sid = 77 ssid = >Features redun- dmlt or correlated to those features already in .A.4 will produce 252 1 0.9 0.~ 0.7 0.6 0.5 ENTROPY: Wall St. Journal Training 0.4 20 A dO dO .</S><S sid = 79 ssid = >The chosen feature is added to M and used in the ME Model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  H94-1048.txt | Citing Article:  W06-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The difference in noun attachments between these two sets is striking, but (Ratnaparkhi et al, 1994) do not discuss this (and we also do not have an explanation for this).</S> | Reference Offset:  ['17','88'] | Reference Text:  <S sid = 17 ssid = >Then our model assigns a probability to either of the possible attachments.</S><S sid = 88 ssid = >The size of the training sets, test sets, and the results are shown in Tables 1 & 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  H94-1048.txt | Citing Article:  W06-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >But it makes obvious that (Ratnaparkhi et al, 1994) were tackling a problem different from (Hindle and Rooth, 1993) given the fact that their baseline was at 59% guessing noun attachment (rather than 67% in the Hindle and Rooth experiments).</S> | Reference Offset:  ['100','142'] | Reference Text:  <S sid = 100 ssid = >In the first trial, they were given only the four head words to make the attachment decision, and in the next, they were given the headwords along with the sentence in which they occurred.</S><S sid = 142 ssid = >Hindle, D. and Rooth, M. 1990.</S> | Discourse Facet:  NA | Annotator: Automatic


