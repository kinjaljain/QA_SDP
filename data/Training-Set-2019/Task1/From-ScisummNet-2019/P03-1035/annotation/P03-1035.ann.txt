Citance Number: 1 | Reference Article:  P03-1035.txt | Citing Article:  P04-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The superiority of the unified approach has been demonstrated empirically in Gao et al (2003), and will also be discussed in Section 5.</S> | Reference Offset:  ['41','146'] | Reference Text:  <S sid = 41 ssid = >These two problems are better solved simultaneously in a unified approach.</S><S sid = 146 ssid = >Our basic solution is the bootstrapping approach described in Gao et al. (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-1035.txt | Citing Article:  P04-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All feature functions in Figure 1, except the NW function, are derived from models presented in (Gao et al, 2003).</S> | Reference Offset:  ['101','146'] | Reference Text:  <S sid = 101 ssid = >We then use an information gain-like metric described in (Chien, 1997; Gao et al., 2002) to estimate how likely a candidate is to form a morphologically derived word, and remove ‘bad’ candidates.</S><S sid = 146 ssid = >Our basic solution is the bootstrapping approach described in Gao et al. (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-1035.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','213'] | Reference Text:  <S sid = 79 ssid = >Each candidate is tagged with its word class and the class 2 In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NEJM), measure, e-mail, phone number, and WWW. model probability P(S’|C), where S’ is any substring of S. Second, Viterbi search is used to select (from the lattice) the most probable word segmentation (i.e. word class sequence C*) according to Eq.</S><S sid = 213 ssid = >Given the comparison results, we can say with confidence that our system achieves at least the performance of state-of-the-art word segmentation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-1035.txt | Citing Article:  I05-3034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The class mode score we used can be written as generate Score ()= P (|) P ()## w w c c The P (C) and P (?#? C) is similar to the one defined by Gao et al (2003).</S> | Reference Offset:  ['9','55'] | Reference Text:  <S sid = 9 ssid = >There are no word boundaries in written Chinese text.</S><S sid = 55 ssid = >According to Bayes’ decision rule and dropping the constant denominator, we can equivalently perform the following maximization: Following the Chinese word definition in Section 3, we define word class C as follows: (1) Each lexicon word is defined as a class; (2) each morphologically derived word is defined as a class; (3) each type of factoids is defined as a class, e.g. all time expressions belong to a class TIME; and (4) each type of named entities is defined as a class, e.g. all person names belong to a class PN.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-1035.txt | Citing Article:  P05-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A Chinese resume C=c1',c2',...,ck' is first tokenized into C= w1,w2,...,wk with a Chinese word segmentation system LSP (Gao et al., 2003).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Improved Source-Channel Models For Chinese Word Segmentation</S><S sid = 1 ssid = >This paper presents a Chinese word segmentation system that uses improved sourcechannel models of Chinese sentence generation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-1035.txt | Citing Article:  P05-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Then, we use a back-off schema (Katz, 1987) to deal with the data sparseness problem when estimating the probability P (L) (Gao et al, 2003).</S> | Reference Offset:  ['144','152'] | Reference Text:  <S sid = 144 ssid = >Ideally, given an annotated corpus, where each sentence is segmented into words which are tagged by their classes, the trigram word class probabilities can be calculated using MLE, together with a backoff schema (Katz, 1987) to deal with the sparse data problem.</S><S sid = 152 ssid = >To solve the first problem, we use two methods to resolve segmentation ambiguities in the initial segmented training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-1035.txt | Citing Article:  P05-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We selected SVMlight (Joachims, 1999) as the SVM classifier toolkit and LSP (Gao et al, 2003) for Chinese word segmentation and named entity identification.</S> | Reference Offset:  ['8','27'] | Reference Text:  <S sid = 8 ssid = >We then present a Chinese word segmentation system which provides a solution to the four fundamental problems of word-level Chinese language processing: word segmentation, morphological analysis, factoid detection, and named entity recognition (NER).</S><S sid = 27 ssid = >Cheng et al., 1999), given an input character string, only words that are stored in the dictionary can be identified.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-1035.txt | Citing Article:  P06-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Gao et al (2003), an approach based on source-channel model for Chinese word segmentation was proposed.</S> | Reference Offset:  ['0','52'] | Reference Text:  <S sid = 0 ssid = >Improved Source-Channel Models For Chinese Word Segmentation</S><S sid = 52 ssid = >This approach is based on the improved source-channel models described below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-1035.txt | Citing Article:  P06-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The word segmentation system is developed based on a source-channel model similar to that described in (Gao et al, 2003).</S> | Reference Offset:  ['0','52'] | Reference Text:  <S sid = 0 ssid = >Improved Source-Channel Models For Chinese Word Segmentation</S><S sid = 52 ssid = >This approach is based on the improved source-channel models described below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-1035.txt | Citing Article:  I05-3001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >That is if we collect all words seen in the training data and store them into a lexicon, then each word in a test set is either a lexicon word or an OOV (out of vocabulary) word (Gao et al., 2003).</S> | Reference Offset:  ['82','95'] | Reference Text:  <S sid = 82 ssid = >❑ Morphologically derived words: Similar to lexicon words, but a morph-lexicon is used instead of the word lexicon (see Section 5.1).</S><S sid = 95 ssid = >We simply collect all morphologically derived word forms of the above five types and incorporate them into the lexicon, called morph lexicon.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-1035.txt | Citing Article:  C10-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our experiments we identify SL (Chinese) NEs implicitly found by the word segmentation algorithm stated in Gao et al (2003), and the dictionaries for translating NEs include the same one used for QSL-TFIDF, and the LDC Chinese/English NE dictionary.</S> | Reference Offset:  ['0','33'] | Reference Text:  <S sid = 0 ssid = >Improved Source-Channel Models For Chinese Word Segmentation</S><S sid = 33 ssid = >Lin et al., 1993) identify unknown words without identifying their types.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-1035.txt | Citing Article:  P07-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Chinese side of all corpora are segmented into words by our implementation of (Gao et al, 2003).</S> | Reference Offset:  ['37','146'] | Reference Text:  <S sid = 37 ssid = >Teahan et al., 2000) are trained on a segmented corpus which is not always available.</S><S sid = 146 ssid = >Our basic solution is the bootstrapping approach described in Gao et al. (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-1035.txt | Citing Article:  C04-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gao et al (2003) uses class-based language for word segmentation where some word category information can be incorporated.</S> | Reference Offset:  ['56','101'] | Reference Text:  <S sid = 56 ssid = >We therefore convert the word segmentation W into a word class sequence C. Eq.</S><S sid = 101 ssid = >We then use an information gain-like metric described in (Chien, 1997; Gao et al., 2002) to estimate how likely a candidate is to form a morphologically derived word, and remove ‘bad’ candidates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-1035.txt | Citing Article:  P13-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To identify entities, we use a CRF-based named entity tagger (Finkel et al, 2005) and a Chinese word breaker (Gao et al, 2003) for English and Chinese corpora, respectively.</S> | Reference Offset:  ['33','146'] | Reference Text:  <S sid = 33 ssid = >Lin et al., 1993) identify unknown words without identifying their types.</S><S sid = 146 ssid = >Our basic solution is the bootstrapping approach described in Gao et al. (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


