Citance Number: 1 | Reference Article:  P01-1030.txt | Citing Article:  P01-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See (Brown et al, 1993) or (Germann et al, 2001) for a detailed discussion of this translation model and a description of its parameters.</S> | Reference Offset:  ['7','77'] | Reference Text:  <S sid = 7 ssid = >Brown et al. (1993) introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.</S><S sid = 77 ssid = >The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P01-1030.txt | Citing Article:  P01-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The decoding algorithm that we use is a greedy one - see (Germann et al, 2001) for details.</S> | Reference Offset:  ['7','171'] | Reference Text:  <S sid = 7 ssid = >Brown et al. (1993) introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.</S><S sid = 171 ssid = >The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P01-1030.txt | Citing Article:  P01-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As discussed by Germann et al (2001), the word-for-word gloss is constructed by aligning each French word fL with its most likely English translation efk.</S> | Reference Offset:  ['20','95'] | Reference Text:  <S sid = 20 ssid = >If two French words align to the same English word, then that English word is said to have a fertility of two.</S><S sid = 95 ssid = >The gloss is constructed by aligning each French word f with its most likely English translation ef(ef argmaxt(ef)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P01-1030.txt | Citing Article:  D12-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly to the work by Germann et al (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step.</S> | Reference Offset:  ['58','77'] | Reference Text:  <S sid = 58 ssid = >Return to the second step (pop).</S><S sid = 77 ssid = >The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P01-1030.txt | Citing Article:  W06-1616.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al, 2001).</S> | Reference Offset:  ['0','115'] | Reference Text:  <S sid = 0 ssid = >Fast Decoding And Optimal Decoding For Machine Translation</S><S sid = 115 ssid = >Because any TSP problem instance can be transformed into a decoding problem instance, Model 4 decoding is provably NP-complete in the length of f. It is interesting to consider the reverse direction—is it possible to transform a decoding problem instance into a TSP instance?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P01-1030.txt | Citing Article:  W06-1616.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Germann et al (2001) present an ILP formulation of the Machine Translation (MT) decoding task in order to conduct exact inference.</S> | Reference Offset:  ['0','115'] | Reference Text:  <S sid = 0 ssid = >Fast Decoding And Optimal Decoding For Machine Translation</S><S sid = 115 ssid = >Because any TSP problem instance can be transformed into a decoding problem instance, Model 4 decoding is provably NP-complete in the length of f. It is interesting to consider the reverse direction—is it possible to transform a decoding problem instance into a TSP instance?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P01-1030.txt | Citing Article:  N07-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al, 2001), that are modeled using a standard TSP formulation.</S> | Reference Offset:  ['0','90'] | Reference Text:  <S sid = 0 ssid = >Fast Decoding And Optimal Decoding For Machine Translation</S><S sid = 90 ssid = >Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P01-1030.txt | Citing Article:  N03-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also included in the figure the performance of an IBM Model 4 word based translation system (M4), which uses a greedy decoder [Germann et al, 2001].</S> | Reference Offset:  ['18','172'] | Reference Text:  <S sid = 18 ssid = >In this paper, we work with IBM Model 4, which revolves around the notion of a word alignment over a pair of sentences (see Figure 1).</S><S sid = 172 ssid = >Even when the greedy decoder uses an optimized-forspeed set of operations in which at most one word is translated, moved, or inserted at a time and at most 3-word-long segments are swapped—which is labeled “greedy” in Table 2—the translation accuracy is affected only slightly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P01-1030.txt | Citing Article:  N04-4003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Germann et al, 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models.</S> | Reference Offset:  ['2','170'] | Reference Text:  <S sid = 2 ssid = >The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).</S><S sid = 170 ssid = >Greedyand greedyare greedy decoders optimized for speed. models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P01-1030.txt | Citing Article:  W04-2708.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The system uses models GIZA++ and ISI ReWrite decoder (Germann et al., 2001).</S> | Reference Offset:  ['77','170'] | Reference Text:  <S sid = 77 ssid = >The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.</S><S sid = 170 ssid = >Greedyand greedyare greedy decoders optimized for speed. models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P01-1030.txt | Citing Article:  H05-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al, 2001).</S> | Reference Offset:  ['23','90'] | Reference Text:  <S sid = 23 ssid = >There are several sets of decisions to be made.</S><S sid = 90 ssid = >Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P01-1030.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Och et al (2001) and Germann et al (2001) both implemented optimal decoders and benchmarked approximative algorithms against them.</S> | Reference Offset:  ['16','90'] | Reference Text:  <S sid = 16 ssid = >This paper reports on measurements of speed, search errors, and translation quality in the context of a traditional stack decoder (Jelinek, 1969; Brown et al., 1995) and two new decoders.</S><S sid = 90 ssid = >Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P01-1030.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Germann et al (2001) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem.</S> | Reference Offset:  ['4','171'] | Reference Text:  <S sid = 4 ssid = >In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.</S><S sid = 171 ssid = >The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P01-1030.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While acceptably fast for the kind of evaluation used in Germann et al (2001), namely sentences of up to 20 words, its speed becomes an issue for more realistic applications.</S> | Reference Offset:  ['0','158'] | Reference Text:  <S sid = 0 ssid = >Fast Decoding And Optimal Decoding For Machine Translation</S><S sid = 158 ssid = >In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P01-1030.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this subsection we recapitulate the greedy hill climbing algorithm presented in Germann et al (2001).</S> | Reference Offset:  ['90','171'] | Reference Text:  <S sid = 90 ssid = >Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).</S><S sid = 171 ssid = >The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P01-1030.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thirdly, the results of our experiments with randomized searches show that greedy decoding does not perform as well on longer sentences as one might conclude from the findings in Germann et al (2001).</S> | Reference Offset:  ['158','171'] | Reference Text:  <S sid = 158 ssid = >In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.</S><S sid = 171 ssid = >The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P01-1030.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al (2001) and presented improvements that drastically reduce the decoder's complexity and speed to practically linear time.</S> | Reference Offset:  ['4','175'] | Reference Text:  <S sid = 4 ssid = >In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.</S><S sid = 175 ssid = >One may also run the greedy decoder using a time threshold, as any instance of anytime algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P01-1030.txt | Citing Article:  C02-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Germann et al (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.</S> | Reference Offset:  ['4','17'] | Reference Text:  <S sid = 4 ssid = >In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.</S><S sid = 17 ssid = >The first is a fast greedy decoder, and the second is a slow optimal decoder based on generic mathematical programming techniques.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P01-1030.txt | Citing Article:  C02-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','178'] | Reference Text:  <S sid = 44 ssid = >After heads and non-heads are placed, NULL-generated words are permuted into the remaining vacant slots randomly.</S><S sid = 178 ssid = >This work was supported by DARPA-ITO grant N66001-00-1-9814.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P01-1030.txt | Citing Article:  C02-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al, 2001).</S> | Reference Offset:  ['87','113'] | Reference Text:  <S sid = 87 ssid = >First, we can consider only certain English words as candidates for zero-fertility, namely words which both occur frequently and have a high probability of being assigned frequency zero.</S><S sid = 113 ssid = >TranslateAndInsert iterates over alignments, where is the size of the list of words with high probability of having fertility 0 (1024 words in our implementation).</S> | Discourse Facet:  NA | Annotator: Automatic


