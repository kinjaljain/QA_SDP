Citance Number: 1 | Reference Article:  P11-2031.txt | Citing Article:  W11-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011).</S> | Reference Offset:  ['0','17'] | Reference Text:  <S sid = 0 ssid = >Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</S><S sid = 17 ssid = >This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P11-2031.txt | Citing Article:  P14-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability.</S> | Reference Offset:  ['45','83'] | Reference Text:  <S sid = 45 ssid = >Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1.2 with English ranking parameters and all modules. the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev: (mi − m)2 n − 1 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance.</S><S sid = 83 ssid = >Therefore, we must include multiple optimizer replications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P11-2031.txt | Citing Article:  P14-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications.</S> | Reference Offset:  ['78','83'] | Reference Text:  <S sid = 78 ssid = >To determine whether an experimental manipulation results in a statistically reliable difference for an evaluation metric, we use a stratified approximate randomization (AR) test.</S><S sid = 83 ssid = >Therefore, we must include multiple optimizer replications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P11-2031.txt | Citing Article:  W12-3154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011).</S> | Reference Offset:  ['12','25'] | Reference Text:  <S sid = 12 ssid = >Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure.</S><S sid = 25 ssid = >The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P11-2031.txt | Citing Article:  P14-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments.</S> | Reference Offset:  ['17','25'] | Reference Text:  <S sid = 17 ssid = >This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).</S><S sid = 25 ssid = >The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P11-2031.txt | Citing Article:  W12-3150.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights.</S> | Reference Offset:  ['22','89'] | Reference Text:  <S sid = 22 ssid = >Dyer et al., 2010).</S><S sid = 89 ssid = >In this simulation, we randomly selected n optimizer outputs from our large pool and ran the AR test to determine the significance; we repeated this procedure 250 times.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P11-2031.txt | Citing Article:  P13-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons.</S> | Reference Offset:  ['17','35'] | Reference Text:  <S sid = 17 ssid = >This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).</S><S sid = 35 ssid = >Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P11-2031.txt | Citing Article:  W12-3130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation.</S> | Reference Offset:  ['17','45'] | Reference Text:  <S sid = 17 ssid = >This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).</S><S sid = 45 ssid = >Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1.2 with English ranking parameters and all modules. the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev: (mi − m)2 n − 1 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P11-2031.txt | Citing Article:  W12-3152.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011).</S> | Reference Offset:  ['12','35'] | Reference Text:  <S sid = 12 ssid = >Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure.</S><S sid = 35 ssid = >Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P11-2031.txt | Citing Article:  P13-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011).</S> | Reference Offset:  ['0','17'] | Reference Text:  <S sid = 0 ssid = >Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</S><S sid = 17 ssid = >This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P11-2031.txt | Citing Article:  P13-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011).</S> | Reference Offset:  ['61','93'] | Reference Text:  <S sid = 61 ssid = >To compute ssel, assume we have n independent optimization runs which produced weight vectors that were used to translate a test set n times.</S><S sid = 93 ssid = >Based on the average performance of the systems reported in Table 1, we expect significance over a large enough number of independent trials.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P11-2031.txt | Citing Article:  W12-3148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011).</S> | Reference Offset:  ['3','35'] | Reference Text:  <S sid = 3 ssid = >In this paper, we consider how to make such experiments more statistically reliable.</S><S sid = 35 ssid = >Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P11-2031.txt | Citing Article:  W12-3126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system.</S> | Reference Offset:  ['35','90'] | Reference Text:  <S sid = 35 ssid = >Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).</S><S sid = 90 ssid = >The p-values reported are the pvalues at the edges of the 95% confidence interval (CI) according to AR seen in the 250 simulated comparison scenarios.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P11-2031.txt | Citing Article:  W12-3126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art.</S> | Reference Offset:  ['20','53'] | Reference Text:  <S sid = 20 ssid = >However, it could be that a particu- “experimental” system (System B), which previous lar sample is on the “low” side of the distribution research has suggested will perform better. over optimizer outcomes (i.e., it results in relatively The first system pair contrasts a baseline phrasepoorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchiThe danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were conpaired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with identified as useless.</S><S sid = 53 ssid = >However, with only a single test corpus, we may have unreliable results because of idiosyncrasies in the test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P11-2031.txt | Citing Article:  P14-2074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script.</S> | Reference Offset:  ['35','84'] | Reference Text:  <S sid = 35 ssid = >Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).</S><S sid = 84 ssid = >Also, since metric scores (such as BLEU) are in general not comparable across test sets, we stratify, exchanging only hypotheses that correspond to the same sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


