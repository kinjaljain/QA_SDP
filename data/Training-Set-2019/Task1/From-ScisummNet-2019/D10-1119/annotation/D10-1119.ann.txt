Citance Number: 1 | Reference Article:  D10-1119.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification</S><S sid = 4 ssid = >We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D10-1119.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences.</S> | Reference Offset:  ['153','206'] | Reference Text:  <S sid = 153 ssid = >For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition.</S><S sid = 206 ssid = >The approach uses higher-order unification to define the space of possible grammars in a language- and representation-independent manner, paired with an algorithm that learns a probabilistic parsing model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D10-1119.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7.</S> | Reference Offset:  ['7','136'] | Reference Text:  <S sid = 7 ssid = >Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).</S><S sid = 136 ssid = >For example, Kate & Mooney (2006) present a method (KRISP) that extends an existing SVM learning algorithm to recover logical representations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D10-1119.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model.</S> | Reference Offset:  ['4','109'] | Reference Text:  <S sid = 4 ssid = >We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.</S><S sid = 109 ssid = >Figure 2 presents the unification-based learning algorithm, UBL.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D10-1119.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6.</S> | Reference Offset:  ['61','128'] | Reference Text:  <S sid = 61 ssid = >These parses closely resemble the types of analyses that will be possible under the grammars we learn in the experiments described in Section 8.</S><S sid = 128 ssid = >Discussion The alternation between refining the lexicon and updating the parameters drives the learning process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D10-1119.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['67','216'] | Reference Text:  <S sid = 67 ssid = >The sum over parses in Eq.</S><S sid = 216 ssid = >Zettlemoyer was supported by a US NSF International Research Fellowship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D10-1119.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting.</S> | Reference Offset:  ['151','208'] | Reference Text:  <S sid = 151 ssid = >Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work.</S><S sid = 208 ssid = >For future work, we are interested in exploring the generality of the approach while extending it to new understanding problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D10-1119.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600.</S> | Reference Offset:  ['7','166'] | Reference Text:  <S sid = 7 ssid = >Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).</S><S sid = 166 ssid = >The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer & Collins (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D10-1119.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system.</S> | Reference Offset:  ['143','169'] | Reference Text:  <S sid = 143 ssid = >Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.</S><S sid = 169 ssid = >We use the same folds as Wong & Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D10-1119.txt | Citing Article:  P13-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG).</S> | Reference Offset:  ['16','30'] | Reference Text:  <S sid = 16 ssid = >Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 1996, 2000).</S><S sid = 30 ssid = >We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).</S><S sid = 11 ssid = >In addition to data like the above, this approach can also learn from examples such as: Sentence: hangi eyaletin texas ye siniri vardir Meaning: answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['67','216'] | Reference Text:  <S sid = 67 ssid = >The sum over parses in Eq.</S><S sid = 216 ssid = >Zettlemoyer was supported by a US NSF International Research Fellowship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either.</S> | Reference Offset:  ['87','143'] | Reference Text:  <S sid = 87 ssid = >Section 5 describes how we estimate the parameters of a probabilistic parsing model and how this parsing model can be used to guide the selection of items to add to the lexicon.</S><S sid = 143 ssid = >Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4).</S> | Reference Offset:  ['94','99'] | Reference Text:  <S sid = 94 ssid = >The new syntactic category for g is determined based on its type, T(g).</S><S sid = 99 ssid = >For example, there is no distinct syntactic category N for nouns (which have semantic type he, ti).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}.</S> | Reference Offset:  ['106','192'] | Reference Text:  <S sid = 106 ssid = >The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data.</S><S sid = 192 ssid = >For this representation, lexical items such as: can be used to construct the desired output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation.</S> | Reference Offset:  ['144','169'] | Reference Text:  <S sid = 144 ssid = >These algorithms are all language independent but representation specific.</S><S sid = 169 ssid = >We use the same folds as Wong & Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements.</S> | Reference Offset:  ['143','160'] | Reference Text:  <S sid = 143 ssid = >Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.</S><S sid = 160 ssid = >The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D10-1119.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010).</S> | Reference Offset:  ['28','109'] | Reference Text:  <S sid = 28 ssid = >Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases.</S><S sid = 109 ssid = >Figure 2 presents the unification-based learning algorithm, UBL.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D10-1119.txt | Citing Article:  P13-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings.</S> | Reference Offset:  ['7','143'] | Reference Text:  <S sid = 7 ssid = >Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).</S><S sid = 143 ssid = >Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D10-1119.txt | Citing Article:  P13-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions.</S> | Reference Offset:  ['46','170'] | Reference Text:  <S sid = 46 ssid = >The higher-order unification problem (Huet, 1975) involves finding a substitution for the free variables in a pair of lambda-calculus expressions that, when applied, makes the expressions equal each other.</S><S sid = 170 ssid = >The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper.</S> | Discourse Facet:  NA | Annotator: Automatic


