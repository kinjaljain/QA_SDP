Citance Number: 1 | Reference Article:  P07-1065.txt | Citing Article:  D07-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly.</S> | Reference Offset:  ['122','124'] | Reference Text:  <S sid = 122 ssid = >In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.</S><S sid = 124 ssid = >The framework presented here shows that with some consideration for its workings, the randomised nature of the Bloom filter need not be a significant impediment to is use in applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1065.txt | Citing Article:  D07-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently.</S> | Reference Offset:  ['4','90'] | Reference Text:  <S sid = 4 ssid = >We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams.</S><S sid = 90 ssid = >We also analyse the sub-sequence filtering scheme directly by creating a BF with only 3-grams and a BF containing both 2-grams and 3-grams and comparing their actual error rates when presented with 3-grams that are all known to be negatives.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1065.txt | Citing Article:  D07-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated.</S> | Reference Offset:  ['50','107'] | Reference Text:  <S sid = 50 ssid = >Errors for the log-frequency BF scheme are onesided: frequencies will never be underestimated.</S><S sid = 107 ssid = >Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1065.txt | Citing Article:  D07-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007).</S> | Reference Offset:  ['50','107'] | Reference Text:  <S sid = 50 ssid = >Errors for the log-frequency BF scheme are onesided: frequencies will never be underestimated.</S><S sid = 107 ssid = >Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1065.txt | Citing Article:  D07-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error.</S> | Reference Offset:  ['108','109'] | Reference Text:  <S sid = 108 ssid = >We presented 500K negatives to the filter and recorded the frequency of overestimation errors of each size.</S><S sid = 109 ssid = >As shown in Section 3.1, the probability of overestimating an item’s frequency under the log-frequency BF scheme decays exponentially in the size of this overestimation error.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1065.txt | Citing Article:  D07-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.</S> | Reference Offset:  ['13','123'] | Reference Text:  <S sid = 13 ssid = >In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.</S><S sid = 123 ssid = >We hope the present work will help establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1065.txt | Citing Article:  P14-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches.</S> | Reference Offset:  ['35','115'] | Reference Text:  <S sid = 35 ssid = >This is known as a one-sided error.</S><S sid = 115 ssid = >We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1065.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability.</S> | Reference Offset:  ['2','29'] | Reference Text:  <S sid = 2 ssid = >Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.</S><S sid = 29 ssid = >Bits in the filter can, therefore, be shared by distinct items allowing significant space savings but introducing a non-zero probability of false positives at test time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1065.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a).</S> | Reference Offset:  ['12','115'] | Reference Text:  <S sid = 12 ssid = >Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation.</S><S sid = 115 ssid = >We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1065.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling.</S> | Reference Offset:  ['13','17'] | Reference Text:  <S sid = 13 ssid = >In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.</S><S sid = 17 ssid = >In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1065.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model.</S> | Reference Offset:  ['53','115'] | Reference Text:  <S sid = 53 ssid = >The error rate Err is, This implies that, unlike a conventional lossless data structure, the model’s accuracy depends on other components in system and how it is queried.</S><S sid = 115 ssid = >We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1065.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases.</S> | Reference Offset:  ['15','114'] | Reference Text:  <S sid = 15 ssid = >This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.</S><S sid = 114 ssid = >By testing for 2-grams prior to querying for the 3-grams, we can avoid performing some queries that may otherwise have incurred errors using the fact that a 3-gram cannot be present if one of its constituent 2-grams is absent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1065.txt | Citing Article:  W09-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007).</S> | Reference Offset:  ['17','122'] | Reference Text:  <S sid = 17 ssid = >In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.</S><S sid = 122 ssid = >In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1065.txt | Citing Article:  W10-1728.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data.</S> | Reference Offset:  ['69','73'] | Reference Text:  <S sid = 69 ssid = >We use the French-English section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003) and the English Gigaword Corpus (LDC2003T05; GW) for additional language modelling.</S><S sid = 73 ssid = >Our baseline LM and other comparison models are conventional n-gram models smoothed using modified Kneser-Ney and built using the SRILM Toolkit (Stolcke, 2002); as is standard practice these models drop entries for n-grams of size 3 and above when the corresponding discounted count is less than 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1065.txt | Citing Article:  W09-0420.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system).</S> | Reference Offset:  ['101','102'] | Reference Text:  <S sid = 101 ssid = >The base 2 quantisation performs worse for smaller amounts of memory, possibly due to the larger set of events it is required to store.</S><S sid = 102 ssid = >Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1065.txt | Citing Article:  D12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For language modeling, we use RandLM (Talbot and Osborne, 2007).</S> | Reference Offset:  ['3','58'] | Reference Text:  <S sid = 3 ssid = >Here we explore the use of BFs for language modelling in statistical machine translation. show how a BF containing can enable us to use much larger corpora and higher-order models complementing a con- LM within an SMT system.</S><S sid = 58 ssid = >This does not use any frequency information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1065.txt | Citing Article:  W10-1724.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM.</S> | Reference Offset:  ['57','74'] | Reference Text:  <S sid = 57 ssid = >A standard BF can implement a Boolean ‘language model’ test: have we seen some fragment of language before?</S><S sid = 74 ssid = >The baseline language model, EP-KN-3, is a trigram model trained on the English portion of the parallel corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1065.txt | Citing Article:  W11-2123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures.</S> | Reference Offset:  ['76','122'] | Reference Text:  <S sid = 76 ssid = >Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.</S><S sid = 122 ssid = >In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1065.txt | Citing Article:  W11-2123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy.</S> | Reference Offset:  ['76','122'] | Reference Text:  <S sid = 76 ssid = >Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.</S><S sid = 122 ssid = >In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1065.txt | Citing Article:  N09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately.</S> | Reference Offset:  ['76','77'] | Reference Text:  <S sid = 76 ssid = >Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.</S><S sid = 77 ssid = >We give the gzip compressed size as an optimistic lower bound on the size of any lossless representation of each model.2 2Note, in particular, that gzip compressed files do not support direct random access as required by our application.</S> | Discourse Facet:  NA | Annotator: Automatic


