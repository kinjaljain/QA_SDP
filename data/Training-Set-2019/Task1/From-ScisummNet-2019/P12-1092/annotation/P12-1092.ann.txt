Citance Number: 1 | Reference Article:  P12-1092.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category.</S> | Reference Offset:  ['99','203'] | Reference Text:  <S sid = 99 ssid = >We downloaded these embeddings from Turian et al (2010).</S><S sid = 203 ssid = >Erk and Pado?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P12-1092.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers.</S> | Reference Offset:  ['11','99'] | Reference Text:  <S sid = 11 ssid = >Reisinger and Mooney (2010b) introduced a multi-prototype VSM whereword sense discrimination is first applied by clus tering contexts, and then prototypes are built using the contexts of the sense-labeled words.</S><S sid = 99 ssid = >We downloaded these embeddings from Turian et al (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P12-1092.txt | Citing Article:  P14-1133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context.</S> | Reference Offset:  ['111','192'] | Reference Text:  <S sid = 111 ssid = >C&W* is the word embeddings trained and provided by C&W. OurModel* is trained without stop words, while Our Model g uses only global context.</S><S sid = 192 ssid = >Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P12-1092.txt | Citing Article:  P14-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012).</S> | Reference Offset:  ['99','198'] | Reference Text:  <S sid = 99 ssid = >We downloaded these embeddings from Turian et al (2010).</S><S sid = 198 ssid = >Two other recent papers (Dhillon et al, 2011; Reddy et al, 2011) present models for constructing word representations that deal with context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P12-1092.txt | Citing Article:  P13-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models, 2) Collobert and Weston's (C&W) deep-learning architecture, and 3) Huang et al's polysemous neural language model (HUANG) (Huang et al, 2012).</S> | Reference Offset:  ['99','192'] | Reference Text:  <S sid = 99 ssid = >We downloaded these embeddings from Turian et al (2010).</S><S sid = 192 ssid = >Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P12-1092.txt | Citing Article:  P14-2131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','212'] | Reference Text:  <S sid = 78 ssid = >Other rare words not in the dictionary are mapped to an UNKNOWN token.For all experiments, our models use 50 dimensional embeddings.</S><S sid = 212 ssid = >Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P12-1092.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper).</S> | Reference Offset:  ['184','192'] | Reference Text:  <S sid = 184 ssid = >Neural language models (Bengio et al, 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Schwenk and Gauvain, 2002; Emami et al, 2003) have been shown to be very powerful at languagemodeling, a task where models are asked to ac curately predict the next word given previously seen words.</S><S sid = 192 ssid = >Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P12-1092.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).</S> | Reference Offset:  ['8','14'] | Reference Text:  <S sid = 8 ssid = >These representations can be used to induce similarity measures by computingdistances between the vectors, leading to many useful applications, such as information retrieval (Manning et al, 2008), document classification (Sebas tiani, 2002) and question answering (Tellex et al, 2003).</S><S sid = 14 ssid = >Several studies in psychology have also shown that global context can help language comprehension (Hess et al., 1995) and acquisition (Li et al, 2000).We introduce a new neural-network-based lan guage model that distinguishes and uses both local and global context via a joint training objective.</S> | Discourse Facet:  NA | Annotator: Automatic


