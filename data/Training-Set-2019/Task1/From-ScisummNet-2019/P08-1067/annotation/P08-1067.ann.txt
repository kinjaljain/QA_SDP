Citance Number: 1 | Reference Article:  P08-1067.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.</S><S sid = 148 ssid = >We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1067.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program.</S> | Reference Offset:  ['24','116'] | Reference Text:  <S sid = 24 ssid = >Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges.</S><S sid = 116 ssid = >Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1067.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers.</S> | Reference Offset:  ['11','116'] | Reference Text:  <S sid = 11 ssid = >Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005).</S><S sid = 116 ssid = >Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1067.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary.</S> | Reference Offset:  ['36','144'] | Reference Text:  <S sid = 36 ssid = >We first establish a unified framework for parse reranking with both n-best lists and packed forests.</S><S sid = 144 ssid = >We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1067.txt | Citing Article:  W10-2910.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008).</S> | Reference Offset:  ['57','119'] | Reference Text:  <S sid = 57 ssid = >In n-best reranking, all features are treated equivalently by the decoder, which simply computes the value of each one on each candidate parse.</S><S sid = 119 ssid = >This result confirms that our feature set design is appropriate, and the averaged perceptron learner is a reasonable candidate for reranking.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Huang (2008) for more details.</S> | Reference Offset:  ['42','141'] | Reference Text:  <S sid = 42 ssid = >” which is an instance of the WordEdges feature (see Figure 2(c) and Section 3.2 for details).</S><S sid = 141 ssid = >Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance.</S> | Reference Offset:  ['63','124'] | Reference Text:  <S sid = 63 ssid = >2(b) is non-local.</S><S sid = 124 ssid = >For both systems, we first use only the local features, and then all the features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.</S><S sid = 148 ssid = >We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order.</S> | Reference Offset:  ['63','124'] | Reference Text:  <S sid = 63 ssid = >2(b) is non-local.</S><S sid = 124 ssid = >For both systems, we first use only the local features, and then all the features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008).</S> | Reference Offset:  ['19','22'] | Reference Text:  <S sid = 19 ssid = >This result is also better than any previously reported systems trained on the Treebank.</S><S sid = 22 ssid = >Shown in Figure 1, this sentence has (at least) two derivations depending on the attachment of the prep. phrase PP3,6 “with a mirror”: it can either be attached to the verb “saw”, or be attached to “him”, which will be further combined with the verb to form the same VP as above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy.</S> | Reference Offset:  ['12','113'] | Reference Text:  <S sid = 12 ssid = >However, we miss the benefits of non-local features that are not representable here.</S><S sid = 113 ssid = >The development set and the test set are parsed with a model trained on all 39832 training sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1067.txt | Citing Article:  P13-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.</S><S sid = 148 ssid = >We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1067.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Huang (2008) proposed to use a parse forest to incorporate non-local features.</S> | Reference Offset:  ['63','124'] | Reference Text:  <S sid = 63 ssid = >2(b) is non-local.</S><S sid = 124 ssid = >For both systems, we first use only the local features, and then all the features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1067.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).</S> | Reference Offset:  ['139','141'] | Reference Text:  <S sid = 139 ssid = >McClosky et al. (2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data.</S><S sid = 141 ssid = >Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1067.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.</S><S sid = 148 ssid = >We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1067.txt | Citing Article:  P13-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list).</S> | Reference Offset:  ['0','124'] | Reference Text:  <S sid = 0 ssid = >Forest Reranking: Discriminative Parsing with Non-Local Features</S><S sid = 124 ssid = >For both systems, we first use only the local features, and then all the features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1067.txt | Citing Article:  P09-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair.</S> | Reference Offset:  ['25','61'] | Reference Text:  <S sid = 25 ssid = >More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges.</S><S sid = 61 ssid = >For example, the Rule feature in Fig.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1067.txt | Citing Article:  P09-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests.</S> | Reference Offset:  ['36','114'] | Reference Text:  <S sid = 36 ssid = >We first establish a unified framework for parse reranking with both n-best lists and packed forests.</S><S sid = 114 ssid = >We implemented both n-best and forest reranking systems in Python and ran our experiments on a 64bit Dual-Core Intel Xeon with 3.0GHz CPUs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1067.txt | Citing Article:  P09-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation.</S> | Reference Offset:  ['82','106'] | Reference Text:  <S sid = 82 ssid = >We also use the notation (e, j) to denote the derivation along hyperedge e, using the jith subderivation for tail ui, so (e, 1) is the best derivation along e. The exact decoding algorithm, shown in Pseudocode 2, is an instance of the bottom-up Viterbi algorithm, which traverses the hypergraph in a topological order, and at each node v, calculates its 1-best derivation using each incoming hyperedge e E IN(v).</S><S sid = 106 ssid = >Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost a(v) for each node v, and then compute the merit aQ(e) for each hyperedge: Intuitively, this merit is the cost of the best derivation that traverses e, and the difference S(e) = aQ(e) − Q(TOP) can be seen as the distance away from the globally best derivation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1067.txt | Citing Article:  E09-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined.</S> | Reference Offset:  ['41','83'] | Reference Text:  <S sid = 41 ssid = >For example, one such feature f2000 might be a question “how many times is a VP of length 5 surrounded by the word ‘has’ and the period?</S><S sid = 83 ssid = >The cost of e, c(e), is the score of its Pseudocode 2 Exact Decoding with Local Features Pseudocode 3 Cube Pruning for Non-local Features (pre-computed) local features w · fL(e).</S> | Discourse Facet:  NA | Annotator: Automatic


