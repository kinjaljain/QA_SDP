Citance Number: 1 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail).</S> | Reference Offset:  ['0','181'] | Reference Text:  <S sid = 0 ssid = >Reinforcement Learning for Mapping Instructions to Actions</S><S sid = 181 ssid = >In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work (Branavan et al, 2009) is only able to handle low-level instructions.</S> | Reference Offset:  ['12','33'] | Reference Text:  <S sid = 12 ssid = >We aim to map this text to the corresponding low-level commands and parameters.</S><S sid = 33 ssid = >Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments.</S> | Reference Offset:  ['33','117'] | Reference Text:  <S sid = 33 ssid = >Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S><S sid = 117 ssid = >Reward Function Environment feedback can be used as a reward function in this domain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009).</S> | Reference Offset:  ['116','137'] | Reference Text:  <S sid = 116 ssid = >In total, there are 4,438 features.</S><S sid = 137 ssid = >In total, there are 8,094 features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09.</S> | Reference Offset:  ['33','161'] | Reference Text:  <S sid = 33 ssid = >Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S><S sid = 161 ssid = >Our method seamlessly combines these two kinds of rewards. sider two naive baselines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009).</S> | Reference Offset:  ['99','160'] | Reference Text:  <S sid = 99 ssid = >Thus, with this reward policy gradient is equivalent to stochastic gradient ascent with a maximum likelihood objective.</S><S sid = 160 ssid = >As shown there, policy gradient with this reward is equivalent to stochastic gradient ascent with a maximum likelihood objective. when only a subset of training documents is annotated, and environment reward is used for the remainder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1010.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The Environment The environment state £ specifies the set of objects available for interaction, and their properties.</S><S sid = 193 ssid = >(JAIR), 15:31–90.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1010.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009].</S> | Reference Offset:  ['33','72'] | Reference Text:  <S sid = 33 ssid = >Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S><S sid = 72 ssid = >Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters θ by performing stochastic gradient ascent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1010.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >The Environment The environment state £ specifies the set of objects available for interaction, and their properties.</S><S sid = 193 ssid = >(JAIR), 15:31–90.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1010.txt | Citing Article:  P10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also compare against the policy gradient learning algorithm of Branavan et al (2009).</S> | Reference Offset:  ['33','83'] | Reference Text:  <S sid = 33 ssid = >Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S><S sid = 83 ssid = >Algorithm 1 details the complete policy gradient algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


