Citance Number: 1 | Reference Article:  P02-1014.txt | Citing Article:  W02-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Ng and Cardie (2002) for a detailed description of the features.</S> | Reference Offset:  ['30','118'] | Reference Text:  <S sid = 30 ssid = >We use the C4.5 decision tree induction system (Quinlan, 1993) to train a classifier that, given a description of two NPs in a document, NP✂ and NP✄ , decides whether or not they are coreferent.</S><S sid = 118 ssid = >(See the Hand-selected Features block in Table 2.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1014.txt | Citing Article:  P14-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder.</S> | Reference Offset:  ['55','56'] | Reference Text:  <S sid = 55 ssid = >Best-first clustering.</S><S sid = 56 ssid = >Rather than a right-to-left search from each anaphoric NP for the first coreferent NP, we hypothesized that a right-to-left search for a highly likely antecedent might offer more precise, if not generally better coreference chains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1014.txt | Citing Article:  P14-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach.</S> | Reference Offset:  ['10','97'] | Reference Text:  <S sid = 10 ssid = >This paper presents an NP coreference system that investigates two types of extensions to the Soon et al. corpus-based approach.</S><S sid = 97 ssid = >Two final features make use of an in-house naive pronoun resolution algorithm (PRO RESOLVE) and a rule-based coreference resolution system (RULE RESOLVE), each of which relies on the original and expanded feature sets described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1014.txt | Citing Article:  P05-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002).</S> | Reference Offset:  ['19','81'] | Reference Text:  <S sid = 19 ssid = >First, we find that performance drops significantly when using the full feature set, even though the learning algorithms investigated have built-in feature selection mechanisms.</S><S sid = 81 ssid = >In addition, we include four new semantic features to allow finer-grained semantic compatibility tests.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1014.txt | Citing Article:  W11-1814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the testing phase, we used the best-first clustering as in Ng and Cardie (2002).</S> | Reference Offset:  ['55','138'] | Reference Text:  <S sid = 55 ssid = >Best-first clustering.</S><S sid = 138 ssid = >We plan to continue investigations along these lines, developing, for example, a true best-first clustering coreference framework and exploring a “supervised clustering” approach to the problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1014.txt | Citing Article:  P06-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported: 73.3%), the anaphora labelled portion of AQUAINT used in Cherry and Bergsma (2005) (1078 instances, highest accuracy: 71.4%), and the anaphoric pronoun subset of the MUC7 (1997) coreference evaluation formal test set (169 instances, highest precision of 62.1 reported on all pronouns in (Ng and Cardie, 2002)).</S> | Reference Offset:  ['42','43'] | Reference Text:  <S sid = 42 ssid = >The MUC-6 corpus produces a training set of 26455 instances (5.4% positive) from 4381 NPs and a test set of 28443 instances (5.2% positive) from 4565 NPs.</S><S sid = 43 ssid = >For the MUC-7 corpus, we obtain a training set of 35895 instances (4.4% positive) from 5270 NPs and a test set of 22699 instances (3.9% positive) from 3558 NPs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1014.txt | Citing Article:  C10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features.</S> | Reference Offset:  ['12','31'] | Reference Text:  <S sid = 12 ssid = >Second, in an attempt to understand whether incorporating additional knowledge can improve the performance of a corpus-based coreference resolution system, we expand the Soon et al. feature set from 12 features to an arguably deeper set of 53.</S><S sid = 31 ssid = >Each training instance represents the two NPs under consideration and consists of the 12 Soon et al. features, which are described in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1014.txt | Citing Article:  C10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases.</S> | Reference Offset:  ['3','64'] | Reference Text:  <S sid = 3 ssid = >Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.</S><S sid = 64 ssid = >We hypothesized, however, that splitting this feature into several primitive features, depending on the type of NP, might give the learning algorithm additional flexibility in creating coreference rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1014.txt | Citing Article:  P10-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations.</S> | Reference Offset:  ['14','57'] | Reference Text:  <S sid = 14 ssid = >Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.</S><S sid = 57 ssid = >As a result, we modify the coreference clustering algorithm to select as the antecedent of NP✄ the NP with the highest coreference likelihood value from among preceding NPs with coreference class values above 0.5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1014.txt | Citing Article:  D09-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation.</S> | Reference Offset:  ['41','95'] | Reference Text:  <S sid = 41 ssid = >We evaluate the Duplicated Soon Baseline system using the standard MUC-6 (1995) and MUC7 (1998) coreference corpora, training the coreference classifier on the 30 “dry run” texts, and applying the coreference resolution algorithm on the 20–30 “formal evaluation” texts.</S><S sid = 95 ssid = >For instance, the CONTAINS PN feature effectively disallows coreference between NPs that contain distinct proper names but are not themselves proper names (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1014.txt | Citing Article:  D09-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system.</S> | Reference Offset:  ['17','27'] | Reference Text:  <S sid = 17 ssid = >Because sources of linguistic information in a learning-based system are represented as features, we can, in contrast, incorporate them selectively rather than as universal hard constraints.</S><S sid = 27 ssid = >Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1014.txt | Citing Article:  C10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning.</S> | Reference Offset:  ['97','106'] | Reference Text:  <S sid = 97 ssid = >Two final features make use of an in-house naive pronoun resolution algorithm (PRO RESOLVE) and a rule-based coreference resolution system (RULE RESOLVE), each of which relies on the original and expanded feature sets described above.</S><S sid = 106 ssid = >This rule covers 38 examples, but has 18 exceptions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1014.txt | Citing Article:  C10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b).</S> | Reference Offset:  ['1','27'] | Reference Text:  <S sid = 1 ssid = >We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively.</S><S sid = 27 ssid = >Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1014.txt | Citing Article:  C10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances.</S> | Reference Offset:  ['5','31'] | Reference Text:  <S sid = 5 ssid = >Aone and Bennett (1995), McCarthy and Lehnert (1995)).</S><S sid = 31 ssid = >Each training instance represents the two NPs under consideration and consists of the 12 Soon et al. features, which are described in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1014.txt | Citing Article:  W12-4508.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem.</S> | Reference Offset:  ['4','135'] | Reference Text:  <S sid = 4 ssid = >Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classification task (e.g.</S><S sid = 135 ssid = >We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1014.txt | Citing Article:  W06-1640.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)).</S> | Reference Offset:  ['135','145'] | Reference Text:  <S sid = 135 ssid = >We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.</S><S sid = 145 ssid = >Sidner (1979), Harabagiu et al. (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1014.txt | Citing Article:  W06-1640.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)).</S> | Reference Offset:  ['27','145'] | Reference Text:  <S sid = 27 ssid = >Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).</S><S sid = 145 ssid = >Sidner (1979), Harabagiu et al. (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1014.txt | Citing Article:  W06-1640.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002).</S> | Reference Offset:  ['22','97'] | Reference Text:  <S sid = 22 ssid = >Overall, the learning framework and linguistic knowledge source modifications boost performance of Soon’s learning-based coreference resolution approach from an F-measure of 62.6 to 70.4, and from 60.4 to 63.4 for the MUC-6 and MUC-7 data sets, respectively.</S><S sid = 97 ssid = >Two final features make use of an in-house naive pronoun resolution algorithm (PRO RESOLVE) and a rule-based coreference resolution system (RULE RESOLVE), each of which relies on the original and expanded feature sets described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1014.txt | Citing Article:  W06-1640.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution.</S> | Reference Offset:  ['14','130'] | Reference Text:  <S sid = 14 ssid = >Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.</S><S sid = 130 ssid = >In particular, our tree makes use of many of the features that are not present in the original Soon feature set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1014.txt | Citing Article:  W06-1640.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002).</S> | Reference Offset:  ['55','138'] | Reference Text:  <S sid = 55 ssid = >Best-first clustering.</S><S sid = 138 ssid = >We plan to continue investigations along these lines, developing, for example, a true best-first clustering coreference framework and exploring a “supervised clustering” approach to the problem.</S> | Discourse Facet:  NA | Annotator: Automatic


