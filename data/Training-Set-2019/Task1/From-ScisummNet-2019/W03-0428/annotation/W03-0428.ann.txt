Citance Number: 1 | Reference Article:  W03-0428.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers.</S> | Reference Offset:  ['20','65'] | Reference Text:  <S sid = 20 ssid = >In addition to this HMM view, it may also be convenient to think of the local emission models as type-conditional -gram models.</S><S sid = 65 ssid = >Sequence-sensitive features can be included by chaining our local classifiers together and performing joint inference, i.e., by building a conditional markov model (CMM), also known as a maximum entropy markov model (McCallum et al., 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-0428.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models.</S> | Reference Offset:  ['20','65'] | Reference Text:  <S sid = 20 ssid = >In addition to this HMM view, it may also be convenient to think of the local emission models as type-conditional -gram models.</S><S sid = 65 ssid = >Sequence-sensitive features can be included by chaining our local classifiers together and performing joint inference, i.e., by building a conditional markov model (CMM), also known as a maximum entropy markov model (McCallum et al., 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-0428.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003).</S> | Reference Offset:  ['8','54'] | Reference Text:  <S sid = 8 ssid = >A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997).</S><S sid = 54 ssid = >Given the amount of improvement from using a model backed by character -grams instead of word -grams, the immediate question is whether this benefit is complementary to the benefit from features which have traditionally been of use in word level systems, such as syntactic context features, topic features, and so on.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-0428.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.</S> | Reference Offset:  ['52','82'] | Reference Text:  <S sid = 52 ssid = >However, this reduced performance (by 2.0% with context on).</S><S sid = 82 ssid = >In the final, much richer chained maxent setting, the reduction from the best model minus -gram features to the reported best model was about 25% â€“ smaller, but still substantial.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-0428.txt | Citing Article:  P05-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['30','83'] | Reference Text:  <S sid = 30 ssid = >We then use empirical, unsmoothed estimates for statestate transitions.</S><S sid = 83 ssid = >This paper also again demonstrates how the ease of incorporating features into a discriminative maxent model allows for productive feature engineering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-0428.txt | Citing Article:  E06-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['30','83'] | Reference Text:  <S sid = 30 ssid = >We then use empirical, unsmoothed estimates for statestate transitions.</S><S sid = 83 ssid = >This paper also again demonstrates how the ease of incorporating features into a discriminative maxent model allows for productive feature engineering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-0428.txt | Citing Article:  I08-5010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models.</S> | Reference Offset:  ['21','59'] | Reference Text:  <S sid = 21 ssid = >Indeed, the character emission model in this section is directly based on the -gram proper-name classification engine described in (Smarr and Manning, 2002).</S><S sid = 59 ssid = >Using the substring features alone scored 73.10%, already breaking the the phrase-based CoNLL baseline, though lower than the no-context HMM, which better models the context inside phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-0428.txt | Citing Article:  I08-5015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information.</S> | Reference Offset:  ['8','51'] | Reference Text:  <S sid = 8 ssid = >A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997).</S><S sid = 51 ssid = >We did also try to incorporate gazetteer information by adding -gram counts from gazetteer entries to the training counts that back the above character emission model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-0428.txt | Citing Article:  P05-2023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005).</S> | Reference Offset:  ['65','77'] | Reference Text:  <S sid = 65 ssid = >Sequence-sensitive features can be included by chaining our local classifiers together and performing joint inference, i.e., by building a conditional markov model (CMM), also known as a maximum entropy markov model (McCallum et al., 2000).</S><S sid = 77 ssid = >In particular, repeated sub-elements (usually last names) of multi-word person names were given type PERSON, and a crude heuristic restoration of B- prefixes was performed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-0428.txt | Citing Article:  W10-3102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003).</S> | Reference Offset:  ['3','79'] | Reference Text:  <S sid = 3 ssid = >Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data).</S><S sid = 79 ssid = >Table 4 gives a more detailed breakdown of this score, and also gives the results of this system on the English test set, and both German data sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W03-0428.txt | Citing Article:  P11-3019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003).</S> | Reference Offset:  ['0','12'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition With Character-Level Models</S><S sid = 12 ssid = >We present two models in which the basic units are characters and character -grams, instead of words and word phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W03-0428.txt | Citing Article:  I08-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003).</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition With Character-Level Models</S><S sid = 8 ssid = >A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W03-0428.txt | Citing Article:  W04-1217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information.</S> | Reference Offset:  ['2','65'] | Reference Text:  <S sid = 2 ssid = >The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.</S><S sid = 65 ssid = >Sequence-sensitive features can be included by chaining our local classifiers together and performing joint inference, i.e., by building a conditional markov model (CMM), also known as a maximum entropy markov model (McCallum et al., 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W03-0428.txt | Citing Article:  W07-1712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003).</S> | Reference Offset:  ['4','8'] | Reference Text:  <S sid = 4 ssid = >This number represents a 25% error reduction over the same model without word-internal (substring) features.</S><S sid = 8 ssid = >A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W03-0428.txt | Citing Article:  D11-1144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003).</S> | Reference Offset:  ['7','10'] | Reference Text:  <S sid = 7 ssid = >However, because of data sparsity, sophisticated unknown word models are generally required for good performance.</S><S sid = 10 ssid = >Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W03-0428.txt | Citing Article:  D11-1144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%.</S> | Reference Offset:  ['34','57'] | Reference Text:  <S sid = 34 ssid = >For emissions, we must estimate a quantity of the form , for example, .1 We use an -gram model of order .2 The -gram estimates are smoothed via deleted interpolation.</S><S sid = 57 ssid = >The overall F-score was 52.29%, well below the official CoNLL baseline of 71.18%.6 We next added -gram features; specifically, we framed each word with special start and end symbols, and then added every contiguous substring to the feature list.</S> | Discourse Facet:  NA | Annotator: Automatic


