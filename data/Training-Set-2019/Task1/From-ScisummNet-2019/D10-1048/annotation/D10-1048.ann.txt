Citance Number: 1 | Reference Article:  D10-1048.txt | Citing Article:  P11-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['64','206'] | Reference Text:  <S sid = 64 ssid = >In a significant departure from previous work, each model in our framework gets (possibly incomplete) clustering information for each mention from the earlier coreference models in the multi-pass system.</S><S sid = 206 ssid = >We also thank Nicholas Rizzolo and Dan Roth for helping us replicate their experimental setup, and Heng Ji and Dekang Lin for providing their gender lexicon.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D10-1048.txt | Citing Article:  P14-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al, 2010).</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >A Multi-Pass Sieve for Coreference Resolution</S><S sid = 10 ssid = >Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D10-1048.txt | Citing Article:  W12-4514.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our system is an extension of Stanford's multi-pass sieve system, (Raghunathan et al, 2010) and (Lee et al, 2011), by adding novel constraints and sieves.</S> | Reference Offset:  ['0','142'] | Reference Text:  <S sid = 0 ssid = >A Multi-Pass Sieve for Coreference Resolution</S><S sid = 142 ssid = >To measure the contribution of our multi-pass system, we also present results from a single-pass variant of our system that uses all applicable features from the multi-pass system (marked as “single pass” in the table).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D10-1048.txt | Citing Article:  W12-4514.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast, (Raghunathan et al, 2010) proposed a rule based model which obtained competitive result with less time.</S> | Reference Offset:  ['83','197'] | Reference Text:  <S sid = 83 ssid = >One exception to this rule is the model deployed in the first pass; it only links mentions if their entire extents match exactly.</S><S sid = 197 ssid = >Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model’s cluster output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D10-1048.txt | Citing Article:  W11-1902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We made three considerable extensions to the Raghunathan et al (2010) model.</S> | Reference Offset:  ['37','62'] | Reference Text:  <S sid = 37 ssid = >This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al.</S><S sid = 62 ssid = >Subjects are more probable antecedents for pronouns (Kertz et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D10-1048.txt | Citing Article:  W11-1902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Please see (Raghunathan et al, 2010) for more details.</S> | Reference Offset:  ['37','62'] | Reference Text:  <S sid = 37 ssid = >This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al.</S><S sid = 62 ssid = >Subjects are more probable antecedents for pronouns (Kertz et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D10-1048.txt | Citing Article:  W11-1902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The core of our coreference resolution system is an incremental extension of the system described in Raghunathan et al (2010).</S> | Reference Offset:  ['0','122'] | Reference Text:  <S sid = 0 ssid = >A Multi-Pass Sieve for Coreference Resolution</S><S sid = 122 ssid = >With one exception (Pass 2), all the previous coreference models focus on nominal coreference resolution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D10-1048.txt | Citing Article:  W11-1902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Proper Head Word Match: This sieve marks two mentions headed by proper nouns as coreferent if they have the same head word and satisfy the following constraints: Not i-within-i same as Raghunathan et al (2010).</S> | Reference Offset:  ['102','117'] | Reference Text:  <S sid = 102 ssid = >To address this issue, this pass implements several features that must all be matched in order to yield a link: Cluster head match – the mention head word matches any head word in the antecedent cluster.</S><S sid = 117 ssid = >This pass relaxes the cluster head match heuristic by allowing the mention head to match any word in the cluster of the candidate antecedent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D10-1048.txt | Citing Article:  S12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity (Raghunathan et al, 2010).</S> | Reference Offset:  ['60','84'] | Reference Text:  <S sid = 60 ssid = >This guarantees syntactic salience and also favors document proximity.</S><S sid = 84 ssid = >This model is triggered for all nominal mentions regardless of discourse salience, because it is possible that indefinite mentions are repeated in a document when concepts are discussed but not instantiated, e.g., a sports bar below: We now describe the coreference models implemented in the sieve.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D10-1048.txt | Citing Article:  S12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >By matching the performance of the DT system in the first two rows of the table, the AC system proves that it can successfully learn the relative importance of the deterministic sieves, which in (Raghunathanet al, 2010) and (Lee et al, 2011) have been manually ordered using a separate development dataset.</S> | Reference Offset:  ['24','85'] | Reference Text:  <S sid = 24 ssid = >We implemented all components in our approach using only deterministic models.</S><S sid = 85 ssid = >For clarity, we summarize them in Table 1 and show the cumulative performance as they are added to the sieve in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D10-1048.txt | Citing Article:  W12-4501.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chen built upon the sieve architecture proposed in Raghunathan et al (2010) and added one more sieve - head match - for Chinese and modified two sieves.</S> | Reference Offset:  ['85','117'] | Reference Text:  <S sid = 85 ssid = >For clarity, we summarize them in Table 1 and show the cumulative performance as they are added to the sieve in Table 2.</S><S sid = 117 ssid = >This pass relaxes the cluster head match heuristic by allowing the mention head to match any word in the cluster of the candidate antecedent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D10-1048.txt | Citing Article:  P13-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We incorporate lexicalized feature sets into two different coreference architectures: Reconcile (Stoyanov et al, 2010), a pairwise coreference classifier, and Sieve (Raghunathan et al, 2010), a rule-based system.</S> | Reference Offset:  ['0','37'] | Reference Text:  <S sid = 0 ssid = >A Multi-Pass Sieve for Coreference Resolution</S><S sid = 37 ssid = >This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D10-1048.txt | Citing Article:  P13-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahmanand Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al, 2010).</S> | Reference Offset:  ['0','156'] | Reference Text:  <S sid = 0 ssid = >A Multi-Pass Sieve for Coreference Resolution</S><S sid = 156 ssid = >Like us, they use a rich set of features and deterministic decisions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D10-1048.txt | Citing Article:  W12-4510.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Compared with machine learning methods, (Raghunathan et al, 2010) proposed rule-base models which have been witnessed good performance.</S> | Reference Offset:  ['35','37'] | Reference Text:  <S sid = 35 ssid = >Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works.</S><S sid = 37 ssid = >This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D10-1048.txt | Citing Article:  P13-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al, 2010).</S> | Reference Offset:  ['27','172'] | Reference Text:  <S sid = 27 ssid = >Additionally, we propose several simple, yet powerful, new features.</S><S sid = 172 ssid = >This flexibility is in sharp contrast to supervised classifiers that require their models to be retrained on labeled data, and unsupervised systems that do not offer a clear insertion point for new features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D10-1048.txt | Citing Article:  W11-1811.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains (Raghunathan et al, 2010), and also on other bio data (hsiang Lin and Liang, 2004).</S> | Reference Offset:  ['128','140'] | Reference Text:  <S sid = 128 ssid = >Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009).</S><S sid = 140 ssid = >We exclude from this analysis two notable works that report results only on a version of the task that includes finding mentions (Haghighi and Klein, 2010; Stoyanov, 2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D10-1048.txt | Citing Article:  D12-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our multi-sieve approach is different from (Raghunathan et al 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available.</S> | Reference Offset:  ['0','183'] | Reference Text:  <S sid = 0 ssid = >A Multi-Pass Sieve for Coreference Resolution</S><S sid = 183 ssid = >For proper nouns, 50% of recall errors are due to mention lengthening, mentions that are longer than their earlier mentions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D10-1048.txt | Citing Article:  D12-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Raghunathan et al 2010) recorded the best result on CoNLL 2011 shared task.</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >To the best of our knowledge, we are the first to apply this theory to coreference resolution.</S><S sid = 140 ssid = >We exclude from this analysis two notable works that report results only on a version of the task that includes finding mentions (Haghighi and Klein, 2010; Stoyanov, 2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D10-1048.txt | Citing Article:  D12-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The ordering should be such that (a) maximum amount of information is injected at early stages (b) the precision at the early stages is as high as possible (Raghunathan et al 2010).</S> | Reference Offset:  ['74','171'] | Reference Text:  <S sid = 74 ssid = >Instead, in each of our models, we exploit the cluster information received from the previous stages by resolving only mentions that are currently first in textual order in their cluster.</S><S sid = 171 ssid = >For instance, once a new high precision feature (or group of features) is inserted as its own stage, it will benefit later stages with more precise clusters, but it will not interfere with their particular algorithmic decisions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D10-1048.txt | Citing Article:  D12-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Third, while our division to sieves may resemble witchcraft, it is motivated by the intuition that mentions appearing close to one another are easier instances of co-ref as well as linguistic insights of (Raghunathan et al 2010).</S> | Reference Offset:  ['77','144'] | Reference Text:  <S sid = 77 ssid = >The intuition behind this heuristic is two-fold.</S><S sid = 144 ssid = >On the corpora where our model is not best, it ranks a close second.</S> | Discourse Facet:  NA | Annotator: Automatic


