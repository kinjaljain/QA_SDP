Citance Number: 1 | Reference Article:  P09-1019.txt | Citing Article:  N10-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009).</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices</S><S sid = 9 ssid = >Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1019.txt | Citing Article:  N10-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities.</S> | Reference Offset:  ['90','91'] | Reference Text:  <S sid = 90 ssid = >We therefore approximate the quantity f(e, w, E) with f*(e, w,G) that counts the edge e with n-gram w that has the highest arc posterior probability relative to predecessors in the entire lattice G. f*(e, w,G) can be computed locally, and the n-gram posterior probability based on f* can be determined as follows: Algorithm 3 MBR Decoding on Lattices (Algorithm 3).</S><S sid = 91 ssid = >However, there are important differences when computing the n-gram posterior probabilities (Step 3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1019.txt | Citing Article:  N10-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009).</S> | Reference Offset:  ['64','179'] | Reference Text:  <S sid = 64 ssid = >This operation is a “max” operation and it is identical to the algorithm described in (Macherey et al., 2008) for phrase lattices.</S><S sid = 179 ssid = >We have presented efficient algorithms which extend previous work on lattice-based MERT (Macherey et al., 2008) and MBR decoding (Tromble et al., 2008) to work with hypergraphs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1019.txt | Citing Article:  N10-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009).</S> | Reference Offset:  ['82','140'] | Reference Text:  <S sid = 82 ssid = >Next, the posterior probability of each n-gram is computed.</S><S sid = 140 ssid = >Table 4 compares Hypergraph MBR (HGMBR) with MAP and MBR decoding on 1000 best lists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1019.txt | Citing Article:  P14-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec.</S> | Reference Offset:  ['10','114'] | Reference Text:  <S sid = 10 ssid = >These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004).</S><S sid = 114 ssid = >We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show how MERT can be used to tune MBR parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1019.txt | Citing Article:  P11-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009).</S> | Reference Offset:  ['130','142'] | Reference Text:  <S sid = 130 ssid = >Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system.</S><S sid = 142 ssid = >In other cases, Hypergraph MBR performs at least as well as N-best MBR.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1019.txt | Citing Article:  N10-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009).</S> | Reference Offset:  ['16','70'] | Reference Text:  <S sid = 16 ssid = >We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.</S><S sid = 70 ssid = >The suggested algorithm has similar properties as the algorithm presented in (Macherey et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1019.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration.</S> | Reference Offset:  ['148','158'] | Reference Text:  <S sid = 148 ssid = >MERT is then performed to optimize the BLEU score on a development set; For MERT, we use 40 random initial parameters as well as parameters computed using corpus based statistics (Tromble et al., 2008).</S><S sid = 158 ssid = >In both tables, the following results are reported: Lattice/HGMBR with default parameters (−5,1.5, 2, 3, 4) computed using corpus statistics (Tromble et al., 2008), Lattice/HGMBR with parameters derived from MERT both without/with the baseline model cost feature (mert−b, mert+b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1019.txt | Citing Article:  P10-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices.</S> | Reference Offset:  ['89','103'] | Reference Text:  <S sid = 89 ssid = >The key idea behind this new algorithm is to rewrite the n-gram posterior probability (Equation 4) as follows: where f(e, w, E) is a score assigned to edge e on path E containing n-gram w: { 1 w ∈ e,p(e|G) > p(e'|G), e' precedes e on E 0 otherwise In other words, for each path E, we count the edge that contributes n-gram w and has the highest edge posterior probability relative to its predecessors on the path E; there is exactly one such edge on each lattice path E. We note that f(e, w, E) relies on the full path E which means that it cannot be computed based on local statistics.</S><S sid = 103 ssid = >Lattice MBR Decoding (Equation 3) assumes a linear form for the gain function (Equation 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1019.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009).</S> | Reference Offset:  ['135','142'] | Reference Text:  <S sid = 135 ssid = >For comparison, we prune hypergraphs to the same density (# of edges per edge on the best path) and achieve identical running times for computing the error surface.</S><S sid = 142 ssid = >In other cases, Hypergraph MBR performs at least as well as N-best MBR.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1019.txt | Citing Article:  W10-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009).</S> | Reference Offset:  ['64','98'] | Reference Text:  <S sid = 64 ssid = >This operation is a “max” operation and it is identical to the algorithm described in (Macherey et al., 2008) for phrase lattices.</S><S sid = 98 ssid = >The algorithm to perform Lattice MBR is given in Algorithm 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1019.txt | Citing Article:  P11-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009).</S> | Reference Offset:  ['16','82'] | Reference Text:  <S sid = 16 ssid = >We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.</S><S sid = 82 ssid = >Next, the posterior probability of each n-gram is computed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1019.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009).</S> | Reference Offset:  ['115','144'] | Reference Text:  <S sid = 115 ssid = >We report results on two tasks.</S><S sid = 144 ssid = >This shows the usefulness of Hypergraph MBR decoding as an efficient alternative to Nbest MBR.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1019.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets.</S> | Reference Offset:  ['109','186'] | Reference Text:  <S sid = 109 ssid = >The lattice MBR decoder (Equation 3) can be written as a linear model: E� = argmaxE,Eg �Ni=0 Bigi(E', F), where g0(E',F) = IE'I and gi(E', F) = The linear approximation to BLEU may not hold in practice for unseen test sets or languagepairs.</S><S sid = 186 ssid = >This may not be optimal in practice for unseen test sets and language pairs, and the resulting linear loss may be quite different from the corpus level BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1019.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009).</S> | Reference Offset:  ['0','72'] | Reference Text:  <S sid = 0 ssid = >Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices</S><S sid = 72 ssid = >We first review Minimum Bayes-Risk (MBR) decoding for statistical MT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1019.txt | Citing Article:  P11-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009).</S> | Reference Offset:  ['112','167'] | Reference Text:  <S sid = 112 ssid = >A weight assignment of 1.0 for this feature function and zeros for the other feature functions would imply that the MAP translation is chosen.</S><S sid = 167 ssid = >We hypothesize that the default MBR parameters (Tromble et al., 2008) are well tuned.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1019.txt | Citing Article:  P12-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs.</S> | Reference Offset:  ['75','179'] | Reference Text:  <S sid = 75 ssid = >MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004).</S><S sid = 179 ssid = >We have presented efficient algorithms which extend previous work on lattice-based MERT (Macherey et al., 2008) and MBR decoding (Tromble et al., 2008) to work with hypergraphs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1019.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009).</S> | Reference Offset:  ['14','85'] | Reference Text:  <S sid = 14 ssid = >For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008).</S><S sid = 85 ssid = >We will refer to this procedure as FSAMBR.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1019.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009).</S> | Reference Offset:  ['16','174'] | Reference Text:  <S sid = 16 ssid = >We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.</S><S sid = 174 ssid = >In particular, MERT avoids the need for manually tuning these parameters by language pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1019.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009).</S> | Reference Offset:  ['3','13'] | Reference Text:  <S sid = 3 ssid = >We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.</S><S sid = 13 ssid = >SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT.</S> | Discourse Facet:  NA | Annotator: Automatic


