Citance Number: 1 | Reference Article:  J02-2003.txt | Citing Article:  W04-3213.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002).</S> | Reference Offset:  ['157','191'] | Reference Text:  <S sid = 157 ssid = >An example generalization: Determining top((soup), stir, obj).</S><S sid = 191 ssid = >It is not clear that the class with the highest association score is always the most appropriate level of generalization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J02-2003.txt | Citing Article:  E12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark and Weir (2002) investigate the task of generalizing a single relation concept pair.</S> | Reference Offset:  ['244','287'] | Reference Text:  <S sid = 244 ssid = >11 We note that this procedure does not guarantee that the correct pair is more likely than the incorrect pair, because of noise in the data from the parser and also because a highly plausible incorrect pair could be generated by chance.</S><S sid = 287 ssid = >The approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the α parameter, and ultimately does not improve on the best performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J02-2003.txt | Citing Article:  W07-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent.</S> | Reference Offset:  ['189','200'] | Reference Text:  <S sid = 189 ssid = >The key question is how to determine a suitable level of generalization for concept c, or, alternatively, how to find a suitable class to represent concept c (assuming the choice is from those classes that contain all concepts dominated by some hypernym of c).</S><S sid = 200 ssid = >Li and Abe use MDL to select a set of classes from a hierarchy, together with their associated probabilities, to represent the selectional preferences of a particular verb.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J02-2003.txt | Citing Article:  D07-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002).</S> | Reference Offset:  ['19','209'] | Reference Text:  <S sid = 19 ssid = >The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000).</S><S sid = 209 ssid = >Also, since WordNet is a DAG, Li and Abe turn WordNet into a tree by copying each subgraph with multiple parents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J02-2003.txt | Citing Article:  E09-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize.</S> | Reference Offset:  ['20','197'] | Reference Text:  <S sid = 20 ssid = >The problem with estimating a probability model defined over a large vocabulary of predicates and noun senses is that this involves a huge number of parameters, which results in a sparse-data problem.</S><S sid = 197 ssid = >Note that the solution to the vertical-ambiguity problem presented in the previous sections is able to generalize appropriately in such cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J02-2003.txt | Citing Article:  W06-3815.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002).</S> | Reference Offset:  ['31','208'] | Reference Text:  <S sid = 31 ssid = >We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task.</S><S sid = 208 ssid = >In order to determine the probability of a noun, the probability of a class is assumed to be distributed uniformly among the members of that class: Since WordNet is a hierarchy with noun senses, rather than nouns, at the nodes, Li and Abe deal with the issue of word sense ambiguity using the method described in Section 3, by dividing the count for a noun equally among the concepts whose synsets contain the noun.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J02-2003.txt | Citing Article:  W06-3815.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998).</S> | Reference Offset:  ['86','138'] | Reference Text:  <S sid = 86 ssid = >This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well.</S><S sid = 138 ssid = >The procedure for finding a suitable class, c', to generalize concept c in position r of verb v works as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J02-2003.txt | Citing Article:  W05-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002.</S> | Reference Offset:  ['12','16'] | Reference Text:  <S sid = 12 ssid = >In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods.</S><S sid = 16 ssid = >Such probabilities can be useful for a variety of natural language processing (NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J02-2003.txt | Citing Article:  D07-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat.</S> | Reference Offset:  ['56','192'] | Reference Text:  <S sid = 56 ssid = >The probability of a concept appearing as an argument of a predicate is written p(c | v, r), where c is a concept in WordNet, v is a predicate, and r is an argument position.3 The focus in this article is on the arguments of verbs, but the techniques discussed can be applied to any predicate that takes nominal arguments, such as adjectives.</S><S sid = 192 ssid = >For example, this approach does not always generalize appropriately for arguments that are negatively associated with some verb.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J02-2003.txt | Citing Article:  D07-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002).</S> | Reference Offset:  ['87','170'] | Reference Text:  <S sid = 87 ssid = >Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000).</S><S sid = 170 ssid = >We also see that, for some cases, the value of α has little effect on the level.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J02-2003.txt | Citing Article:  S12-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation.</S> | Reference Offset:  ['87','206'] | Reference Text:  <S sid = 87 ssid = >Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000).</S><S sid = 206 ssid = >Li and Abe refer to such a partition as a “cut” and the cut together with the probabilities as a “tree cut model.” The probabilities of the classes in a cut, P, satisfy the following constraint: (urban area), (geographical area), (region), (location), (object), (entity).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J02-2003.txt | Citing Article:  S12-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002).</S> | Reference Offset:  ['229','243'] | Reference Text:  <S sid = 229 ssid = >The task we used to compare the class-based estimation techniques is a decision task previously used by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999).</S><S sid = 243 ssid = >An evaluation using a PP attachment task was attempted in Clark and Weir (2000), but the evaluation was limited by the relatively small size of the Penn Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J02-2003.txt | Citing Article:  P05-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >Class-Based Probability Estimation Using A Semantic Hierarchy</S><S sid = 16 ssid = >Such probabilities can be useful for a variety of natural language processing (NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J02-2003.txt | Citing Article:  S12-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference.</S> | Reference Offset:  ['31','287'] | Reference Text:  <S sid = 31 ssid = >We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task.</S><S sid = 287 ssid = >The approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the α parameter, and ultimately does not improve on the best performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J02-2003.txt | Citing Article:  S12-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow the approach by Clark and Weir (2002) to create the test data.</S> | Reference Offset:  ['225','287'] | Reference Text:  <S sid = 225 ssid = >This did create a problem with overgeneralization: Many of the cuts returned by MDL were overgeneralizing at the (entity) node.</S><S sid = 287 ssid = >The approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the α parameter, and ultimately does not improve on the best performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J02-2003.txt | Citing Article:  W04-2411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy.</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >Class-Based Probability Estimation Using A Semantic Hierarchy</S><S sid = 10 ssid = >There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J02-2003.txt | Citing Article:  W04-2411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities.</S> | Reference Offset:  ['203','206'] | Reference Text:  <S sid = 203 ssid = >The hierarchy is also assumed to be in the form of a tree.</S><S sid = 206 ssid = >Li and Abe refer to such a partition as a “cut” and the cut together with the probabilities as a “tree cut model.” The probabilities of the classes in a cut, P, satisfy the following constraint: (urban area), (geographical area), (region), (location), (object), (entity).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J02-2003.txt | Citing Article:  W04-2411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above.</S> | Reference Offset:  ['31','120'] | Reference Text:  <S sid = 31 ssid = >We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task.</S><S sid = 120 ssid = >For example, Clark and Weir (2000) describes a prepositional phrase attachment algorithm that employs probability estimates obtained using the WordNet method described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J02-2003.txt | Citing Article:  I08-2105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy.</S> | Reference Offset:  ['86','127'] | Reference Text:  <S sid = 86 ssid = >This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well.</S><S sid = 127 ssid = >Ribas (1995), Li and Abe (1998), McCarthy (2000), and Wagner (2000) all use some kind of thresholding when dealing with counts in the hierarchy (although not in the context of a chi-square test).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J02-2003.txt | Citing Article:  D08-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community.</S> | Reference Offset:  ['17','19'] | Reference Text:  <S sid = 17 ssid = >To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity: Fred ate strawberries with a spoon.</S><S sid = 19 ssid = >The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


