Citance Number: 1 | Reference Article:  N03-1017.txt | Citing Article:  W03-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003).</S> | Reference Offset:  ['8','71'] | Reference Text:  <S sid = 8 ssid = >Phrase translation clearly helps, as we will also show with the experiments in this paper.</S><S sid = 71 ssid = >The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N03-1017.txt | Citing Article:  P14-2046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following phrase-based methods in statistical machine translation (Koehn et al., 2003).</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Statistical Phrase-Based Translation</S><S sid = 7 ssid = >Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems' are augmented with phrase translation capability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N03-1017.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases.</S> | Reference Offset:  ['5','129'] | Reference Text:  <S sid = 5 ssid = >Learning only syntactically motivated phrases degrades the performance of our systems.</S><S sid = 129 ssid = >How long do phrases have to be to achieve high performance?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N03-1017.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system.</S> | Reference Offset:  ['106','110'] | Reference Text:  <S sid = 106 ssid = >First, we compared the performance of the three methods for phrase extraction head-on, using the same decoder (Section 2) and the same trigram language model.</S><S sid = 110 ssid = >We also included in the figure the performance of an IBM Model 4 wordbased translation system (M4), which uses a greedy decoder [Germann et al., 2001].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N03-1017.txt | Citing Article:  W08-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Consider the lexical model pw (ry|rx), defined following Koehn et al (2003).</S> | Reference Offset:  ['73','148'] | Reference Text:  <S sid = 73 ssid = >They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase).</S><S sid = 148 ssid = >Note that phrase translation with a lexical weight is a special case of the alignment template model [Och et al., 1999] with one word class for each word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N03-1017.txt | Citing Article:  W08-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003).</S> | Reference Offset:  ['70','195'] | Reference Text:  <S sid = 70 ssid = >As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993].</S><S sid = 195 ssid = >For all language pairs the phrase model (based on word alignments, Section 3.1) outperforms IBM Model 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N03-1017.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule "grow-diag-final-and" (Koehn et al., 2003).</S> | Reference Offset:  ['70','78'] | Reference Text:  <S sid = 70 ssid = >As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993].</S><S sid = 78 ssid = >As a byproduct, it generates word alignments for this data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N03-1017.txt | Citing Article:  W06-3105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S> | Reference Offset:  ['135','204'] | Reference Text:  <S sid = 135 ssid = >Allowing for longer phrases increases the phrase translation table size (see Table 2).</S><S sid = 204 ssid = >It matters how phrases are extracted.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N03-1017.txt | Citing Article:  W06-3105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >we achieved results similar to Koehn et al (2003a).</S> | Reference Offset:  ['11','71'] | Reference Text:  <S sid = 11 ssid = >Our experiments show that high levels of performance can be achieved with fairly simple means.</S><S sid = 71 ssid = >The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N03-1017.txt | Citing Article:  W04-3219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003).</S> | Reference Offset:  ['2','113'] | Reference Text:  <S sid = 2 ssid = >Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models.</S><S sid = 113 ssid = >All systems improve with more data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N03-1017.txt | Citing Article:  D11-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003).</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Statistical Phrase-Based Translation</S><S sid = 7 ssid = >Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems' are augmented with phrase translation capability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N03-1017.txt | Citing Article:  C08-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003).</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Statistical Phrase-Based Translation</S><S sid = 7 ssid = >Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems' are augmented with phrase translation capability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N03-1017.txt | Citing Article:  W12-4402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration.</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Statistical Phrase-Based Translation</S><S sid = 7 ssid = >Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems' are augmented with phrase translation capability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N03-1017.txt | Citing Article:  W12-4402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012).</S> | Reference Offset:  ['44','185'] | Reference Text:  <S sid = 44 ssid = >We recombine search hypotheses as done by Och et al. [2001].</S><S sid = 185 ssid = >For more information on these models, please refer to Brown et al. [1993].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N03-1017.txt | Citing Article:  W06-3122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['64','205'] | Reference Text:  <S sid = 64 ssid = >With our decoder, translating 1755 sentence of length 5-15 words takes about 10 minutes on a 2 GHz Linux system.</S><S sid = 205 ssid = >The results suggest that choosing the right alignment heuristic is more important than which model is used to create the initial word alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N03-1017.txt | Citing Article:  N07-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003).</S> | Reference Offset:  ['100','153'] | Reference Text:  <S sid = 100 ssid = >Note that this approach is consistent with the approach taken by Marcu and Wong themselves, who use conditional models during decoding.</S><S sid = 153 ssid = >We remedy this problem with a heuristic approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N03-1017.txt | Citing Article:  P12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003).</S> | Reference Offset:  ['186','190'] | Reference Text:  <S sid = 186 ssid = >Again, we use the heuristics from the Section 4.5 to reconcile the mono-directional alignments obtained through training parameters using models of increasing complexity.</S><S sid = 190 ssid = >Using different expansion heuristics during symmetrizing the word alignments has a bigger effect.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N03-1017.txt | Citing Article:  P11-2079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010).</S> | Reference Offset:  ['0','6'] | Reference Text:  <S sid = 0 ssid = >Statistical Phrase-Based Translation</S><S sid = 6 ssid = >Various researchers have improved the quality of statistical machine translation system with the use of phrase translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N03-1017.txt | Citing Article:  P10-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4, the refinement and phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al, 2007) to decode.</S> | Reference Offset:  ['7','70'] | Reference Text:  <S sid = 7 ssid = >Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems' are augmented with phrase translation capability.</S><S sid = 70 ssid = >As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N03-1017.txt | Citing Article:  W10-3707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['64','205'] | Reference Text:  <S sid = 64 ssid = >With our decoder, translating 1755 sentence of length 5-15 words takes about 10 minutes on a 2 GHz Linux system.</S><S sid = 205 ssid = >The results suggest that choosing the right alignment heuristic is more important than which model is used to create the initial word alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


