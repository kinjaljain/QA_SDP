Citance Number: 1 | Reference Article:  N04-1033.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a phrase-based translation approach as described in (Zens and Ney, 2004).</S> | Reference Offset:  ['252','257'] | Reference Text:  <S sid = 252 ssid = >In (Zens et al., 2002), a simple phrase-based approach is described that served as starting point for the system in this work.</S><S sid = 257 ssid = >We described a phrase-based translation approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1033.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible.</S> | Reference Offset:  ['24','262'] | Reference Text:  <S sid = 24 ssid = >In Section 4, we will describe a monotone search algorithm.</S><S sid = 262 ssid = >We described a highly efficient monotone search algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1033.txt | Citing Article:  W12-3137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant.</S> | Reference Offset:  ['23','220'] | Reference Text:  <S sid = 23 ssid = >Then, we will describe refinements of the baseline model.</S><S sid = 220 ssid = >Thus the lexical choice of words is of the same quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1033.txt | Citing Article:  P09-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004).</S> | Reference Offset:  ['90','98'] | Reference Text:  <S sid = 90 ssid = >The monotone search can be efficiently computed with dynamic programming.</S><S sid = 98 ssid = >We obtain the following dynamic programming recursion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1033.txt | Citing Article:  P06-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004).</S> | Reference Offset:  ['199','252'] | Reference Text:  <S sid = 199 ssid = >The alignment template system (Och et al., 1999) is similar to the system described in this work.</S><S sid = 252 ssid = >In (Zens et al., 2002), a simple phrase-based approach is described that served as starting point for the system in this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1033.txt | Citing Article:  W06-3118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table.</S> | Reference Offset:  ['52','62'] | Reference Text:  <S sid = 52 ssid = >Finally, we have to estimate the phrase translation probabilities p(˜f|˜e).</S><S sid = 62 ssid = >This will be used to smooth the phrase translation probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1033.txt | Citing Article:  W10-1717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities.</S> | Reference Offset:  ['72','153'] | Reference Text:  <S sid = 72 ssid = >We are using relative frequencies to estimate the phrase translation probabilities.</S><S sid = 153 ssid = >It should be pointed out that in practice the monotone search will perform better than what the preceding estimates indicate.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1033.txt | Citing Article:  P06-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['96','281'] | Reference Text:  <S sid = 96 ssid = >Q(J + 1, $) is the probability of the optimum translation.</S><S sid = 281 ssid = >This work has been partially funded by the EU project TransType 2, IST-2001-32091.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1033.txt | Citing Article:  P08-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004).</S> | Reference Offset:  ['18','257'] | Reference Text:  <S sid = 18 ssid = >As a decision rule, we obtain: This approach is a generalization of the source-channel approach.</S><S sid = 257 ssid = >We described a phrase-based translation approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1033.txt | Citing Article:  P08-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004).</S> | Reference Offset:  ['0','24'] | Reference Text:  <S sid = 0 ssid = >Improvements In Phrase-Based Statistical Machine Translation</S><S sid = 24 ssid = >In Section 4, we will describe a monotone search algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1033.txt | Citing Article:  E09-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Logic MONOTONE) This is the algorithm of Zens and Ney (2004).</S> | Reference Offset:  ['24','262'] | Reference Text:  <S sid = 24 ssid = >In Section 4, we will describe a monotone search algorithm.</S><S sid = 262 ssid = >We described a highly efficient monotone search algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1033.txt | Citing Article:  E09-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint.</S> | Reference Offset:  ['50','185'] | Reference Text:  <S sid = 50 ssid = >Therefore, there is no constraint on the reordering within the phrases.</S><S sid = 185 ssid = >For details, see (Tillmann and Ney, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1033.txt | Citing Article:  W09-0439.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur.</S> | Reference Offset:  ['191','195'] | Reference Text:  <S sid = 191 ssid = >We use the Downhill Simplex algorithm from (Press et al., 2002).</S><S sid = 195 ssid = >In the experiments, the Downhill Simplex algorithm converged after about 200 iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1033.txt | Citing Article:  W07-0717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['96','281'] | Reference Text:  <S sid = 96 ssid = >Q(J + 1, $) is the probability of the optimum translation.</S><S sid = 281 ssid = >This work has been partially funded by the EU project TransType 2, IST-2001-32091.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1033.txt | Citing Article:  W07-0717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004).</S> | Reference Offset:  ['38','42'] | Reference Text:  <S sid = 38 ssid = >We take the union ofboth alignments to obtain a symmetrized word alignment matrix.</S><S sid = 42 ssid = >It means that two phrases are considered to be translations of each other, if the words are aligned only within the phrase pair and not to words outside.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1033.txt | Citing Article:  P12-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages.</S> | Reference Offset:  ['212','215'] | Reference Text:  <S sid = 212 ssid = >The unigram method hurts performance.</S><S sid = 215 ssid = >The translation results of the different systems are shown in Table 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1033.txt | Citing Article:  P12-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This finding fails to echo the promising results in the previous study (Zens and Ney,2004).</S> | Reference Offset:  ['6','206'] | Reference Text:  <S sid = 6 ssid = >The translation results for the Xerox and Canadian Hansards task are very promising.</S><S sid = 206 ssid = >We start with the Verbmobil results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1033.txt | Citing Article:  P08-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004).</S> | Reference Offset:  ['0','34'] | Reference Text:  <S sid = 0 ssid = >Improvements In Phrase-Based Statistical Machine Translation</S><S sid = 34 ssid = >So, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1033.txt | Citing Article:  W09-0438.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words.</S> | Reference Offset:  ['216','266'] | Reference Text:  <S sid = 216 ssid = >Obviously, the monotone phrase-based system outperforms the monotone single-word based system.</S><S sid = 266 ssid = >The described search is monotone at the phrase level.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1033.txt | Citing Article:  W09-1704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Improvements In Phrase-Based Statistical Machine Translation</S><S sid = 1 ssid = >In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups.</S> | Discourse Facet:  NA | Annotator: Automatic


