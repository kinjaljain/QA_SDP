Citance Number: 1 | Reference Article:  W06-1615.txt | Citing Article:  D07-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005).</S> | Reference Offset:  ['130','180'] | Reference Text:  <S sid = 130 ssid = >MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).</S><S sid = 180 ssid = >We use the parser described by McDonald et al. (2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-1615.txt | Citing Article:  D07-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al, 2006) in the domain adaptation task given by the CoNLL 2007.</S> | Reference Offset:  ['0','222'] | Reference Text:  <S sid = 0 ssid = >Domain Adaptation With Structural Correspondence Learning</S><S sid = 222 ssid = >Structural correspondence learning is a marriage of ideas from single domain semi-supervised learning and domain adaptation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-1615.txt | Citing Article:  P11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Prettenhofer and Stein (2010) investigate cross lingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al, 2006).</S> | Reference Offset:  ['0','222'] | Reference Text:  <S sid = 0 ssid = >Domain Adaptation With Structural Correspondence Learning</S><S sid = 222 ssid = >Structural correspondence learning is a marriage of ideas from single domain semi-supervised learning and domain adaptation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-1615.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There has been a lot of work in domain adaption for NLP (Dai et al, 2007) (Jiang and Zhai, 2007) and one suitable choice for our problem is the approach based on structural correspondence learning (SCL) as in (Blitzer et al, 2006) and (Blitzer et al, 2007b).</S> | Reference Offset:  ['19','130'] | Reference Text:  <S sid = 19 ssid = >There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).</S><S sid = 130 ssid = >MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-1615.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These features can either be identified with heuristics (Blitzer et al, 2006) or by automatic selection (Blitzer et al, 2007b).</S> | Reference Offset:  ['19','130'] | Reference Text:  <S sid = 19 ssid = >There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).</S><S sid = 130 ssid = >MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-1615.txt | Citing Article:  D07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al, 2006).</S> | Reference Offset:  ['180','182'] | Reference Text:  <S sid = 180 ssid = >We use the parser described by McDonald et al. (2005b).</S><S sid = 182 ssid = >We train the parser and PoS tagger on the same size of WSJ data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-1615.txt | Citing Article:  D07-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work by McClosky et al (2006) and Blitzer et al (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation.</S> | Reference Offset:  ['19','127'] | Reference Text:  <S sid = 19 ssid = >There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).</S><S sid = 127 ssid = >For our experiments we use a version of the discriminative online large-margin learning algorithm MIRA (Crammer et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-1615.txt | Citing Article:  P11-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SCL is the structural correspondence learning technique of Blitzer et al (2006).</S> | Reference Offset:  ['0','22'] | Reference Text:  <S sid = 0 ssid = >Domain Adaptation With Structural Correspondence Learning</S><S sid = 22 ssid = >We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-1615.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al, 2006), which is an extension of the method described by (Ando and Zhang, 558 2005).</S> | Reference Offset:  ['21','227'] | Reference Text:  <S sid = 21 ssid = >Structural learning models the correlations which are most useful for semi-supervised learning.</S><S sid = 227 ssid = >SCL consistently outperformed both supervised and semi-supervised learning with no labeled target domain training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-1615.txt | Citing Article:  W10-2607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SCL (Blitzer et al, 2006) is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification.</S> | Reference Offset:  ['58','92'] | Reference Text:  <S sid = 58 ssid = >In our experiments, the supervised task is part of speech tagging.</S><S sid = 92 ssid = >That is, 0x is the desired mapping to the (low dimensional) shared feature representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-1615.txt | Citing Article:  W10-2607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al, 2006).</S> | Reference Offset:  ['16','178'] | Reference Text:  <S sid = 16 ssid = >Pivot features are features which behave in the same way for discriminative learning in both domains.</S><S sid = 178 ssid = >Using SCL features still does, however.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-1615.txt | Citing Article:  W10-2607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As in (Blitzer et al, 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data.</S> | Reference Offset:  ['19','228'] | Reference Text:  <S sid = 19 ssid = >There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).</S><S sid = 228 ssid = >We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from Florian et al. (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-1615.txt | Citing Article:  W10-2607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this way our problem resembles the part-of-speech tagging task (Blitzer et al, 2006), where the category of each word is predicted using values of the left, right, and current word token.</S> | Reference Offset:  ['82','141'] | Reference Text:  <S sid = 82 ssid = >That is, we do not use any feature derived from the right word when solving a right token pivot predictor.</S><S sid = 141 ssid = >Ticks to the left or right indicate relative positive or negative values for a word under this projection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-1615.txt | Citing Article:  W10-1757.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Blitzer et al, 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP.</S> | Reference Offset:  ['70','108'] | Reference Text:  <S sid = 70 ssid = >Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a).</S><S sid = 108 ssid = >We found it necessary to make a change to the ASO algorithm as described in Ando and Zhang (2005a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-1615.txt | Citing Article:  W11-0310.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al (2006)).</S> | Reference Offset:  ['5','192'] | Reference Text:  <S sid = 5 ssid = >We introduce learning automatically induce correspondences among features from different domains.</S><S sid = 192 ssid = >Aside from Florian et al. (2004), several authors have also given techniques for adapting classification to new domains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-1615.txt | Citing Article:  P11-2021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, we compare two domain adaptation approaches to utilize unlabeled speech data: bootstrapping, and Blitzer et al's Structural Correspondence Learning (SCL) (Blitzer et al, 2006).</S> | Reference Offset:  ['0','19'] | Reference Text:  <S sid = 0 ssid = >Domain Adaptation With Structural Correspondence Learning</S><S sid = 19 ssid = >There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-1615.txt | Citing Article:  P11-2021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain independent features.</S> | Reference Offset:  ['216','223'] | Reference Text:  <S sid = 216 ssid = >Our ASO baseline uses unlabeled data from the target domain.</S><S sid = 223 ssid = >It uses unlabeled data and frequently-occurring pivot features from both source and target domains to find correspondences among features from these domains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-1615.txt | Citing Article:  W09-2420.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blitzer et al (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of Speech tagger.</S> | Reference Offset:  ['0','22'] | Reference Text:  <S sid = 0 ssid = >Domain Adaptation With Structural Correspondence Learning</S><S sid = 22 ssid = >We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-1615.txt | Citing Article:  P14-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al, 2006).</S> | Reference Offset:  ['35','40'] | Reference Text:  <S sid = 35 ssid = >Figure 1 shows two PoS-tagged sentences, one each from the Wall Street Journal (hereafter WSJ) and MEDLINE.</S><S sid = 40 ssid = >The word “signal” in this sentence is a noun, but a tagger trained on the WSJ incorrectly classifies it as an adjective.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-1615.txt | Citing Article:  P14-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blitzer et al (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.</S> | Reference Offset:  ['10','165'] | Reference Text:  <S sid = 10 ssid = >However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data.</S><S sid = 165 ssid = >In this case, we make use of the out-of-domain data by using features of the source domain tagger’s predictions in training and testing the target domain tagger (Florian et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


