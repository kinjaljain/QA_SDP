Citance Number: 1 | Reference Article:  W01-0521.txt | Citing Article:  W01-0908.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001).</S> | Reference Offset:  ['83','95'] | Reference Text:  <S sid = 83 ssid = >The lexical bigrams are contained in the most specific distribution for P,,,,.</S><S sid = 95 ssid = >In particular, lexical bigram statistics appear to be corpus-specific, and our results show that they are of no use when attempting to generalize to new training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W01-0521.txt | Citing Article:  P14-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001).</S> | Reference Offset:  ['48','65'] | Reference Text:  <S sid = 48 ssid = >The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.</S><S sid = 65 ssid = >Lexical cooccurrence statistics seem to be of no benefit when attempting to generalize to a new corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W01-0521.txt | Citing Article:  N06-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy.</S> | Reference Offset:  ['8','16'] | Reference Text:  <S sid = 8 ssid = >Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al.</S><S sid = 16 ssid = >Roland et al. (2000) find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W01-0521.txt | Citing Article:  W10-3911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001).</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Corpus Variation And Parser Performance</S><S sid = 21 ssid = >We take as our baseline parser the statistical model of Model 1 of Collins (1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W01-0521.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001).</S> | Reference Offset:  ['13','62'] | Reference Text:  <S sid = 13 ssid = >A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.</S><S sid = 62 ssid = >Results are shown in Table 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W01-0521.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001).</S> | Reference Offset:  ['15','19'] | Reference Text:  <S sid = 15 ssid = >Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998).</S><S sid = 19 ssid = >The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W01-0521.txt | Citing Article:  P11-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here.</S> | Reference Offset:  ['102','103'] | Reference Text:  <S sid = 102 ssid = >Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.</S><S sid = 103 ssid = >Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W01-0521.txt | Citing Article:  W07-2203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data.</S> | Reference Offset:  ['34','36'] | Reference Text:  <S sid = 34 ssid = >We conducted separate experiments using WSJ data, Brown data, and a combination of the two as training material.</S><S sid = 36 ssid = >For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W01-0521.txt | Citing Article:  W04-2410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001).</S> | Reference Offset:  ['19','53'] | Reference Text:  <S sid = 19 ssid = >The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.</S><S sid = 53 ssid = >The large number of possible pairs of words in the vocabulary make the training data necessarily sparse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W01-0521.txt | Citing Article:  D09-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001).</S> | Reference Offset:  ['10','13'] | Reference Text:  <S sid = 10 ssid = >The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.</S><S sid = 13 ssid = >A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W01-0521.txt | Citing Article:  D08-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists.</S> | Reference Offset:  ['3','93'] | Reference Text:  <S sid = 3 ssid = >We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.</S><S sid = 93 ssid = >The standard WSJ task seems to be simplified by its homogenous style.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W01-0521.txt | Citing Article:  D08-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001).</S> | Reference Offset:  ['36','38'] | Reference Text:  <S sid = 36 ssid = >For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.</S><S sid = 38 ssid = >However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W01-0521.txt | Citing Article:  I08-2096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001).</S> | Reference Offset:  ['6','48'] | Reference Text:  <S sid = 6 ssid = >The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.</S><S sid = 48 ssid = >The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W01-0521.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Gildea (2001) for the exact setup.</S> | Reference Offset:  ['102','103'] | Reference Text:  <S sid = 102 ssid = >Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.</S><S sid = 103 ssid = >Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W01-0521.txt | Citing Article:  W09-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head.</S> | Reference Offset:  ['24','51'] | Reference Text:  <S sid = 24 ssid = >The first distribution gives probability of the syntactic category H of the head child of a parent node with category P, head word Hhw with the head tag (the part of speech tag of the head word) Hht: The head word and head tag of the new node H are defined to be the same as those of its parent.</S><S sid = 51 ssid = >(The head word of a parent is the same as the head word of its head child.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W01-0521.txt | Citing Article:  W06-2902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole.</S> | Reference Offset:  ['41','47'] | Reference Text:  <S sid = 41 ssid = >Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.</S><S sid = 47 ssid = >The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W01-0521.txt | Citing Article:  W06-2902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability.</S> | Reference Offset:  ['6','21'] | Reference Text:  <S sid = 6 ssid = >The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.</S><S sid = 21 ssid = >We take as our baseline parser the statistical model of Model 1 of Collins (1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W01-0521.txt | Citing Article:  W06-2902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.</S> | Reference Offset:  ['41','47'] | Reference Text:  <S sid = 41 ssid = >Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.</S><S sid = 47 ssid = >The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W01-0521.txt | Citing Article:  W06-2902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above).</S> | Reference Offset:  ['44','63'] | Reference Text:  <S sid = 44 ssid = >Combining the WSJ and Brown training data in one model improves performance further, but by less than 0.5% absolute.</S><S sid = 63 ssid = >Perhaps the most striking result is just how little the elimination of lexical bigrams affects the baseline system: performance on the WSJ corpus decreases by less than 0.5% absolute.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W01-0521.txt | Citing Article:  W10-2604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests.</S> | Reference Offset:  ['18','62'] | Reference Text:  <S sid = 18 ssid = >The variation in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful in parsing another.</S><S sid = 62 ssid = >Results are shown in Table 3.</S> | Discourse Facet:  NA | Annotator: Automatic


