Citance Number: 1 | Reference Article:  P03-1004.txt | Citing Article:  C04-1190.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)).</S> | Reference Offset:  ['1','9'] | Reference Text:  <S sid = 1 ssid = >Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).</S><S sid = 9 ssid = >In the field of Natural Language Processing, many successes have been reported.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-1004.txt | Citing Article:  P14-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003).</S> | Reference Offset:  ['118','121'] | Reference Text:  <S sid = 118 ssid = >Let s = c1c2 · · · cm be a sequence of Japanese characters, t = t1t2 · · · tm be a sequence of Japanese character types 3 associated with each character, and yi ∈ {+1, −1}, (i = (1, 2,...,m− 1)) be a boundary marker.</S><S sid = 121 ssid = >Note that we distinguish the relative position of each character and character type.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-1004.txt | Citing Article:  P14-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection.</S> | Reference Offset:  ['10','114'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 114 ssid = >In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-1004.txt | Citing Article:  C10-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s | s ∈ S, fj ∈ s} from each feature fj to support vector s ∈ S to only consider support vector s relevant to given x such that s Tx 6= 0.</S> | Reference Offset:  ['26','162'] | Reference Text:  <S sid = 26 ssid = >One is PKI (Polynomial Kernel Inverted), which is an extension of Inverted Index in Information Retrieval.</S><S sid = 162 ssid = >One is PKI (Polynomial Kernel Inverted), which is an extension of Inverted Index.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-1004.txt | Citing Article:  C10-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space.</S><S sid = 168 ssid = >µd ¶µ</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-1004.txt | Citing Article:  C10-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features.</S> | Reference Offset:  ['96','98'] | Reference Text:  <S sid = 96 ssid = >We use a TRIE to efficiently store the set Q.</S><S sid = 98 ssid = >Although there are many implementations for TRIE, we use a Double-Array (Aoe, 1989) in our task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-1004.txt | Citing Article:  P08-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears.</S> | Reference Offset:  ['62','162'] | Reference Text:  <S sid = 62 ssid = >This is a naive extension of Inverted Indexing in Information Retrieval.</S><S sid = 162 ssid = >One is PKI (Polynomial Kernel Inverted), which is an extension of Inverted Index.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-1004.txt | Citing Article:  P08-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >PKE - Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003).</S> | Reference Offset:  ['10','161'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 161 ssid = >Then, we introduced two fast classification algorithms for this kernel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-1004.txt | Citing Article:  P08-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion.</S> | Reference Offset:  ['104','159'] | Reference Text:  <S sid = 104 ssid = >The experiments using a Cubic Kernel are suitable to evaluate the effectiveness of the basket mining approach applied in the PKE, since a Cubic Kernel projects the original feature space F into F3 space, which is too large to be handled only using a naive exhaustive method.</S><S sid = 159 ssid = >PKE takes a basket mining approach to enumerating effective feature combinations more efficiently than their exhaustive method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-1004.txt | Citing Article:  E12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger.</S> | Reference Offset:  ['10','23'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 23 ssid = >These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-1004.txt | Citing Article:  P05-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding.</S> | Reference Offset:  ['1','8'] | Reference Text:  <S sid = 1 ssid = >Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).</S><S sid = 8 ssid = >Kernel methods (e.g., Support Vector Machines (Vapnik, 1995)) attract a great deal of attention recently.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-1004.txt | Citing Article:  N07-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy.</S> | Reference Offset:  ['1','23'] | Reference Text:  <S sid = 1 ssid = >Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).</S><S sid = 23 ssid = >These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-1004.txt | Citing Article:  W07-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets.</S> | Reference Offset:  ['10','114'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 114 ssid = >In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-1004.txt | Citing Article:  D09-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w.</S> | Reference Offset:  ['10','114'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 114 ssid = >In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P03-1004.txt | Citing Article:  D09-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task.</S> | Reference Offset:  ['93','114'] | Reference Text:  <S sid = 93 ssid = >In our mining task, we can give a real number to minimum support, since each transaction (support example Xj) has possibly non-integer frequency Cdyjαj.</S><S sid = 114 ssid = >In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P03-1004.txt | Citing Article:  D09-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This result conforms to the results reported in (Kudo and Matsumoto, 2003).</S> | Reference Offset:  ['10','91'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 91 ssid = >The result can be obtained by merging these two results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P03-1004.txt | Citing Article:  W09-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space.</S><S sid = 168 ssid = >µd ¶µ</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P03-1004.txt | Citing Article:  W09-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.</S> | Reference Offset:  ['29','82'] | Reference Text:  <S sid = 29 ssid = >In order to build PKE, we extend the PrefixSpan (Pei et al., 2001), an efficient Basket Mining algorithm, to enumerate effective feature combinations from a set of support examples.</S><S sid = 82 ssid = >There are many subset mining algorithms proposed, however, we will focus on the PrefixSpan algorithm, which is an efficient algorithm for sequential pattern mining, originally proposed by (Pei et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P03-1004.txt | Citing Article:  W10-2926.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space.</S><S sid = 168 ssid = >µd ¶µ</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P03-1004.txt | Citing Article:  D12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al, 2007) for dependency parsing.</S> | Reference Offset:  ['10','124'] | Reference Text:  <S sid = 10 ssid = >Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).</S><S sid = 124 ssid = >In previous research, we presented a state-of-the-art SVMs-based Japanese dependency parser (Kudo and Matsumoto, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


