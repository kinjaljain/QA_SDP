Citance Number: 1 | Reference Article:  P07-1019.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since we approach decoding as xR transduction, the process is identical to that of constituency based algorithms (e.g. Huang and Chiang, 2007).</S> | Reference Offset:  ['2','21'] | Reference Text:  <S sid = 2 ssid = >We develop faster approaches for problem based on parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems.</S><S sid = 21 ssid = >This process can be formalized as a deductive system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1019.txt | Citing Article:  P14-2022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes.</S> | Reference Offset:  ['27','58'] | Reference Text:  <S sid = 27 ssid = >These two deductive systems represent the search space of decoding without a language model.</S><S sid = 58 ssid = >The different target sides then constitute a third dimension of the grid, forming a cube of possible combinations (Chiang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1019.txt | Citing Article:  D10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007).</S> | Reference Offset:  ['13','42'] | Reference Text:  <S sid = 13 ssid = >We push the idea behind this method further and make the following contributions in this paper: Cube pruning and cube growing are collectively called forest rescoring since they both approximately rescore the packed forest of derivations from −LM decoding.</S><S sid = 42 ssid = >In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1019.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A hyper graph is analogous to a parse forest (Huang and Chiang, 2007).</S> | Reference Offset:  ['76','114'] | Reference Text:  <S sid = 76 ssid = >In other words, we would like to have an estimate of the best item not explored yet (analogous to the heuristic function in A* search).</S><S sid = 114 ssid = >Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1019.txt | Citing Article:  W11-2167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007).</S> | Reference Offset:  ['11','26'] | Reference Text:  <S sid = 11 ssid = >Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring.</S><S sid = 26 ssid = >The resulting translation t2t1 is the inverted concatenation as specified by the target-side of the SCFG rule with the additional cost c′ being the cost of this rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1019.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration.</S> | Reference Offset:  ['6','102'] | Reference Text:  <S sid = 6 ssid = >Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999).</S><S sid = 102 ssid = >We set the decoder phrase-table limit to 100 as suggested in (Koehn, 2004) and the distortion limit to 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1019.txt | Citing Article:  D12-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007).</S> | Reference Offset:  ['117','118'] | Reference Text:  <S sid = 117 ssid = >Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a).</S><S sid = 118 ssid = >At the lowest level of search error, the relative speed-up from cube growing and cube pruning compared with full-integration is by a factor of 9.8 and 4.1, respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1019.txt | Citing Article:  P10-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems.</S> | Reference Offset:  ['8','27'] | Reference Text:  <S sid = 8 ssid = >In practice, one must prune the search space aggressively to reduce it to a reasonable size.</S><S sid = 27 ssid = >These two deductive systems represent the search space of decoding without a language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1019.txt | Citing Article:  W11-2142.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007).</S> | Reference Offset:  ['42','68'] | Reference Text:  <S sid = 42 ssid = >In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation.</S><S sid = 68 ssid = >This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1019.txt | Citing Article:  N12-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search.</S> | Reference Offset:  ['37','68'] | Reference Text:  <S sid = 37 ssid = >The experiments in this paper use trigram models.</S><S sid = 68 ssid = >This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1019.txt | Citing Article:  P11-1128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Tree-to-string decoding with STSG is usually treated as forest rescoring (Huang and Chiang, 2007) that involves two steps.</S> | Reference Offset:  ['0','114'] | Reference Text:  <S sid = 0 ssid = >Forest Rescoring: Faster Decoding with Integrated Language Models</S><S sid = 114 ssid = >Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1019.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007).</S> | Reference Offset:  ['61','66'] | Reference Text:  <S sid = 61 ssid = >See Figure 2 for the pseudocode for cube pruning.</S><S sid = 66 ssid = >Although much faster than full-integration, cube pruning still computes a fixed amount of +LM items at each node, many of which will not be useful for arriving at the 1-best hypothesis at the root.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1019.txt | Citing Article:  W11-2211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In hierarchical phrase-based translation (Chiang, 2005) a weighted synchronous context-free grammar is induced from parallel text, the search is based on CYK+ parsing (Chappelier and Rajman, 1998) and typically carried out using the cube pruning algorithm (Huang and Chiang, 2007).</S> | Reference Offset:  ['12','15'] | Reference Text:  <S sid = 12 ssid = >By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system.</S><S sid = 15 ssid = >We establish in this section a unified framework for translation with an integrated n-gram language model in both phrase-based systems and syntaxbased systems based on synchronous context-free grammars (SCFGs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1019.txt | Citing Article:  D08-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Forest Rescoring: Faster Decoding with Integrated Language Models</S><S sid = 1 ssid = >Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1019.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a sub translation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right.</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.</S><S sid = 7 ssid = >The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1019.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The complexity of this dynamic programming algorithm for g-gram decoding is O (2nn2|V|g-1) where n is the sentence length and |V| is the English vocabulary size (Huang and Chiang, 2007).</S> | Reference Offset:  ['6','7'] | Reference Text:  <S sid = 6 ssid = >Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999).</S><S sid = 7 ssid = >The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1019.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An alternative approach to computing a synchronous parse forest is based on cube pruning (Huang and Chiang, 2007).</S> | Reference Offset:  ['41','122'] | Reference Text:  <S sid = 41 ssid = >Cube pruning (Chiang, 2007) reduces the search space significantly based on the observation that when the above method is combined with beam search, only a small fraction of the possible +LM items at a node will escape being pruned, and moreover we can select with reasonable accuracy those top-k items without computing all possible items first.</S><S sid = 122 ssid = >We have presented a novel extension of cube pruning called cube growing, and shown how both can be seen as general forest rescoring techniques applicable to both phrase-based and syntax-based decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1019.txt | Citing Article:  W12-3137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We utilize the cube pruning algorithm (Huang and Chiang, 2007) for decoding and optimize the model weights with MERT.</S> | Reference Offset:  ['68','101'] | Reference Text:  <S sid = 68 ssid = >This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1).</S><S sid = 101 ssid = >The weights for the log-linear model are tuned on a separate development set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1019.txt | Citing Article:  P11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007).</S> | Reference Offset:  ['42','117'] | Reference Text:  <S sid = 42 ssid = >In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation.</S><S sid = 117 ssid = >Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1019.txt | Citing Article:  N09-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Huang and Chiang (2007) describe a variation of cube pruning called cube growing, and they apply it to a source-tree to target string translator.</S> | Reference Offset:  ['110','114'] | Reference Text:  <S sid = 110 ssid = >In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006).</S><S sid = 114 ssid = >Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


