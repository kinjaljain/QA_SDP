Citance Number: 1 | Reference Article:  P02-1060.txt | Citing Article:  P03-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['118','119'] | Reference Text:  <S sid = 118 ssid = >While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.</S><S sid = 119 ssid = >In the near feature, we would like to incorporate the following into our system:</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1060.txt | Citing Article:  W10-2416.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM).</S> | Reference Offset:  ['105','115'] | Reference Text:  <S sid = 105 ssid = >Another important question is about the effect of different sub-features.</S><S sid = 115 ssid = >Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1060.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger.</S> | Reference Offset:  ['93','95'] | Reference Text:  <S sid = 93 ssid = >In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data.</S><S sid = 95 ssid = >For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1060.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method.</S> | Reference Offset:  ['54','70'] | Reference Text:  <S sid = 54 ssid = >This paper will focus on difference between our tagger and other traditional HMM-based taggers, as used in BBN's IdentiFinder.</S><S sid = 70 ssid = >This kind of internal sub-feature has been widely used in machine-learning systems, such as BBN's IdendiFinder and New York Univ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1060.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002).</S> | Reference Offset:  ['19','107'] | Reference Text:  <S sid = 19 ssid = >These systems are mainly rule-based.</S><S sid = 107 ssid = >2) 2 f is very useful for NER and increases the performance further by 10% to 87.4%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1060.txt | Citing Article:  C04-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002).</S> | Reference Offset:  ['0','36'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition Using An HMM-Based Chunk Tagger</S><S sid = 36 ssid = >The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL'2000 [Tjong+00].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1060.txt | Citing Article:  P05-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002).</S> | Reference Offset:  ['61','62'] | Reference Text:  <S sid = 61 ssid = >In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features.</S><S sid = 62 ssid = >The internal sub-features are found within the word and/or word string itself to capture internal evidence while external sub-features are derived within the context to capture external evidence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1060.txt | Citing Article:  C04-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage.</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition Using An HMM-Based Chunk Tagger</S><S sid = 7 ssid = >Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1060.txt | Citing Article:  C04-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity.</S> | Reference Offset:  ['0','55'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition Using An HMM-Based Chunk Tagger</S><S sid = 55 ssid = >Ideally, it can be estimated by using the forward-backward algorithm [Rabiner89] recursively for the 1st-order [Rabiner89] or 2nd -order HMMs [Watson+92].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1060.txt | Citing Article:  W06-2201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002).</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition Using An HMM-Based Chunk Tagger</S><S sid = 7 ssid = >Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1060.txt | Citing Article:  P06-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002).</S> | Reference Offset:  ['9','33'] | Reference Text:  <S sid = 9 ssid = >Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.</S><S sid = 33 ssid = >As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1060.txt | Citing Article:  P06-2074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB.</S> | Reference Offset:  ['100','102'] | Reference Text:  <S sid = 100 ssid = >More generally how does the performance vary as the training data size changes?</S><S sid = 102 ssid = >It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain.</S> | Reference Offset:  ['1','54'] | Reference Text:  <S sid = 1 ssid = >This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.</S><S sid = 54 ssid = >This paper will focus on difference between our tagger and other traditional HMM-based taggers, as used in BBN's IdentiFinder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002).</S> | Reference Offset:  ['93','95'] | Reference Text:  <S sid = 93 ssid = >In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data.</S><S sid = 95 ssid = >For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002).</S> | Reference Offset:  ['36','56'] | Reference Text:  <S sid = 36 ssid = >The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL'2000 [Tjong+00].</S><S sid = 56 ssid = >However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). arg max log (  |) Then we assume conditional probability word sequence and F1n = f 1 f2 ... fn is the word-feature sequence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002).</S> | Reference Offset:  ['48','57'] | Reference Text:  <S sid = 48 ssid = >The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.</S><S sid = 57 ssid = >In the meantime, NE-chunk tag ti is structural and consists of three parts: Obviously, there exist some constraints between ti −1 and ti on the boundary and entity categories, as shown in Table 1, where &quot;valid&quot; / &quot;invalid&quot; means the tag sequence ti−1ti is valid / invalid while &quot;valid on&quot; means ti−1ti is valid with an additional condition ECi −1 = ECi .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002).</S> | Reference Offset:  ['61','95'] | Reference Text:  <S sid = 61 ssid = >In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features.</S><S sid = 95 ssid = >For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3.</S> | Reference Offset:  ['30','101'] | Reference Text:  <S sid = 30 ssid = >Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence.</S><S sid = 101 ssid = >The result is shown in Figure 2 for MUC-7 NE task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002).</S> | Reference Offset:  ['85','88'] | Reference Text:  <S sid = 85 ssid = >Initially, we also consider part-of-speech (POS) sub-feature.</S><S sid = 88 ssid = >Therefore, POS is discarded.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1060.txt | Citing Article:  W03-1307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002).</S> | Reference Offset:  ['9','83'] | Reference Text:  <S sid = 9 ssid = >Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.</S><S sid = 83 ssid = >During decoding, the NEs already recognized from the document are stored in a list.</S> | Discourse Facet:  NA | Annotator: Automatic


