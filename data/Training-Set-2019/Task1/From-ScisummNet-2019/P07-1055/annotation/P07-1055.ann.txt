Citance Number: 1 | Reference Article:  P07-1055.txt | Citing Article:  D12-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the best-performing model that fuses HCRF-Coarse and the supervised model (McDonald et al2007) by interpolation.</S> | Reference Offset:  ['32','69'] | Reference Text:  <S sid = 32 ssid = >The current work means any model of fine-to-coarse analysis should differs from that in Pang and Lee through the use of account for this. a single joint structured model for both sentence and In Section 2 we describe a simple structured document level analysis. model that jointly learns and infers sentiment on dif- Many problems in natural language processing ferent levels of granularity.</S><S sid = 69 ssid = >This is achieved by using k-best Viterbi techniques to return the k-best global labelings for each document label in line 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1055.txt | Citing Article:  C10-2057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald et al (2007) also dealt with sentiment analysis, via the global joint-structural approach.</S> | Reference Offset:  ['18','21'] | Reference Text:  <S sid = 18 ssid = >The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity.</S><S sid = 21 ssid = >In this domain, hard’s parsing (Taskar et al., 2004; McDonald et al., 2005), sentiment can only be determined in context (i.e., machine translation (Liang et al., 2006) and summahard to assemble versus a hard workout).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1055.txt | Citing Article:  P09-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, McDonald et al (2007) have investigated a model for jointly performing sentence and document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited.</S> | Reference Offset:  ['48','110'] | Reference Text:  <S sid = 48 ssid = >Figure 1 presents a model for jointly classifying the sentiment of both the sentences and the document.</S><S sid = 110 ssid = >Sentence level analysis is dependent on neighbouring sentences as well as the paragraph level analysis, and the paragraph analysis is dependent on each of the sentences within it, the neighbouring paragraphs, and the document level analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1055.txt | Citing Article:  N10-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The data used in our initial English-only experiments were a set of 554 consumer reviews described in (McDonald et al, 2007).</S> | Reference Offset:  ['120','133'] | Reference Text:  <S sid = 120 ssid = >Of the original 600 reviews that were gathered, we discarded duplicate reviews, reviews with insufficient text, and spam.</S><S sid = 133 ssid = >The model described in Section 2 will be called Joint-Structured.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1055.txt | Citing Article:  D10-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document and sentence-level classification accuracy (McDonald et al, 2007).</S> | Reference Offset:  ['7','69'] | Reference Text:  <S sid = 7 ssid = >Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker level in debates (Thomas et al., 2006).</S><S sid = 69 ssid = >This is achieved by using k-best Viterbi techniques to return the k-best global labelings for each document label in line 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1055.txt | Citing Article:  D10-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Alternative approaches include explicitly ac counting for this structure by treating subjective sentence extraction as a sequence-labeling problem, such as in McDonald et al (2007).</S> | Reference Offset:  ['34','41'] | Reference Text:  <S sid = 34 ssid = >This includes parsing and relaanalysis to a sequential classification problem us- tion extraction (Miller et al., 2000), entity labeling ing constrained Viterbi inference.</S><S sid = 41 ssid = >We then discuss the feaants (Taskar et al., 2003; Tsochantaridis et al., 2004; ture space and give an algorithm for learning the paMcDonald et al., 2005; Daum´e III et al., 2006). rameters based on large-margin structured learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1055.txt | Citing Article:  D10-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald et al (2007) propose a model which jointly identifies global polarity as well as paragraph and sentence-level polarity, all of which are observed in training data.</S> | Reference Offset:  ['84','144'] | Reference Text:  <S sid = 84 ssid = >Only features observed in the training data were considered.</S><S sid = 144 ssid = >This inconsistency may be a result of the model overfitting on the small set of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1055.txt | Citing Article:  D10-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While our approach uses a similar hierarchy, McDonald et al (2007) is concerned with recovering the labels at all levels, whereas in this work we are interested in using latent document content structure as a means to benefit task predictions.</S> | Reference Offset:  ['108','161'] | Reference Text:  <S sid = 108 ssid = >For example, longer documents might benefit from an analysis on the paragraph level as well as the sentence and document levels.</S><S sid = 161 ssid = >The first uses the Sentence- this work (Liang et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1055.txt | Citing Article:  P10-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al, 2006), and employing document subcomponent information (McDonald et al, 2007).</S> | Reference Offset:  ['7','41'] | Reference Text:  <S sid = 7 ssid = >Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker level in debates (Thomas et al., 2006).</S><S sid = 41 ssid = >We then discuss the feaants (Taskar et al., 2003; Tsochantaridis et al., 2004; ture space and give an algorithm for learning the paMcDonald et al., 2005; Daum´e III et al., 2006). rameters based on large-margin structured learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1055.txt | Citing Article:  P09-2079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >NGram Back-off Features: Similar to McDonald et al (2007), we utilize backed-off versions of lexical bi grams and trigrams, where all possible combinations of the words in the ngram are replaced by their POS tags, creating features such as w j POS k, POS j w k, POS j POS k for each lexical bigram and similarly for trigrams.</S> | Reference Offset:  ['80','121'] | Reference Text:  <S sid = 80 ssid = >For example, if the sentence label set is Y(s) = {subj, obj} and the document set is Y(d) = {pos, neg}, then the system might contain the following feature, { 1 if p E P(si) and y!</S><S sid = 121 ssid = >All reviews were labeled by online customers as having a positive or negative polarity on the document level, i.e., Y(d) _ {pos, neg}.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1055.txt | Citing Article:  P09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most recently, McDonald et al (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.</S> | Reference Offset:  ['1','167'] | Reference Text:  <S sid = 1 ssid = >In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.</S><S sid = 167 ssid = >In both cases there structured model that learns to predict sentiment on different levels of granularity for a text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1055.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','171'] | Reference Text:  <S sid = 53 ssid = >The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in a document and inference was solved using a min-cut algorithm.</S><S sid = 171 ssid = >Furthermore, extensions to the sentence-document model were discussed and it was argued that a nested hierarchical structure would be beneficial since it would allow for efficient inference algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1055.txt | Citing Article:  C10-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007).</S> | Reference Offset:  ['6','21'] | Reference Text:  <S sid = 6 ssid = >Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006).</S><S sid = 21 ssid = >In this domain, hard’s parsing (Taskar et al., 2004; McDonald et al., 2005), sentiment can only be determined in context (i.e., machine translation (Liang et al., 2006) and summahard to assemble versus a hard workout).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1055.txt | Citing Article:  P11-2100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald et al (2007) later showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels.</S> | Reference Offset:  ['0','17'] | Reference Text:  <S sid = 0 ssid = >Structured Models for Fine-to-Coarse Sentiment Analysis</S><S sid = 17 ssid = >We call the problem of identifying the sentiment of the document and of all its subcomponents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1055.txt | Citing Article:  P11-2100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald et al (2007) introduced a fully supervised model in which predictions of coarse-grained (document) and fine-grained (sentence) sentiment are learned and inferred jointly.</S> | Reference Offset:  ['0','17'] | Reference Text:  <S sid = 0 ssid = >Structured Models for Fine-to-Coarse Sentiment Analysis</S><S sid = 17 ssid = >We call the problem of identifying the sentiment of the document and of all its subcomponents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1055.txt | Citing Article:  C08-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >But even in such approaches, McDonald et al (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text.</S> | Reference Offset:  ['5','147'] | Reference Text:  <S sid = 5 ssid = >Extracting sentiment from text is a challenging problem with applications throughout Natural Language Processing and Information Retrieval.</S><S sid = 147 ssid = >This suggests that the Joint- use the document information to classify sentences, Structured model might be relying too much on use the sentence information to classify documents, the sentence level sentiment features – in order to and repeat until convergence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1055.txt | Citing Article:  P10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al, 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006).</S> | Reference Offset:  ['7','30'] | Reference Text:  <S sid = 7 ssid = >Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker level in debates (Thomas et al., 2006).</S><S sid = 30 ssid = >The top subjective sentences are cascaded approach would also be insufficient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1055.txt | Citing Article:  W12-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald (McDonald et al 2007) has reported some success mixing fine and course labeling in sentiment analysis.</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Structured Models for Fine-to-Coarse Sentiment Analysis</S><S sid = 21 ssid = >In this domain, hard’s parsing (Taskar et al., 2004; McDonald et al., 2005), sentiment can only be determined in context (i.e., machine translation (Liang et al., 2006) and summahard to assemble versus a hard workout).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1055.txt | Citing Article:  D08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most recently, McDonald et al (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.</S> | Reference Offset:  ['1','167'] | Reference Text:  <S sid = 1 ssid = >In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.</S><S sid = 167 ssid = >In both cases there structured model that learns to predict sentiment on different levels of granularity for a text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1055.txt | Citing Article:  C10-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, with CRFs, Zhao et al (2008) and McDonald et al (2007) performed sentiment classification in sentence and document level.</S> | Reference Offset:  ['7','41'] | Reference Text:  <S sid = 7 ssid = >Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker level in debates (Thomas et al., 2006).</S><S sid = 41 ssid = >We then discuss the feaants (Taskar et al., 2003; Tsochantaridis et al., 2004; ture space and give an algorithm for learning the paMcDonald et al., 2005; Daum´e III et al., 2006). rameters based on large-margin structured learning.</S> | Discourse Facet:  NA | Annotator: Automatic


