Citance Number: 1 | Reference Article:  P95-1037.txt | Citing Article:  W10-1409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The conversion uses head propagation rules to find the head on the right-hand side of the CFG rules, first proposed for English in (Magerman, 1995).</S> | Reference Offset:  ['11','105'] | Reference Text:  <S sid = 11 ssid = >Since most natural language rules are not absolute, the disambiguation criteria discovered in this work are never applied deterministically.</S><S sid = 105 ssid = >The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P95-1037.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank, also modified the treebank to contain different labels for standard and for base noun phrases.</S> | Reference Offset:  ['136','148'] | Reference Text:  <S sid = 136 ssid = >The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels.</S><S sid = 148 ssid = >The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels.2 The WSJ portion of the Penn Treebank is divided into 25 sections, numbered 00 - 24.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P95-1037.txt | Citing Article:  P09-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies.</S> | Reference Offset:  ['6','95'] | Reference Text:  <S sid = 6 ssid = >Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents.</S><S sid = 95 ssid = >A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P95-1037.txt | Citing Article:  W01-0521.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) dier in their details but are based on similar features.</S> | Reference Offset:  ['29','120'] | Reference Text:  <S sid = 29 ssid = >The grammarian is accomplishing two critical tasks: identifying the features which are relevant to each decision, and deciding which choice to select based on the values of the relevant features.</S><S sid = 120 ssid = >The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P95-1037.txt | Citing Article:  C10-2013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >FTB-UC-DEP is a dependency tree bank derived from FTB-UC using the classic technique of head propagation rules, first proposed for English by Magerman (1995).</S> | Reference Offset:  ['13','118'] | Reference Text:  <S sid = 13 ssid = >These probabilities are estimated using statistical decision tree models.</S><S sid = 118 ssid = >After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in (Magerman, 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P95-1037.txt | Citing Article:  W02-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al.</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >Statistical Decision-Tree Models For Parsing</S><S sid = 13 ssid = >These probabilities are estimated using statistical decision tree models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P95-1037.txt | Citing Article:  W02-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a similar, but more limited, strategy to the one used by Magerman (1995).</S> | Reference Offset:  ['85','156'] | Reference Text:  <S sid = 85 ssid = >For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994).</S><S sid = 156 ssid = >In fact, no information other than the words is used from the test corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P95-1037.txt | Citing Article:  P05-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In both cases, we report PARSEVAL labeled bracket scores (Magerman, 1995), with the brackets labeled by syntactic categories but not grammatical functions.</S> | Reference Offset:  ['95','160'] | Reference Text:  <S sid = 95 ssid = >A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label.</S><S sid = 160 ssid = >Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P95-1037.txt | Citing Article:  N03-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We apply canonical lexical head projection rules (Magerman, 1995) in order to lexicalize syntactic trees.</S> | Reference Offset:  ['86','105'] | Reference Text:  <S sid = 86 ssid = >An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees.</S><S sid = 105 ssid = >The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P95-1037.txt | Citing Article:  W03-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall (Magerman, 1995).</S> | Reference Offset:  ['5','160'] | Reference Text:  <S sid = 5 ssid = >Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.</S><S sid = 160 ssid = >Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P95-1037.txt | Citing Article:  P10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These features are also computed for the head of the phrase, determined using a set of head finding rules in the style of Magerman (1995) adapted to TiGer.</S> | Reference Offset:  ['33','105'] | Reference Text:  <S sid = 33 ssid = >This probability P(flh) is determined by asking a sequence of questions qi q2...q„ about the context, where the ith question asked is uniquely determined by the answers to the i —1 previous questions.</S><S sid = 105 ssid = >The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P95-1037.txt | Citing Article:  W05-1514.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The head word is identified by using the head-percolation table (Magerman, 1995).</S> | Reference Offset:  ['100','105'] | Reference Text:  <S sid = 100 ssid = >The word feature can take on any value of any word.</S><S sid = 105 ssid = >The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P95-1037.txt | Citing Article:  W09-0103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, statistical parsers from Magerman (1995) on use features based on head-dependent relationships.</S> | Reference Offset:  ['10','91'] | Reference Text:  <S sid = 10 ssid = >The candidate disambiguators are the words in the sentence, relationships among the words, and relationships among constituents already constructed in the parsing process.</S><S sid = 91 ssid = >For more discussion of the use of binary decision-tree questions, see (Magerman, 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P95-1037.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The head word is identified by using the head percolation table (Magerman, 1995).</S> | Reference Offset:  ['100','105'] | Reference Text:  <S sid = 100 ssid = >The word feature can take on any value of any word.</S><S sid = 105 ssid = >The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P95-1037.txt | Citing Article:  P01-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For each possible constituent in a parse tree, rules first described in (Magerman, 1995) and (Jelinek et al, 1994) identify the head-child and propagate the head-word to its parent.</S> | Reference Offset:  ['89','103'] | Reference Text:  <S sid = 89 ssid = >These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992).</S><S sid = 103 ssid = >The extension can take on any of the following five values: right - the node is the first child of a constituent; left - the node is the last child of a constituent; up - the node is neither the first nor the last child of a constituent; unary - the node is a child of a unary constituent; root - the node is the root of the tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P95-1037.txt | Citing Article:  D10-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets.</S> | Reference Offset:  ['5','135'] | Reference Text:  <S sid = 5 ssid = >Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.</S><S sid = 135 ssid = >The training and test sentences were annotated by the University of Lancaster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P95-1037.txt | Citing Article:  P05-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG).</S> | Reference Offset:  ['21','138'] | Reference Text:  <S sid = 21 ssid = >In a problem like parsing, where long-distance lexical information is crucial to disambiguate interpretations accurately, local models like probabilistic context-free grammars are inadequate.</S><S sid = 138 ssid = >The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based, unification-style probabilistic context-free grammar for parsing this domain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P95-1037.txt | Citing Article:  N09-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn tree bank using head percolation rules used in (Magerman, 1995), which do not contain any order information.</S> | Reference Offset:  ['155','156'] | Reference Text:  <S sid = 155 ssid = >No information about the legal tags for a word are extracted from the test corpus.</S><S sid = 156 ssid = >In fact, no information other than the words is used from the test corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P95-1037.txt | Citing Article:  P00-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','169'] | Reference Text:  <S sid = 53 ssid = >Using this definition, an n-gram model can be represented by a decision-tree model with n — 1 questions.</S><S sid = 169 ssid = >The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P95-1037.txt | Citing Article:  P00-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','169'] | Reference Text:  <S sid = 53 ssid = >Using this definition, an n-gram model can be represented by a decision-tree model with n — 1 questions.</S><S sid = 169 ssid = >The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


