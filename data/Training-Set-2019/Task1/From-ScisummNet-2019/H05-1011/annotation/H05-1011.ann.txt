Citance Number: 1 | Reference Article:  H05-1011.txt | Citing Article:  W07-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Moore, 2005) has proposed an approach which does not impose any restrictions on the form of model features.</S> | Reference Offset:  ['20','104'] | Reference Text:  <S sid = 20 ssid = >In this paper, we take a different approach to word alignment, based on discriminative training of a weighted linear combination of a small number of features.</S><S sid = 104 ssid = >Weinitially explored limiting the number of associations considered for each word type simply as an ef ficiency heuristic, but we were surprised to discover that the most extreme form of such pruning actually reduced alignment error rate over any less restrictive form or not pruning on this basis at all.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  H05-1011.txt | Citing Article:  W07-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >LLR and CLP are the word association statistics used in Moore's work (Moore, 2005).</S> | Reference Offset:  ['39','86'] | Reference Text:  <S sid = 39 ssid = >We compute LLR scores using the following formula presented by Moore (2004): LLR(f, e) = ? f??{f,?f} ? e??{e,?e} C(f?, e?)</S><S sid = 86 ssid = >Adding a link for a new pair of words can affect the nonmonotonicity scores, the one-to-many score, and the unlinked word score differently, depending on 2The conditional link probabilities used in the current work are those used in Method 4 of the earlier work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  H05-1011.txt | Citing Article:  W07-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A variation of this feature was used by (Moore, 2005) in his paper.</S> | Reference Offset:  ['3','113'] | Reference Text:  <S sid = 3 ssid = >In this paper, we demonstrate a discriminative approachto training simple word alignment mod els that are comparable in accuracy tothe more complex generative models nor mally used.</S><S sid = 113 ssid = >We iterate this pro cedure until a local optimum is found.Next, we used a fixed weight of 1.0 for the wordassociation feature, which we expect to be most im portant feature in the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  H05-1011.txt | Citing Article:  C10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In fact, LLR can still be used for extracting positive associations by filtering in a pre-processing step words with possibly negative associations (Moore, 2005).</S> | Reference Offset:  ['46','88'] | Reference Text:  <S sid = 46 ssid = >The LLR score for a pair of words is high if the words have either a strong positive association or a strong negative association.</S><S sid = 88 ssid = >Never theless, we have found a beam-search procedure that seems highly effective in finding good alignments when used with these models.For each sentence pair, we create a list of associa tion types and their corresponding scores, consisting of the associations for which we have determined ascore and for which the words involved in the asso ciation type occur in the sentence pair.3 We sort the resulting list of association types from best to worst according to their scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  H05-1011.txt | Citing Article:  C10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, to ensure that only positive association counts, we set the probability to zero if p (x, y) < p (x) p (y), where the probabilities are estimated using relative frequencies (Moore, 2005).</S> | Reference Offset:  ['46','143'] | Reference Text:  <S sid = 46 ssid = >The LLR score for a pair of words is high if the words have either a strong positive association or a strong negative association.</S><S sid = 143 ssid = >We collected link counts and co-occurrence counts from these alignments for estimating conditional link probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  H05-1011.txt | Citing Article:  P06-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We take advantage of this, building on our existing framework (Moore, 2005), to substantially reduce the alignment error rate (AER) we previously reported, given the same training and test data.</S> | Reference Offset:  ['0','138'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Framework For Bilingual Word Alignment</S><S sid = 138 ssid = >We used one ofthese subsets as a development set for parameter op timization, and held out the other for a final test set.We report the performance of our alignment mod els in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003): recall = |A ? S| |S| precision = |A ? P | |A| AER = 1?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  H05-1011.txt | Citing Article:  P06-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As in our previous work (Moore, 2005), we train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them.</S> | Reference Offset:  ['20','168'] | Reference Text:  <S sid = 20 ssid = >In this paper, we take a different approach to word alignment, based on discriminative training of a weighted linear combination of a small number of features.</S><S sid = 168 ssid = >When the first version of this paper was submitted for review, we could honestly state, ?We are not aware of any previous work on discriminative word alignment models.?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  H05-1011.txt | Citing Article:  C08-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Firstly, as denoted by Moore (2005), one needs to tune numerous parameters in order to optimize the results for a particular alignment task, which can be very time consuming.</S> | Reference Offset:  ['13','30'] | Reference Text:  <S sid = 13 ssid = >Since evaluating each combina tion of parameter values in this way can take hours to days on a large training corpus, it seems safe to say that these parameters are rarely if ever truly jointly optimized for a particular alignment task.</S><S sid = 30 ssid = >In practice, however, effective discriminative models for word alignment require only a few parameters, which can be optimized on a set of annotated sentence pairs comparable in size to what is needed to tune the free parameters used in the generative approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  H05-1011.txt | Citing Article:  W06-1672.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Full details are provided in the reference.what other links are present in the alignment.</S><S sid = 176 ssid = >After many years using the same small set of alignment models, we now have an easy way to experiment with a wide variety of knowledge sources to improve word-alignment accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  H05-1011.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >d is an absolute discount parameter as in (Moore, 2005).</S> | Reference Offset:  ['77','144'] | Reference Text:  <S sid = 77 ssid = >We find itbetter, however, to adjust these probabilities by sub tracting a small fixed discount from the link count: LPd(f, e) = links 1 (f, e)?</S><S sid = 144 ssid = >We trained CLP-based models from these counts for a range of values for the discount used in the conditional link probability estimation, finding a value of 0.4 to be a roughly optimal value of the discount parameter for the development set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  H05-1011.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Moore, 2005) uses an averaged perceptron for training with a customized beam search.</S> | Reference Offset:  ['26','105'] | Reference Text:  <S sid = 26 ssid = >We optimize the model weights using a modified version of averaged perceptron learning as describedby Collins (2002).</S><S sid = 105 ssid = >We optimize the feature weights using a modified version of averaged perceptron learning as described by Collins (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  H05-1011.txt | Citing Article:  P10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >2) Conditional link probability (Moore, 2005).</S> | Reference Offset:  ['73','143'] | Reference Text:  <S sid = 73 ssid = >2.2 The Conditional-Link-Probability-Based.</S><S sid = 143 ssid = >We collected link counts and co-occurrence counts from these alignments for estimating conditional link probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  H05-1011.txt | Citing Article:  W10-2917.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Moore (2005) uses statistics like log-likelihood-ratio and conditional likelihood-probability to measure word associations; Liu et al (2005) and Taskar et al (2005) use results from IBM Model 3 and Model 4, respectively.</S> | Reference Offset:  ['37','171'] | Reference Text:  <S sid = 37 ssid = >In our first model, we use a log-likelihood-ratio (LLR) statistic as our measure of word association.</S><S sid = 171 ssid = >Liu et al (2005) also develop a log-linear model,based on IBM Model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  H05-1011.txt | Citing Article:  W10-2917.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >d is a discounting constant which is set to 0.4 following Moore (2005).</S> | Reference Offset:  ['39','114'] | Reference Text:  <S sid = 39 ssid = >We compute LLR scores using the following formula presented by Moore (2004): LLR(f, e) = ? f??{f,?f} ? e??{e,?e} C(f?, e?)</S><S sid = 114 ssid = >Allowing all weights tovary allows many equivalent sets of weights that dif fer only by a constant scale factor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  H05-1011.txt | Citing Article:  W10-2917.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moore (2005) proposes a similar framework, but with more features and a different search method.</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Framework For Bilingual Word Alignment</S><S sid = 20 ssid = >In this paper, we take a different approach to word alignment, based on discriminative training of a weighted linear combination of a small number of features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  H05-1011.txt | Citing Article:  I08-4001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to obtain the word alignment satisfying the ITG constraint, Wu (1997) propose a DPalgorithm, and we (Chao and Li, 2007) have transferred the constraint to four simple position judgment procedures in an explicit way, so that we can incorporate the ITG constraint as a feature into a log linear word alignment model (Moore, 2005).</S> | Reference Offset:  ['57','171'] | Reference Text:  <S sid = 57 ssid = >We sort the word pairs in the alignment, first by source word position, and then by target word position.</S><S sid = 171 ssid = >Liu et al (2005) also develop a log-linear model,based on IBM Model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  H05-1011.txt | Citing Article:  P14-1138.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These models are roughly clustered into two groups: generative models, such as those pro posed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006).</S> | Reference Offset:  ['7','171'] | Reference Text:  <S sid = 7 ssid = >The standard approach to word alignment makes use of various com binations of five generative models developed at IBM by Brown et al (1993), sometimes augmented by an HMM-based model or Och and Ney?s ?Model 6?</S><S sid = 171 ssid = >Liu et al (2005) also develop a log-linear model,based on IBM Model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  H05-1011.txt | Citing Article:  P10-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story.</S> | Reference Offset:  ['15','19'] | Reference Text:  <S sid = 15 ssid = >Generative models require a generative ?story?</S><S sid = 19 ssid = >To model this explicitly, they had to come up with a different generative story.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  H05-1011.txt | Citing Article:  P11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features.</S> | Reference Offset:  ['60','175'] | Reference Text:  <S sid = 60 ssid = >For example, suppose we have the sorted alignment ((1,1)(2,4)(2,5)(3,2)(5,6)).</S><S sid = 175 ssid = >The results of our work and other recent efforts on discriminatively trained alignment models showthat results comparable to or better than those ob tained with the IBM models are possible within aframework that makes it easy to add arbitrary ad ditional features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  H05-1011.txt | Citing Article:  P06-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Full details are provided in the reference.what other links are present in the alignment.</S><S sid = 176 ssid = >After many years using the same small set of alignment models, we now have an easy way to experiment with a wide variety of knowledge sources to improve word-alignment accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


