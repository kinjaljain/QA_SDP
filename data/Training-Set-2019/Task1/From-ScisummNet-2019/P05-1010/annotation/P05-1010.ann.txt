Citance Number: 1 | Reference Article:  P05-1010.txt | Citing Article:  W05-1512.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours.</S> | Reference Offset:  ['18','96'] | Reference Text:  <S sid = 18 ssid = >Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.</S><S sid = 96 ssid = >It is in contrast with our approach where (approximated) posterior probability is optimized.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1010.txt | Citing Article:  P14-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The first method simply limits the number of candidate parse trees compared in Eq.</S><S sid = 144 ssid = >It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1010.txt | Citing Article:  N10-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These scores are the same as the variational rule scores of Matsuzaki et al (2005).</S> | Reference Offset:  ['88','117'] | Reference Text:  <S sid = 88 ssid = >We use to denote the rule probability of rule and to denote the probability with which is generated as a root node.</S><S sid = 117 ssid = >The parsing performances were measured using F scores of the parse trees that were obtained by re-ranking of 1000-best parses by a PCFG.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The first method simply limits the number of candidate parse trees compared in Eq.</S><S sid = 144 ssid = >It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.</S> | Reference Offset:  ['10','20'] | Reference Text:  <S sid = 10 ssid = >This paper defines a generative model of parse trees that we call PCFG with latent annotations (PCFG-LA).</S><S sid = 20 ssid = >PCFG-LA is a generative probabilistic model of parse trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The first method simply limits the number of candidate parse trees compared in Eq.</S><S sid = 144 ssid = >It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],.</S> | Reference Offset:  ['48','141'] | Reference Text:  <S sid = 48 ssid = >In the definition below, denotes the non-terminal label of the-th node.</S><S sid = 141 ssid = >In contrast, our method induces all parameters automatically, except that manually written head-rules are used in binarization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005).</S> | Reference Offset:  ['31','125'] | Reference Text:  <S sid = 31 ssid = >The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992).</S><S sid = 125 ssid = >A model created using CENTER-PARENT with was used throughout this experiment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data.</S> | Reference Offset:  ['102','113'] | Reference Text:  <S sid = 102 ssid = >We used sections 2 through 20 of the Penn WSJ corpus as training data and section 21 as heldout data.</S><S sid = 113 ssid = >We obtained an observable grammar for each model by reading off grammar rules from the binarized training trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find.</S> | Reference Offset:  ['4','15'] | Reference Text:  <S sid = 4 ssid = >Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.</S><S sid = 15 ssid = >Because exact inference with a PCFG-LA, i.e., selection of the most probable parse, is NP-hard, we are forced to use some approximation of it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The first method simply limits the number of candidate parse trees compared in Eq.</S><S sid = 144 ssid = >It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank.</S> | Reference Offset:  ['106','114'] | Reference Text:  <S sid = 106 ssid = >To see the degree of dependency of trained models on initializations, four instances of the same model were trained with different initial values of parameters.3 The model used in this experiment was created by CENTER-PARENT binarization and was set to 16.</S><S sid = 114 ssid = >For each binarization method, PCFG-LA models with different numbers of latent annotation symbols, , and , were trained.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing.</S> | Reference Offset:  ['115','124'] | Reference Text:  <S sid = 115 ssid = >The relationships between the number of parameters in the models and their parsing performances are shown in Figure 7.</S><S sid = 124 ssid = >The relationships between the average parse time and parsing performance using the three parsing methods described in Section 3 are shown in Figure 8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1010.txt | Citing Article:  D08-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc.</S> | Reference Offset:  ['11','126'] | Reference Text:  <S sid = 11 ssid = >This model is an extension of PCFG models in which non-terminal symbols are annotated with latent variables.</S><S sid = 126 ssid = >The data points were made by varying configurable parameters of each method, which control the number of candidate parses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1010.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The first method simply limits the number of candidate parse trees compared in Eq.</S><S sid = 144 ssid = >It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1010.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The first method simply limits the number of candidate parse trees compared in Eq.</S><S sid = 144 ssid = >It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1010.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005).</S> | Reference Offset:  ['31','93'] | Reference Text:  <S sid = 31 ssid = >The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992).</S><S sid = 93 ssid = >Once we have computed and , the parse tree that maximizes is found using a Viterbi algorithm, as in PCFG parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1010.txt | Citing Article:  P12-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions.</S> | Reference Offset:  ['21','53'] | Reference Text:  <S sid = 21 ssid = >In this model, an observed parse tree is considered as an incomplete data, and the correplete data) and observed tree (incomplete data). sponding complete data is a tree with latent annotations.</S><S sid = 53 ssid = >If node has a right sibling , let be the mother node of.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1010.txt | Citing Article:  W12-1904.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework.</S> | Reference Offset:  ['116','133'] | Reference Text:  <S sid = 116 ssid = >Note that models created using different binarization methods have different numbers of parameters for the same .</S><S sid = 133 ssid = >The different lines for the second and the third methods correspond to different values of .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1010.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005).</S> | Reference Offset:  ['18','122'] | Reference Text:  <S sid = 18 ssid = >Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.</S><S sid = 122 ssid = >However, both the memory size and the training time are more than linear in , and the training time for the largest ( ) models was about 15 hours for the models created using CENTER-PARENT, CENTER-HEAD, and LEFT and about 20 hours for the model created using RIGHT.</S> | Discourse Facet:  NA | Annotator: Automatic


