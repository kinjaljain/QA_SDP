Citance Number: 1 | Reference Article:  D11-1142.txt | Citing Article:  D12-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reverb (Fader et al 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations.</S> | Reference Offset:  ['59','62'] | Reference Text:  <S sid = 59 ssid = >Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor.</S><S sid = 62 ssid = >Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D11-1142.txt | Citing Article:  P14-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Literature on automatic relation discovery (Fader et al, 2011) has shown that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations.</S> | Reference Offset:  ['8','107'] | Reference Text:  <S sid = 8 ssid = >Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007).</S><S sid = 107 ssid = >The results in Table 3 are similar to Banko and Etzioni’s findings that a set of eight POS patterns cover a large fraction of binary verbal relation phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D11-1142.txt | Citing Article:  P13-2051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To this end, we used a random sample from the large scale web-based ReVerb corpus (Fader et al, 2011), comprising tuple extractions of predicate templates with their argument instantiations.</S> | Reference Offset:  ['108','109'] | Reference Text:  <S sid = 108 ssid = >However, their analysis was based on a set of sentences known to contain either a company acquisition or birthplace relationship, while our results are on a random sample of Web sentences.</S><S sid = 109 ssid = >We applied Banko and Etzioni’s verbal patterns to our random sample of 300 Web sentences, and found that they cover approximately 69% of the relation phrases in the corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D11-1142.txt | Citing Article:  P12-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Two example systems implementing this paradigm are TEXTRUN NER (Yates et al, 2007) and REVERB (Fader et al, 2011).</S> | Reference Offset:  ['11','12'] | Reference Text:  <S sid = 11 ssid = >(Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009).</S><S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D11-1142.txt | Citing Article:  P12-3014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The propositions are usually produced by an extraction method, such as TextRunner (Banko et al, 2007) or ReVerb (Fader et al, 2011).</S> | Reference Offset:  ['11','12'] | Reference Text:  <S sid = 11 ssid = >(Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009).</S><S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D11-1142.txt | Citing Article:  P12-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al (2011)) automatically extracted from the ClueWeb09 web crawl, where each extraction comprises a predicate and two arguments.</S> | Reference Offset:  ['197','217'] | Reference Text:  <S sid = 197 ssid = >Table 6 summarizes the correct extractions that were extracted by other systems and were not extracted by REVERB.</S><S sid = 217 ssid = >For instance, since many of REVERB’s errors are due to incorrect arguments, improved methods for argument extraction are in order.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D11-1142.txt | Citing Article:  N12-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['68','219'] | Reference Text:  <S sid = 68 ssid = >For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010).</S><S sid = 219 ssid = >This research was supported in part by NSF grant IIS-0803481, ONR grant N00014-081-0431, and DARPA contract FA8750-09-C-0179, and carried out at the University of Washington’s Turing Center.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D11-1142.txt | Citing Article:  P13-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al, 2011), a large scale publicly available web based open extractions data set, containing about 15 million unique template extractions.</S> | Reference Offset:  ['125','182'] | Reference Text:  <S sid = 125 ssid = >We set D to be the set of all relation phrases that take at least k distinct argument pairs in the set of extractions.</S><S sid = 182 ssid = >Incoherent extractions were a large fraction of the errors made by previous systems, accounting for approximately 13% of TEXTRUNNER’s extractions, 15% of WOEpOs’s, and 30% of WOEparse’s.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D11-1142.txt | Citing Article:  P14-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In terms of the latter, Cai and Yates (2013) and Berant et al (2013) applied pattern matching and relation intersection between Freebase relations and predicate argument triples from the ReVerb OpenIE system (Fader et al, 2011).</S> | Reference Offset:  ['11','12'] | Reference Text:  <S sid = 11 ssid = >(Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009).</S><S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D11-1142.txt | Citing Article:  D12-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare OLLIE to two state-of-the-art Open IE systems: (1) REVERB (Fader et al2011), which uses shallow syntactic processing to identify relation phrases that begin with a verb and occur between the argument phrases; 2 (2) WOEparse (Wuand Weld, 2010), which uses bootstrapping from entries in Wikipedia info-boxes to learn extraction pat terns in dependency parses.</S> | Reference Offset:  ['2','104'] | Reference Text:  <S sid = 2 ssid = >This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions.</S><S sid = 104 ssid = >Previous work has shown that dependency paths do indeed boost the recall of relation extraction systems (Wu and Weld, 2010; Mintz et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D11-1142.txt | Citing Article:  P13-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We say a schema is a textual schema if it has been extracted from free text, such as the Nell (Carlson et al, 2010) and ReVerb (Fader et al, 2011) extracted databases.</S> | Reference Offset:  ['12','197'] | Reference Text:  <S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S><S sid = 197 ssid = >Table 6 summarizes the correct extractions that were extracted by other systems and were not extracted by REVERB.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D11-1142.txt | Citing Article:  P13-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MATCHER uses an API for the ReVerb Open IEsystem (Fader et al, 2011) to collect I (rT), for each rT.</S> | Reference Offset:  ['11','12'] | Reference Text:  <S sid = 11 ssid = >(Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009).</S><S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D11-1142.txt | Citing Article:  P12-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We obtained 155,409 positive instances from the English sentences using an off-the-shelf relation extraction system, ReVerb (Fader et al., 2011).</S> | Reference Offset:  ['8','148'] | Reference Text:  <S sid = 8 ssid = >Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007).</S><S sid = 148 ssid = >This process resulted in a set of 67, 562 positive instances, and 356,834 negative instances.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D11-1142.txt | Citing Article:  D12-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The REVERB extractor (Fader et al 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities.</S> | Reference Offset:  ['22','120'] | Reference Text:  <S sid = 22 ssid = >An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002).</S><S sid = 120 ssid = >Finally, REVERB is “relation first” rather than “arguments first”, which enables it to avoid a common error made by previous methods—confusing a noun in the relation phrase for an argument, e.g. the noun deal in made a deal with.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D11-1142.txt | Citing Article:  P14-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhang and Weld (2013) is based on REVERB (Fader et al, 2011), which uses a regular expression on part-of-speech tags to produce the extractions.</S> | Reference Offset:  ['11','88'] | Reference Text:  <S sid = 11 ssid = >(Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009).</S><S sid = 88 ssid = >For example, one could create a model of relations expressed in Figure 1: A simple part-of-speech-based regular expression reduces the number of incoherent extractions like was central torpedo and covers relations expressed via light verb constructions like gave a talk at.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D11-1142.txt | Citing Article:  E12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For convenience, we identify part-whole relations in Rule 12 based on the output produced by ReVerb (Fader et al 2011), an open information extraction system.</S> | Reference Offset:  ['0','12'] | Reference Text:  <S sid = 0 ssid = >Identifying Relations for Open Information Extraction</S><S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D11-1142.txt | Citing Article:  P14-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fader et al (2011) utilizes a confidence function.</S> | Reference Offset:  ['12','139'] | Reference Text:  <S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S><S sid = 139 ssid = >We trained the confidence function by manually labeling the extractions from a set of 1, 000 sentences from the Web and Wikipedia as correct or incorrect.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D11-1142.txt | Citing Article:  P12-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Fader et al (2011) found that this set covers 69% of their corpus).</S> | Reference Offset:  ['12','125'] | Reference Text:  <S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S><S sid = 125 ssid = >We set D to be the set of all relation phrases that take at least k distinct argument pairs in the set of extractions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D11-1142.txt | Citing Article:  P13-1158.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al, 2011).</S> | Reference Offset:  ['12','197'] | Reference Text:  <S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S><S sid = 197 ssid = >Table 6 summarizes the correct extractions that were extracted by other systems and were not extracted by REVERB.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D11-1142.txt | Citing Article:  P13-1158.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We performed an end-to-end evaluation against a database of 15 million facts automatically extracted from general web text (Fader et al, 2011).</S> | Reference Offset:  ['12','197'] | Reference Text:  <S sid = 12 ssid = >The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S><S sid = 197 ssid = >Table 6 summarizes the correct extractions that were extracted by other systems and were not extracted by REVERB.</S> | Discourse Facet:  NA | Annotator: Automatic


