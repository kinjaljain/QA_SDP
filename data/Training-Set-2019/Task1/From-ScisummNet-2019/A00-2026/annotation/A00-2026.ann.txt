Citance Number: 1 | Reference Article:  A00-2026.txt | Citing Article:  P01-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000).</S> | Reference Offset:  ['4','35'] | Reference Text:  <S sid = 4 ssid = >NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.</S><S sid = 35 ssid = >Its performance is intended to serve as a baseline result to the more sophisticated models discussed later.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A00-2026.txt | Citing Article:  P04-3009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase.</S> | Reference Offset:  ['4','39'] | Reference Text:  <S sid = 4 ssid = >NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.</S><S sid = 39 ssid = >The local and non-local information is integrated with use of features in a maximum entropy probability model, and a highly pruned search procedure attempts to find the best scoring word sequence according to the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A00-2026.txt | Citing Article:  W05-1601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning.</S> | Reference Offset:  ['0','113'] | Reference Text:  <S sid = 0 ssid = >Trainable Methods For Surface Natural Language Generation</S><S sid = 113 ssid = >NLG2 and NLG3 are also statistical learning approaches but generate from an actual semantic representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A00-2026.txt | Citing Article:  W05-1627.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs.</S> | Reference Offset:  ['23','116'] | Reference Text:  <S sid = 23 ssid = >All systems in this paper use attribute-value pairs as a semantic representation, which suffice as a representation for a limited domain like air travel.</S><S sid = 116 ssid = >In the air travel domain, the length of a phrase fragment to describe an attribute is usually only a few words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A00-2026.txt | Citing Article:  P06-1130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features.</S> | Reference Offset:  ['4','106'] | Reference Text:  <S sid = 4 ssid = >NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.</S><S sid = 106 ssid = >NLG2 and NLG3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with a given attribute and local context, whereas (Elhadad et al., 1997) uses a rule-based approach to decide the word choice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A00-2026.txt | Citing Article:  W10-1617.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input.</S> | Reference Offset:  ['29','99'] | Reference Text:  <S sid = 29 ssid = >All the trainable NLG systems in this paper assume the existence of a large corpus of phrases in which the values of interest have been replaced with their corresponding attributes, or in other words, a corpus of generation templates.</S><S sid = 99 ssid = >The NLG2 and NLG3 systems automatically attempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A00-2026.txt | Citing Article:  C08-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain.</S> | Reference Offset:  ['23','25'] | Reference Text:  <S sid = 23 ssid = >All systems in this paper use attribute-value pairs as a semantic representation, which suffice as a representation for a limited domain like air travel.</S><S sid = 25 ssid = >The goal, more specifically, is then to learn the optimal attribute ordering and lexical choice for the text to be generated from the attribute-value pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A00-2026.txt | Citing Article:  C02-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000).</S> | Reference Offset:  ['4','113'] | Reference Text:  <S sid = 4 ssid = >NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.</S><S sid = 113 ssid = >NLG2 and NLG3 are also statistical learning approaches but generate from an actual semantic representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A00-2026.txt | Citing Article:  P05-2026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000).</S> | Reference Offset:  ['6','19'] | Reference Text:  <S sid = 6 ssid = >We present experiments in which we generate phrases to describe flights in the air travel domain.</S><S sid = 19 ssid = >The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A00-2026.txt | Citing Article:  W11-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few.</S> | Reference Offset:  ['19','110'] | Reference Text:  <S sid = 19 ssid = >The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998).</S><S sid = 110 ssid = >Our approach differs from the corpus-based surface generation approaches of (Langkilde and Knight, 1998) and (Berger et al., 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


