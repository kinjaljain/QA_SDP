Citance Number: 1 | Reference Article:  P06-3002.txt | Citing Article:  N07-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','147'] | Reference Text:  <S sid = 54 ssid = >It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S><S sid = 147 ssid = >Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-3002.txt | Citing Article:  N07-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures.</S> | Reference Offset:  ['42','143'] | Reference Text:  <S sid = 42 ssid = >The resulting taggers are evaluated against outputs of supervised taggers for various languages.</S><S sid = 143 ssid = >We supported the claim of language-independence by validating the output of our system against supervised systems in three languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-3002.txt | Citing Article:  P10-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We train SVM Tool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections.</S> | Reference Offset:  ['41','91'] | Reference Text:  <S sid = 41 ssid = >Finally, we train a Viterbi tagger with this lexicon and augment it with an affix classifier for unknown words.</S><S sid = 91 ssid = >Unlike in supervised scenarios, our task is not to train a tagger model from a small corpus of hand-tagged data, but from our clusters of derived syntactic categories and a considerably large, yet unlabeled corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-3002.txt | Citing Article:  W07-0212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nevertheless, many of the practically useful spell checkers incorporate context information and the current analysis on SpellNet can be extended for such spell-checkers by conceptualizing a network of words that capture the word co-occurrence patterns (Biemann, 2006).</S> | Reference Offset:  ['25','53'] | Reference Text:  <S sid = 25 ssid = >The word’s global context is the sum of all its contexts.</S><S sid = 53 ssid = >For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-3002.txt | Citing Article:  P10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','147'] | Reference Text:  <S sid = 54 ssid = >It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S><S sid = 147 ssid = >Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-3002.txt | Citing Article:  D08-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous graph-theoretic work (Biemann, 2006) uses order 1 representations.</S> | Reference Offset:  ['35','50'] | Reference Text:  <S sid = 35 ssid = >This work constructs an unsupervised POS tagger from scratch.</S><S sid = 50 ssid = >(van Dongen, 2000; Biemann 2006), find the number of clusters automatically1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-3002.txt | Citing Article:  P07-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to test the argument above, and as an attempt to improve the results from the previous experiment, POS-tags were induced using Biemann's unsupervised POS-tagger (Biemann, 2006).</S> | Reference Offset:  ['4','35'] | Reference Text:  <S sid = 4 ssid = >Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.</S><S sid = 35 ssid = >This work constructs an unsupervised POS tagger from scratch.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-3002.txt | Citing Article:  W10-2901.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additionally, we used an unsupervised part-of-speech tagger (see (Biemann, 2006)) to tag the NEGRA corpus to be able to present a complete unsupervised parsing process relying on word strings only.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering</S><S sid = 1 ssid = >An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-3002.txt | Citing Article:  D12-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.</S> | Reference Offset:  ['20','37'] | Reference Text:  <S sid = 20 ssid = >Contexts in that sense are often restricted to the most frequent words.</S><S sid = 37 ssid = >In a first stage, we employ a clustering algorithm on distributional similarity, which groups a subset of the most frequent 10,000 words of a corpus into several hundred clusters (partitioning 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-3002.txt | Citing Article:  W09-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','147'] | Reference Text:  <S sid = 54 ssid = >It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S><S sid = 147 ssid = >Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-3002.txt | Citing Article:  W09-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','147'] | Reference Text:  <S sid = 54 ssid = >It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S><S sid = 147 ssid = >Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-3002.txt | Citing Article:  P11-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','147'] | Reference Text:  <S sid = 54 ssid = >It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S><S sid = 147 ssid = >Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-3002.txt | Citing Article:  D07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Perhaps due to the overly simplistic methods employed to compute morphological information, morphology has only been used as what Biemann (2006) called add-on's in existing POS induction algorithms, which remain primarily distributional in nature.</S> | Reference Offset:  ['21','28'] | Reference Text:  <S sid = 21 ssid = >The words used to describe syntactic contexts will be called feature words in the remainder.</S><S sid = 28 ssid = >An extension to this generic scheme is presented in (Clark, 2003), where morphological Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 7–12, Sydney, July 2006. c�2006 Association for Computational Linguistics information is used for determining the word class of rare words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-3002.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Biemann (2006) described a graph-based clustering methods for word classes.</S> | Reference Offset:  ['1','53'] | Reference Text:  <S sid = 1 ssid = >An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.</S><S sid = 53 ssid = >For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


