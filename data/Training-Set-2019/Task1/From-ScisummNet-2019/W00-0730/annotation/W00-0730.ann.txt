Citance Number: 1 | Reference Article:  W00-0730.txt | Citing Article:  N01-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto,2000a).</S> | Reference Offset:  ['40','50'] | Reference Text:  <S sid = 40 ssid = >We defined the certainty score as the number of votes for the class (tag) obtained through the pairwise voting.</S><S sid = 50 ssid = >We set the beam width N to 5 tentatively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W00-0730.txt | Citing Article:  N01-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5.</S> | Reference Offset:  ['1','48'] | Reference Text:  <S sid = 1 ssid = >In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification.</S><S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W00-0730.txt | Citing Article:  C10-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For SVM, we used the YAMCHA (Kudo and Matsumoto, 2000) sequence labeling system,1 which uses the TinySVM package for classification.</S> | Reference Offset:  ['30','51'] | Reference Text:  <S sid = 30 ssid = >Second approach is pairwise classification.</S><S sid = 51 ssid = >SVMs training is carried out with the Slight package, which is designed and optimized to handle large sparse feature vector and large numbers of training examples (Joachims, 2000; Joachims, 1999a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W00-0730.txt | Citing Article:  N04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For our experiments, we used TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and test software.</S> | Reference Offset:  ['48','53'] | Reference Text:  <S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S><S sid = 53 ssid = >Figure 1 shows the results of our experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W00-0730.txt | Citing Article:  P06-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kudo and Matsumoto (2000) used an SVM based algorithm, and achieved NP chunking results of 93.72% precision, 94.02% recall and 93.87 F-measure for the same shared task data, using only the words and their PoS tags.</S> | Reference Offset:  ['54','57'] | Reference Text:  <S sid = 54 ssid = >The all the values of the chunking F-measure are almost 93.5.</S><S sid = 57 ssid = >Although we select features for learning in very straight way — using all available features such as the words their POS tags without any cut-off threshold for the number of occurrence, we archive high performance for test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W00-0730.txt | Citing Article:  P06-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They used the same features as Kudo and Matsumoto (2000), and achieved over-all chunking performance of 92.06% precision, 92.09% recall and 92.08 F-measure (The results for NP chunks alone were not reported).</S> | Reference Offset:  ['22','54'] | Reference Text:  <S sid = 22 ssid = >For example, NP could be considered as two types of chunk, I-NP or B-NP.</S><S sid = 54 ssid = >The all the values of the chunking F-measure are almost 93.5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W00-0730.txt | Citing Article:  N04-4036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use TinySVM2 along with YamCha3 (Kudo and Matsumoto (2000, 2001)) as the SVM training and test software.</S> | Reference Offset:  ['39','48'] | Reference Text:  <S sid = 39 ssid = >The algorithm scans the test data from left to right and calls the SVM classifiers for all pairs of chunk tags for obtaining the certainty score.</S><S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W00-0730.txt | Citing Article:  P06-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel.</S> | Reference Offset:  ['19','49'] | Reference Text:  <S sid = 19 ssid = >Among the many kinds of Kernel functions available, we will focus on the d-th polynomial kernel: K(xi, xi) = (xi • xi + 1)d Use of d-th polynomial kernel function allows us to build an optimal separating hyperplane which takes into account all combination of features up to d. We believe SVMs have advantage over conventional statistical learning algorithms, such as Decision Tree, and Maximum Entropy Models, from the following two aspects:</S><S sid = 49 ssid = >For the kernel function, we use the 2-nd polynomial function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W00-0730.txt | Citing Article:  N04-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.</S> | Reference Offset:  ['39','48'] | Reference Text:  <S sid = 39 ssid = >The algorithm scans the test data from left to right and calls the SVM classifiers for all pairs of chunk tags for obtaining the certainty score.</S><S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W00-0730.txt | Citing Article:  P06-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This type of sequential tagging method regard as a chunking procedure (Kudo and Matsumoto, 2000) at sentence level.</S> | Reference Offset:  ['21','48'] | Reference Text:  <S sid = 21 ssid = >Each chunk type belongs to I or B tags.</S><S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W00-0730.txt | Citing Article:  P06-2013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kudo and Matsumoto (Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task (Sang and Buchholz, 2000).</S> | Reference Offset:  ['1','48'] | Reference Text:  <S sid = 1 ssid = >In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification.</S><S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W00-0730.txt | Citing Article:  D09-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thissetting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use.</S> | Reference Offset:  ['2','33'] | Reference Text:  <S sid = 2 ssid = >SVMs are so-called large margin classifiers and are well-known as their good generalization performance.</S><S sid = 33 ssid = >The reasons that we use pairwise classifiers are as follows: For the features, we decided to use all the information available in the surrounding contexts, such as the words, their POS tags as well as the chunk labels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W00-0730.txt | Citing Article:  P07-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Goldberg et al, 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well.</S> | Reference Offset:  ['24','55'] | Reference Text:  <S sid = 24 ssid = >We simply formulate the chunking task as a classification problem of these 22 types of chunk.</S><S sid = 55 ssid = >Especially, our method performs well for the chunk types of high frequency, such as NP, VP and PP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W00-0730.txt | Citing Article:  P07-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kudo and Matsumoto (2000) used SVM as a classification engine and achieved an F Score of 93.79 on the shared task NPs.</S> | Reference Offset:  ['1','54'] | Reference Text:  <S sid = 1 ssid = >In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification.</S><S sid = 54 ssid = >The all the values of the chunking F-measure are almost 93.5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W00-0730.txt | Citing Article:  N07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software.</S> | Reference Offset:  ['30','48'] | Reference Text:  <S sid = 30 ssid = >Second approach is pairwise classification.</S><S sid = 48 ssid = >We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W00-0730.txt | Citing Article:  W02-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Even with a more feasible method, pairwise (Kreel, 1998), which is employed in (Kudo and Matsumoto, 2000), we can not train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation.</S> | Reference Offset:  ['6','40'] | Reference Text:  <S sid = 6 ssid = >In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as follows: xi is a feature vector of the i-th sample represented by an n dimensional vector. yi is the class (positive(+1) or negative(-1) class) label of the i-th data.</S><S sid = 40 ssid = >We defined the certainty score as the number of votes for the class (tag) obtained through the pairwise voting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W00-0730.txt | Citing Article:  W02-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al, 2001).</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >Use Of Support Vector Learning For Chunk Identification</S><S sid = 4 ssid = >Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pattern recognition problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W00-0730.txt | Citing Article:  W10-2415.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking.</S> | Reference Offset:  ['3','54'] | Reference Text:  <S sid = 3 ssid = >We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling.</S><S sid = 54 ssid = >The all the values of the chunking F-measure are almost 93.5.</S> | Discourse Facet:  NA | Annotator: Automatic


