Citance Number: 1 | Reference Article:  P96-1025.txt | Citing Article:  W12-3201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >A New Statistical Parser Based On Bigram Lexical Dependencies</S><S sid = 1 ssid = >This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P96-1025.txt | Citing Article:  C08-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Maximum likelihood estimation is used to estimate P(wROOT|S) on training data set with the smoothing method used in (Collins, 1996).</S> | Reference Offset:  ['55','162'] | Reference Text:  <S sid = 55 ssid = >Let V be the vocabulary of all words seen in training data, T be the set of all part-of-speech tags, and TRAIN be the training set, a set of reduced sentences.</S><S sid = 162 ssid = >Estimates based on relaxing the distance measure could also be used for smoothing at present we only back-off on words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P96-1025.txt | Citing Article:  C08-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To overcome the data sparseness problem, we not only apply the smoothing method used in (Collins, 1996) for a lexicalized head to back off it to its part-of-speech, but also assign a very small value to P (cpi|wi) when there is no cpi modifying wi in the constructed case patterns.</S> | Reference Offset:  ['36','162'] | Reference Text:  <S sid = 36 ssid = >Head-words 2.</S><S sid = 162 ssid = >Estimates based on relaxing the distance measure could also be used for smoothing at present we only back-off on words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P96-1025.txt | Citing Article:  C08-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(9) The smoothing method used in (Collins, 1996) is applied during estimation.</S> | Reference Offset:  ['87','162'] | Reference Text:  <S sid = 87 ssid = >(Collins 95) describes how a backed-off estimation strategy is used for making prepositional phrase attachment decisions.</S><S sid = 162 ssid = >Estimates based on relaxing the distance measure could also be used for smoothing at present we only back-off on words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P96-1025.txt | Citing Article:  W06-1636.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Extending on the notion of a Base NP, introduced by Collins (1996), we mark any nonterminal that dominates only preterminals as Base.</S> | Reference Offset:  ['39','42'] | Reference Text:  <S sid = 39 ssid = >VBD is identified as the head-child of VP —> <VBD NP NP>.</S><S sid = 42 ssid = >The triple of nonterminals at the start, middle and end of the arrow specify the nature of the dependency relationship — <NP , S , VP> represents a subject-verb dependency, <PP ,NP ,NP> denotes prepositional phrase modification of an NP, and so on4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P96-1025.txt | Citing Article:  P99-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the discourse structure analysis, we suggest a statistical model with discourse segment boundaries (DSBs) similar to the idea of gaps suggested for a statistical parsing (Collins (1996)).</S> | Reference Offset:  ['15','88'] | Reference Text:  <S sid = 15 ssid = >First, the statistical model assigns a probability to every candidate parse tree for a sentence.</S><S sid = 88 ssid = >The idea is to back-off to estimates based on less context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P96-1025.txt | Citing Article:  W06-2920.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bi lexical dependencies, i.e. dependencies between two words.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >A New Statistical Parser Based On Bigram Lexical Dependencies</S><S sid = 1 ssid = >This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P96-1025.txt | Citing Article:  P98-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In general, the likelihood of a head-dependent relation decreases as distance increases (Collins, 1996).</S> | Reference Offset:  ['36','59'] | Reference Text:  <S sid = 36 ssid = >Head-words 2.</S><S sid = 59 ssid = >Additional context, in particular the relative order of the two words and the distance between them, will also strongly influence the likelihood of one word modifying the other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P96-1025.txt | Citing Article:  C04-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >model (Collins, 1996) to Japanese dependency analysis.</S> | Reference Offset:  ['32','33'] | Reference Text:  <S sid = 32 ssid = >The dependency model is limited to relationships between words in reduced sentences such as Example 1.</S><S sid = 33 ssid = >The mapping from trees to dependency structures is central to the dependency model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P96-1025.txt | Citing Article:  W04-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is therefore necessary to either discard infrequent rules, do manual editing, use a different rule format such as individual dependencies (Collins, 1996) or gain full linguistic control and insight by using a hand written grammar, each of which sacrifices total completeness.</S> | Reference Offset:  ['11','114'] | Reference Text:  <S sid = 11 ssid = >In this way it is similar to 'By 'modifier' we mean the linguistic notion of either an argument or adjunct.</S><S sid = 114 ssid = >In training data 96% of commas follow this rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P96-1025.txt | Citing Article:  W04-0857.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To generate the training examples forthe classifier, we generate a parse tree for every sentence in the SENSEVAL-3 training data, using the Collins (1996) statistical parser.</S> | Reference Offset:  ['15','62'] | Reference Text:  <S sid = 15 ssid = >First, the statistical model assigns a probability to every candidate parse tree for a sentence.</S><S sid = 62 ssid = >The parse tree in training data indicates a relationship in only one of these cases, so this sentence would contribute an estimate of that the two words are related.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P96-1025.txt | Citing Article:  E06-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a modified version of the backed-off smoothing used by Collins (1996) to alleviate sparse data problems.</S> | Reference Offset:  ['76','87'] | Reference Text:  <S sid = 76 ssid = >Conditioning on the exact distance between two words by making Aj,h, = hj — j leads to severe sparse data problems.</S><S sid = 87 ssid = >(Collins 95) describes how a backed-off estimation strategy is used for making prepositional phrase attachment decisions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P96-1025.txt | Citing Article:  P04-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We generate our training data from the Wall Street Journal Section of the Penn Tree Bank (PTB), by transforming it to projective dependency structures, following (Collins, 1996), and extracting rules from the result.</S> | Reference Offset:  ['9','24'] | Reference Text:  <S sid = 9 ssid = >This paper describes a new parser which is much simpler than SPATTER, yet performs at least as well when trained and tested on the same Wall Street Journal data.</S><S sid = 24 ssid = >The tagger performs at around 97% accuracy on Wall Street Journal Text, and is trained on the first 40,000 sentences of the Penn Treebank (Marcus et al. 93).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P96-1025.txt | Citing Article:  P04-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We first transform the PTB into projective dependencies structures following (Collins, 1996).</S> | Reference Offset:  ['33','159'] | Reference Text:  <S sid = 33 ssid = >The mapping from trees to dependency structures is central to the dependency model.</S><S sid = 159 ssid = >The method is equally applicable to tree or dependency representations of syntactic structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P96-1025.txt | Citing Article:  P05-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree.</S> | Reference Offset:  ['1','15'] | Reference Text:  <S sid = 1 ssid = >This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.</S><S sid = 15 ssid = >First, the statistical model assigns a probability to every candidate parse tree for a sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P96-1025.txt | Citing Article:  P02-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While the model of Collins (1996) is technically unsound (Collins, 1999), our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model.</S> | Reference Offset:  ['15','121'] | Reference Text:  <S sid = 15 ssid = >First, the statistical model assigns a probability to every candidate parse tree for a sentence.</S><S sid = 121 ssid = >Ideally we would like to integrate POS tagging into the parsing model rather than treating it as a separate stage.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P96-1025.txt | Citing Article:  P02-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The estimation method is based on Collins (1996).</S> | Reference Offset:  ['87','105'] | Reference Text:  <S sid = 87 ssid = >(Collins 95) describes how a backed-off estimation strategy is used for making prepositional phrase attachment decisions.</S><S sid = 105 ssid = >The probability of a baseNP sequence in an unreduced sentence S is then: The estimation method is analogous to that described in the sparse data section of this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P96-1025.txt | Citing Article:  C10-2086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Relationship: in this paper, we not only use the label of the constituent label as Collins (1996), but also use some well-known context in parsing to define the head-modifier relationship r (.), including the POS of the modifier m, the POS of the head h, the dependency direction d, the parent label of the dependency label l, the grandfather label of the dependency relation p, the POS of adjacent siblings of the modifier s.</S> | Reference Offset:  ['49','127'] | Reference Text:  <S sid = 49 ssid = >5For the head-word of the entire sentence h, 0, with R3=--<Label of the root of the parse tree >.</S><S sid = 127 ssid = >A dynamic programming algorithm is used: if two proposed constituents span the same set of words, have the same label, head, and distance from with the punctuation rule described in section 2.7; (3) is model (2) with POS tags ignored when lexical information is present; (4) is model (3) with probability distributions from the POS tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P96-1025.txt | Citing Article:  P10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of (Collins, 1996), named Collins distance for convenience.</S> | Reference Offset:  ['59','139'] | Reference Text:  <S sid = 59 ssid = >Additional context, in particular the relative order of the two words and the distance between them, will also strongly influence the likelihood of one word modifying the other.</S><S sid = 139 ssid = >For 'no distance measure' the distance measure is Question 1 alone (i.e. whether tb-3 precedes Or follows ti)h,)• parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P96-1025.txt | Citing Article:  W01-1206.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Capturing question or answer dependencies can be cast as a straightforward process of mapping syntactic trees to sets of binary head modifier relationships, as first noted in (Collins, 1996).</S> | Reference Offset:  ['27','37'] | Reference Text:  <S sid = 27 ssid = >Arrows show modifier head dependencies.</S><S sid = 37 ssid = >Head-modifier relationships are now extracted from the tree in Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


