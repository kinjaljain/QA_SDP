Citance Number: 1 | Reference Article:  W10-3001.txt | Citing Article:  W11-0406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information.</S> | Reference Offset:  ['1','181'] | Reference Text:  <S sid = 1 ssid = >The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.</S><S sid = 181 ssid = >The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W10-3001.txt | Citing Article:  W11-0406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel.</S> | Reference Offset:  ['1','181'] | Reference Text:  <S sid = 1 ssid = >The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.</S><S sid = 181 ssid = >The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W10-3001.txt | Citing Article:  P14-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor.</S> | Reference Offset:  ['23','34'] | Reference Text:  <S sid = 23 ssid = >Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).</S><S sid = 34 ssid = >As uncertainty detection is extremely important for biomedical information extraction and most existing approaches have targeted such applications, participants were asked to develop systems for hedge detection in biological scientific articles.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W10-3001.txt | Citing Article:  P14-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2: Features used in sentence-level sentiment prediction.</S> | Reference Offset:  ['26','175'] | Reference Text:  <S sid = 26 ssid = >Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features.</S><S sid = 175 ssid = >Task2 systems differ in the number of class labels used as target and in the machine learning approaches applied.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W10-3001.txt | Citing Article:  P14-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','189'] | Reference Text:  <S sid = 59 ssid = >The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.</S><S sid = 189 ssid = >This work was supported in part by the National Office for Research and Technology (NKTH, http://www.nkth.gov.hu/) of the Hungarian government within the framework of the projects TEXTREND, BELAMI and MASZEKER.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W10-3001.txt | Citing Article:  P14-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al, 2010), and (dis) agreement detection (Yin et al, 2012) in on line debates; (2) a Linear CRF for (dis) agreement identification in broadcast conversations (Wang et al, 2011).</S> | Reference Offset:  ['23','145'] | Reference Text:  <S sid = 23 ssid = >Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).</S><S sid = 145 ssid = >Regarding cross submissions, Zhao et al. (2010) and Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W10-3001.txt | Citing Article:  P11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields.</S> | Reference Offset:  ['29','181'] | Reference Text:  <S sid = 29 ssid = >The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope.</S><S sid = 181 ssid = >The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W10-3001.txt | Citing Article:  P11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Councill et al (2010) present a supervised scope detector using their own annotation.</S> | Reference Offset:  ['145','147'] | Reference Text:  <S sid = 145 ssid = >Regarding cross submissions, Zhao et al. (2010) and Ji et al.</S><S sid = 147 ssid = >Zhao et al. (2010) extended the biological cue word dictionary of their system – using it as a feature for classification – by the frequent cues of the Wikipedia dataset, while Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W10-3001.txt | Citing Article:  P11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As annotation tool, we use Jubilee (Choi et al,2010).</S> | Reference Offset:  ['68','145'] | Reference Text:  <S sid = 68 ssid = >However, the use of the above words or grammatical devices does not necessarily entail their being a weasel cue since their use may be justifiable in their contexts.</S><S sid = 145 ssid = >Regarding cross submissions, Zhao et al. (2010) and Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper.</S> | Reference Offset:  ['23','145'] | Reference Text:  <S sid = 23 ssid = >Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).</S><S sid = 145 ssid = >Regarding cross submissions, Zhao et al. (2010) and Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora.</S> | Reference Offset:  ['77','182'] | Reference Text:  <S sid = 77 ssid = >Two uncertainty detection tasks (sentence classification and in-sentence hedge scope detection) in two domains (biological publications and Wikipedia articles) with three types of submissions (closed, cross and open) were given to the participants of the CoNLL-2010 Shared Task.</S><S sid = 182 ssid = >The challenge consisted of a sentence identification task on uncertainty (Task1) and an in-sentence hedge scope detection task (Task2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules.</S> | Reference Offset:  ['157','174'] | Reference Text:  <S sid = 157 ssid = >It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g.</S><S sid = 174 ssid = >The identification of the scope for a certain cue was typically carried out by classifying each token in the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L).</S> | Reference Offset:  ['44','159'] | Reference Text:  <S sid = 44 ssid = >However, the whole phrase is speculative therefore it is marked as a hedge cue.</S><S sid = 159 ssid = >(2010) is the accurate detection of scope boundaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5).</S> | Reference Offset:  ['27','143'] | Reference Text:  <S sid = 27 ssid = >Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues.</S><S sid = 143 ssid = >The system of Kilicoglu and Bergler (2010) is the only open submission.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note.</S> | Reference Offset:  ['157','163'] | Reference Text:  <S sid = 157 ssid = >It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g.</S><S sid = 163 ssid = >Five systems followed a pure token classification approach (TC) for cue detection while others used sequential labeling techniques (usually Conditional Random Fields) to identify cue phrases in sentences (SL).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','189'] | Reference Text:  <S sid = 59 ssid = >The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.</S><S sid = 189 ssid = >This work was supported in part by the National Office for Research and Technology (NKTH, http://www.nkth.gov.hu/) of the Hungarian government within the framework of the projects TEXTREND, BELAMI and MASZEKER.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W10-3001.txt | Citing Article:  W12-3805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','189'] | Reference Text:  <S sid = 59 ssid = >The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.</S><S sid = 189 ssid = >This work was supported in part by the National Office for Research and Technology (NKTH, http://www.nkth.gov.hu/) of the Hungarian government within the framework of the projects TEXTREND, BELAMI and MASZEKER.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W10-3001.txt | Citing Article:  D11-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well.</S> | Reference Offset:  ['1','181'] | Reference Text:  <S sid = 1 ssid = >The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.</S><S sid = 181 ssid = >The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W10-3001.txt | Citing Article:  W12-3804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes.</S> | Reference Offset:  ['1','29'] | Reference Text:  <S sid = 1 ssid = >The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.</S><S sid = 29 ssid = >The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W10-3001.txt | Citing Article:  W12-3804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection.</S> | Reference Offset:  ['23','145'] | Reference Text:  <S sid = 23 ssid = >Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).</S><S sid = 145 ssid = >Regarding cross submissions, Zhao et al. (2010) and Ji et al.</S> | Discourse Facet:  NA | Annotator: Automatic


