Citance Number: 1 | Reference Article:  P08-1076.txt | Citing Article:  P09-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent research indicates that using labeled and unlabeled data in semi-supervised learning (SSL) environment, with an emphasis on graph-based methods, can improve the performance of information extraction from data for tasks such as question classification (Tri et al, 2006), web classification (Liu et al., 2006), relation extraction (Chen et al, 2006), passage-retrieval (Otterbacher et al, 2009), various natural language processing tasks such as part of-speech tagging, and named-entity recognition (Suzuki and Isozaki, 2008), word-sense disambiguation (Niu et al, 2005), etc.</S> | Reference Offset:  ['1','169'] | Reference Text:  <S sid = 1 ssid = >This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.</S><S sid = 169 ssid = >We observed that ASOsemi prefers ‘nugget extraction’ tasks to ’field segmentation’ tasks (Grenager et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1076.txt | Citing Article:  W09-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['190','191'] | Reference Text:  <S sid = 190 ssid = >Moreover, our experimental analysis revealed that it may also induce an improvement in the expected performance for unseen data in terms of the unlabeled data coverage.</S><S sid = 191 ssid = >Our results may encourage the adoption of the SSL method for many other real world applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1076.txt | Citing Article:  N09-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semi supervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition.</S> | Reference Offset:  ['1','9'] | Reference Text:  <S sid = 1 ssid = >This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.</S><S sid = 9 ssid = >In this paper, we focus on traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1076.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We describe an extension of semi supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008).</S> | Reference Offset:  ['40','186'] | Reference Text:  <S sid = 40 ssid = >Actually, there is a difference in that generative models are directed graphical models while our conditional PM is an undirected.</S><S sid = 186 ssid = >We proposed a simple yet powerful semi-supervised conditional model, which we call JESS-CM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1076.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008).</S> | Reference Offset:  ['28','157'] | Reference Text:  <S sid = 28 ssid = >As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007).</S><S sid = 157 ssid = >Basically, the U.app increase leads to improved performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1076.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that it is possible to iterate the method steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008) but in our experiments we only performed these steps once.</S> | Reference Offset:  ['15','183'] | Reference Text:  <S sid = 15 ssid = >One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible.</S><S sid = 183 ssid = >Note that ASO-semi is also an ‘indirect approach’.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1076.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow a similar approach to that of (Suzuki and Isozaki, 2008) in partitioning f (x, y), where the k different feature vectors correspond to different feature types or feature templates.</S> | Reference Offset:  ['89','99'] | Reference Text:  <S sid = 89 ssid = >As regards the design of the feature functions fi, Table 3 shows the feature templates used in our experiments.</S><S sid = 99 ssid = >With our design, one feature template corresponded to one HMM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1076.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem.</S> | Reference Offset:  ['1','183'] | Reference Text:  <S sid = 1 ssid = >This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.</S><S sid = 183 ssid = >Note that ASO-semi is also an ‘indirect approach’.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1076.txt | Citing Article:  P10-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline.</S> | Reference Offset:  ['27','186'] | Reference Text:  <S sid = 27 ssid = >We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) (Lafferty et al., 2001).</S><S sid = 186 ssid = >We proposed a simple yet powerful semi-supervised conditional model, which we call JESS-CM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1076.txt | Citing Article:  P10-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >22-24 was 4.2%, which is comparable to related work in the literature, e.g. Suzuki and Isozaki (2008) (7%) and Spoustova et al (2009) (4-5%).</S> | Reference Offset:  ['28','80'] | Reference Text:  <S sid = 28 ssid = >As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007).</S><S sid = 80 ssid = >For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1076.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In comparison, there are 79 templates in (Suzuki and Isozaki, 2008).</S> | Reference Offset:  ['89','102'] | Reference Text:  <S sid = 89 ssid = >As regards the design of the feature functions fi, Table 3 shows the feature templates used in our experiments.</S><S sid = 102 ssid = >As a result, 47, 39 and 79 distinct HMMs are embedded in the potential functions of JESS-CM for POS tagging, chunking and NER experiments, respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1076.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['190','191'] | Reference Text:  <S sid = 190 ssid = >Moreover, our experimental analysis revealed that it may also induce an improvement in the expected performance for unseen data in terms of the unlabeled data coverage.</S><S sid = 191 ssid = >Our results may encourage the adoption of the SSL method for many other real world applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1076.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wong and Ng (2007) and Suzuki and Isozaki (2008) are similar in that they run a baseline discriminative classifier on unlabeled data to generate pseudo examples, which are then used to train a different type of classifier for the same problem.</S> | Reference Offset:  ['28','73'] | Reference Text:  <S sid = 28 ssid = >As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007).</S><S sid = 73 ssid = >Moreover, it is necessary to split features into several sets, and then train several corresponding discriminative models separately and preliminarily.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1076.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Suzuki and Isozaki (2008), on the other hand, used the automatically labeled corpus to train HMMs.</S> | Reference Offset:  ['68','83'] | Reference Text:  <S sid = 68 ssid = >In addition, the calculation cost for estimating parameters of embedded joint PMs (HMMs) is independent of the number of HMMs, J, that we used (Suzuki et al., 2007).</S><S sid = 83 ssid = >The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1076.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although the method in (Suzuki and Isozaki 2008) is quite general, it is hard to see how it can be applied to the query classification problem.</S> | Reference Offset:  ['159','181'] | Reference Text:  <S sid = 159 ssid = >Thus, it strongly encourages us to use an SSL approach that includes JESS-CM to construct a general tagger and chunker for actual use.</S><S sid = 181 ssid = >There is an essential difference between this method and JESSCM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1076.txt | Citing Article:  C10-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Suzuki and Isozaki (2008) also found a log linear relationship between unlabeled data (up to a billion words) and performance on three NLP tasks.</S> | Reference Offset:  ['4','20'] | Reference Text:  <S sid = 4 ssid = >We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement.</S><S sid = 20 ssid = >We used up to 1G-words (one billion tokens) of unlabeled data to explore the performance improvement with respect to the unlabeled data size.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1076.txt | Citing Article:  D09-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another approach (Suzuki and Isozaki, 2008) embeds a joint probability model.</S> | Reference Offset:  ['48','180'] | Reference Text:  <S sid = 48 ssid = >Hereafter in this paper, we refer to this conditional model as a ‘Joint probability model Embedding style SemiSupervised Conditional Model’, or JESS-CM for short.</S><S sid = 180 ssid = >There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1076.txt | Citing Article:  D10-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['190','191'] | Reference Text:  <S sid = 190 ssid = >Moreover, our experimental analysis revealed that it may also induce an improvement in the expected performance for unseen data in terms of the unlabeled data coverage.</S><S sid = 191 ssid = >Our results may encourage the adoption of the SSL method for many other real world applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1076.txt | Citing Article:  P12-2076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Incorporating binary and real features yields a rough approximation of generative models in semi supervised CRFs (Suzuki and Isozaki, 2008).</S> | Reference Offset:  ['40','70'] | Reference Text:  <S sid = 40 ssid = >Actually, there is a difference in that generative models are directed graphical models while our conditional PM is an undirected.</S><S sid = 70 ssid = >SSL based on a hybrid generative/discriminative approach proposed in (Suzuki et al., 2007) has been defined as a log-linear model that discriminatively combines several discriminative models, pDi , and generative models, pGj , such that: where Λ={λi}Ii=1, and Γ={{γi}Ii=1, {γj}I+J j=I+1}.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1076.txt | Citing Article:  P11-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Suzuki and Isozaki (2008) is one such example.</S> | Reference Offset:  ['115','187'] | Reference Text:  <S sid = 115 ssid = >An example of non-convergence is the oscillation of the estimated O.</S><S sid = 187 ssid = >It is applicable to large amounts of unlabeled data, for example, at the giga-word level.</S> | Discourse Facet:  NA | Annotator: Automatic


