Citance Number: 1 | Reference Article:  W08-2102.txt | Citing Article:  P09-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser.</S> | Reference Offset:  ['26','107'] | Reference Text:  <S sid = 26 ssid = >We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing.</S><S sid = 107 ssid = >To deal with this problem, we use a simple initial model to prune the search space of the more complex model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W08-2102.txt | Citing Article:  P13-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W08-2102.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach can be seen as trade-off between phrase based reranking experiments (Collins, 2000) and the approach of Carreras et al (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism.</S> | Reference Offset:  ['4','151'] | Reference Text:  <S sid = 4 ssid = >We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.</S><S sid = 151 ssid = >We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W08-2102.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text).</S> | Reference Offset:  ['41','157'] | Reference Text:  <S sid = 41 ssid = >Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features.</S><S sid = 157 ssid = >Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W08-2102.txt | Citing Article:  C10-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar.</S> | Reference Offset:  ['4','33'] | Reference Text:  <S sid = 4 ssid = >We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.</S><S sid = 33 ssid = >A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser.1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDonald et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W08-2102.txt | Citing Article:  P12-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W08-2102.txt | Citing Article:  P13-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading model sis excluded.</S> | Reference Offset:  ['153','157'] | Reference Text:  <S sid = 153 ssid = >The accuracy of our approach also shows some improvement over results in (Petrov and Klein, 2007).</S><S sid = 157 ssid = >Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W08-2102.txt | Citing Article:  D12-1133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W08-2102.txt | Citing Article:  P10-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008).</S> | Reference Offset:  ['7','157'] | Reference Text:  <S sid = 7 ssid = >In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector.</S><S sid = 157 ssid = >Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W08-2102.txt | Citing Article:  E12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W08-2102.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W08-2102.txt | Citing Article:  D10-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W08-2102.txt | Citing Article:  D12-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Carreras et al., 2008) and edge annotation (Huang, 2008).</S> | Reference Offset:  ['41','157'] | Reference Text:  <S sid = 41 ssid = >Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features.</S><S sid = 157 ssid = >Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W08-2102.txt | Citing Article:  D12-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008).</S> | Reference Offset:  ['96','116'] | Reference Text:  <S sid = 96 ssid = >For the first-order model, the methods described in (Eisner, 2000) can be used for the parsing algorithm.</S><S sid = 116 ssid = >We now turn to how the marginals Âµ(x, h, m, l) are defined and computed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W08-2102.txt | Citing Article:  D09-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al, 2008)).</S> | Reference Offset:  ['44','167'] | Reference Text:  <S sid = 44 ssid = >The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003; Shen and Joshi, 2005).</S><S sid = 167 ssid = >A key to the approach has been to use a splittable grammar that allows efficient dynamic programming algorithms, in combination with pruning using a lower-order model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W08-2102.txt | Citing Article:  D09-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008).</S> | Reference Offset:  ['4','157'] | Reference Text:  <S sid = 4 ssid = >We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.</S><S sid = 157 ssid = >Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W08-2102.txt | Citing Article:  D09-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To continue our example, the resulting entry would be as follows: es gibt? S NP there VP is To give a more formal description of how syn tactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al, 2008).</S> | Reference Offset:  ['91','136'] | Reference Text:  <S sid = 91 ssid = >The combination of the left and right modifier structures has led to flat structures, for example the rule VP â ADVP VP NP in the above tree.</S><S sid = 136 ssid = >For example, on section 22 of the treebank, if derivations are first extracted using the method described in this section, then mapped back to parse trees using the method described in section 2.3, the resulting parse trees score 100% precision and 99.81% recall in labeled constituent accuracy, indicating that very little information is lost in this process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W08-2102.txt | Citing Article:  D10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >Next, consider combining the left and right structures of a spine.</S><S sid = 176 ssid = >Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.</S> | Discourse Facet:  NA | Annotator: Automatic


