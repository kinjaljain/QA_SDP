Citance Number: 1 | Reference Article:  P03-1013.txt | Citing Article:  P04-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing.</S> | Reference Offset:  ['72','136'] | Reference Text:  <S sid = 72 ssid = >Of the remaining 2,000 sentences, the first 1,000 served as the test set, and the last 1000 as the development set.</S><S sid = 136 ssid = >The poor performance of the lexicalized models could be due to a lack of sufficient training data: our Negra training set contains approximately 18,000 sentences, and is therefore significantly smaller than the Penn Treebank training set (about 40,000 sentences).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-1013.txt | Citing Article:  E12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','241'] | Reference Text:  <S sid = 60 ssid = >Another prediction was that adding Negra-specific information to the models will increase parsing performance.</S><S sid = 241 ssid = >Testing our sister-head model on these languages is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-1013.txt | Citing Article:  W08-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003).</S> | Reference Offset:  ['45','174'] | Reference Text:  <S sid = 45 ssid = >The aim of the present paper is to test if this finding carries over to German and to the Negra corpus.</S><S sid = 174 ssid = >Again, we give results obtained using TnT tags and using perfect tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-1013.txt | Citing Article:  P06-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure.</S> | Reference Offset:  ['44','165'] | Reference Text:  <S sid = 44 ssid = >Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., Carroll and Rooth 1998; Charniak 1997, 2000; Collins 1997).</S><S sid = 165 ssid = >The Collins (1997) model does not use context-free rules, but generates the next category using zeroth order Markov chains (see Section 3.3), hence no information about the previous sisters is included.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-1013.txt | Citing Article:  W05-1512.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003).</S> | Reference Offset:  ['82','222'] | Reference Text:  <S sid = 82 ssid = >Lexicalization requires that each rule in a grammar has one of the categories on its righthand side annotated as the head.</S><S sid = 222 ssid = >Collins et al. (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-1013.txt | Citing Article:  C08-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006).</S> | Reference Offset:  ['217','222'] | Reference Text:  <S sid = 217 ssid = >There is some research on treebank-based parsing of languages other than English.</S><S sid = 222 ssid = >Collins et al. (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-1013.txt | Citing Article:  W09-2605.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English.</S> | Reference Offset:  ['2','204'] | Reference Text:  <S sid = 2 ssid = >We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.</S><S sid = 204 ssid = >On the whole, the results of Experiment 2 are at odds with what is known about parsing for English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-1013.txt | Citing Article:  P06-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing.</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Probabilistic Parsing For German Using Sister-Head Dependencies</S><S sid = 7 ssid = >Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., Collins 1997; Charniak 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-1013.txt | Citing Article:  W08-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures.</S> | Reference Offset:  ['9','35'] | Reference Text:  <S sid = 9 ssid = >The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German.</S><S sid = 35 ssid = >The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-1013.txt | Citing Article:  W08-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%.</S> | Reference Offset:  ['9','228'] | Reference Text:  <S sid = 9 ssid = >The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German.</S><S sid = 228 ssid = >The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-1013.txt | Citing Article:  W07-2219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000).</S> | Reference Offset:  ['12','218'] | Reference Text:  <S sid = 12 ssid = >Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000).</S><S sid = 218 ssid = >The work by Collins et al. (1999) and Bikel and Chiang (2000) has demonstrated the applicability of the Collins (1997) model for Czech and Chinese.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-1013.txt | Citing Article:  W07-2219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance.</S> | Reference Offset:  ['3','21'] | Reference Text:  <S sid = 3 ssid = >Learning curves show that this effect is not due to lack of training data.</S><S sid = 21 ssid = >Learning curves show that the poor performance of the lexicalized models is not due to lack of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-1013.txt | Citing Article:  W06-2920.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','241'] | Reference Text:  <S sid = 60 ssid = >Another prediction was that adding Negra-specific information to the models will increase parsing performance.</S><S sid = 241 ssid = >Testing our sister-head model on these languages is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-1013.txt | Citing Article:  W06-1614.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','241'] | Reference Text:  <S sid = 60 ssid = >Another prediction was that adding Negra-specific information to the models will increase parsing performance.</S><S sid = 241 ssid = >Testing our sister-head model on these languages is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P03-1013.txt | Citing Article:  W06-1614.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','241'] | Reference Text:  <S sid = 60 ssid = >Another prediction was that adding Negra-specific information to the models will increase parsing performance.</S><S sid = 241 ssid = >Testing our sister-head model on these languages is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P03-1013.txt | Citing Article:  W06-1614.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing.</S> | Reference Offset:  ['115','122'] | Reference Text:  <S sid = 115 ssid = >Adding grammatical functions reduces both figures slightly, and coverage drops by about 15%.</S><S sid = 122 ssid = >Perfect tagging results in a performance increase of over 10% for the models with grammatical functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P03-1013.txt | Citing Article:  P05-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003).</S> | Reference Offset:  ['175','197'] | Reference Text:  <S sid = 175 ssid = >The row ‘Split PP’ contains the performance figures obtained by including split PPs in both the training and in the testing set.</S><S sid = 197 ssid = >However, we observed that LR and LP are artificially inflated if split PPs are used for testing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P03-1013.txt | Citing Article:  P05-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004).</S> | Reference Offset:  ['120','228'] | Reference Text:  <S sid = 120 ssid = >Performance using perfect tags (an upper bound of model performance) is 2–3% higher for the baseline and for the C&R model.</S><S sid = 228 ssid = >The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P03-1013.txt | Citing Article:  P05-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','241'] | Reference Text:  <S sid = 60 ssid = >Another prediction was that adding Negra-specific information to the models will increase parsing performance.</S><S sid = 241 ssid = >Testing our sister-head model on these languages is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P03-1013.txt | Citing Article:  P05-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004).</S> | Reference Offset:  ['224','237'] | Reference Text:  <S sid = 224 ssid = >We presented the first probabilistic full parsing model for German trained on Negra, a syntactically annotated corpus.</S><S sid = 237 ssid = >The best performance was obtained for a model that uses sister-head dependencies for all categories.</S> | Discourse Facet:  NA | Annotator: Automatic


