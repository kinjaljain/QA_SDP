Citance Number: 1 | Reference Article:  W04-1013.txt | Citing Article:  P04-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).</S> | Reference Offset:  ['0','175'] | Reference Text:  <S sid = 0 ssid = >ROUGE: A Package For Automatic Evaluation Of Summaries</S><S sid = 175 ssid = >We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-1013.txt | Citing Article:  P14-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used ROUGE (Lin, 2004) as an evaluation criterion.</S> | Reference Offset:  ['176','177'] | Reference Text:  <S sid = 176 ssid = >In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries.</S><S sid = 177 ssid = >In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-1013.txt | Citing Article:  N10-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004).</S> | Reference Offset:  ['13','177'] | Reference Text:  <S sid = 13 ssid = >Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S><S sid = 177 ssid = >In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-1013.txt | Citing Article:  S12-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure.</S> | Reference Offset:  ['116','130'] | Reference Text:  <S sid = 116 ssid = >We call the skip-bigram-based Fmeasure, i.e.</S><S sid = 130 ssid = >To achieve this, we extend ROUGE-S with the addition of unigram as counting unit.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-1013.txt | Citing Article:  C08-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;.</S> | Reference Offset:  ['13','124'] | Reference Text:  <S sid = 13 ssid = >Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S><S sid = 124 ssid = >For example, if we set dskip to 0 then ROUGE-S is equivalent to bigram overlap Fmeasure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-1013.txt | Citing Article:  N06-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results.</S> | Reference Offset:  ['49','131'] | Reference Text:  <S sid = 49 ssid = >He used LCS as an approximate string matching algorithm.</S><S sid = 131 ssid = >The extended version is called ROUGE-SU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-1013.txt | Citing Article:  W08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation.</S> | Reference Offset:  ['13','25'] | Reference Text:  <S sid = 13 ssid = >Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S><S sid = 25 ssid = >Please see Papineni et al. (2001) for details about BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-1013.txt | Citing Article:  N12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries.</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >ROUGE: A Package For Automatic Evaluation Of Summaries</S><S sid = 20 ssid = >Formally, ROUGE-N is an n-gram recall between a candidate summary and a set of reference summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-1013.txt | Citing Article:  W08-1808.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task.</S> | Reference Offset:  ['56','104'] | Reference Text:  <S sid = 56 ssid = >Therefore, only Rlcs is considered.</S><S sid = 104 ssid = >Therefore, only Rwlcs is considered.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-1013.txt | Citing Article:  P13-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004).</S> | Reference Offset:  ['0','23'] | Reference Text:  <S sid = 0 ssid = >ROUGE: A Package For Automatic Evaluation Of Summaries</S><S sid = 23 ssid = >A closely related measure, BLEU, used in automatic evaluation of machine translation, is a precision-based measure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-1013.txt | Citing Article:  N09-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >N=2, for the purpose of explanation.</S><S sid = 180 ssid = >The author would like to thank the anonymous reviewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-1013.txt | Citing Article:  N07-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >N=2, for the purpose of explanation.</S><S sid = 180 ssid = >The author would like to thank the anonymous reviewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-1013.txt | Citing Article:  P11-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['177','178'] | Reference Text:  <S sid = 177 ssid = >In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S><S sid = 178 ssid = >The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-1013.txt | Citing Article:  P11-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004).</S> | Reference Offset:  ['116','175'] | Reference Text:  <S sid = 116 ssid = >We call the skip-bigram-based Fmeasure, i.e.</S><S sid = 175 ssid = >We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-1013.txt | Citing Article:  W06-1643.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%.</S> | Reference Offset:  ['39','152'] | Reference Text:  <S sid = 39 ssid = >Given M references, we compute the best score over M sets of M-1 references.</S><S sid = 152 ssid = >We found that correlations were not affected by stemming or removal of stopwords in this data set, ROUGE-2 performed better among the ROUGE-N variants, ROUGE-L, ROUGE-W, and ROUGE-S were all performing well, and using multiple references improved performance though not much.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-1013.txt | Citing Article:  W06-1643.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004).</S> | Reference Offset:  ['170','179'] | Reference Text:  <S sid = 170 ssid = >In the single document summarization tasks we had over 100 samples; while we only had about 30 samples in the multi-document tasks.</S><S sid = 179 ssid = >However, how to achieve high correlation with human judgments in multi-document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-1013.txt | Citing Article:  D11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively.</S> | Reference Offset:  ['13','25'] | Reference Text:  <S sid = 13 ssid = >Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S><S sid = 25 ssid = >Please see Papineni et al. (2001) for details about BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-1013.txt | Citing Article:  D11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004).</S> | Reference Offset:  ['0','177'] | Reference Text:  <S sid = 0 ssid = >ROUGE: A Package For Automatic Evaluation Of Summaries</S><S sid = 177 ssid = >In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-1013.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures.</S> | Reference Offset:  ['156','164'] | Reference Text:  <S sid = 156 ssid = >We found that ROUGE-1, ROUGE-L, ROUGESU4 and 9, and ROUGE-W were very good measures in this category, ROUGE-N with N > 1 performed significantly worse than all other measures, and exclusion of stopwords improved performance in general except for ROUGE-1.</S><S sid = 164 ssid = >These results again suggested that exclusion of stopwords achieved better performance especially in multi-document summaries of 50 words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-1013.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used ROUGE (Lin, 2004) for evaluating the content of summaries.</S> | Reference Offset:  ['175','176'] | Reference Text:  <S sid = 175 ssid = >We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</S><S sid = 176 ssid = >In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


