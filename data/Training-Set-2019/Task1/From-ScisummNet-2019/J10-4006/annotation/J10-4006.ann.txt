Citance Number: 1 | Reference Article:  J10-4006.txt | Citing Article:  P14-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts.</S> | Reference Offset:  ['208','547'] | Reference Text:  <S sid = 208 ssid = >The labeled tensor is nothing other than a formalization of distributional data extracted in the word–link–word–score format, which is customary in many structured DSMs.</S><S sid = 547 ssid = >In our experiments we do not make use of the contexts of the target word pairs that are provided with the test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J10-4006.txt | Citing Article:  W11-2505.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure.</S> | Reference Offset:  ['0','752'] | Reference Text:  <S sid = 0 ssid = >Distributional Memory: A General Framework for Corpus-Based Semantics</S><S sid = 752 ssid = >A general framework for distributional semantics should satisfy the following two requirements: (1) representing corpus-derived data in such a way as to capture aspects of meaning that have so far been modeled with different, prima facie incompatible data structures; (2) using this common representation to address a large battery of semantic experiments, achieving a performance at least comparable to that of state-of-art, taskspecific DSMs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived.</S> | Reference Offset:  ['0','58'] | Reference Text:  <S sid = 0 ssid = >Distributional Memory: A General Framework for Corpus-Based Semantics</S><S sid = 58 ssid = >With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S> | Reference Offset:  ['2','8'] | Reference Text:  <S sid = 2 ssid = >As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S><S sid = 8 ssid = >As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l.</S> | Reference Offset:  ['99','208'] | Reference Text:  <S sid = 99 ssid = >As in structured DSMs, we adopt word–link–word tuples as the most suitable way to capture distributional facts.</S><S sid = 208 ssid = >The labeled tensor is nothing other than a formalization of distributional data extracted in the word–link–word–score format, which is customary in many structured DSMs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively).</S> | Reference Offset:  ['214','260'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 260 ssid = >These terms were selected based on their frequency in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000 most frequent verbs and adjectives), augmenting the list with lemmas that we found in various standard test sets, such as the TOEFL and SAT lists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC).</S> | Reference Offset:  ['61','523'] | Reference Text:  <S sid = 61 ssid = >DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of word– link–word tuples.</S><S sid = 523 ssid = >This is essentially the same task as the TOEFL, but applied to word pairs instead of words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1).</S> | Reference Offset:  ['214','736'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 736 ssid = >Bootstrapped confidence intervals are obtained as described in Section 6.1.3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['267','815'] | Reference Text:  <S sid = 267 ssid = >For example, there is a sbj intr−1 link between an intransitive verb and its subject: (talk, sbj intr−1, soldier).</S><S sid = 815 ssid = >This separation is in line with what is commonly assumed in cognitive science and formal linguistics, and we hope it will contribute to make corpus-based modeling a core part of the ongoing study of semantic knowledge in humans and machines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J10-4006.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010).</S> | Reference Offset:  ['2','8'] | Reference Text:  <S sid = 2 ssid = >As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S><S sid = 8 ssid = >As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J10-4006.txt | Citing Article:  P12-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010).</S> | Reference Offset:  ['65','358'] | Reference Text:  <S sid = 65 ssid = >In all cases, we compare the performance of several DM implementations to state-of-the-art results.</S><S sid = 358 ssid = >In many of the experiments herein, DM is not only compared to the results available in the literature, but also to our implementation of state-of-the-art DSMs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J10-4006.txt | Citing Article:  W11-2508.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010.</S> | Reference Offset:  ['214','598'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 598 ssid = >automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J10-4006.txt | Citing Article:  W12-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)).</S> | Reference Offset:  ['0','58'] | Reference Text:  <S sid = 0 ssid = >Distributional Memory: A General Framework for Corpus-Based Semantics</S><S sid = 58 ssid = >With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J10-4006.txt | Citing Article:  W12-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review).</S> | Reference Offset:  ['214','431'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 431 ssid = >See Baroni et al. (2010) for the full list.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J10-4006.txt | Citing Article:  W12-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >we used local mutual information (LMI) as proposed by Baroni and Lenci (2010).</S> | Reference Offset:  ['214','269'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 269 ssid = >The weights assigned to the tuples by the scoring function σ are given by Local Mutual Information (LMI) computed on the raw corpus-derived word–link–word cooccurrence counts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J10-4006.txt | Citing Article:  W11-2503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM.</S> | Reference Offset:  ['214','664'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 664 ssid = >TypeDM is again our best model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J10-4006.txt | Citing Article:  W11-2503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined.</S> | Reference Offset:  ['214','334'] | Reference Text:  <S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S><S sid = 334 ssid = >We refer to it as the centroid of the vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J10-4006.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available.</S> | Reference Offset:  ['315','664'] | Reference Text:  <S sid = 315 ssid = >Coherent with this approach, we make our best DM model (TypeDM) publicly available from http://clic.cimec.unitn.it/dm.</S><S sid = 664 ssid = >TypeDM is again our best model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J10-4006.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information.</S> | Reference Offset:  ['431','464'] | Reference Text:  <S sid = 431 ssid = >See Baroni et al. (2010) for the full list.</S><S sid = 464 ssid = >The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus 1998) consists of 100 noun–verb pairs rated by 36 subjects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J10-4006.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010).</S> | Reference Offset:  ['65','214'] | Reference Text:  <S sid = 65 ssid = >In all cases, we compare the performance of several DM implementations to state-of-the-art results.</S><S sid = 214 ssid = >We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


