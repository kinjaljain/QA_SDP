Citance Number: 1 | Reference Article:  E03-1071.txt | Citing Article:  W03-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm.</S> | Reference Offset:  ['12','105'] | Reference Text:  <S sid = 12 ssid = >We also investigate how the use of a correction feature affects the performance of ME taggers.</S><S sid = 105 ssid = >This paper has demonstrated, both analytically and empirically, that GIS does not require a correction feature Eliminating the correction feature simplifies further the already very simple estimation algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E03-1071.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003).</S> | Reference Offset:  ['48','76'] | Reference Text:  <S sid = 48 ssid = >CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).</S><S sid = 76 ssid = >Table 6 gives results for cutoff values between 1 and 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E03-1071.txt | Citing Article:  W06-1619.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','142'] | Reference Text:  <S sid = 44 ssid = >The parameters o-, are usually collapsed into one parameter which can be set using heldout data.</S><S sid = 142 ssid = >Computational Linguistics, 27(2): 199-229.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E03-1071.txt | Citing Article:  W06-1619.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003).</S> | Reference Offset:  ['48','98'] | Reference Text:  <S sid = 48 ssid = >CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).</S><S sid = 98 ssid = >Following Clark (2002), all categories that occurred at least 10 times in the training data were used, resulting in a tagset of 398 categories.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E03-1071.txt | Citing Article:  P05-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al, 1994).</S> | Reference Offset:  ['18','118'] | Reference Text:  <S sid = 18 ssid = >The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &quot;supertagger&quot;, which assigns tags from the much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002).</S><S sid = 118 ssid = >A maximum entropy part-ofspeech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E03-1071.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger.</S> | Reference Offset:  ['53','84'] | Reference Text:  <S sid = 53 ssid = >The contextual predicates used by the two taggers are given in Table 2, where w, is the ith word and ti is the ith tag.</S><S sid = 84 ssid = >Table 8 shows the difference in the number of contextual predicates and features between the original and final taggers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E03-1071.txt | Citing Article:  W07-2206.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003).</S> | Reference Offset:  ['118','137'] | Reference Text:  <S sid = 118 ssid = >A maximum entropy part-ofspeech tagger.</S><S sid = 137 ssid = >Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E03-1071.txt | Citing Article:  C04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here we use the Maximum Entropy models described in Curran and Clark (2003).</S> | Reference Offset:  ['121','125'] | Reference Text:  <S sid = 121 ssid = >Maximum Entropy Models for Natural Language Ambiguity Resolution.</S><S sid = 125 ssid = >Learning to parse natural language with maximum entropy models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E03-1071.txt | Citing Article:  C04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights.</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing.</S><S sid = 7 ssid = >Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E03-1071.txt | Citing Article:  C04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories.</S> | Reference Offset:  ['55','58'] | Reference Text:  <S sid = 55 ssid = >The supertagger uses POS tags as additional features, which Clark (2002) found improved performance significantly, and does not use the morphological features, since the POS tags provide equivalent information.</S><S sid = 58 ssid = >The tagger returns the most probable sequence for the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E03-1071.txt | Citing Article:  C04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger.</S> | Reference Offset:  ['48','55'] | Reference Text:  <S sid = 48 ssid = >CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).</S><S sid = 55 ssid = >The supertagger uses POS tags as additional features, which Clark (2002) found improved performance significantly, and does not use the morphological features, since the POS tags provide equivalent information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E03-1071.txt | Citing Article:  P07-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.</S> | Reference Offset:  ['48','60'] | Reference Text:  <S sid = 48 ssid = >CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).</S><S sid = 60 ssid = >We develop and test our improved POS tagger (c &c) using the standard parser development methodology on the Penn Treebank WSJ corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E03-1071.txt | Citing Article:  W03-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C & C tagger (Curran and Clark, 2003).</S> | Reference Offset:  ['88','118'] | Reference Text:  <S sid = 88 ssid = >We also performed 10-fold cross-validation using MXPOST and TNT, a publicly available Markov model PO S tagger (Brants, 2000).</S><S sid = 118 ssid = >A maximum entropy part-ofspeech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E03-1071.txt | Citing Article:  D07-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches.</S> | Reference Offset:  ['114','118'] | Reference Text:  <S sid = 114 ssid = >Using maximum entropy for text classification.</S><S sid = 118 ssid = >A maximum entropy part-ofspeech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E03-1071.txt | Citing Article:  D07-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The C & C tagger (Curran and Clark, 2003) was trained on the Wall Street Journal texts in the Penn Treebank and then used to tag the NET Bible (the source text).</S> | Reference Offset:  ['18','60'] | Reference Text:  <S sid = 18 ssid = >The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &quot;supertagger&quot;, which assigns tags from the much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002).</S><S sid = 60 ssid = >We develop and test our improved POS tagger (c &c) using the standard parser development methodology on the Penn Treebank WSJ corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E03-1071.txt | Citing Article:  P06-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark,2003).</S> | Reference Offset:  ['18','60'] | Reference Text:  <S sid = 18 ssid = >The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &quot;supertagger&quot;, which assigns tags from the much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002).</S><S sid = 60 ssid = >We develop and test our improved POS tagger (c &c) using the standard parser development methodology on the Penn Treebank WSJ corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E03-1071.txt | Citing Article:  S10-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000).</S> | Reference Offset:  ['132','136'] | Reference Text:  <S sid = 132 ssid = >2000.</S><S sid = 136 ssid = >2000.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E03-1071.txt | Citing Article:  S10-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b).</S> | Reference Offset:  ['114','118'] | Reference Text:  <S sid = 114 ssid = >Using maximum entropy for text classification.</S><S sid = 118 ssid = >A maximum entropy part-ofspeech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E03-1071.txt | Citing Article:  P03-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003).</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing.</S><S sid = 7 ssid = >Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E03-1071.txt | Citing Article:  W06-3328.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003).</S> | Reference Offset:  ['17','19'] | Reference Text:  <S sid = 17 ssid = >We explore the combination of Gaussian smoothing and a simple cutoff for two tagging tasks.</S><S sid = 19 ssid = >Elimination of the correction feature and use of appropriate smoothing methods result in state of the art performance for both tagging tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


