Citance Number: 1 | Reference Article:  P05-1065.txt | Citing Article:  N07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student's lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005).</S> | Reference Offset:  ['27','141'] | Reference Text:  <S sid = 27 ssid = >This section highlights examples and features of some commonly used measures of reading level and discusses current research on the topic of reading level assessment using NLP techniques.</S><S sid = 141 ssid = >The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1065.txt | Citing Article:  N07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Prior work on first language readability by Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases.</S> | Reference Offset:  ['16','141'] | Reference Text:  <S sid = 16 ssid = >In this work, we develop a method of reading level assessment that uses support vector machines (SVMs) to combine features from statistical language models (LMs), parse trees, and other traditional features used in reading level assessment.</S><S sid = 141 ssid = >The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1065.txt | Citing Article:  P13-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.</S><S sid = 159 ssid = >We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1065.txt | Citing Article:  C10-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.</S><S sid = 159 ssid = >We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1065.txt | Citing Article:  E09-3003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability.</S> | Reference Offset:  ['74','130'] | Reference Text:  <S sid = 74 ssid = >In addition to using the likelihood ratio for classification, we can use scores from language models as features in another classifier (e.g. an SVM).</S><S sid = 130 ssid = >By combining language model scores with other features in an SVM framework, we achieve our best results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1065.txt | Citing Article:  E09-3003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005).</S> | Reference Offset:  ['0','156'] | Reference Text:  <S sid = 0 ssid = >Reading Level Assessment Using Support Vector Machines And Statistical Language Models</S><S sid = 156 ssid = >Combining information from statistical LMs with other features using support vector machines provided the best results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1065.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A corpus of Weekly Reader articles was previously used in work by Schwarm and Ostendorf (2005).</S> | Reference Offset:  ['50','152'] | Reference Text:  <S sid = 50 ssid = >Our work is currently focused on a corpus obtained from Weekly Reader, an educational newspaper with versions targeted at different grade levels (Weekly Reader, 2004).</S><S sid = 152 ssid = >We found that our SVM classifier, trained on the Weekly Reader corpus, classified four of these articles as grade 4 and seven articles as grade 5 (with one overlap with grade 4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1065.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Schwarm and Ostendorf (2005) studied four parse tree features (average parse tree height, average number of SBARs, noun phrases, and verb phrases per sentences).</S> | Reference Offset:  ['29','141'] | Reference Text:  <S sid = 29 ssid = >The widelyused Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al., 1975) (as cited in (Collins-Thompson and Callan, 2004)).</S><S sid = 141 ssid = >The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1065.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For comparison, we replicated 6 out-of-vocabulary features described in Schwarm and Ostendorf (2005).</S> | Reference Offset:  ['76','113'] | Reference Text:  <S sid = 76 ssid = >Perplexity scores are used as features in the SVM model described in Section 4.3.</S><S sid = 113 ssid = >For comparison to other methods, e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1065.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also replicated the 12 perplexity features implemented by Schwarm and Ostendorf (2005) (see Section 3.2).</S> | Reference Offset:  ['76','104'] | Reference Text:  <S sid = 76 ssid = >Perplexity scores are used as features in the SVM model described in Section 4.3.</S><S sid = 104 ssid = >This resulted in 12 LM perplexity features per article based on trigram, bigram and unigram LMs trained on Britannica (adult), Britannica Elementary, CNN (adult) and CNN abridged text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1065.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 8 compares a classifier trained on the four parse features of Schwarm and Ostendorf (2005) to a classifier trained on our expanded set of parse features.</S> | Reference Offset:  ['45','98'] | Reference Text:  <S sid = 45 ssid = >The smoothed unigram classifier is also more generalizable, since it can be trained on any collection of data.</S><S sid = 98 ssid = >The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1065.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The most closely related previous study is the work of Schwarm and Ostendorf (2005).</S> | Reference Offset:  ['23','81'] | Reference Text:  <S sid = 23 ssid = >Section 2 describes related work on reading level assessment.</S><S sid = 81 ssid = >Lee and Myaeng’s (2002) genre and subject detection work and Boulis and Ostendorf’s (2005) work on feature selection for topic classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1065.txt | Citing Article:  E09-2013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, relatedly, (Schwarm and Ostendorf, 2005) use a statistical language model to train SVM classifiers to classify text for grade levels 2-5.</S> | Reference Offset:  ['96','103'] | Reference Text:  <S sid = 96 ssid = >Our SVM classifiers for reading level use the following features: The OOV scores are relative to the most common 100, 200 and 500 words in the lowest grade level (grade 2) 2.</S><S sid = 103 ssid = >Thus we made use of the Britannica and CNN articles to train models of three n-gram orders on “child” text and “adult” text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1065.txt | Citing Article:  W08-0909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A measure by Schwarmand Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features.</S> | Reference Offset:  ['31','97'] | Reference Text:  <S sid = 31 ssid = >These methods are quick and easy to calculate but have drawbacks: sentence length is not an accurate measure of syntactic complexity, and syllable count does not necessarily indicate the difficulty of a word.</S><S sid = 97 ssid = >For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1065.txt | Citing Article:  W11-2308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.</S><S sid = 159 ssid = >We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1065.txt | Citing Article:  W12-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.</S><S sid = 159 ssid = >We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1065.txt | Citing Article:  W12-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Schwarm and Ostendorf (2005) implemented four parse tree features (average parse tree height, aver age number of SBARs, NPs per sentence and VPs per sentence) in their work.</S> | Reference Offset:  ['29','141'] | Reference Text:  <S sid = 29 ssid = >The widelyused Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al., 1975) (as cited in (Collins-Thompson and Callan, 2004)).</S><S sid = 141 ssid = >The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1065.txt | Citing Article:  W12-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to verify the impact of our choice of features, we also did a replication of the parsed syntactic feature measures reported by (Schwarm and Ostendorf, 2005) on the WeeklyReader corpus and obtained essentially the same accuracy as the one published (50.7% vs. 50.91%), supporting the comparability of the WeeklyReader data used.</S> | Reference Offset:  ['84','129'] | Reference Text:  <S sid = 84 ssid = >Information gain measures the difference in entropy when w is and is not included as a feature.</S><S sid = 129 ssid = >Although our training corpus is small the feature selection described in Section 4.2 allows us to use these higher-order trigram models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1065.txt | Citing Article:  D08-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.</S><S sid = 159 ssid = >We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1065.txt | Citing Article:  D08-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Syntactic complexity is an obvious factor: indeed (Heilman et al, 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.</S> | Reference Offset:  ['28','118'] | Reference Text:  <S sid = 28 ssid = >Many traditional methods of reading level assessment focus on simple approximations of syntactic complexity such as sentence length.</S><S sid = 118 ssid = >Martin et al. (1997).</S> | Discourse Facet:  NA | Annotator: Automatic


