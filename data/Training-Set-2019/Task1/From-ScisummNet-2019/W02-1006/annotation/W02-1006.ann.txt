Citance Number: 1 | Reference Article:  W02-1006.txt | Citing Article:  P03-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we used the WSD program reported in (Lee and Ng, 2002).</S> | Reference Offset:  ['52','92'] | Reference Text:  <S sid = 52 ssid = >This parameter is also used by (Ng and Lee, 1996).</S><S sid = 92 ssid = >Our reported results in this paper used the linear kernel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-1006.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word.</S> | Reference Offset:  ['3','92'] | Reference Text:  <S sid = 3 ssid = >The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.</S><S sid = 92 ssid = >Our reported results in this paper used the linear kernel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-1006.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus.</S> | Reference Offset:  ['13','14'] | Reference Text:  <S sid = 13 ssid = >Another approach involves the use of dictionary or thesaurus to perform WSD.</S><S sid = 14 ssid = >In this paper, we focus on a corpus-based, supervised learning approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-1006.txt | Citing Article:  W06-2911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output.</S> | Reference Offset:  ['2','77'] | Reference Text:  <S sid = 2 ssid = >Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.</S><S sid = 77 ssid = >We also investigated the effect of feature selection on syntactic-relation features that are words (i.e., POS, voice, and relative position are excluded).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-1006.txt | Citing Article:  W06-2911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM.</S> | Reference Offset:  ['52','153'] | Reference Text:  <S sid = 52 ssid = >This parameter is also used by (Ng and Lee, 1996).</S><S sid = 153 ssid = >Also, using the combination of four knowledge sources gives better performance than using any single individual knowledge source for most algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-1006.txt | Citing Article:  D09-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD.</S> | Reference Offset:  ['28','116'] | Reference Text:  <S sid = 28 ssid = >Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.</S><S sid = 116 ssid = >We ran the different learning algorithms using various knowledge sources.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-1006.txt | Citing Article:  D09-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002).</S> | Reference Offset:  ['60','140'] | Reference Text:  <S sid = 60 ssid = >This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).</S><S sid = 140 ssid = >Note that we are able to obtain state-of-the-art results using a single learning algorithm (SVM), without resorting to combining multiple learning algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-1006.txt | Citing Article:  H05-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation.</S> | Reference Offset:  ['1','108'] | Reference Text:  <S sid = 1 ssid = >In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.</S><S sid = 108 ssid = >Our evaluation is based on all the official training and test data of SENSEVAL-2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-1006.txt | Citing Article:  H05-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.</S><S sid = 159 ssid = >The performance drop from 61.8% may be due to the different collocations used in the two systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-1006.txt | Citing Article:  H05-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations).</S> | Reference Offset:  ['2','145'] | Reference Text:  <S sid = 2 ssid = >Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.</S><S sid = 145 ssid = >In SENSEVAL-1, hopkins used hierarchical decision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996). tilburg also used dictionary examples as additional training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-1006.txt | Citing Article:  P07-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002).</S> | Reference Offset:  ['3','21'] | Reference Text:  <S sid = 3 ssid = >The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.</S><S sid = 21 ssid = >There is a large body of prior research on WSD.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-1006.txt | Citing Article:  P07-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002).</S> | Reference Offset:  ['2','148'] | Reference Text:  <S sid = 2 ssid = >Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.</S><S sid = 148 ssid = >For example, local collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-1006.txt | Citing Article:  C08-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness).</S> | Reference Offset:  ['52','109'] | Reference Text:  <S sid = 52 ssid = >This parameter is also used by (Ng and Lee, 1996).</S><S sid = 109 ssid = >For SENSEVAL-1, we used the 36 trainable words for our evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-1006.txt | Citing Article:  P07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002).</S> | Reference Offset:  ['28','52'] | Reference Text:  <S sid = 28 ssid = >Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.</S><S sid = 52 ssid = >This parameter is also used by (Ng and Lee, 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-1006.txt | Citing Article:  P06-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier.</S> | Reference Offset:  ['115','145'] | Reference Text:  <S sid = 115 ssid = >Otherwise, the classifier for is used.</S><S sid = 145 ssid = >In SENSEVAL-1, hopkins used hierarchical decision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996). tilburg also used dictionary examples as additional training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-1006.txt | Citing Article:  D08-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words.</S> | Reference Offset:  ['2','28'] | Reference Text:  <S sid = 2 ssid = >Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.</S><S sid = 28 ssid = >Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-1006.txt | Citing Article:  D07-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002).</S> | Reference Offset:  ['2','60'] | Reference Text:  <S sid = 2 ssid = >Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.</S><S sid = 60 ssid = >This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-1006.txt | Citing Article:  D07-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['158','159'] | Reference Text:  <S sid = 158 ssid = >Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.</S><S sid = 159 ssid = >The performance drop from 61.8% may be due to the different collocations used in the two systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-1006.txt | Citing Article:  D07-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the same syntactic relations as (Lee and Ng, 2002).</S> | Reference Offset:  ['52','73'] | Reference Text:  <S sid = 52 ssid = >This parameter is also used by (Ng and Lee, 1996).</S><S sid = 73 ssid = >We use different types of syntactic relations, depending on the POS of .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-1006.txt | Citing Article:  D07-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense).</S> | Reference Offset:  ['84','114'] | Reference Text:  <S sid = 84 ssid = >The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes.</S><S sid = 114 ssid = >During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used.</S> | Discourse Facet:  NA | Annotator: Automatic


