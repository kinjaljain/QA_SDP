Citance Number: 1 | Reference Article:  N12-1047.txt | Citing Article:  W12-3160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Cherry and Foster, 2012), which is closer to the usual loss used for max-margin in machine learing.</S> | Reference Offset:  ['36','62'] | Reference Text:  <S sid = 36 ssid = >The loss `i(~w) is 0 only if w~ separates each e ∈ Ei from ez by a margin proportional to their BLEU differentials.</S><S sid = 62 ssid = >In terms of (4), PRO defines where (x)+ = max(0, x).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N12-1047.txt | Citing Article:  N12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cherry and Foster (2012) have concurrently performed a similar analysis.</S> | Reference Offset:  ['115','130'] | Reference Text:  <S sid = 115 ssid = >To solve the necessary quadratic programming sub-problems, we use a multiclass SVM similar to LIBLINEAR (Hsieh et al., 2008).</S><S sid = 130 ssid = >We tested 20, 15, 10, 8 and 5 shards during development. tings that performed well in general.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N12-1047.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The system was tuned with batch lattice MIRA (Cherry and Foster, 2012).</S> | Reference Offset:  ['120','193'] | Reference Text:  <S sid = 120 ssid = >These methods can be split into two groups: MIRA variants (online, batch k-best, batch lattice), and direct optimizers (PRO, MR and SVM).</S><S sid = 193 ssid = >Our experiments show Batch Lattice MIRA to be the most consistent of the tested methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N12-1047.txt | Citing Article:  W12-3135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al, 2008) and batch MIRA (Cherry and Foster, 2012).</S> | Reference Offset:  ['6','117'] | Reference Text:  <S sid = 6 ssid = >The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003).</S><S sid = 117 ssid = >We have reviewed three tuning methods and introduced three tuning methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N12-1047.txt | Citing Article:  P13-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al, 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models.</S> | Reference Offset:  ['34','86'] | Reference Text:  <S sid = 34 ssid = >First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al.</S><S sid = 86 ssid = >This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N12-1047.txt | Citing Article:  P13-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, we ran the k-best batchMIRA (kbMIRA) (Cherry and Foster, 2012) implementation in Moses.</S> | Reference Offset:  ['86','125'] | Reference Text:  <S sid = 86 ssid = >This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation.</S><S sid = 125 ssid = >To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N12-1047.txt | Citing Article:  P13-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cherry and Foster (2012) reported the same result, and their implementation is available in Moses.</S> | Reference Offset:  ['86','141'] | Reference Text:  <S sid = 86 ssid = >This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation.</S><S sid = 141 ssid = >Six-feature lexicalized distortion models were estimated and applied as in Moses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N12-1047.txt | Citing Article:  P13-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines.</S> | Reference Offset:  ['7','14'] | Reference Text:  <S sid = 7 ssid = >However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces.</S><S sid = 14 ssid = >Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s established outer loop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N12-1047.txt | Citing Article:  P13-2071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012).</S> | Reference Offset:  ['94','120'] | Reference Text:  <S sid = 94 ssid = >Batch k-best MIRA inherits all of the MERT architecture.</S><S sid = 120 ssid = >These methods can be split into two groups: MIRA variants (online, batch k-best, batch lattice), and direct optimizers (PRO, MR and SVM).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N12-1047.txt | Citing Article:  P13-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See (Cherry and Foster, 2012) for details on objectives.</S> | Reference Offset:  ['55','195'] | Reference Text:  <S sid = 55 ssid = >Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA.</S><S sid = 195 ssid = >Thanks to Mark Hopkins, Zhifei Li and Jonathan May for their advice while implementing the methods in this review, and to Kevin Gimpel, Roland Kuhn and the anonymous reviewers for their valuable comments on an earlier draft.</S> | Discourse Facet:  NA | Annotator: Automatic


