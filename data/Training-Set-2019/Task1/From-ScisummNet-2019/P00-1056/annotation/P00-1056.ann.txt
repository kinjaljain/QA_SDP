Citance Number: 1 | Reference Article:  P00-1056.txt | Citing Article:  P01-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000).</S> | Reference Offset:  ['10','23'] | Reference Text:  <S sid = 10 ssid = >An alignment Ã¢ for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model.</S><S sid = 23 ssid = >So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P00-1056.txt | Citing Article:  P14-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003).</S> | Reference Offset:  ['8','17'] | Reference Text:  <S sid = 8 ssid = >We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).</S><S sid = 17 ssid = >In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P00-1056.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively.</S> | Reference Offset:  ['1','12'] | Reference Text:  <S sid = 1 ssid = >In this paper, we present and compare various single-word based alignment models for statistical machine translation.</S><S sid = 12 ssid = >For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P00-1056.txt | Citing Article:  W02-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002).</S> | Reference Offset:  ['8','17'] | Reference Text:  <S sid = 8 ssid = >We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).</S><S sid = 17 ssid = >In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P00-1056.txt | Citing Article:  W02-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment.</S> | Reference Offset:  ['4','15'] | Reference Text:  <S sid = 4 ssid = >As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.</S><S sid = 15 ssid = >This allows an automatic evaluation, once a reference alignment has been produced.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P00-1056.txt | Citing Article:  W02-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b).</S> | Reference Offset:  ['19','26'] | Reference Text:  <S sid = 19 ssid = >The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.</S><S sid = 26 ssid = >Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P00-1056.txt | Citing Article:  W02-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)).</S> | Reference Offset:  ['8','28'] | Reference Text:  <S sid = 8 ssid = >We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).</S><S sid = 28 ssid = >The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P00-1056.txt | Citing Article:  W02-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b).</S> | Reference Offset:  ['19','26'] | Reference Text:  <S sid = 19 ssid = >The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.</S><S sid = 26 ssid = >Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P00-1056.txt | Citing Article:  W02-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The performance of the four decoders was measured with respect to the alignments provided by human experts (Och and Ney, 2000b).</S> | Reference Offset:  ['3','36'] | Reference Text:  <S sid = 3 ssid = >We present different methods to combine alignments.</S><S sid = 36 ssid = >Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EMalgorithm, the counts are collected only over a subset of promising alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P00-1056.txt | Citing Article:  W05-0823.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000).</S> | Reference Offset:  ['7','28'] | Reference Text:  <S sid = 7 ssid = >In (statistical) alignment models Pr(fil , aflef), a 'hidden' alignment a is introduced which describes a mapping from source word fi to a target word Ca,.</S><S sid = 28 ssid = >The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P00-1056.txt | Citing Article:  W09-0435.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003).</S> | Reference Offset:  ['8','17'] | Reference Text:  <S sid = 8 ssid = >We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).</S><S sid = 17 ssid = >In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P00-1056.txt | Citing Article:  P05-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures.</S> | Reference Offset:  ['11','28'] | Reference Text:  <S sid = 11 ssid = >So far, no well established evaluation criterion exists in the literature for these alignment models.</S><S sid = 28 ssid = >The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P00-1056.txt | Citing Article:  P10-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf).</S> | Reference Offset:  ['4','17'] | Reference Text:  <S sid = 4 ssid = >As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.</S><S sid = 17 ssid = >In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P00-1056.txt | Citing Article:  P03-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences.</S> | Reference Offset:  ['4','14'] | Reference Text:  <S sid = 4 ssid = >As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.</S><S sid = 14 ssid = >We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P00-1056.txt | Citing Article:  P03-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences.</S> | Reference Offset:  ['4','14'] | Reference Text:  <S sid = 4 ssid = >As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.</S><S sid = 14 ssid = >We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P00-1056.txt | Citing Article:  P03-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['19','40'] | Reference Text:  <S sid = 19 ssid = >The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.</S><S sid = 40 ssid = >This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P00-1056.txt | Citing Article:  P03-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4.</S> | Reference Offset:  ['5','17'] | Reference Text:  <S sid = 5 ssid = >We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.</S><S sid = 17 ssid = >In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P00-1056.txt | Citing Article:  P03-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This demonstrates that we are competitive with the methods described in (Och and Ney, 2000).</S> | Reference Offset:  ['3','17'] | Reference Text:  <S sid = 3 ssid = >We present different methods to combine alignments.</S><S sid = 17 ssid = >In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P00-1056.txt | Citing Article:  W08-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['19','40'] | Reference Text:  <S sid = 19 ssid = >The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.</S><S sid = 40 ssid = >This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P00-1056.txt | Citing Article:  P11-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004).</S> | Reference Offset:  ['1','19'] | Reference Text:  <S sid = 1 ssid = >In this paper, we present and compare various single-word based alignment models for statistical machine translation.</S><S sid = 19 ssid = >The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.</S> | Discourse Facet:  NA | Annotator: Automatic


