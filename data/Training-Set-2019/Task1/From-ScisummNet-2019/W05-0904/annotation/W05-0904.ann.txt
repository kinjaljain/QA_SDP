Citance Number: 1 | Reference Article:  W05-0904.txt | Citing Article:  P06-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics.</S> | Reference Offset:  ['30','127'] | Reference Text:  <S sid = 30 ssid = >However, if we evaluate their fluency based on the syntactic similarity with the reference, we will get our desired results.</S><S sid = 127 ssid = >The syntactic metrics, except the kernel based ones, all outperform BLEU in sentence-level fluency evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W05-0904.txt | Citing Article:  P14-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR.</S> | Reference Offset:  ['92','128'] | Reference Text:  <S sid = 92 ssid = >The results show that in both systems our syntactic metrics all achieve a better performance in the correlation with human judgments of fluency.</S><S sid = 128 ssid = >For the overall evaluation of sentences for fluency and adequacy, the metric based on headword chain performs better than BLEU in both sentencelevel and corpus-level correlation with human judgments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W05-0904.txt | Citing Article:  W12-3117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation.</S> | Reference Offset:  ['1','2'] | Reference Text:  <S sid = 1 ssid = >Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.</S><S sid = 2 ssid = >We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W05-0904.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarities are captured from different viewpoints: DP-HWC (i) -l This metric corresponds to the HWC metric presented by Liu and Gildea (2005).</S> | Reference Offset:  ['25','75'] | Reference Text:  <S sid = 25 ssid = >For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.</S><S sid = 75 ssid = >For this reason, the two methods described in section 3.1 are used to compute the similarity of dependency trees between the MT hypothesis and its references, and the corresponding metrics are denoted DSTM for dependency subtree metric and DTKM for dependency tree kernel metric.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W05-0904.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This metric corresponds to the STM metric presented by Liu and Gildea (2005).</S> | Reference Offset:  ['25','49'] | Reference Text:  <S sid = 25 ssid = >For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.</S><S sid = 49 ssid = >Therefore, the final score of STM is (6/7+3/4+1/2)/3=0.702.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W05-0904.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >Syntactic Features For Evaluation Of Machine Translation</S><S sid = 3 ssid = >Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W05-0904.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser.</S> | Reference Offset:  ['58','60'] | Reference Text:  <S sid = 58 ssid = >Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.</S><S sid = 60 ssid = >We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W05-0904.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc.</S> | Reference Offset:  ['2','58'] | Reference Text:  <S sid = 2 ssid = >We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.</S><S sid = 58 ssid = >Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W05-0904.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005).</S> | Reference Offset:  ['87','96'] | Reference Text:  <S sid = 87 ssid = >To do this, the syntactic metrics (computed with the Collins (1999) parser) as well as BLEU were used to evaluate hypotheses in the test set from ACL05 MT workshop, which provides both fluency and adequacy scores for each sentence, and their Pearson coefficients of correlation with the human fluency scores were computed.</S><S sid = 96 ssid = >In our experiments, every hypothesis is evaluated by referring to three human translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W05-0904.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This finding has been previously reported, among others, in Liu and Gildea (2005).</S> | Reference Offset:  ['43','138'] | Reference Text:  <S sid = 43 ssid = >Since PRON only occurs once in the reference, its clipped count should be 1 rather than 2.</S><S sid = 138 ssid = >This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W05-0904.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric.</S> | Reference Offset:  ['18','119'] | Reference Text:  <S sid = 18 ssid = >As MT systems improve, the shortcomings of n-gram based evaluation are becoming more apparent.</S><S sid = 119 ssid = >This shows that corpus-level evaluation, compared with the sentence-level evaluation, is much less sensitive to the sparse data problem and thus leaves more space for making use of comprehensive evaluation metrics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W05-0904.txt | Citing Article:  W10-1750.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees.</S> | Reference Offset:  ['60','100'] | Reference Text:  <S sid = 60 ssid = >We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999).</S><S sid = 100 ssid = >A similar trend can be found in syntax tree and dependency tree based metrics, but the decreasing ratios are much lower than BLEU, which indicates that the syntactic metrics are less affected by the sparse data problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W05-0904.txt | Citing Article:  W08-0332.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use three different kinds of metrics: DR-STM Semantic Tree Matching, a la Liu and Gildea (2005), but over DRS instead of over constituency trees.</S> | Reference Offset:  ['55','65'] | Reference Text:  <S sid = 55 ssid = >In order to derive a similarity measure ranging from zero to one, we use the cosine of the vectors H: we can compute the cosine similarity using the kernel method, without ever computing the entire of vector of counts H. Our kernel-based subtree metric TKM is then defined as the maximum of the cosine measure over the references: The advantage of using the tree kernel is that it can capture the similarity of subtrees of different shapes; the weak point is that it can only use the reference trees one by one, while STM can use them simultaneously.</S><S sid = 65 ssid = >The dependency tree contains both the lexical and syntactic information, which inspires us to use it for the MT evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W05-0904.txt | Citing Article:  P08-3005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005).</S> | Reference Offset:  ['21','130'] | Reference Text:  <S sid = 21 ssid = >Evidence that we are reaching the limits of ngram based evaluation was provided by Charniak et al. (2003), who found that a syntax-based language model improved the fluency and semantic accuracy of their system, but lowered their BLEU score.</S><S sid = 130 ssid = >Our syntax-based measures require the existence of a parser for the language in question, however it is worth noting that a parser is required for the target language only, as all our measures of similarity are defined across hypotheses and references in the same language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W05-0904.txt | Citing Article:  P08-3005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set.</S> | Reference Offset:  ['59','73'] | Reference Text:  <S sid = 59 ssid = >Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al. (2000) and Lin (2004).</S><S sid = 73 ssid = >Using HWCM to denote the headword chain based metric, it is computed as follows: where D is chosen as the maximum length chain considered.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W05-0904.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005).</S> | Reference Offset:  ['19','102'] | Reference Text:  <S sid = 19 ssid = >State-of-the-art MT output often contains roughly the correct words and concepts, but does not form a coherent sentence.</S><S sid = 102 ssid = >Though our syntactic metrics are proposed for evaluating the sentences’ fluency, we are curious how well they do in the overall evaluation of sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W05-0904.txt | Citing Article:  P07-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees.</S> | Reference Offset:  ['25','58'] | Reference Text:  <S sid = 25 ssid = >For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.</S><S sid = 58 ssid = >Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W05-0904.txt | Citing Article:  P07-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This phenomenon has been previously observed by Liu and Gildea (2005).</S> | Reference Offset:  ['43','138'] | Reference Text:  <S sid = 43 ssid = >Since PRON only occurs once in the reference, its clipped count should be 1 rather than 2.</S><S sid = 138 ssid = >This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W05-0904.txt | Citing Article:  W10-1753.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching.</S> | Reference Offset:  ['3','65'] | Reference Text:  <S sid = 3 ssid = >Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.</S><S sid = 65 ssid = >The dependency tree contains both the lexical and syntactic information, which inspires us to use it for the MT evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W05-0904.txt | Citing Article:  W10-1753.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement.</S> | Reference Offset:  ['6','10'] | Reference Text:  <S sid = 6 ssid = >The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on the assumption that “The closer a machine translation is to a professional human translation, the better it is” (Papineni et al., 2002).</S><S sid = 10 ssid = >BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities (Papineni et al., 2002; Doddington, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


