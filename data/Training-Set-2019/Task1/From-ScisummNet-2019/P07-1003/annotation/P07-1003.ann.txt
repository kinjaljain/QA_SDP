Citance Number: 1 | Reference Article:  P07-1003.txt | Citing Article:  W08-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates.</S> | Reference Offset:  ['21','110'] | Reference Text:  <S sid = 21 ssid = >Our model generates word alignments that better respect the parse trees upon which they are conditioned, without sacrificing alignment quality.</S><S sid = 110 ssid = >This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1003.txt | Citing Article:  P10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline.</S> | Reference Offset:  ['100','118'] | Reference Text:  <S sid = 100 ssid = >Using the simple but effective joint training technique of Liang et al. (2006), we initialized the model with lexical parameters from a jointly trained implementation of IBM Model 1.</S><S sid = 118 ssid = >Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1003.txt | Citing Article:  P09-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary.</S> | Reference Offset:  ['116','132'] | Reference Text:  <S sid = 116 ssid = >For Chinese, we trained on the FBIS corpus and the LDC bilingual dictionary, then tested on 491 hand-aligned sentences from the 2002 Hansards data from the NAACL 2003 Shared Task.3 We trained on 100k sentences for each language.</S><S sid = 132 ssid = >We observed a similar 50% reduction for the Chinese data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1003.txt | Citing Article:  P09-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features.</S> | Reference Offset:  ['84','118'] | Reference Text:  <S sid = 84 ssid = >However, we found this variant to slightly underperform the full model described above.</S><S sid = 118 ssid = >Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1003.txt | Citing Article:  P09-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','141'] | Reference Text:  <S sid = 44 ssid = >Figure 1B exposes the consequences: a wide array of desired rules are lost during extraction.</S><S sid = 141 ssid = >While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1003.txt | Citing Article:  P10-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007).</S> | Reference Offset:  ['83','132'] | Reference Text:  <S sid = 83 ssid = >This model can be simplified by removing all conditioning on node types.</S><S sid = 132 ssid = >We observed a similar 50% reduction for the Chinese data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1003.txt | Citing Article:  P10-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.</S> | Reference Offset:  ['61','111'] | Reference Text:  <S sid = 61 ssid = >However, its distortion model considers only string distance, disregarding the constituent structure of the English sentence.</S><S sid = 111 ssid = >Lopez and Resnik (2005) considers a simpler tree distance distortion model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1003.txt | Citing Article:  D10-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007).</S> | Reference Offset:  ['37','40'] | Reference Text:  <S sid = 37 ssid = >Consider the example sentence in figure 1A, which demonstrates how a particular type of alignment error prevents the extraction of many useful transducer rules.</S><S sid = 40 ssid = >While alignment errors are undesirable in general, this error is particularly problematic for a syntax-based translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1003.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006).</S> | Reference Offset:  ['22','122'] | Reference Text:  <S sid = 22 ssid = >Using the joint training technique of Liang et al. (2006) to initialize the model parameters, we achieve an AER superior to the GIZA++ implementation of IBM model 4 (Och and Ney, 2003) and a reduction of 56.3% in aligned interior nodes, a measure of agreement between alignments and parses.</S><S sid = 122 ssid = >Our models substantially outperform GIZA++, confirming results in Liang et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1003.txt | Citing Article:  D09-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments.</S> | Reference Offset:  ['40','110'] | Reference Text:  <S sid = 40 ssid = >While alignment errors are undesirable in general, this error is particularly problematic for a syntax-based translation system.</S><S sid = 110 ssid = >This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1003.txt | Citing Article:  D09-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','141'] | Reference Text:  <S sid = 44 ssid = >Figure 1B exposes the consequences: a wide array of desired rules are lost during extraction.</S><S sid = 141 ssid = >While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1003.txt | Citing Article:  N12-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus.</S> | Reference Offset:  ['2','50'] | Reference Text:  <S sid = 2 ssid = >We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.</S><S sid = 50 ssid = >This alignment pattern was observed in our test set and corrected by our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1003.txt | Citing Article:  N12-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','141'] | Reference Text:  <S sid = 44 ssid = >Figure 1B exposes the consequences: a wide array of desired rules are lost during extraction.</S><S sid = 141 ssid = >While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.</S> | Discourse Facet:  NA | Annotator: Automatic


