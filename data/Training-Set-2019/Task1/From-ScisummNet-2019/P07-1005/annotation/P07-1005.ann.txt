Citance Number: 1 | Reference Article:  P07-1005.txt | Citing Article:  P10-4014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1005.txt | Citing Article:  P10-4014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A previous implementation of the IMS system, NUS-PT (Chan et al, 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and fine grained task, respectively.</S> | Reference Offset:  ['19','87'] | Reference Text:  <S sid = 19 ssid = >In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task.</S><S sid = 87 ssid = >Hereafter, we will use symbols to represent the Chinese and English words in the rule: c1, c2, and c3 will represent the Chinese words “4”, “�”, and “YI” respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1005.txt | Citing Article:  D09-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, (Chan et al, 2007) trained a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier.</S> | Reference Offset:  ['63','138'] | Reference Text:  <S sid = 63 ssid = >Using the WSD classifier described in Section 2, we classified the words in each Chinese source sentence to be translated.</S><S sid = 138 ssid = >For example, for the Chinese sentence “.. .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1005.txt | Citing Article:  D12-1128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1005.txt | Citing Article:  D08-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chan et al (2007) use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models.</S> | Reference Offset:  ['26','30'] | Reference Text:  <S sid = 26 ssid = >For our the decoder could weigh the additional alternative experiments, we use the SVM implementation of translations against its own.</S><S sid = 30 ssid = >For local 2005) did not use a state-of-the-art MT system, collocations, we use 3 features, w_1w+1, w_1, and while the experiments in (Vickrey et al., 2005) were w+1, where w_1 (w+1) is the token immediately to not done using a full-fledged MT system and the the left (right) of the current ambiguous word ocevaluation was not on how well each source sentence currence w. For parts-of-speech, we use 3 features, was translated as a whole.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1005.txt | Citing Article:  P11-2056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1005.txt | Citing Article:  N09-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collocations of words are used to refine phrase pair selection dynamically by incorporating scores from the WSD classifier (Chan et al, 2007).</S> | Reference Offset:  ['6','24'] | Reference Text:  <S sid = 6 ssid = >Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.</S><S sid = 24 ssid = >Pharaoh, a state-of-the-art phrase-based MT sys- 2 Word Sense Disambiguation tem (Koehn et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1005.txt | Citing Article:  P10-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1005.txt | Citing Article:  I08-2105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has long been believed that being able to detect the correct sense of a word in a given context - performing word sense disambiguation (WSD) - will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al, 2007) and summarization (Elhadad et al., 1997).</S> | Reference Offset:  ['0','6'] | Reference Text:  <S sid = 0 ssid = >Word Sense Disambiguation Improves Statistical Machine Translation</S><S sid = 6 ssid = >Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1005.txt | Citing Article:  D11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6).</S> | Reference Offset:  ['6','10'] | Reference Text:  <S sid = 6 ssid = >Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.</S><S sid = 10 ssid = >To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1005.txt | Citing Article:  D11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a set of word context features motivated by word sense disambiguation (Chan et al, 2007) to test scalability.</S> | Reference Offset:  ['0','6'] | Reference Text:  <S sid = 0 ssid = >Word Sense Disambiguation Improves Statistical Machine Translation</S><S sid = 6 ssid = >Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1005.txt | Citing Article:  N10-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Independent of these lexical substitution tasks, the connection between word senses and word translation has been explored in Chan et al (2007) and Carpuat and Wu (2007), who predict the probabilities of a target word being translated as an item in a sense inventory, where the sense inventory is a list of possible translations.</S> | Reference Offset:  ['6','60'] | Reference Text:  <S sid = 6 ssid = >Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.</S><S sid = 60 ssid = >Hence, unlike traditional WSD where the sense classes are tied to a specific sense inventory, our “senses” here consist of the English phrases extracted as translations for each Chinese phrase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1005.txt | Citing Article:  P14-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-artWSD system into a hierarchical phrase-based system (Chiang, 2005).</S> | Reference Offset:  ['2','13'] | Reference Text:  <S sid = 2 ssid = >In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero.</S><S sid = 13 ssid = >Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1005.txt | Citing Article:  W11-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1005.txt | Citing Article:  C08-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chan et al (2007) incorporated a WSD system into the hierarchical SMT system, Hiero (Chiang, 2005), and reported statistically significant improvement.</S> | Reference Offset:  ['4','149'] | Reference Text:  <S sid = 4 ssid = >Furthermore, the improvement is statistically significant.</S><S sid = 149 ssid = >We have shown that WSD improves the translation performance of a state-of-the-art hierarchical phrase-based statistical MT system and this improvement is statistically significant.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1005.txt | Citing Article:  P11-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1005.txt | Citing Article:  W08-0302.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al, 2007).</S> | Reference Offset:  ['0','147'] | Reference Text:  <S sid = 0 ssid = >Word Sense Disambiguation Improves Statistical Machine Translation</S><S sid = 147 ssid = >A hierarchical phrase-based model for statistical machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1005.txt | Citing Article:  C10-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT.</S> | Reference Offset:  ['6','24'] | Reference Text:  <S sid = 6 ssid = >Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.</S><S sid = 24 ssid = >Pharaoh, a state-of-the-art phrase-based MT sys- 2 Word Sense Disambiguation tem (Koehn et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1005.txt | Citing Article:  D08-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','156'] | Reference Text:  <S sid = 48 ssid = >A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.</S><S sid = 156 ssid = >David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1005.txt | Citing Article:  D08-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We note that the best performing system (Chan et al, 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity.</S> | Reference Offset:  ['19','35'] | Reference Text:  <S sid = 19 ssid = >In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task.</S><S sid = 35 ssid = >We obtain accuracy that ing two additional features into the MT model which compares favorably to the best participating system operate on the existing rules of the grammar, with- in the task (Carpuat et al., 2004). out introducing competing rules.</S> | Discourse Facet:  NA | Annotator: Automatic


