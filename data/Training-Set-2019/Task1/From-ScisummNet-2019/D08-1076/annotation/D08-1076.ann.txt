Citance Number: 1 | Reference Article:  D08-1076.txt | Citing Article:  W09-0432.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.</S> | Reference Offset:  ['171','177'] | Reference Text:  <S sid = 171 ssid = >The performance drop in iteration 1 is also attributed to overfitting the candidate repository.</S><S sid = 177 ssid = >The reduced performance for N-best MERT is a consequence of the performance drop in the first iteration which causes the final weights to be far off from the initial parameter set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1076.txt | Citing Article:  W12-3159.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009).</S> | Reference Offset:  ['61','142'] | Reference Text:  <S sid = 61 ssid = >For a description on how to generate lattices, see (Ueffing et al., 2002).</S><S sid = 142 ssid = >In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1076.txt | Citing Article:  W12-3159.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference.</S> | Reference Offset:  ['0','142'] | Reference Text:  <S sid = 0 ssid = >Lattice-based Minimum Error Rate Training for Statistical Machine Translation</S><S sid = 142 ssid = >In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice.</S> | Reference Offset:  ['67','102'] | Reference Text:  <S sid = 67 ssid = >To develop the algorithm for computing the upper envelope of all translation hypotheses that are encoded in a phrase lattice, we first consider a node vPVf with some incoming and outgoing arcs: Each path that starts at the source node s and ends in v defines a partial translation hypothesis which can be represented as a line (cf.</S><S sid = 102 ssid = >The memory efficiency of the suggested algorithm results from the following theorem which provides a novel upper bound for the number of cost minimizing paths in a directed acyclic graph with arcspecific affine cost functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1).</S> | Reference Offset:  ['46','70'] | Reference Text:  <S sid = 46 ssid = >An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).</S><S sid = 70 ssid = >We now assume that the upper envelope for these partial translation hypotheses is known.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Macherey et al (2008) use methods from computational geometry to compute the upper envelope.</S> | Reference Offset:  ['46','50'] | Reference Text:  <S sid = 46 ssid = >An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).</S><S sid = 50 ssid = >By construction, the upper envelope consists of at most K line segments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs.</S> | Reference Offset:  ['22','187'] | Reference Text:  <S sid = 22 ssid = >As a consequence, the line optimization algorithm needs to repeatedly translate the development corpus and enlarge the candidate repositories with newly found hypotheses in order to avoid overfitting on Cs and preventing the optimization procedure from stopping in a poor local optimum.</S><S sid = 187 ssid = >One possible usecase is the computation of consensus translations from the outputs of multiple machine translation systems where this framework allows us to estimate the system prior weights directly on confusion networks (Rosti et al., 2007; Macherey and Och, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008).</S> | Reference Offset:  ['46','142'] | Reference Text:  <S sid = 46 ssid = >An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).</S><S sid = 142 ssid = >In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice.</S> | Reference Offset:  ['102','120'] | Reference Text:  <S sid = 102 ssid = >The memory efficiency of the suggested algorithm results from the following theorem which provides a novel upper bound for the number of cost minimizing paths in a directed acyclic graph with arcspecific affine cost functions.</S><S sid = 120 ssid = >This convex hull is again a convex polygon which consists of at most N1 + N2 edges, and therefore, the number of cost minimizing paths in G1' (and thus also in G) is upper bounded by NiN2.� ❑ Corollary: The upper envelope for a phrase lattice Gf~p = (Vf, Efq) consists of at most |IEf |I line segments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT.</S> | Reference Offset:  ['160','174'] | Reference Text:  <S sid = 160 ssid = >In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT.</S><S sid = 174 ssid = >This is illustrated in Figure 3, where lattice MERT and N-best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1076.txt | Citing Article:  W12-6219.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008).</S> | Reference Offset:  ['33','176'] | Reference Text:  <S sid = 33 ssid = >Section 7 reports on experiments conducted on the NIST 2008 translation tasks.</S><S sid = 176 ssid = >While only the aren task shows improvements on the development data, lattice MERT provides consistent gains over N-best MERT on all three blind test sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1076.txt | Citing Article:  W10-1702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Weights on feature functions are found by lattice MERT (Macherey et al, 2008).</S> | Reference Offset:  ['135','148'] | Reference Text:  <S sid = 135 ssid = >While this update scheme provides a ranking of the feature functions according to their discriminative power (each iteration picks the feature function for which changing the corresponding weight yields the highest gain), it does not take possible correlations between the feature functions into account.</S><S sid = 148 ssid = >The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1076.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008).</S> | Reference Offset:  ['6','189'] | Reference Text:  <S sid = 6 ssid = >The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.</S><S sid = 189 ssid = >Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1076.txt | Citing Article:  D11-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008).</S> | Reference Offset:  ['0','184'] | Reference Text:  <S sid = 0 ssid = >Lattice-based Minimum Error Rate Training for Statistical Machine Translation</S><S sid = 184 ssid = >The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1076.txt | Citing Article:  W09-0426.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008).</S> | Reference Offset:  ['19','184'] | Reference Text:  <S sid = 19 ssid = >The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum.</S><S sid = 184 ssid = >The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1076.txt | Citing Article:  W09-0426.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008).</S> | Reference Offset:  ['184','186'] | Reference Text:  <S sid = 184 ssid = >The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.</S><S sid = 186 ssid = >While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1076.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it.</S> | Reference Offset:  ['4','20'] | Reference Text:  <S sid = 4 ssid = >Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder.</S><S sid = 20 ssid = >Candidate translations in MERT are typically represented as N-best lists which contain the N most probable translation hypotheses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1076.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008).</S> | Reference Offset:  ['142','144'] | Reference Text:  <S sid = 142 ssid = >In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).</S><S sid = 144 ssid = >A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1076.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only.</S> | Reference Offset:  ['142','174'] | Reference Text:  <S sid = 142 ssid = >In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).</S><S sid = 174 ssid = >This is illustrated in Figure 3, where lattice MERT and N-best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1076.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008).</S> | Reference Offset:  ['60','142'] | Reference Text:  <S sid = 60 ssid = >In this section, the algorithm for computing the upper envelope on N-best lists is extended to phrase lattices.</S><S sid = 142 ssid = >In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).</S> | Discourse Facet:  NA | Annotator: Automatic


