Citance Number: 1 | Reference Article:  N06-1056.txt | Citing Article:  P06-1115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser.</S> | Reference Offset:  ['13','155'] | Reference Text:  <S sid = 13 ssid = >In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules (Kate et al., 2005).</S><S sid = 155 ssid = >Figure 6 shows the performance of WASP compared to four other algorithms: SILT (Kate et al., 2005), COCKTAIL (Tang and Mooney, 2001), SCISSOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N06-1056.txt | Citing Article:  P09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006).</S> | Reference Offset:  ['122','155'] | Reference Text:  <S sid = 122 ssid = >A similar feature set is used by Zettlemoyer and Collins (2005).</S><S sid = 155 ssid = >Figure 6 shows the performance of WASP compared to four other algorithms: SILT (Kate et al., 2005), COCKTAIL (Tang and Mooney, 2001), SCISSOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N06-1056.txt | Citing Article:  P09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing.</S> | Reference Offset:  ['132','147'] | Reference Text:  <S sid = 132 ssid = >Following Zettlemoyer and Collins (2005), only rules that are used in the best parses for the training set are retained in the final lexicon.</S><S sid = 147 ssid = >A semantic parser was learned from the training set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N06-1056.txt | Citing Article:  D08-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques.</S> | Reference Offset:  ['0','175'] | Reference Text:  <S sid = 0 ssid = >Learning For Semantic Parsing With Statistical Machine Translation</S><S sid = 175 ssid = >Statistical machine translation by parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N06-1056.txt | Citing Article:  D08-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006).</S> | Reference Offset:  ['121','146'] | Reference Text:  <S sid = 121 ssid = >The number of features is quite modest (less than 3,000 in our experiments).</S><S sid = 146 ssid = >Standard 10-fold cross validation was used in our experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N06-1056.txt | Citing Article:  P14-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia).</S> | Reference Offset:  ['32','122'] | Reference Text:  <S sid = 32 ssid = >The second domain is GEOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Zelle and Mooney, 1996; Kate et al., 2005).</S><S sid = 122 ssid = >A similar feature set is used by Zettlemoyer and Collins (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N06-1056.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','177'] | Reference Text:  <S sid = 44 ssid = >The right-hand side (RHS) of an SCFG rule is a pair of strings, (α, Q), where the non-terminals in 0 are a permutation of the non-terminals in α.</S><S sid = 177 ssid = >This research was supported by Defense Advanced Research Projects Agency under grant HR0011-041-0007.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL).</S> | Reference Offset:  ['167','175'] | Reference Text:  <S sid = 167 ssid = >Our work shows that ideas from compiler theory (SCFG) and machine translation (word alignment models) can be successfully applied to semantic parsing, a closelyrelated task whose goal is to translate a natural language into a formal language.</S><S sid = 175 ssid = >Statistical machine translation by parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)).</S> | Reference Offset:  ['32','138'] | Reference Text:  <S sid = 32 ssid = >The second domain is GEOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Zelle and Mooney, 1996; Kate et al., 2005).</S><S sid = 138 ssid = >To build a corpus for GEOQUERY, 880 English questions were gathered from various sources, which were manually translated into the functional GEOQUERY language (Tang and Mooney, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG.</S> | Reference Offset:  ['14','20'] | Reference Text:  <S sid = 14 ssid = >The algorithm learns a semantic parser given a set of NL sentences annotated with their correct MRs.</S><S sid = 20 ssid = >Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic.</S> | Reference Offset:  ['15','32'] | Reference Text:  <S sid = 15 ssid = >It requires no prior knowledge of the NL syntax, although it assumes that an unambiguous, context-free grammar (CFG) of the target MRL is available.</S><S sid = 32 ssid = >The second domain is GEOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Zelle and Mooney, 1996; Kate et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence.</S> | Reference Offset:  ['117','125'] | Reference Text:  <S sid = 117 ssid = >We propose a maximum-entropy model that defines a conditional probability distribution over derivations (d) given the observed NL string (e): where fi is a feature function, and Zλ(e) is a normalizing factor.</S><S sid = 125 ssid = >The maximum conditional likelihood criterion is used for estimating the model parameters, Ai.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For details regarding non-isomorphic NL/MR parse trees, removal of bad links from alignments, and extraction of word gaps (e.g. the token (1) in the last rule of Figure 3), see Wong and Mooney (2006).</S> | Reference Offset:  ['45','111'] | Reference Text:  <S sid = 45 ssid = >Below are some SCFG rules that can be used for generating the parse trees in Figure 3: Each SCFG rule X → (α, Q) is a combination of a production of the NL semantic grammar, X → α, and a production of the MRL grammar, X → Q.</S><S sid = 111 ssid = >Given an alignment, a, we count the number of links that would prevent a rule from being extracted for each production in the MR parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N06-1056.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006).</S> | Reference Offset:  ['94','138'] | Reference Text:  <S sid = 94 ssid = >For each of these productions, X —* Q, a rule X —* (α, Q) is extracted such that α consists of the words to which the production is linked, e.g.</S><S sid = 138 ssid = >To build a corpus for GEOQUERY, 880 English questions were gathered from various sources, which were manually translated into the functional GEOQUERY language (Tang and Mooney, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N06-1056.txt | Citing Article:  P13-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL.</S> | Reference Offset:  ['166','172'] | Reference Text:  <S sid = 166 ssid = >It is also similar to the hierarchical phrase-based model of Chiang (2005), in which hierarchical phrase pairs, essentially SCFG rules, are learned through the use of a simpler, phrase-based alignment model.</S><S sid = 172 ssid = >In the future, we would like to develop a word-based alignment model that is aware of the MRL syntax, so that better lexicons can be learned.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N06-1056.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string.</S> | Reference Offset:  ['56','58'] | Reference Text:  <S sid = 56 ssid = >, ((bowner our {4}) (do our {6} (pos (left (half our)))))) Here the MR string is said to be a translation of the NL string.</S><S sid = 58 ssid = >The semantic parsing model of WASP thus consists of an SCFG, G, and a probabilistic model, parameterized by A, that takes a possible derivation, d, and returns its likelihood of being correct given an input sentence, e. The output translation, f*, for a sentence, e, is defined as: where m(d) is the MR string that a derivation d yields, and D(G|e) is the set of all possible derivations of G that yield e. In other words, the output MR is the yield of the most probable derivation that yields e in the NL stream.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N06-1056.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers.</S> | Reference Offset:  ['34','88'] | Reference Text:  <S sid = 34 ssid = >To describe the semantic parsing model of WASP, it is best to start with an example.</S><S sid = 88 ssid = >Note that the structure of a parse tree is preserved through linearization, and for each MR there is a unique linearized parse, since the MRL grammar is unambiguous.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N06-1056.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish.</S> | Reference Offset:  ['138','140'] | Reference Text:  <S sid = 138 ssid = >To build a corpus for GEOQUERY, 880 English questions were gathered from various sources, which were manually translated into the functional GEOQUERY language (Tang and Mooney, 2001).</S><S sid = 140 ssid = >250 of the queries were also translated into Spanish, Japanese and Turkish, resulting in a smaller, multilingual data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N06-1056.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system.</S> | Reference Offset:  ['8','46'] | Reference Text:  <S sid = 8 ssid = >Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame.</S><S sid = 46 ssid = >Each rule corresponds to a transformation rule in Kate et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N06-1056.txt | Citing Article:  N07-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system.</S> | Reference Offset:  ['147','159'] | Reference Text:  <S sid = 147 ssid = >A semantic parser was learned from the training set.</S><S sid = 159 ssid = >WASP also outperforms SILT in terms of recall, where lexical learning is done by a local bottom-up search, which is much less effective than the wordalignment-based algorithm in WASP.</S> | Discourse Facet:  NA | Annotator: Automatic


