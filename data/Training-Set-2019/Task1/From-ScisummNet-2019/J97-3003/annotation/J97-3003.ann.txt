Citance Number: 1 | Reference Article:  J97-3003.txt | Citing Article:  C10-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One could also expand the lexicon, by adapting algorithms for analyzing unknown words (e.g., Mikheev, 1997).</S> | Reference Offset:  ['100','208'] | Reference Text:  <S sid = 100 ssid = >We put all the hapax words from the Brown Corpus that were found in the cELEx-derived lexicon into the test collection (test lexicon) and all other words from the cELEx-derived lexicon into the training lexicon.</S><S sid = 208 ssid = >Because of the similarities in the algorithms with the LISP implemented Xerox tagger, we could directly use the Xerox guessing rule set with the HMM tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J97-3003.txt | Citing Article:  P07-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have used LTPOS (Mikheev, 1997), which performed the task almost error less.</S> | Reference Offset:  ['176','215'] | Reference Text:  <S sid = 176 ssid = >There are a number of standard parameters (Lewis 1991) used for measuring performance on this kind of task.</S><S sid = 215 ssid = >These words were not used in the training of the tagger either.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J97-3003.txt | Citing Article:  W02-1706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Step 6 contains a call to the other main LT TTT program, LTOPS (Mikheev, 1997), which performs both sentence identification and POS tagging.</S> | Reference Offset:  ['178','205'] | Reference Text:  <S sid = 178 ssid = >To see how well the guesser performs, we can compare the results of the guessing with the PUS-tags known to be true for the word (i.e., listed in the lexicon).</S><S sid = 205 ssid = >Our main interest is in how the advantage of one rule set over another will affect the tagging performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J97-3003.txt | Citing Article:  W05-0617.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The search for such rules has previously been conducted in the context of supervised part-of-speech tagging (Mikheev, 1997).</S> | Reference Offset:  ['204','264'] | Reference Text:  <S sid = 204 ssid = >The task of unknown-word guessing is, however, a subtask of the overall part-of-speech tagging process.</S><S sid = 264 ssid = >Evaluation of tagging accuracy on unknown words using texts and words unseen at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author (85%).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J97-3003.txt | Citing Article:  N03-3006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Tagging and chunking is done by a standard tagger and chunker, LTPos (Mikheev,1997).</S> | Reference Offset:  ['211','223'] | Reference Text:  <S sid = 211 ssid = >We also tried hybrid tagging using the output of the HMM tagger as the input to Brill's final state tagger, but it gave poorer results than either of the taggers and we decided not to consider this tagging option.</S><S sid = 223 ssid = >In our tagging experiments, we measured the error rate of tagging on unknown words using different guessers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J97-3003.txt | Citing Article:  W04-1907.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997).</S> | Reference Offset:  ['16','253'] | Reference Text:  <S sid = 16 ssid = >However, some domain-specific words or infrequently used morphological variants of general-purpose words can be missing from the lexicon and thus, their Pos-classes should be guessed by the system and only then sent to the disambiguation module.</S><S sid = 253 ssid = >Mistagging of the first type occurred when a guesser provided a broader POS-class for an unknown word than a lexicon would, and the tagger had difficulties with its disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J97-3003.txt | Citing Article:  W03-0505.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence identifier (Mikheev, 1997).</S> | Reference Offset:  ['244','261'] | Reference Text:  <S sid = 244 ssid = >This can be accounted for by the fact that the unguessed capitalized words were taken by default to be proper nouns and that the Brill tagger and the HMM tagger had slightly different strategies to apply to the first word of a sentence.</S><S sid = 261 ssid = >We have presented a technique for fully automated statistical acquisition of rules that guess possible POS-tags for words unknown to the lexicon.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J97-3003.txt | Citing Article:  W03-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >PoS tagging can be performed using LTPOS (Mikheev, 1997).</S> | Reference Offset:  ['206','223'] | Reference Text:  <S sid = 206 ssid = >Therefore, we performed an evaluation of the impact of the word guessers on tagging accuracy.</S><S sid = 223 ssid = >In our tagging experiments, we measured the error rate of tagging on unknown words using different guessers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J97-3003.txt | Citing Article:  W03-0428.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997).</S> | Reference Offset:  ['275','276'] | Reference Text:  <S sid = 275 ssid = >The two additional types of features used by Brill's guesser are implicitly represented in our approach as well: One of the Brill schemata checks the context of an unknown word.</S><S sid = 276 ssid = >In our approach we guess the words using their features only and provide several possibilities for a word; then at the disambiguation phase the context is used to choose the right tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J97-3003.txt | Citing Article:  W05-0616.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the possible part(s)-of-speech of unknown words (Mikheev,1997).</S> | Reference Offset:  ['2','7'] | Reference Text:  <S sid = 2 ssid = >In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.</S><S sid = 7 ssid = >In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J97-3003.txt | Citing Article:  W04-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997).</S> | Reference Offset:  ['16','253'] | Reference Text:  <S sid = 16 ssid = >However, some domain-specific words or infrequently used morphological variants of general-purpose words can be missing from the lexicon and thus, their Pos-classes should be guessed by the system and only then sent to the disambiguation module.</S><S sid = 253 ssid = >Mistagging of the first type occurred when a guesser provided a broader POS-class for an unknown word than a lexicon would, and the tagger had difficulties with its disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J97-3003.txt | Citing Article:  W04-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The next stage in the linguistic analysis module performs noun group and verb group chunking using fsg match with the specialised hand-written rule sets which were the core part of LT CHUNK (Finch and Mikheev, 1997).</S> | Reference Offset:  ['80','174'] | Reference Text:  <S sid = 80 ssid = >Thus if the ending s predicts that a word can be a plural noun or a 3d form of a verb, the information that this word was capitalized can narrow the considered set of POS-tags to plural proper noun.</S><S sid = 174 ssid = >There are two important questions that arise at the rule acquisition stage: how to choose the scoring threshold 0, and what the performance of the rule sets produced with different thresholds is.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J97-3003.txt | Citing Article:  I08-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The stripping-recoding rules could be manually encoded, mined from a monolingual corpus usinga learning method such as (Mikheev, 1997), or supplied by a source terminology extraction system that handles morphological variations.</S> | Reference Offset:  ['4','263'] | Reference Text:  <S sid = 4 ssid = >Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.</S><S sid = 263 ssid = >Using such training data, three types of guessing rules are induced: prefix morphological rules, suffix morphological rules, and ending-guessing rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J97-3003.txt | Citing Article:  W02-0607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Mikheev (1997), we therefore adjust reliability using lower confidence limit statistics.</S> | Reference Offset:  ['135','139'] | Reference Text:  <S sid = 135 ssid = >We tackle this problem by calculating the lower confidence limit 711 for the rule estimate, which can be seen as the minimal expected value of p for the rule if we were to draw a large number of samples.</S><S sid = 139 ssid = >It has two parameters: a, the level of confidence and df, the number of degrees of freedom, which is one less than the sample size (df = n -1). td(c_a)/2 can be looked up in the tables for the t-distribution listed in every textbook on statistics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J97-3003.txt | Citing Article:  P01-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The identification of sentence boundaries, mark-up of sentence elements and POS tagging is done by the statistical program lt pos (Mikheev, 1997).</S> | Reference Offset:  ['90','261'] | Reference Text:  <S sid = 90 ssid = >Since a word can take on several different POS-tags, in the lexicon it can be represented as a [string/Pos-class] record, where the Pos-class is a set of one or more POS-tags.</S><S sid = 261 ssid = >We have presented a technique for fully automated statistical acquisition of rules that guess possible POS-tags for words unknown to the lexicon.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J97-3003.txt | Citing Article:  W04-1211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams'n'Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997).</S> | Reference Offset:  ['24','198'] | Reference Text:  <S sid = 24 ssid = >The Xerox tagger (Cutting et al. 1992) comes with a set of rules that assign an unknown word a set of possible POS-tags (i.e., Pos-class) on the basis of its ending segment.</S><S sid = 198 ssid = >As the baseline standard, we took the ending-guessing rule set supplied with the Xerox tagger (Cutting et al. 1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J97-3003.txt | Citing Article:  W04-1211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word 'guesser' (Mikheev, 1997).</S> | Reference Offset:  ['265','267'] | Reference Text:  <S sid = 265 ssid = >Tagging accuracy on unknown words using the cascading guesser was 87.7-88.7%.</S><S sid = 267 ssid = >When the unknown words were made known to the lexicon, the accuracy of tagging was 93.6-94.3% which makes the accuracy drop caused by the cascading guesser to be less than 6% in general.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J97-3003.txt | Citing Article:  P08-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mikheev (1997) suggested a guessing-rule technique, based on prefix morphological rules ,suffix morphological rules, and ending-guessing rules.</S> | Reference Offset:  ['9','263'] | Reference Text:  <S sid = 9 ssid = >Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.</S><S sid = 263 ssid = >Using such training data, three types of guessing rules are induced: prefix morphological rules, suffix morphological rules, and ending-guessing rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J97-3003.txt | Citing Article:  P07-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Transitions to the normal and pivotal stage occur when an estimator of the relative frequency is high enough, for example by taking the lower bound of the confidence interval (Mikheev,1997).</S> | Reference Offset:  ['21','135'] | Reference Text:  <S sid = 21 ssid = >As argued in Church (1988), who proposes a more elaborated heuristic, Dermatas and Kokkinakis (1995) proposed a simple probabilistic approach to unknown-word guessing: verb present, 3d person verb, present, non-3d Example take took taking taken takes take Meaning Example Tag the probability that an unknown word has a particular POS-tag is estimated from the probability distribution of hapax words (words that occur only once) in the previously seen texts.'</S><S sid = 135 ssid = >We tackle this problem by calculating the lower confidence limit 711 for the rule estimate, which can be seen as the minimal expected value of p for the rule if we were to draw a large number of samples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J97-3003.txt | Citing Article:  W01-0502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lexical knowledge (for unknown words) and the word lemma (for known words) provide, w.h.p, one sided error (Mikheev, 1997).</S> | Reference Offset:  ['225','241'] | Reference Text:  <S sid = 225 ssid = >The error rate for a category of words was calculated as follows: Total _Words _in _Set _X Wrongly _Tagged _Words _from _Set _X Thus, for instance, the error rate of tagging the unknown words is the proportion of the mistagged unknown words to all unknown words.</S><S sid = 241 ssid = >It shows the overall error rate on unknown words and also displays the distribution of the error rate and the coverage between unknown proper nouns and the other unknown words.</S> | Discourse Facet:  NA | Annotator: Automatic


