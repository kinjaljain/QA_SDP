Citance Number: 1 | Reference Article:  D11-1033.txt | Citing Article:  W12-3148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT.</S> | Reference Offset:  ['35','46'] | Reference Text:  <S sid = 35 ssid = >This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).</S><S sid = 46 ssid = >We apply this criterion for the first time to the task of selecting training data for machine translation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D11-1033.txt | Citing Article:  P14-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models.</S> | Reference Offset:  ['36','45'] | Reference Text:  <S sid = 36 ssid = >The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010).</S><S sid = 45 ssid = >Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D11-1033.txt | Citing Article:  P14-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','151'] | Reference Text:  <S sid = 47 ssid = >We furthermore extend this idea for MT-specific purposes.</S><S sid = 151 ssid = >The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D11-1033.txt | Citing Article:  D12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The conventional wisdom is that more data is better; the larger the training corpus, the more accurate the model can be.</S><S sid = 11 ssid = >This would empirically provide more accurate lexical probabilities, and thus better target the task at hand.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D11-1033.txt | Citing Article:  D12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection.</S> | Reference Offset:  ['45','94'] | Reference Text:  <S sid = 45 ssid = >Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.</S><S sid = 94 ssid = >We consider three methods for extracting domaintargeted parallel data from a general corpus: sourceside cross-entropy (Cross-Ent), source-side crossentropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (bML), which is novel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D11-1033.txt | Citing Article:  W12-3139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr.</S> | Reference Offset:  ['35','36'] | Reference Text:  <S sid = 35 ssid = >This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).</S><S sid = 36 ssid = >The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D11-1033.txt | Citing Article:  E12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','151'] | Reference Text:  <S sid = 47 ssid = >We furthermore extend this idea for MT-specific purposes.</S><S sid = 151 ssid = >The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D11-1033.txt | Citing Article:  P14-2002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','151'] | Reference Text:  <S sid = 47 ssid = >We furthermore extend this idea for MT-specific purposes.</S><S sid = 151 ssid = >The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.</S> | Discourse Facet:  NA | Annotator: Automatic


