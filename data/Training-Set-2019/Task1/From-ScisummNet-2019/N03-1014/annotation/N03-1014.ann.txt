Citance Number: 1 | Reference Article:  N03-1014.txt | Citing Article:  W04-0305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003).</S> | Reference Offset:  ['8','55'] | Reference Text:  <S sid = 8 ssid = >Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) are based on a history-based model of parser actions.</S><S sid = 55 ssid = >Choosing this representation is a challenge for any history-based statistical parser, because the history is of unbounded size.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N03-1014.txt | Citing Article:  W04-0305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003).</S> | Reference Offset:  ['14','194'] | Reference Text:  <S sid = 14 ssid = >We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).</S><S sid = 194 ssid = >Previous work on applying SSNs to natural language parsing (Henderson, 2000) has not been general enough to be applied to the Penn Treebank, so it is not possible to compare results directly to this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N03-1014.txt | Citing Article:  W04-0305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003).</S> | Reference Offset:  ['1','200'] | Reference Text:  <S sid = 1 ssid = >We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.</S><S sid = 200 ssid = >The neural network architecture we use, Simple Synchrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to impose linguistically appropriate soft biases on the learning process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N03-1014.txt | Citing Article:  W04-0305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003).</S> | Reference Offset:  ['13','63'] | Reference Text:  <S sid = 13 ssid = >In the work presented here, we automatically induce a finite set of real valued features to represent the parse history.</S><S sid = 63 ssid = >In addition the SSN has a hidden layer, which computes a finite vector of real valued features from a sequence of inputs specifying the derivation history .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N03-1014.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model.</S> | Reference Offset:  ['14','61'] | Reference Text:  <S sid = 14 ssid = >We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).</S><S sid = 61 ssid = >The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N03-1014.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order.</S> | Reference Offset:  ['118','147'] | Reference Text:  <S sid = 118 ssid = >Our pruning strategy is designed specifically for left-corner parsing.</S><S sid = 147 ssid = >This allows use to trade off parsing accuracy for parsing speed, which is a much more important issue than training time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N03-1014.txt | Citing Article:  N06-2026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task.</S> | Reference Offset:  ['1','2'] | Reference Text:  <S sid = 1 ssid = >We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.</S><S sid = 2 ssid = >The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N03-1014.txt | Citing Article:  N09-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance.</S> | Reference Offset:  ['14','61'] | Reference Text:  <S sid = 14 ssid = >We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).</S><S sid = 61 ssid = >The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N03-1014.txt | Citing Article:  P05-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours.</S> | Reference Offset:  ['63','64'] | Reference Text:  <S sid = 63 ssid = >In addition the SSN has a hidden layer, which computes a finite vector of real valued features from a sequence of inputs specifying the derivation history .</S><S sid = 64 ssid = >This hidden layer vector is the history representation .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N03-1014.txt | Citing Article:  H05-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks.</S> | Reference Offset:  ['18','195'] | Reference Text:  <S sid = 18 ssid = >The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.</S><S sid = 195 ssid = >This paper has presented a method for estimating the parameters of a history-based statistical parser which does not require any a priori independence assumptions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N03-1014.txt | Citing Article:  H05-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system.</S> | Reference Offset:  ['18','72'] | Reference Text:  <S sid = 18 ssid = >The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.</S><S sid = 72 ssid = >Training a Simple Synchrony Network (SSN) is similar to training a log-linear model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N03-1014.txt | Citing Article:  H05-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations.</S> | Reference Offset:  ['172','200'] | Reference Text:  <S sid = 172 ssid = >To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head.</S><S sid = 200 ssid = >The neural network architecture we use, Simple Synchrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to impose linguistically appropriate soft biases on the learning process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N03-1014.txt | Citing Article:  H05-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003).</S> | Reference Offset:  ['172','196'] | Reference Text:  <S sid = 172 ssid = >To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head.</S><S sid = 196 ssid = >A neural network is trained simultaneously to estimate the probabilities of parser actions and to induce a finite representation of the unbounded parse history.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N03-1014.txt | Citing Article:  H05-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >H03 indicates the model illustrated in (Henderson, 2003).</S> | Reference Offset:  ['39','61'] | Reference Text:  <S sid = 39 ssid = >To precisely specify this ordering, it is sufficient to characterize the state of the parser as a stack of nodes which are in the process of being parsed, as illustrated on the right in figure 1.</S><S sid = 61 ssid = >The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N03-1014.txt | Citing Article:  H05-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs.</S> | Reference Offset:  ['51','134'] | Reference Text:  <S sid = 51 ssid = >We also reduced the computational cost of terminal prediction by replacing the very large number of lower frequency tag-word pairs with tag-“unknown-word” pairs, which are also used for tag-word pairs which were not in the training set.</S><S sid = 134 ssid = >This resulted in a vocabulary size of 512 tag-word pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N03-1014.txt | Citing Article:  D07-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing.</S> | Reference Offset:  ['14','61'] | Reference Text:  <S sid = 14 ssid = >We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).</S><S sid = 61 ssid = >The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N03-1014.txt | Citing Article:  D07-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003).</S> | Reference Offset:  ['27','61'] | Reference Text:  <S sid = 27 ssid = >The neural network is used to estimate the parameters of this probability model.</S><S sid = 61 ssid = >The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N03-1014.txt | Citing Article:  W06-2303.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.</S> | Reference Offset:  ['2','20'] | Reference Text:  <S sid = 2 ssid = >The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.</S><S sid = 20 ssid = >When a relatively small vocabulary of words is used, performance is only marginally below the best current parser accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N03-1014.txt | Citing Article:  W06-2303.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem.</S> | Reference Offset:  ['94','172'] | Reference Text:  <S sid = 94 ssid = >To achieve this structurally-determined inductive bias, we use Simple Synchrony Networks, which are specifically designed for processing structures.</S><S sid = 172 ssid = >To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N03-1014.txt | Citing Article:  W06-2303.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.</S> | Reference Offset:  ['92','111'] | Reference Text:  <S sid = 92 ssid = >We can exploit this recency bias in inducing history representations by ensuring that information which is known to be important at a given step in the derivation is input directly to that step’s history representation, and that as information becomes less relevant it has increasing numbers of history representations to pass through before reaching the step’s history representation.</S><S sid = 111 ssid = >The pre-defined features of the derivation history which are input to for node top at step are chosen to reflect the information which is directly relevant to choosing the next decision .</S> | Discourse Facet:  NA | Annotator: Automatic


