Citance Number: 1 | Reference Article:  P09-1094.txt | Citing Article:  P13-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc.</S> | Reference Offset:  ['34','86'] | Reference Text:  <S sid = 34 ssid = >Researchers employ the existing SMT models for PG (Quirk et al., 2004).</S><S sid = 86 ssid = >One can refer to (Zhao et al., 2008b) for the details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1094.txt | Citing Article:  P13-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models.</S> | Reference Offset:  ['48','64'] | Reference Text:  <S sid = 48 ssid = >The PTs used in this work are constructed using different corpora and different score functions (Section 3.5).</S><S sid = 64 ssid = >In practice, the units of a sentence may be paraphrased using different PTs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1094.txt | Citing Article:  D10-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries.</S> | Reference Offset:  ['86','169'] | Reference Text:  <S sid = 86 ssid = >One can refer to (Zhao et al., 2008b) for the details.</S><S sid = 169 ssid = >Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1094.txt | Citing Article:  W11-2128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task.</S> | Reference Offset:  ['38','86'] | Reference Text:  <S sid = 38 ssid = >To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).</S><S sid = 86 ssid = >One can refer to (Zhao et al., 2008b) for the details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1094.txt | Citing Article:  C10-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009).</S> | Reference Offset:  ['17','59'] | Reference Text:  <S sid = 17 ssid = >The results show that the proposed method is promising, which generates useful paraphrases for the given applications.</S><S sid = 59 ssid = >Paraphrase Model: Paraphrase generation is a decoding process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1094.txt | Citing Article:  C10-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications.</S> | Reference Offset:  ['58','152'] | Reference Text:  <S sid = 58 ssid = >Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control the adequacy, fluency, and usability of the paraphrases, respectively1.</S><S sid = 152 ssid = >Thus it contains no paraphrase planning stage or the usability sub-model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1094.txt | Citing Article:  C10-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG.</S> | Reference Offset:  ['11','84'] | Reference Text:  <S sid = 11 ssid = >As far as we know, this is the first statistical model specially designed for paraphrase generation.</S><S sid = 84 ssid = >We applied the approach proposed in (Zhao et al., 2008b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1094.txt | Citing Article:  W11-1602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009).</S> | Reference Offset:  ['9','172'] | Reference Text:  <S sid = 9 ssid = >PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).</S><S sid = 172 ssid = >Sentence Simplification: Carroll et al. (1999) has proposed an automatic text simplification method for language-impaired readers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1094.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a).</S> | Reference Offset:  ['9','116'] | Reference Text:  <S sid = 9 ssid = >PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).</S><S sid = 116 ssid = >The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1094.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR.</S> | Reference Offset:  ['34','86'] | Reference Text:  <S sid = 34 ssid = >Researchers employ the existing SMT models for PG (Quirk et al., 2004).</S><S sid = 86 ssid = >One can refer to (Zhao et al., 2008b) for the details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1094.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression.</S> | Reference Offset:  ['14','80'] | Reference Text:  <S sid = 14 ssid = >We consider three paraphrase applications in our experiments, including sentence compression, sentence simplification, and sentence similarity computation.</S><S sid = 80 ssid = >The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1094.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below.</S> | Reference Offset:  ['84','171'] | Reference Text:  <S sid = 84 ssid = >We applied the approach proposed in (Zhao et al., 2008b).</S><S sid = 171 ssid = >However, as most other sentence compression methods, their method allows information loss after compression, which means that the generated sentences are not necessarily paraphrases of the source sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1094.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality.</S> | Reference Offset:  ['80','86'] | Reference Text:  <S sid = 80 ssid = >The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).</S><S sid = 86 ssid = >One can refer to (Zhao et al., 2008b) for the details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1094.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['190','191'] | Reference Text:  <S sid = 190 ssid = >The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).</S><S sid = 191 ssid = >Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1094.txt | Citing Article:  W10-0205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009).</S> | Reference Offset:  ['84','86'] | Reference Text:  <S sid = 84 ssid = >We applied the approach proposed in (Zhao et al., 2008b).</S><S sid = 86 ssid = >One can refer to (Zhao et al., 2008b) for the details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1094.txt | Citing Article:  D11-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions).</S> | Reference Offset:  ['168','169'] | Reference Text:  <S sid = 168 ssid = >Sentence compression: Sentence compression is widely studied, which is mostly reviewed as a word deletion task.</S><S sid = 169 ssid = >Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1094.txt | Citing Article:  D12-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009).</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >Application-driven Statistical Paraphrase Generation</S><S sid = 8 ssid = >Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1094.txt | Citing Article:  W11-1609.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1.</S> | Reference Offset:  ['0','54'] | Reference Text:  <S sid = 0 ssid = >Application-driven Statistical Paraphrase Generation</S><S sid = 54 ssid = >The application in this example is sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1094.txt | Citing Article:  P12-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009).</S> | Reference Offset:  ['38','84'] | Reference Text:  <S sid = 38 ssid = >To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).</S><S sid = 84 ssid = >We applied the approach proposed in (Zhao et al., 2008b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1094.txt | Citing Article:  C10-1152.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).</S> | Reference Offset:  ['9','29'] | Reference Text:  <S sid = 9 ssid = >PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).</S><S sid = 29 ssid = >In the first one, the source sentence s is transformed into its semantic representation r by undertaking a series of NLP processing, including morphology analyzing, syntactic parsing, semantic role labeling, etc.</S> | Discourse Facet:  NA | Annotator: Automatic


