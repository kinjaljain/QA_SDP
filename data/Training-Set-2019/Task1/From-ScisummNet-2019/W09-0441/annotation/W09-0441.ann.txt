Citance Number: 1 | Reference Article:  W09-0441.txt | Citing Article:  W09-0409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment.</S> | Reference Offset:  ['21','56'] | Reference Text:  <S sid = 21 ssid = >To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.</S><S sid = 56 ssid = >While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W09-0441.txt | Citing Article:  P13-2047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003).</S> | Reference Offset:  ['12','89'] | Reference Text:  <S sid = 12 ssid = >This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W09-0441.txt | Citing Article:  D12-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two.</S> | Reference Offset:  ['21','89'] | Reference Text:  <S sid = 21 ssid = >To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W09-0441.txt | Citing Article:  D12-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lemma is added later in the TERplus extension (Snover et al 2009).</S> | Reference Offset:  ['45','89'] | Reference Text:  <S sid = 45 ssid = >For exact details on these constraints, see Snover et al. (2006).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W09-0441.txt | Citing Article:  W10-1751.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation.</S><S sid = 173 ssid = >HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/âˆ¼snover/terp/.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W09-0441.txt | Citing Article:  W10-1755.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments.</S> | Reference Offset:  ['12','89'] | Reference Text:  <S sid = 12 ssid = >This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W09-0441.txt | Citing Article:  W12-3107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two.</S> | Reference Offset:  ['21','89'] | Reference Text:  <S sid = 21 ssid = >To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W09-0441.txt | Citing Article:  W12-3107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009).</S> | Reference Offset:  ['60','89'] | Reference Text:  <S sid = 60 ssid = >Two words are determined to be synonyms if they share the same synonym set according to WordNet (Fellbaum, 1998).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W09-0441.txt | Citing Article:  W12-3107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lemma is added later in the TERplus extension (Snover et al, 2009).</S> | Reference Offset:  ['45','89'] | Reference Text:  <S sid = 45 ssid = >For exact details on these constraints, see Snover et al. (2006).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W09-0441.txt | Citing Article:  D10-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4.</S> | Reference Offset:  ['29','87'] | Reference Text:  <S sid = 29 ssid = >Automatic MT evaluation metrics compare the hypothesis against this set of reference translations and assign a score to the similarity; higher scores are given to hypotheses that are more similar to the references.</S><S sid = 87 ssid = >Document level Adequacy scores are determined by taking the length weighted average of the segment level scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W09-0441.txt | Citing Article:  N12-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009).</S> | Reference Offset:  ['12','89'] | Reference Text:  <S sid = 12 ssid = >This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W09-0441.txt | Citing Article:  N10-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments.</S> | Reference Offset:  ['21','56'] | Reference Text:  <S sid = 21 ssid = >To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.</S><S sid = 56 ssid = >While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W09-0441.txt | Citing Article:  W11-2121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm.</S> | Reference Offset:  ['55','89'] | Reference Text:  <S sid = 55 ssid = >Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp.</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W09-0441.txt | Citing Article:  W11-2121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995).</S> | Reference Offset:  ['51','59'] | Reference Text:  <S sid = 51 ssid = >TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms.</S><S sid = 59 ssid = >TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W09-0441.txt | Citing Article:  P11-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases.</S> | Reference Offset:  ['89','117'] | Reference Text:  <S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S><S sid = 117 ssid = >The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W09-0441.txt | Citing Article:  W11-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TERp (Snover et al, 2009): Translation Edit.</S> | Reference Offset:  ['12','89'] | Reference Text:  <S sid = 12 ssid = >This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W09-0441.txt | Citing Article:  W11-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy.</S> | Reference Offset:  ['90','94'] | Reference Text:  <S sid = 90 ssid = >The IBM version of BLEU was used in case insensitive mode with an ngram-size of 4 to calculate the BLEU scores.</S><S sid = 94 ssid = >TER was also used in case insensitive mode.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W09-0441.txt | Citing Article:  W10-1748.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis.</S> | Reference Offset:  ['45','93'] | Reference Text:  <S sid = 45 ssid = >For exact details on these constraints, see Snover et al. (2006).</S><S sid = 93 ssid = >When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W09-0441.txt | Citing Article:  D12-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics.</S> | Reference Offset:  ['12','89'] | Reference Text:  <S sid = 12 ssid = >This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W09-0441.txt | Citing Article:  N10-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997).</S> | Reference Offset:  ['12','89'] | Reference Text:  <S sid = 12 ssid = >This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).</S><S sid = 89 ssid = >We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


