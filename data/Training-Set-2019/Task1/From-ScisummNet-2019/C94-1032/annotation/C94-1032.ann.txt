Citance Number: 1 | Reference Article:  C94-1032.txt | Citing Article:  C96-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >N-gram models arc usually used for scoring (Gu et al, 1991) (Nagata, 1994), but their training requires the sentences of the corpus to be manualy segmented, and even class-tagged if class-based N-gram is used, as in (Nagata, 1994).</S> | Reference Offset:  ['180','190'] | Reference Text:  <S sid = 180 ssid = >Figure 5 shows tile percentage of sentences (not words) correctly segmented and tagged.</S><S sid = 190 ssid = >The tagging models previously used are either part of speech I)igram [9, 14] or Character-based IIMM [12].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C94-1032.txt | Citing Article:  P11-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['72','231'] | Reference Text:  <S sid = 72 ssid = >In the forward search, we use a table called parse-list, whose key is the end position of the parse  structure, and wlm,se value is a list of parse structures that have the best partial path scores for each combined state at the end position.</S><S sid = 231 ssid = >IPSJ, Vol.30, No.3, pp.294-301, 1989 (in Japanese).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C94-1032.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedure [Nagata, 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996].</S> | Reference Offset:  ['171','211'] | Reference Text:  <S sid = 171 ssid = >Since the segmentation accuracy of the proposed sys- tem is relatively high (97.7% recall and 97.2% precision for the top candidate) compared to the morphologi- cal analysis accuracy, it is likely that we can improve the part of speech assignment accuracy by refining the statistically-based tagging model.</S><S sid = 211 ssid = >Its word segmentation a d tagging accuracy is approxlmatcly 95%, which is comparable to the star.e- of-the-art stochastic tagger for English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C94-1032.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the Viterbi-like dynamic programming procedure described in [Nagata, 1994] to get the most likely word segmentation.</S> | Reference Offset:  ['54','88'] | Reference Text:  <S sid = 54 ssid = >First, a linear time dynamic programming is used for record- ing the scores of all partial paths in a table 3.</S><S sid = 88 ssid = >For the backward N-best search, how(wet, we want N most likely word segmentation and part of speech sequence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C94-1032.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al, 1996].</S> | Reference Offset:  ['169','171'] | Reference Text:  <S sid = 169 ssid = >For open texts, tile morphological nalyzer achieved 95.1% recall and 94.6% precision for the top candidate, and 97.8% recall and 73.2% precision for the 5 best candidates.</S><S sid = 171 ssid = >Since the segmentation accuracy of the proposed sys- tem is relatively high (97.7% recall and 97.2% precision for the top candidate) compared to the morphologi- cal analysis accuracy, it is likely that we can improve the part of speech assignment accuracy by refining the statistically-based tagging model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C94-1032.txt | Citing Article:  W96-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['72','231'] | Reference Text:  <S sid = 72 ssid = >In the forward search, we use a table called parse-list, whose key is the end position of the parse  structure, and wlm,se value is a list of parse structures that have the best partial path scores for each combined state at the end position.</S><S sid = 231 ssid = >IPSJ, Vol.30, No.3, pp.294-301, 1989 (in Japanese).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C94-1032.txt | Citing Article:  P06-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nagata (1994) proposed a stochastic word segmenter based on a word-gram model to solve the word segmentation problem.</S> | Reference Offset:  ['106','109'] | Reference Text:  <S sid = 106 ssid = >Once word hypotheses for unknown words are gener- ated, the proposed N-best algorithm will find tile most likely word segmentation a d part of speech assignment taking into account he entire sentence.</S><S sid = 109 ssid = >The word model must account for morphology and word for- marion to estimate the part of speech and tile probabil- ity of a word hypothesis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C94-1032.txt | Citing Article:  P06-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese.</S> | Reference Offset:  ['150','151'] | Reference Text:  <S sid = 150 ssid = >First, we selected 1,000 test sentences for all open test, arid used I.he others for training.</S><S sid = 151 ssid = >Tile corpus was divided into 90% R)r training and 10% for test- ing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C94-1032.txt | Citing Article:  C00-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fully stochastic language models (e.g. Nagata 1994), on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve.</S> | Reference Offset:  ['69','211'] | Reference Text:  <S sid = 69 ssid = >The reason is described later.</S><S sid = 211 ssid = >Its word segmentation a d tagging accuracy is approxlmatcly 95%, which is comparable to the star.e- of-the-art stochastic tagger for English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C94-1032.txt | Citing Article:  C00-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best accuracy reported for statistical methods to date is around 95% (e.g. Nagata 1994).</S> | Reference Offset:  ['8','211'] | Reference Text:  <S sid = 8 ssid = >We show in this paper that we can buihl a stochastic Japanese morphological nalyzer that offers approxi- mately 95% accuracy on a statistical language model- ing technique and an efficient two-pass N-best search strategy.</S><S sid = 211 ssid = >Its word segmentation a d tagging accuracy is approxlmatcly 95%, which is comparable to the star.e- of-the-art stochastic tagger for English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C94-1032.txt | Citing Article:  W01-0512.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the Viterbi algorithm to find the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the N best sets.</S> | Reference Offset:  ['13','66'] | Reference Text:  <S sid = 13 ssid = >The proposed algorithm amalgamates and extends three well-known algorithms in different fields: the Minimum Connective-Cost Method [7] for Japanese morphologi- cal analysis, Extended Viterbi Algorithm for charac- ter recognition [6], and "l~ee-Trellis N-Best Search for speech recognition [15].</S><S sid = 66 ssid = >Parse.prev?ous i  the pointer to the (best) previous parse structure as in conventional Viterbi decoding, which is not necessary if we use the backward N best search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C94-1032.txt | Citing Article:  P01-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximum probability solution in Japanese morphological analysis (Nagata, 1994).</S> | Reference Offset:  ['92','208'] | Reference Text:  <S sid = 92 ssid = >But we assume that the readers know the A* algorithm, and exphtin only the way we applied the algorithm to the problem.</S><S sid = 208 ssid = >We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm, but we dont know how to handle unknown words in this algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C94-1032.txt | Citing Article:  A00-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To find the sequence, Nagata proposed a probabilistic language model for non-segmented languages (Nagata, 1994).</S> | Reference Offset:  ['26','132'] | Reference Text:  <S sid = 26 ssid = >w,~ and a sequence of tags T = t i ts .</S><S sid = 132 ssid = >We also connt the number of crossings, which is tile mmtber of c,mes where a bracketed sequence from the standard data overlaps a bracketed sequence from tile system output, but neither sequence is completely coutained in the other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C94-1032.txt | Citing Article:  P09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we apply a dynamic programming search (Nagata, 1994) to k best MIRA.</S> | Reference Offset:  ['12','53'] | Reference Text:  <S sid = 12 ssid = >It consists of the forward dynamic pro- gramming search and the backward A* search.</S><S sid = 53 ssid = >3 Search  S t ra tegy The search algorithm consists of a forward dynamic programming search and a backward A* search.</S> | Discourse Facet:  NA | Annotator: Automatic


