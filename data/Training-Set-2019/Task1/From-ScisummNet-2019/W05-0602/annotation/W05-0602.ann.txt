Citance Number: 1 | Reference Article:  W05-0602.txt | Citing Article:  W10-2903.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['39','107'] | Reference Text:  <S sid = 39 ssid = >Figure 1 showsthe SAPT for a simple sentence in the CLANG do main.</S><S sid = 107 ssid = >i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W05-0602.txt | Citing Article:  P06-1115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser.</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >A Statistical Semantic Parser That Integrates Syntax And Semantics</S><S sid = 2 ssid = >It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W05-0602.txt | Citing Article:  N06-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 6 shows the performance of WASP com pared to four other algorithms: SILT (Kate et al,2005), COCKTAIL (Tang and Mooney, 2001), SCIS SOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005).</S> | Reference Offset:  ['17','88'] | Reference Text:  <S sid = 17 ssid = >Training the system requires sentences an notated with both gold-standard SAPT?s and MR?s. We present experimental results on corpora for bothgeography-database querying and Robocup coaching demonstrating that SCISSOR produces more accurate semantic representations than several previ ous approaches based on symbolic learning (Tang and Mooney, 2001; Kate et al, 2005).</S><S sid = 88 ssid = >Collins?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W05-0602.txt | Citing Article:  W11-0131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of course, other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >A Statistical Semantic Parser That Integrates Syntax And Semantics</S><S sid = 3 ssid = >A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W05-0602.txt | Citing Article:  W11-0105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008).</S> | Reference Offset:  ['10','17'] | Reference Text:  <S sid = 10 ssid = >The first is a Prolog-based language used in a previously-developed corpus of queries to a database on U.S. geography (Zelle and Mooney, 1996).</S><S sid = 17 ssid = >Training the system requires sentences an notated with both gold-standard SAPT?s and MR?s. We present experimental results on corpora for bothgeography-database querying and Robocup coaching demonstrating that SCISSOR produces more accurate semantic representations than several previ ous approaches based on symbolic learning (Tang and Mooney, 2001; Kate et al, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W05-0602.txt | Citing Article:  P06-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax.</S> | Reference Offset:  ['16','17'] | Reference Text:  <S sid = 16 ssid = >Our approach is implemented in a system called SCISSOR (Semantic Composition that IntegratesSyntax and Semantics to get Optimal Representations).</S><S sid = 17 ssid = >Training the system requires sentences an notated with both gold-standard SAPT?s and MR?s. We present experimental results on corpora for bothgeography-database querying and Robocup coaching demonstrating that SCISSOR produces more accurate semantic representations than several previ ous approaches based on symbolic learning (Tang and Mooney, 2001; Kate et al, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W05-0602.txt | Citing Article:  P06-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Extended back-off levels for the semantic parameter PL1 (Li| ...), using the same notation as in Ge and Mooney (2005).</S> | Reference Offset:  ['49','90'] | Reference Text:  <S sid = 49 ssid = >In the example, node N3 is determined to be the semantic head of the sentence, since its semantic label, bowner, matchesN8?s semantic label.</S><S sid = 90 ssid = >In the following section, we follow the notation in (Collins, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W05-0602.txt | Citing Article:  P06-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here, we only describe changes made to SCISSOR for re ranking, for a full description of SCISSOR see Ge and Mooney (2005).</S> | Reference Offset:  ['16','17'] | Reference Text:  <S sid = 16 ssid = >Our approach is implemented in a system called SCISSOR (Semantic Composition that IntegratesSyntax and Semantics to get Optimal Representations).</S><S sid = 17 ssid = >Training the system requires sentences an notated with both gold-standard SAPT?s and MR?s. We present experimental results on corpora for bothgeography-database querying and Robocup coaching demonstrating that SCISSOR produces more accurate semantic representations than several previ ous approaches based on symbolic learning (Tang and Mooney, 2001; Kate et al, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W05-0602.txt | Citing Article:  P06-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Baseline ResultsTable 2 shows the results comparing the baseline learner SCISSOR using both the back-off parameters in Ge and Mooney (2005) (SCISSOR) and the revised parameters in Section 2.2 (SCISSOR+).</S> | Reference Offset:  ['17','63'] | Reference Text:  <S sid = 17 ssid = >Training the system requires sentences an notated with both gold-standard SAPT?s and MR?s. We present experimental results on corpora for bothgeography-database querying and Robocup coaching demonstrating that SCISSOR produces more accurate semantic representations than several previ ous approaches based on symbolic learning (Tang and Mooney, 2001; Kate et al, 2005).</S><S sid = 63 ssid = >This section discusses how sentences for training SCISSOR were manually annotated with SAPT?s. Sentences were parsed by Collins?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W05-0602.txt | Citing Article:  P06-2080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ge and Mooney also presented a statistical method (Ge and Mooney, 2005) by merging syn tactic and semantic information.</S> | Reference Offset:  ['0','37'] | Reference Text:  <S sid = 0 ssid = >A Statistical Semantic Parser That Integrates Syntax And Semantics</S><S sid = 37 ssid = >By in tegrating syntax and semantics in a single statisticalparser that produces an SAPT, we can use both se mantic information to resolve syntactic ambiguitiesand syntactic information to resolve semantic ambi guities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W05-0602.txt | Citing Article:  P06-2080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Structure representation A tree structure representation incorporated with semantic and syntactic information is named semantically augmented parse tree (SAPT) (Ge and Mooney, 2005).</S> | Reference Offset:  ['2','3'] | Reference Text:  <S sid = 2 ssid = >It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.</S><S sid = 3 ssid = >A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W05-0602.txt | Citing Article:  P06-2080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As defined in (Ge and Mooney, 2005), in an SAPT, each internal node in the parse tree is annotated with a semantic label.</S> | Reference Offset:  ['2','38'] | Reference Text:  <S sid = 2 ssid = >It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.</S><S sid = 38 ssid = >In a SAPT, each internal node in the parse tree is annotated with a semantic label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W05-0602.txt | Citing Article:  P06-2080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The procedure of generating a logical form using a SAPT structure originally proposed by (Ge and Mooney, 2005) and it is expressed as Algorithm 1.</S> | Reference Offset:  ['23','45'] | Reference Text:  <S sid = 23 ssid = >In CLANG, tactics and behaviors are expressed in terms of if-then rules.</S><S sid = 45 ssid = >A special semantic label nullis used for nodes that do not correspond to any con cept in the domain.Figure 2 shows the basic algorithm for build ing an MR from an SAPT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W05-0602.txt | Citing Article:  P06-2080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['39','107'] | Reference Text:  <S sid = 39 ssid = >Figure 1 showsthe SAPT for a simple sentence in the CLANG do main.</S><S sid = 107 ssid = >i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W05-0602.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SCISSOR (Ge and Mooney, 2005) takes syntactic parses rather than NL strings and attempts to translate them into MR expressions.</S> | Reference Offset:  ['34','53'] | Reference Text:  <S sid = 34 ssid = >Notation: X MR is the MR of node X . Output: N MR C i := the ith child node of N; 1  i  n C h = GETSEMANTICHEAD(N ) // see Section 3 C h MR = BUILDMR(C h ; K) for each other child C i where i 6= h C i MR = BUILDMR(C i ; K) COMPOSEMR(C h MR , C i MR ; K) // see Section 3 N MR = C h MR Figure 2: Computing an MR from a SAPT.of its children.</S><S sid = 53 ssid = >Once the MR for the head is constructed, the MR of all other (non-head) children are computed recursively,and COMPOSEMR assigns their MR?s to fill the arguments in the head?s MR to construct the com plete MR for the node.</S> | Discourse Facet:  NA | Annotator: Automatic


