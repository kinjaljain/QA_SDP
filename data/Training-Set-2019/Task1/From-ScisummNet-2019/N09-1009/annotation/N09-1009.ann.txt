Citance Number: 1 | Reference Article:  N09-1009.txt | Citing Article:  D12-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols.</S> | Reference Offset:  ['16','107'] | Reference Text:  <S sid = 16 ssid = >Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN.</S><S sid = 107 ssid = >Our extension to the model in Cohen et al. (2008) follows naturally after we have defined the shared LN distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N09-1009.txt | Citing Article:  P10-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags.</S> | Reference Offset:  ['4','157'] | Reference Text:  <S sid = 4 ssid = >We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.</S><S sid = 157 ssid = >Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N09-1009.txt | Citing Article:  N10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).</S> | Reference Offset:  ['21','122'] | Reference Text:  <S sid = 21 ssid = >We provide a variational EM algorithm for inference.</S><S sid = 122 ssid = >In our experiments, we use this variational EM algorithm on a training set, and then use the normal experts’ means to get a point estimate for θ, the grammar weights.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N09-1009.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S><S sid = 176 ssid = >The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N09-1009.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others.</S> | Reference Offset:  ['2','93'] | Reference Text:  <S sid = 2 ssid = >This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.</S><S sid = 93 ssid = >We define a shared logistic normal distribution with N “experts” over a collection of K multinomial distributions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N09-1009.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S><S sid = 176 ssid = >The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N09-1009.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S><S sid = 176 ssid = >The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts.</S> | Reference Offset:  ['24','148'] | Reference Text:  <S sid = 24 ssid = >In §3, we present our model and a variational inference algorithm for it.</S><S sid = 148 ssid = >We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods.</S> | Reference Offset:  ['54','58'] | Reference Text:  <S sid = 54 ssid = >EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y).</S><S sid = 58 ssid = >Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing.</S> | Reference Offset:  ['4','161'] | Reference Text:  <S sid = 4 ssid = >We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.</S><S sid = 161 ssid = >Parts of speech are matched through the single coarse tagset (footnote 4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English.</S> | Reference Offset:  ['155','173'] | Reference Text:  <S sid = 155 ssid = >Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p < 0.05).</S><S sid = 173 ssid = >We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets.</S> | Reference Offset:  ['38','41'] | Reference Text:  <S sid = 38 ssid = >Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the “constituent-context model” (CCM).</S><S sid = 41 ssid = >A tree y is defined by a pair of functions yleft and yright (both 10, 1, 2,..., n} __+ 2{1,2,...,n}) that map each word to its sets of left and right dependents, respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N09-1009.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These are the same training, development, and test sets used by Cohen and Smith (2009).</S> | Reference Offset:  ['16','58'] | Reference Text:  <S sid = 16 ssid = >Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN.</S><S sid = 58 ssid = >Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N09-1009.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results.</S> | Reference Offset:  ['130','173'] | Reference Text:  <S sid = 130 ssid = >This performance measure is also known as attachment accuracy.</S><S sid = 173 ssid = >We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N09-1009.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S><S sid = 176 ssid = >The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N09-1009.txt | Citing Article:  D10-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This has been done successfully in multilingual settings (Cohen and Smith, 2009).</S> | Reference Offset:  ['25','138'] | Reference Text:  <S sid = 25 ssid = >In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them.</S><S sid = 138 ssid = >Following this observation, we compare four different settings in our experiments (all SLN settings include one normal expert for each multinomial on its own, equivalent to the regular LN setting from Cohen et al. ): bilities corresponding to a verbal parent (any parent, using the coarse tags of Cohen et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N09-1009.txt | Citing Article:  D10-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences.</S> | Reference Offset:  ['51','52'] | Reference Text:  <S sid = 51 ssid = >Headden et al. (2009) extended DMV so that the distributions θe condition on the valence as well, with smoothing, and showed significant improvements for short sentences.</S><S sid = 52 ssid = >Our experiments found that these improvements do not hold on longer sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N09-1009.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009).</S> | Reference Offset:  ['119','145'] | Reference Text:  <S sid = 119 ssid = >Our variational inference algorithm is derived similarly to that of Cohen et al. (2008).</S><S sid = 145 ssid = >For the covariance matrices, we follow the setting in Cohen et al. (2008) in our experiments also described in §3.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N09-1009.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper.</S> | Reference Offset:  ['132','167'] | Reference Text:  <S sid = 132 ssid = >Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing.</S><S sid = 167 ssid = >The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&N.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N09-1009.txt | Citing Article:  D11-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).</S> | Reference Offset:  ['9','150'] | Reference Text:  <S sid = 9 ssid = >There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).</S><S sid = 150 ssid = >The baselines include right attachment (where each word is attached to the word to its right), MLE via EM (Klein and Manning, 2004), and empirical Bayes with Dirichlet and LN priors (Cohen et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


