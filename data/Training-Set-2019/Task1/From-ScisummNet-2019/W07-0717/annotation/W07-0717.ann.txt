Citance Number: 1 | Reference Article:  W07-0717.txt | Citing Article:  W08-0334.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights.</S> | Reference Offset:  ['33','122'] | Reference Text:  <S sid = 33 ssid = >We use two different estimates for the conditional probabilities p(�t|g) and p(g|�t): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004).</S><S sid = 122 ssid = >Hildebrand et al (1995) describe a similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W07-0717.txt | Citing Article:  W08-0334.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model.</S> | Reference Offset:  ['44','49'] | Reference Text:  <S sid = 44 ssid = >We adapt both language and translation model features within the overall loglinear combination (1).</S><S sid = 49 ssid = >The most commonly-used framework for mixture models is a linear one: where p(x|h) is either a language or translation model; pc(x|h) is a model trained on component c, and λc is the corresponding weight.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W07-0717.txt | Citing Article:  P14-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information.</S> | Reference Offset:  ['60','62'] | Reference Text:  <S sid = 60 ssid = >Our focus in this paper is on adaptation via mixture weights.</S><S sid = 62 ssid = >This is not the case for dynamic adaptation, where, in the absence of an in-domain development corpus, the only information we can hope to glean are the weights on adapted models compared to other features of the system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W07-0717.txt | Citing Article:  E12-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011).</S> | Reference Offset:  ['0','108'] | Reference Text:  <S sid = 0 ssid = >Mixture-Model Adaptation for SMT</S><S sid = 108 ssid = >However, combined LM and TM adaptation is not better than LM adaptation on its own, indicating that the individual adapted models may be capturing the same information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W07-0717.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly.</S> | Reference Offset:  ['36','90'] | Reference Text:  <S sid = 36 ssid = >Our approach to mixture-model adaptation can be summarized by the following general algorithm: from several different domains.</S><S sid = 90 ssid = >Both types of mixture model are better than the baseline, but the linear mixture is slightly better than the loglinear mixture.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W07-0717.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate.</S> | Reference Offset:  ['11','77'] | Reference Text:  <S sid = 11 ssid = >In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.</S><S sid = 77 ssid = >A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W07-0717.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For details, refer to (Foster and Kuhn, 2007).</S> | Reference Offset:  ['92','119'] | Reference Text:  <S sid = 92 ssid = >We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components.</S><S sid = 119 ssid = >It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W07-0717.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs). Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures.</S> | Reference Offset:  ['89','94'] | Reference Text:  <S sid = 89 ssid = >Table 2 shows a comparison between linear and loglinear mixing frameworks, with uniform weights used in the linear mixture.</S><S sid = 94 ssid = >Due to this result, all experiments we describe below involve linear mixtures only.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W07-0717.txt | Citing Article:  W09-0432.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered.</S> | Reference Offset:  ['10','11'] | Reference Text:  <S sid = 10 ssid = >In cross-domain adaptation, a small sample of parallel in-domain text is available, and it is used to optimize for translating future texts drawn from the same domain.</S><S sid = 11 ssid = >In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W07-0717.txt | Citing Article:  W12-3155.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model.</S> | Reference Offset:  ['77','87'] | Reference Text:  <S sid = 77 ssid = >A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation.</S><S sid = 87 ssid = >Because different development corpora are used for cross-domain and dynamic adaptation, we trained one static baseline model for each of these adaptation settings, on the corresponding development set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W07-0717.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice.</S> | Reference Offset:  ['32','99'] | Reference Text:  <S sid = 32 ssid = >Phrase translation model probabilities are features of the form: log p(s|t,a) ≈ EKk=1 log p(gk|�tk).</S><S sid = 99 ssid = >(Entries in the top right corner are missing due to lack of time.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W07-0717.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1.</S> | Reference Offset:  ['27','35'] | Reference Text:  <S sid = 27 ssid = >The remainder of the paper is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes.</S><S sid = 35 ssid = >To derive the joint counts c(g, t) from which p(s|� and p(�t|s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W07-0717.txt | Citing Article:  W10-1759.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components.</S> | Reference Offset:  ['1','37'] | Reference Text:  <S sid = 1 ssid = >We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.</S><S sid = 37 ssid = >Set mixture weights as a function of the distances from corpus components to the current source text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W07-0717.txt | Citing Article:  E12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006).</S> | Reference Offset:  ['17','92'] | Reference Text:  <S sid = 17 ssid = >This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (Tillmann and Zhang, 2006; Liang et al., 2006).</S><S sid = 92 ssid = >We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W07-0717.txt | Citing Article:  C10-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling.</S> | Reference Offset:  ['83','135'] | Reference Text:  <S sid = 83 ssid = >The training corpus was divided into seven components according to genre; in all cases these were identical to LDC corpora, with the exception of the Newswire component, which was amalgamated from several smaller corpora.</S><S sid = 135 ssid = >The most successful is to weight component models in proportion to maximum-likelihood (EM) weights for the current text given an ngram language model mixture trained on corpus components.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W07-0717.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007).</S> | Reference Offset:  ['57','87'] | Reference Text:  <S sid = 57 ssid = >Perplexity (Jelinek, 1997) is a standard way of evaluating the quality of a language model on a test text.</S><S sid = 87 ssid = >Because different development corpora are used for cross-domain and dynamic adaptation, we trained one static baseline model for each of these adaptation settings, on the corresponding development set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W07-0717.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mixture-modelling for language models is well established (Foster and Kuhn, 2007).</S> | Reference Offset:  ['49','65'] | Reference Text:  <S sid = 49 ssid = >The most commonly-used framework for mixture models is a linear one: where p(x|h) is either a language or translation model; pc(x|h) is a model trained on component c, and λc is the corresponding weight.</S><S sid = 65 ssid = >Loglinear mixture models were not used for dynamic adaptation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W07-0717.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand.</S> | Reference Offset:  ['106','110'] | Reference Text:  <S sid = 106 ssid = >Both LM and TM adaptation are effective, with test-set improvements of approximately 1 BLEU point over the baseline for LM adaptation and somewhat less for TM adaptation.</S><S sid = 110 ssid = >In this setting, TM adaptation is much less effective, not significantly better than the baseline; performance of combined LM and TM adaptation is also lower.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W07-0717.txt | Citing Article:  W11-2211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before.</S> | Reference Offset:  ['64','134'] | Reference Text:  <S sid = 64 ssid = >When using a loglinear combining framework as described in section 3.3, mixture weights are set in the same way as the other loglinear parameters when performing cross-domain adaptation.</S><S sid = 134 ssid = >We have investigated a number of approaches to mixture-based adaptation using genres for Chinese to English translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W07-0717.txt | Citing Article:  P13-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007).</S> | Reference Offset:  ['49','65'] | Reference Text:  <S sid = 49 ssid = >The most commonly-used framework for mixture models is a linear one: where p(x|h) is either a language or translation model; pc(x|h) is a model trained on component c, and λc is the corresponding weight.</S><S sid = 65 ssid = >Loglinear mixture models were not used for dynamic adaptation.</S> | Discourse Facet:  NA | Annotator: Automatic


