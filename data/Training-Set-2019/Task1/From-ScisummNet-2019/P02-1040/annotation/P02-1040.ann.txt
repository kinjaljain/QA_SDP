Citance Number: 1 | Reference Article:  P02-1040.txt | Citing Article:  P02-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002).</S> | Reference Offset:  ['13','22'] | Reference Text:  <S sid = 13 ssid = >How does one measure translation performance?</S><S sid = 22 ssid = >In Section 3, we evaluate the performance of BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1040.txt | Citing Article:  P14-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality.</S> | Reference Offset:  ['22','169'] | Reference Text:  <S sid = 22 ssid = >In Section 3, we evaluate the performance of BLEU.</S><S sid = 169 ssid = >Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1040.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002).</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >Bleu: A Method For Automatic Evaluation Of Machine Translation</S><S sid = 16 ssid = >To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1040.txt | Citing Article:  P10-2026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003).</S> | Reference Offset:  ['22','56'] | Reference Text:  <S sid = 22 ssid = >In Section 3, we evaluate the performance of BLEU.</S><S sid = 56 ssid = >A translation using the same words (1-grams) as in the references tends to satisfy adequacy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1040.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['37','169'] | Reference Text:  <S sid = 37 ssid = >Experiments over large collections of translations presented in Section 5 show that this ranking ability is a general phenomenon, and not an artifact of a few toy examples.</S><S sid = 169 ssid = >Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1040.txt | Citing Article:  W03-0501.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >Bleu: A Method For Automatic Evaluation Of Machine Translation</S><S sid = 3 ssid = >We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1040.txt | Citing Article:  N07-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09].</S> | Reference Offset:  ['21','152'] | Reference Text:  <S sid = 21 ssid = >In Section 2, we describe the baseline metric in detail.</S><S sid = 152 ssid = >Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1040.txt | Citing Article:  P03-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper.</S> | Reference Offset:  ['0','12'] | Reference Text:  <S sid = 0 ssid = >Bleu: A Method For Automatic Evaluation Of Machine Translation</S><S sid = 12 ssid = >We propose such an evaluation method in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1040.txt | Citing Article:  N07-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002).</S> | Reference Offset:  ['22','151'] | Reference Text:  <S sid = 22 ssid = >In Section 3, we evaluate the performance of BLEU.</S><S sid = 151 ssid = >We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1040.txt | Citing Article:  W07-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation.</S> | Reference Offset:  ['137','169'] | Reference Text:  <S sid = 137 ssid = >This outcome suggests that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator.</S><S sid = 169 ssid = >Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1040.txt | Citing Article:  W11-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['87','176'] | Reference Text:  <S sid = 87 ssid = >To some extent, the n-gram precision already accomplishes this.</S><S sid = 176 ssid = >We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of ChineseEnglish MT systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1040.txt | Citing Article:  D10-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric.</S> | Reference Offset:  ['0','169'] | Reference Text:  <S sid = 0 ssid = >Bleu: A Method For Automatic Evaluation Of Machine Translation</S><S sid = 169 ssid = >Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1040.txt | Citing Article:  W08-0312.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor.</S> | Reference Offset:  ['44','169'] | Reference Text:  <S sid = 44 ssid = >Unfortunately, MT systems can overgenerate “reasonable” words, resulting in improbable, but high-precision, translations like that of example 2 below.</S><S sid = 169 ssid = >Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1040.txt | Citing Article:  W07-0409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002).</S> | Reference Offset:  ['116','157'] | Reference Text:  <S sid = 116 ssid = >We compute the brevity penalty BP, The ranking behavior is more immediately apparent in the log domain, log BLEU = min(1 − In our baseline, we use N = 4 and uniform weights wn = 1/N.</S><S sid = 157 ssid = >5 BLEU vs The Human Evaluation Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1040.txt | Citing Article:  C10-2144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores.</S> | Reference Offset:  ['131','169'] | Reference Text:  <S sid = 131 ssid = >Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are statistically very significant.</S><S sid = 169 ssid = >Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1040.txt | Citing Article:  W12-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002).</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >Bleu: A Method For Automatic Evaluation Of Machine Translation</S><S sid = 10 ssid = >We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from 1So we call our method the bilingual evaluation understudy, BLEU. the evaluation bottleneck.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1040.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.</S> | Reference Offset:  ['22','64'] | Reference Text:  <S sid = 22 ssid = >In Section 3, we evaluate the performance of BLEU.</S><S sid = 64 ssid = >2.1.2 Ranking systems using only modified n-gram precision To verify that modified n-gram precision distinguishes between very good translations and bad translations, we computed the modified precision numbers on the output of a (good) human translator and a standard (poor) machine translation system using 4 reference translations for each of 127 source sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1040.txt | Citing Article:  W04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation.</S> | Reference Offset:  ['26','90'] | Reference Text:  <S sid = 26 ssid = >These translations may vary in word choice or in word order even when they use the same words.</S><S sid = 90 ssid = >This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1040.txt | Citing Article:  D10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output.</S> | Reference Offset:  ['122','162'] | Reference Text:  <S sid = 122 ssid = >Table 1 shows the BLEU scores of the 5 systems against two references on this test corpus.</S><S sid = 162 ssid = >We now take the worst system as a reference point and compare the BLEU scores with the human judgment scores of the remaining systems relative to the worst system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1040.txt | Citing Article:  P12-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi.</S> | Reference Offset:  ['13','22'] | Reference Text:  <S sid = 13 ssid = >How does one measure translation performance?</S><S sid = 22 ssid = >In Section 3, we evaluate the performance of BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


