Citance Number: 1 | Reference Article:  N04-1025.txt | Citing Article:  W12-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages.</S> | Reference Offset:  ['57','204'] | Reference Text:  <S sid = 57 ssid = >To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).</S><S sid = 204 ssid = >We also showed that the Smoothed Unigram method is robust for short passages and Web documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1025.txt | Citing Article:  D08-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','211'] | Reference Text:  <S sid = 78 ssid = >This is a well-known issue in language model applications, and it is standard to compensate for this sparseness by smoothing the frequencies in the trained models.</S><S sid = 211 ssid = >We thank the anonymous reviewers for their comments and Luo Si for helpful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1025.txt | Citing Article:  W06-1809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  ['65','199'] | Reference Text:  <S sid = 65 ssid = >Our statistical model is based on a variation of the multinomial naïve Bayes classifier, which we call the ‘Smoothed Unigram’ model.</S><S sid = 199 ssid = >We have shown that reading difficulty can be estimated with a simple language modeling approach using a modified naïve Bayes classifier.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1025.txt | Citing Article:  W06-1809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  ['188','201'] | Reference Text:  <S sid = 188 ssid = >While word difficulty is well-known to be an excellent predictor of reading difficulty (Chall & Edgar, 1995), it was not at all clear how effective our language model approach would be for predicting Web page reading difficulty.</S><S sid = 201 ssid = >Our evaluation suggests that reasonably effective models can be trained with small amounts of easilyacquired data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1025.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents.</S> | Reference Offset:  ['16','204'] | Reference Text:  <S sid = 16 ssid = >As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).</S><S sid = 204 ssid = >We also showed that the Smoothed Unigram method is robust for short passages and Web documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1025.txt | Citing Article:  N07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  ['7','16'] | Reference Text:  <S sid = 7 ssid = >Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).</S><S sid = 16 ssid = >As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1025.txt | Citing Article:  H05-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998).</S> | Reference Offset:  ['57','139'] | Reference Text:  <S sid = 57 ssid = >To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).</S><S sid = 139 ssid = >However, Reading A-Z documents were written to pre-established criteria which includes objective factors such as type/ token ratio (Reading A-Z.com, 2003), so it is not surprising that the correlation is high.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1025.txt | Citing Article:  H05-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','211'] | Reference Text:  <S sid = 78 ssid = >This is a well-known issue in language model applications, and it is standard to compensate for this sparseness by smoothing the frequencies in the trained models.</S><S sid = 211 ssid = >We thank the anonymous reviewers for their comments and Luo Si for helpful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1025.txt | Citing Article:  W11-2308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008).</S> | Reference Offset:  ['111','203'] | Reference Text:  <S sid = 111 ssid = >Previous work (Stenner, 1996, also citing Squires et al., 1983 and Crawford et al., 1975) suggests that a comprehension rate of 75% for a text is a desirable target.</S><S sid = 203 ssid = >As an example of retraining, we showed that the classifier obtained good correlation with difficulty for at least two languages, English and French, with the only algorithm difference being a change in the morphology handling during feature processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1025.txt | Citing Article:  N12-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages.</S> | Reference Offset:  ['16','204'] | Reference Text:  <S sid = 16 ssid = >As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).</S><S sid = 204 ssid = >We also showed that the Smoothed Unigram method is robust for short passages and Web documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1025.txt | Citing Article:  W08-1605.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school.</S> | Reference Offset:  ['65','169'] | Reference Text:  <S sid = 65 ssid = >Our statistical model is based on a variation of the multinomial naïve Bayes classifier, which we call the ‘Smoothed Unigram’ model.</S><S sid = 169 ssid = >We derived the learning curve of our classifier as a function of the mean model training set size in tokens.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1025.txt | Citing Article:  W12-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text.</S> | Reference Offset:  ['2','57'] | Reference Text:  <S sid = 2 ssid = >We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.</S><S sid = 57 ssid = >To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1025.txt | Citing Article:  P05-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)).</S> | Reference Offset:  ['9','134'] | Reference Text:  <S sid = 9 ssid = >Widely-used traditional readability formulas such as Flesch-Kincaid usually perform poorly in this scenario.</S><S sid = 134 ssid = >We also included a fourth predictor: the FleschKincaid score (Kincaid et al. 1975), which is a linear combination of the text’s average sentence length (in tokens), and the average number of syllables per token.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1025.txt | Citing Article:  P05-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  ['15','46'] | Reference Text:  <S sid = 15 ssid = >While traditional formulas are based on linear regression with two or three variables, statistical language models can capture more detailed patterns of individual word usage.</S><S sid = 46 ssid = >The word ‘the’ is very common and varies less in frequency across grade levels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1025.txt | Citing Article:  E09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  ['19','114'] | Reference Text:  <S sid = 19 ssid = >Section 3 summarizes related work on readability, focusing on existing vocabulary-based measures that can be thought of as simplified language model techniques.</S><S sid = 114 ssid = >The results from the most closely related previous work (Si and Callan, 2001) are not directly comparable to ours; among other factors, their task used a dataset trained on science curriculum descriptions, not text written at different levels of difficulty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1025.txt | Citing Article:  E09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  ['49','114'] | Reference Text:  <S sid = 49 ssid = >A comprehensive summary of early readability work may be found in Chall (1958) and Klare (1963).</S><S sid = 114 ssid = >The results from the most closely related previous work (Si and Callan, 2001) are not directly comparable to ours; among other factors, their task used a dataset trained on science curriculum descriptions, not text written at different levels of difficulty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1025.txt | Citing Article:  D12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French.</S> | Reference Offset:  ['122','177'] | Reference Text:  <S sid = 122 ssid = >Finally, we looked at how well the model could be extended to a language other than English – in this study, we give results for French.</S><S sid = 177 ssid = >To test the flexibility of our language model approach, we did a preliminary study for French reading difficulty prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


