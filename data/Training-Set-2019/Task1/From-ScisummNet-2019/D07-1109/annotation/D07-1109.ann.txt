Citance Number: 1 | Reference Article:  D07-1109.txt | Citing Article:  C10-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Work such as (Cai et al, 2007) or (Boyd-Graber et al., 2007) use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambiguation.</S> | Reference Offset:  ['0','43'] | Reference Text:  <S sid = 0 ssid = >A Topic Model for Word Sense Disambiguation</S><S sid = 43 ssid = >These properties allow us to develop LDAWN, which is a fusion of these WORDNET-WALKs and latent Dirichlet alocation (LDA) (Blei et al, 2003),a probabilistic model of documents that is an im provement to pLSI (Hofmann, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1109.txt | Citing Article:  P12-2054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Abney and Light (1999) used tree structured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al, 2007).</S> | Reference Offset:  ['83','203'] | Reference Text:  <S sid = 83 ssid = >This is equivalent to the need for balancing as noted by Abney and Light (1999).</S><S sid = 203 ssid = >the usage of ambiguous terms in selectional restrictions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1109.txt | Citing Article:  P10-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Boyd-Graber et al (2007) integrate a model of random walks on the WordNet graph into an LDA topic model to build an unsupervised word sense disambiguation system.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >A Topic Model for Word Sense Disambiguation</S><S sid = 1 ssid = >We develop latent Dirichlet alocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1109.txt | Citing Article:  S12-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Boyd-Graber et al (2007) describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned.</S> | Reference Offset:  ['0','67'] | Reference Text:  <S sid = 0 ssid = >A Topic Model for Word Sense Disambiguation</S><S sid = 67 ssid = >In thecase of LDAWN, the hidden variables are the parameters of the K WORDNET-WALKs, the topic assign ments of each word in the collection, and the synset path of each word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1109.txt | Citing Article:  S12-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gibbs sampling updates for LDAWN are given in Boyd-Graber et al (2007).</S> | Reference Offset:  ['14','97'] | Reference Text:  <S sid = 14 ssid = >Topic models have re cently been applied to information retrieval (Wei and Croft, 2006), text classification (Blei et al, 2003), and dialogue segmentation (Purver et al, 2006).</S><S sid = 97 ssid = >In LDAWN, the samples contain a configuration of the latent semantic states of the system, revealing the hidden topics and paths that likely led to the observed data.Gibbs sampling reproduces the posterior distri bution by repeatedly sampling each hidden variable conditioned on the current state of the other hidden variables and observations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1109.txt | Citing Article:  D11-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, SemCor is also the dataset used in (Boyd-Graber et al, 2007), where a WordNet based topic model for WSD is introduced.</S> | Reference Offset:  ['22','118'] | Reference Text:  <S sid = 22 ssid = >in WORDNET.</S><S sid = 118 ssid = >Of the two data sets used during the course of our evaluation, the primary dataset was SEMCOR (Miller et al, 1993), which is a subset of the Brown corpus with many nouns manually labeled with the correct WORDNET sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1109.txt | Citing Article:  D11-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >STM10 achieves similar results as in LDAWN (Boyd-Graber et al, 2007) which was specifically designed for WSD.</S> | Reference Offset:  ['20','74'] | Reference Text:  <S sid = 20 ssid = >Finally, we evaluate our system on real-world WSD data, discuss the properties of the topics and disambiguation accuracy results, and draw connections to other WSD algorithms from the research literature.</S><S sid = 74 ssid = >The intuition behind LDAWN is that the words in a topic will have similar meanings and thus share paths within WORDNET.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1109.txt | Citing Article:  D11-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Boyd-Graber et al (2007) are the first to integrate semantics into the topic model framework.</S> | Reference Offset:  ['0','52'] | Reference Text:  <S sid = 0 ssid = >A Topic Model for Word Sense Disambiguation</S><S sid = 52 ssid = >For each topic, k ? {1, . . .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1109.txt | Citing Article:  P10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al, 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al, 2007).</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >A Topic Model for Word Sense Disambiguation</S><S sid = 14 ssid = >Topic models have re cently been applied to information retrieval (Wei and Croft, 2006), text classification (Blei et al, 2003), and dialogue segmentation (Purver et al, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D07-1109.txt | Citing Article:  P10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In another unsupervised system, Boyd-Graber et al (2007) enhance the basic LDA algorithm by incorporating WordNet senses as an additional latent variable.</S> | Reference Offset:  ['1','229'] | Reference Text:  <S sid = 1 ssid = >We develop latent Dirichlet alocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.</S><S sid = 229 ssid = >As most errors were attributable to the hyponomy structure of WORDNET, incorporating the novel use of topicmodeling presented here with a more mature unsu pervised WSD algorithm to replace the underlyingWORDNET-WALK could lead to advances in state of-the-art unsupervised WSD accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D07-1109.txt | Citing Article:  P10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This dataset, which consists of 320,000 articles, is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al (2007).</S> | Reference Offset:  ['118','119'] | Reference Text:  <S sid = 118 ssid = >Of the two data sets used during the course of our evaluation, the primary dataset was SEMCOR (Miller et al, 1993), which is a subset of the Brown corpus with many nouns manually labeled with the correct WORDNET sense.</S><S sid = 119 ssid = >The words in this dataset are lemmatized, and multi-word expressions that are present in WORDNET are identified.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D07-1109.txt | Citing Article:  D10-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graberetal., 2007) and to encode clustering constraints (Andrzejewski et al, 2009).</S> | Reference Offset:  ['14','84'] | Reference Text:  <S sid = 14 ssid = >Topic models have re cently been applied to information retrieval (Wei and Croft, 2006), text classification (Blei et al, 2003), and dialogue segmentation (Purver et al, 2006).</S><S sid = 84 ssid = >The other function that the Dirichlet prior serves is to enable us to encode any information we have about how we suspect the transitions to childrennodes will be distributed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D07-1109.txt | Citing Article:  D10-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN's (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior.</S> | Reference Offset:  ['90','212'] | Reference Text:  <S sid = 90 ssid = >given the overall corpus counts (see Figure 1, which shows prior transition probabilities for ?revolver?).By setting ?s,i, the prior probability of transitioning from synset s to node i, proportional to the to tal number of observed tokens in the children of i, 1026we introduce a probabilistic variation on information content (Resnik, 1995).</S><S sid = 212 ssid = >This technique could indeed be used with any hierarchy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D07-1109.txt | Citing Article:  E12-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >LDAWN (Boyd-Graber et al 2007) models sets of words for the word sense disambiguation task.</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >A Topic Model for Word Sense Disambiguation</S><S sid = 4 ssid = >Word sense disambiguation (WSD) is the task of determining the meaning of an ambiguous word in its context.</S> | Discourse Facet:  NA | Annotator: Automatic


