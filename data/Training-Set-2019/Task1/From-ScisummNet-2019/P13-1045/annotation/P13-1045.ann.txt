Citance Number: 1 | Reference Article:  P13-1045.txt | Citing Article:  P14-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','250'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 250 ssid = >Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P13-1045.txt | Citing Article:  P14-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','250'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 250 ssid = >Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P13-1045.txt | Citing Article:  P14-1130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another application of word vectors is compositional vector grammar (Socher et al, 2013).</S> | Reference Offset:  ['78','129'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 129 ssid = >Socher et al. (2012) proposed to give every single word a matrix and a vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P13-1045.txt | Citing Article:  P14-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013).</S> | Reference Offset:  ['69','129'] | Reference Text:  <S sid = 69 ssid = >This paper uses several ideas of (Socher et al., 2011b).</S><S sid = 129 ssid = >Socher et al. (2012) proposed to give every single word a matrix and a vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P13-1045.txt | Citing Article:  P14-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels.</S> | Reference Offset:  ['110','125'] | Reference Text:  <S sid = 110 ssid = >3.1 and the POS tags come from a PCFG.</S><S sid = 125 ssid = >For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P13-1045.txt | Citing Article:  P14-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','250'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 250 ssid = >Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P13-1045.txt | Citing Article:  P14-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a).</S> | Reference Offset:  ['125','223'] | Reference Text:  <S sid = 125 ssid = >For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).</S><S sid = 223 ssid = >Analysis of Composition Matrices.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P13-1045.txt | Citing Article:  P14-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a).</S> | Reference Offset:  ['78','231'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 231 ssid = >In this small model analysis, we use two pairs of sentences that the original Stanford parser and the CVG did not parse correctly after training on the WSJ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P13-1045.txt | Citing Article:  P14-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes.</S> | Reference Offset:  ['67','125'] | Reference Text:  <S sid = 67 ssid = >Our syntactically untied RNNs outperform them by a significant margin.</S><S sid = 125 ssid = >For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P13-1045.txt | Citing Article:  P14-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge.</S> | Reference Offset:  ['78','88'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 88 ssid = >For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P13-1045.txt | Citing Article:  P14-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013).</S> | Reference Offset:  ['78','125'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 125 ssid = >For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P13-1045.txt | Citing Article:  P14-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)).</S> | Reference Offset:  ['3','210'] | Reference Text:  <S sid = 3 ssid = >The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.</S><S sid = 210 ssid = >The dev set accuracy of the best model is 90.93% labeled F1 on all sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P13-1045.txt | Citing Article:  P14-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013).</S> | Reference Offset:  ['61','243'] | Reference Text:  <S sid = 61 ssid = >Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser.</S><S sid = 243 ssid = >The compositional vectors are learned with a new syntactically untied recursive neural network.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P13-1045.txt | Citing Article:  P14-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','250'] | Reference Text:  <S sid = 78 ssid = >Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).</S><S sid = 250 ssid = >Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S> | Discourse Facet:  NA | Annotator: Automatic


