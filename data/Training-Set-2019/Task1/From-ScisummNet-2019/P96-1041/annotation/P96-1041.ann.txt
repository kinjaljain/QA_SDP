Citance Number: 1 | Reference Article:  P96-1041.txt | Citing Article:  W02-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Smoothing typically adds considerable computational complexity to the system since multiple models need to be estimated and applied together, and it is often considered a black art (Chen and Goodman, 1996).</S> | Reference Offset:  ['22','133'] | Reference Text:  <S sid = 22 ssid = >Typically, n is taken to be two or three, corresponding to a bigram or trigram model, respectively.'</S><S sid = 133 ssid = >We find that the two most widely used techniques, Katz smoothing and Jelinek-Mercer smoothing, perform consistently well across training set sizes for both bigram and trigram models, with Katz smoothing performing better on trigram models produced from large training sets and on bigram models in general.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P96-1041.txt | Citing Article:  P14-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Constant restoring is similar to the additive smoothing, which isused to solve the zero-frequency problem of language models (Chen and Goodman, 1996).</S> | Reference Offset:  ['45','90'] | Reference Text:  <S sid = 45 ssid = >Each n-gram is assigned to one of several buckets based on its frequency predicted from lower-order models.</S><S sid = 90 ssid = >We smooth the unigram distribution using additive smoothing with parameter b.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P96-1041.txt | Citing Article:  D11-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models.</S> | Reference Offset:  ['18','136'] | Reference Text:  <S sid = 18 ssid = >While being relatively simple to implement, we show that these methods yield good performance in bigram models and superior performance in trigram models.</S><S sid = 136 ssid = >Finally, we find that our novel methods average-count and one-count are superior to existing methods for trigram models and perform well on bigram models; method one-count yields marginally worse performance but is extremely easy to implement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P96-1041.txt | Citing Article:  W07-2044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >I used a smoothing method loosely based on the one-count method given in (Chen and Goodman, 1996).</S> | Reference Offset:  ['104','136'] | Reference Text:  <S sid = 104 ssid = >(new-avg-count and new-one-count) The implementation new-avg-count, corresponding to smoothing method average-count, is identical to interp-held-out except that we use the novel bucketing scheme described in section 3.1.</S><S sid = 136 ssid = >Finally, we find that our novel methods average-count and one-count are superior to existing methods for trigram models and perform well on bigram models; method one-count yields marginally worse performance but is extremely easy to implement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P96-1041.txt | Citing Article:  P09-2058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data.</S> | Reference Offset:  ['26','35'] | Reference Text:  <S sid = 26 ssid = >While intuitive, the maximum likelihood estimate is a poor one when the amount of training data is small compared to the size of the model being built, as is generally the case in language modeling.</S><S sid = 35 ssid = >To address this, another smoothing technique is to interpolate the bigram model with a unigram model PmL(wi) = c(w)/N5, a model that reflects how often each word occurs in the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P96-1041.txt | Citing Article:  W08-0326.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified Kneser Ney discounting (Chen and Goodman, 1996).</S> | Reference Offset:  ['3','17'] | Reference Text:  <S sid = 3 ssid = >In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.</S><S sid = 17 ssid = >In addition, we introduce two novel smoothing techniques: the first belonging to the class of smoothing models described by Jelinek and Mercer, the second a very simple linear interpolation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P96-1041.txt | Citing Article:  W09-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Alternatively, smoothing techniques (Chen and Goodman, 1996) redistribute the probabilities, taking into account previously unseen word sequences.</S> | Reference Offset:  ['68','135'] | Reference Text:  <S sid = 68 ssid = >We have found that taking is the number of words with one count, and where )3 and -y are constants.</S><S sid = 135 ssid = >Furthermore, we show that Church-Gale smoothing, which previously had not been compared with common smoothing techniques, outperforms all existing methods on bigram models produced from large training sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P96-1041.txt | Citing Article:  W09-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996).</S> | Reference Offset:  ['28','97'] | Reference Text:  <S sid = 28 ssid = >Then, we have PmL(thelburnish) = 0, which is clearly inaccurate as this probability should be larger than zero.</S><S sid = 97 ssid = >In particular, it is unclear whether to bucket trigrams according to P(wr_21)P(wi) or P(w:=21)P(wilwi_1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P96-1041.txt | Citing Article:  N12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations.</S> | Reference Offset:  ['8','55'] | Reference Text:  <S sid = 8 ssid = >While smoothing is a central issue in language modeling, the literature lacks a definitive comparison between the many existing techniques.</S><S sid = 55 ssid = >This scheme is an instance of Jelinek-Mercer smoothing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P96-1041.txt | Citing Article:  N12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events.</S> | Reference Offset:  ['0','39'] | Reference Text:  <S sid = 0 ssid = >An Empirical Study Of Smoothing Techniques For Language Modeling</S><S sid = 39 ssid = >It is not used directly for n-gram smoothing because, like additive smoothing, it does not perform the interpolation of lower- and higher-order models essential for good performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P96-1041.txt | Citing Article:  D10-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events.</S> | Reference Offset:  ['0','39'] | Reference Text:  <S sid = 0 ssid = >An Empirical Study Of Smoothing Techniques For Language Modeling</S><S sid = 39 ssid = >It is not used directly for n-gram smoothing because, like additive smoothing, it does not perform the interpolation of lower- and higher-order models essential for good performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P96-1041.txt | Citing Article:  P09-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity.</S> | Reference Offset:  ['90','133'] | Reference Text:  <S sid = 90 ssid = >We smooth the unigram distribution using additive smoothing with parameter b.</S><S sid = 133 ssid = >We find that the two most widely used techniques, Katz smoothing and Jelinek-Mercer smoothing, perform consistently well across training set sizes for both bigram and trigram models, with Katz smoothing performing better on trigram models produced from large training sets and on bigram models in general.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P96-1041.txt | Citing Article:  W11-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Giga word corpus (Parker et al, 2009).</S> | Reference Offset:  ['0','5'] | Reference Text:  <S sid = 0 ssid = >An Empirical Study Of Smoothing Techniques For Language Modeling</S><S sid = 5 ssid = >A language model is a probability distribution over strings P(s) that attempts to reflect the frequency with which each string s occurs as a sentence in natural text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P96-1041.txt | Citing Article:  D12-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use one-count smoothing (Chen and Goodman, 1996), where (ti) is based on the number of words that occur with ti once: (ti)= |wi: C (ti ,wi)= 1|.</S> | Reference Offset:  ['19','68'] | Reference Text:  <S sid = 19 ssid = >We take the performance of a method in to be its cross-entropy on test data where Pin (ti) denotes the language model produced with method 711 and where the test data T is composed of sentences (ti, , and contains a total of NT words.</S><S sid = 68 ssid = >We have found that taking is the number of words with one count, and where )3 and -y are constants.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P96-1041.txt | Citing Article:  W12-3131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Modified Knesser-Ney smoothed (Chen and Goodman, 1996) n-gram language models are built from the monolingual English data using the SRI language modeling toolkit (Stolke, 2002).</S> | Reference Offset:  ['0','26'] | Reference Text:  <S sid = 0 ssid = >An Empirical Study Of Smoothing Techniques For Language Modeling</S><S sid = 26 ssid = >While intuitive, the maximum likelihood estimate is a poor one when the amount of training data is small compared to the size of the model being built, as is generally the case in language modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P96-1041.txt | Citing Article:  W09-0417.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney back off (Chen and Goodman, 1996).</S> | Reference Offset:  ['6','12'] | Reference Text:  <S sid = 6 ssid = >Language models are used in speech recognition to resolve acoustically ambiguous utterances.</S><S sid = 12 ssid = >We carry out experiments over many training data sizes on varied corpora using both bigram and trigram models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P96-1041.txt | Citing Article:  D09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996).</S> | Reference Offset:  ['81','95'] | Reference Text:  <S sid = 81 ssid = >Due to space limitations, these descriptions are not comprehensive; a more complete discussion .is presented in Chen (1996).</S><S sid = 95 ssid = >Thus, instead of partitioning the space of P(wi_i)P(wi) values in some uniform way as was done by Church and Gale, we partition the space so that at least crnin non-zero n-grams fall in each bucket.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P96-1041.txt | Citing Article:  D09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used one-count smoothing (Chen and Goodman, 1996).</S> | Reference Offset:  ['104','136'] | Reference Text:  <S sid = 104 ssid = >(new-avg-count and new-one-count) The implementation new-avg-count, corresponding to smoothing method average-count, is identical to interp-held-out except that we use the novel bucketing scheme described in section 3.1.</S><S sid = 136 ssid = >Finally, we find that our novel methods average-count and one-count are superior to existing methods for trigram models and perform well on bigram models; method one-count yields marginally worse performance but is extremely easy to implement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P96-1041.txt | Citing Article:  D10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used.</S> | Reference Offset:  ['6','133'] | Reference Text:  <S sid = 6 ssid = >Language models are used in speech recognition to resolve acoustically ambiguous utterances.</S><S sid = 133 ssid = >We find that the two most widely used techniques, Katz smoothing and Jelinek-Mercer smoothing, perform consistently well across training set sizes for both bigram and trigram models, with Katz smoothing performing better on trigram models produced from large training sets and on bigram models in general.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P96-1041.txt | Citing Article:  W12-3160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We constructed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRI language modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996).</S> | Reference Offset:  ['0','26'] | Reference Text:  <S sid = 0 ssid = >An Empirical Study Of Smoothing Techniques For Language Modeling</S><S sid = 26 ssid = >While intuitive, the maximum likelihood estimate is a poor one when the amount of training data is small compared to the size of the model being built, as is generally the case in language modeling.</S> | Discourse Facet:  NA | Annotator: Automatic


