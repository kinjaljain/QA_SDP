Citance Number: 1 | Reference Article:  A97-1004.txt | Citing Article:  P02-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >finding sentence boundaries (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['0','35'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Identifying Sentence Boundaries</S><S sid = 35 ssid = >We present two systems for identifying sentence boundaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A97-1004.txt | Citing Article:  D12-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003).</S> | Reference Offset:  ['56','61'] | Reference Text:  <S sid = 56 ssid = >The parameters are chosen to maximize the likelihood of the training data, using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm.</S><S sid = 61 ssid = >We corrected punctuation mistakes and erroneous sentence boundaries in the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A97-1004.txt | Citing Article:  W04-3210.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We trained a publicly available sentence splitter (Reynar and Ratnaparkhi, 1997) on a small manually annotated sample (1,000 sentences per domain per language) and applied it to our corpora.</S> | Reference Offset:  ['7','70'] | Reference Text:  <S sid = 7 ssid = >Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g.</S><S sid = 70 ssid = >Since 39441 training sentences is considerably more than might exist in a new domain or a language other than English, we experimented with the quantity of training data required to maintain performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A97-1004.txt | Citing Article:  D07-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also used MXTerminator (Reynar and Ratnaparkhi, 1997) for sentence segmentation, MINIPAR (Lin, 1993) for lemmatization and dependency parsing, and MATLAB3 for SVD computation.</S> | Reference Offset:  ['12','48'] | Reference Text:  <S sid = 12 ssid = >However, these punctuation marks are not used exclusively to mark sentence breaks.</S><S sid = 48 ssid = >The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratna.parkhi, 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A97-1004.txt | Citing Article:  P12-2049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sentence segmentation Off-the-shelf sentence segmentators tend to be trained on newswire texts (Reynar and Ratnaparkhi, 1997), which significantly differ from the noisy text in our corpus.</S> | Reference Offset:  ['66','74'] | Reference Text:  <S sid = 66 ssid = >Performance on the WSJ corpus was, as we expected, higher than performance on the Brown corpus since we trained the model on financial newspaper text.</S><S sid = 74 ssid = >For example, Riley's performance on the Brown corpus is higher than ours, but his system is trained on the Brown corpus and uses thirty times as much data as our system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A97-1004.txt | Citing Article:  N09-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period.</S> | Reference Offset:  ['39','53'] | Reference Text:  <S sid = 39 ssid = >We use information about the token containing the potential sentence boundary, as well as contextual information about the tokens immediately to the left and to the right.</S><S sid = 53 ssid = >The contextual information deemed useful for sentence-boundary detection, which. we described earlier, must be encoded using features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A97-1004.txt | Citing Article:  N09-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One common objection to supervised SBD systems is an observation in (Reynar and Ratnaparkhi, 1997), that training data and test data must be a good match, limiting the applicability of a model trained from a specific genre.</S> | Reference Offset:  ['4','63'] | Reference Text:  <S sid = 4 ssid = >The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language.</S><S sid = 63 ssid = >The first test set, WSJ, is Palmer and Hearst's initial test data and the second is the entire Brown corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A97-1004.txt | Citing Article:  N03-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detection (Reynar and Ratnaparkhi, 1997), and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications (Brill, 1995).</S> | Reference Offset:  ['0','48'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Identifying Sentence Boundaries</S><S sid = 48 ssid = >The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratna.parkhi, 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A97-1004.txt | Citing Article:  W08-1801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The corpus was prepared using MXTerminator (Reynar and Ratnaparkhi,1997) for sentence segmentation, BBN Identifinder (Bikel et al, 1999) for named entity recognition, as well as the aforementioned ASSERT for identification of verb predicate-argument structures and PropBank-style semantic role labeling of the arguments.</S> | Reference Offset:  ['10','33'] | Reference Text:  <S sid = 10 ssid = >(Cutting et al., 1992)).</S><S sid = 33 ssid = >His performance on I he Brown corpus is 99.8%, using a model learned from a. corpus of 25 million words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A97-1004.txt | Citing Article:  W07-1414.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As the text part may consist of more than one sentence, we first perform sentence splitting using Mxterminator (Reynar and Ratnaparkhi, 1997), a maximum 83 entropy-based end of sentence classifier trained onthe Penn Treebank data.</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Identifying Sentence Boundaries</S><S sid = 20 ssid = >Instead, we present a solution based on a maximum entropy model which requires a few hints about what. information to use and a corpus annotated with sentence boundaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A97-1004.txt | Citing Article:  P05-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['11','30'] | Reference Text:  <S sid = 11 ssid = >On first glance, it may appear that using a short list, of sentence-final punctuation marks, such as ., ?, and !, is sufficient.</S><S sid = 30 ssid = >They obtained similar results using the decision tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A97-1004.txt | Citing Article:  P08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Obtained by segmenting (Reynar and Ratnaparkhi, 1997) the interviewee turns, and discarding sentences with only one word.</S> | Reference Offset:  ['30','54'] | Reference Text:  <S sid = 30 ssid = >They obtained similar results using the decision tree.</S><S sid = 54 ssid = >For example, a useful feature might be: This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A97-1004.txt | Citing Article:  N04-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >finding sentence boundaries (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['0','35'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Identifying Sentence Boundaries</S><S sid = 35 ssid = >We present two systems for identifying sentence boundaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A97-1004.txt | Citing Article:  N04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 1 presents information about article length (measured in sentences, as determined by the sentence separator of Reynar and Ratnaparkhi (1997)), vocabulary size, and token/type ratio for each domain.</S> | Reference Offset:  ['39','49'] | Reference Text:  <S sid = 39 ssid = >We use information about the token containing the potential sentence boundary, as well as contextual information about the tokens immediately to the left and to the right.</S><S sid = 49 ssid = >For each potential sentence boundary token (., ?, and !</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A97-1004.txt | Citing Article:  W04-3103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each abstract set was prepared for annotation as follows: the order of the abstracts was randomized and the abstracts were broken into sentences using Mxterminator (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['63','71'] | Reference Text:  <S sid = 63 ssid = >The first test set, WSJ, is Palmer and Hearst's initial test data and the second is the entire Brown corpus.</S><S sid = 71 ssid = >Table 3 shows performance on the WSJ corpus as a. function of training set size using the best performing system and the more portable system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A97-1004.txt | Citing Article:  P11-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It can be resolved fairly easily with rules in the form of regular expressions or in a machine-learning framework (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['4','47'] | Reference Text:  <S sid = 4 ssid = >The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language.</S><S sid = 47 ssid = >As a. result, no hand-crafted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A97-1004.txt | Citing Article:  W11-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The contents of these URLs were collected and only distinct web pages were retained. We use an HTMLparser3 to extract the textual con tents, and perform sentence segmentation (Reynar and Ratnaparkhi, 1997) on the parsed web pages.</S> | Reference Offset:  ['9','59'] | Reference Text:  <S sid = 9 ssid = >Others perform the division implicitly without discussing performance (e.g.</S><S sid = 59 ssid = >All experiments use a simple decision rule to classify each potential sentence boundary: a potential sentence boundary is an actual sentence boundary if and only if p(yesic) > .5, where and where c is the context including the potential sentence boundary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  A97-1004.txt | Citing Article:  P07-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['0','33'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Identifying Sentence Boundaries</S><S sid = 33 ssid = >His performance on I he Brown corpus is 99.8%, using a model learned from a. corpus of 25 million words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  A97-1004.txt | Citing Article:  W07-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sentence splitting, using mxterminator (Reynar and Ratnaparkhi, 1997).</S> | Reference Offset:  ['11','53'] | Reference Text:  <S sid = 11 ssid = >On first glance, it may appear that using a short list, of sentence-final punctuation marks, such as ., ?, and !, is sufficient.</S><S sid = 53 ssid = >The contextual information deemed useful for sentence-boundary detection, which. we described earlier, must be encoded using features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  A97-1004.txt | Citing Article:  W11-1607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006).</S> | Reference Offset:  ['10','66'] | Reference Text:  <S sid = 10 ssid = >(Cutting et al., 1992)).</S><S sid = 66 ssid = >Performance on the WSJ corpus was, as we expected, higher than performance on the Brown corpus since we trained the model on financial newspaper text.</S> | Discourse Facet:  NA | Annotator: Automatic


