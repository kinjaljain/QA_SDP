Citance Number: 1 | Reference Article:  J00-3004.txt | Citing Article:  W02-1815.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result.</S> | Reference Offset:  ['99','223'] | Reference Text:  <S sid = 99 ssid = >In our work we use the prediction by partial matching (PPM) symbolwise compression scheme (Cleary and Witten 1984), which has become a benchmark in the compression community.</S><S sid = 223 ssid = >For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J00-3004.txt | Citing Article:  W04-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Teahan (Teahan et al, 2000) has successfully applied escape method D to segment Chinese text.</S> | Reference Offset:  ['5','223'] | Reference Text:  <S sid = 5 ssid = >It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.</S><S sid = 223 ssid = >For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J00-3004.txt | Citing Article:  C04-1175.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Treating each character individually as in (Teahan et al, 2000) requires a large amount of training data in order to calculate all the probabilities in the tables, as well as a large amount of table space and time to lookup data from the tables.</S> | Reference Offset:  ['84','281'] | Reference Text:  <S sid = 84 ssid = >Good performance can be acquired using simple rules only if the training corpus is large enough.</S><S sid = 281 ssid = >5.6 Effect of the Amount of Training Data For the Rocling corpus, we experimented with different amounts of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J00-3004.txt | Citing Article:  C04-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000).</S> | Reference Offset:  ['79','80'] | Reference Text:  <S sid = 79 ssid = >Hockenmaier and Brew (1998) present an algorithm, based on Palmer's (1997) experiments, that applies a symbolic machine learning technique—transformation-based error-driven learning (Brill 1995)—to the problem of Chinese word segmentation.</S><S sid = 80 ssid = >Using a set of rule templates and four distinct initial-state annotators, Palmer concludes that the learning technique works well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J00-3004.txt | Citing Article:  C04-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the N-gram generative language modeling based approach of Teahan et al (2000) does not use domain knowledge.</S> | Reference Offset:  ['59','103'] | Reference Text:  <S sid = 59 ssid = >Several different algorithms have been proposed, which, generally speaking, can be classified into dictionary-based and statistical-based methods, although other techniques that involve more linguistic information, such as syntactic and semantic knowledge, have been reported in the natural language processing literature.</S><S sid = 103 ssid = >PPM is an n-gram approach that uses finite-context models of characters, where the previous few (say three) characters predict the upcoming one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J00-3004.txt | Citing Article:  C04-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcoming of CRF-based segmenters is that it requires significantly longer training time.</S> | Reference Offset:  ['73','291'] | Reference Text:  <S sid = 73 ssid = >Ponte and Croft (1996) introduce two models for word segmentation: word-based and bigram models.</S><S sid = 291 ssid = >For order 3 models, most words are segmented with the same error rate as for order 5 models, though some words are missed when order 2 models are used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J00-3004.txt | Citing Article:  W06-0129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach reduces to the cross-entropy/compression-based approach of (Teahan et al 2000).</S> | Reference Offset:  ['0','48'] | Reference Text:  <S sid = 0 ssid = >A Compression-Based Algorithm For Chinese Word Segmentation</S><S sid = 48 ssid = >This approach means that we can capitalize on existing research in text compression to create good models for word segmentation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J00-3004.txt | Citing Article:  P03-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, the probabilistic models used in these methods (e.g. Teahan et al, 2000) are trained on a segmented corpus which is not always available.</S> | Reference Offset:  ['223','291'] | Reference Text:  <S sid = 223 ssid = >For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).</S><S sid = 291 ssid = >For order 3 models, most words are segmented with the same error rate as for order 5 models, though some words are missed when order 2 models are used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J00-3004.txt | Citing Article:  W08-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is in contrast to supervised word segmentation algorithms (e.g., Teahan et al, 2000), which are typically used for segmenting text in documents written in languages that do not put spaces between their words like Chinese.</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >Chinese is written without using spaces or other word delimiters.</S><S sid = 7 ssid = >Chinese is written without using spaces or other word delimiters.</S> | Discourse Facet:  NA | Annotator: Automatic


