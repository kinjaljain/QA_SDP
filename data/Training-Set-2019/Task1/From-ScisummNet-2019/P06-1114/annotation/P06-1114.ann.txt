Citance Number: 1 | Reference Article:  P06-1114.txt | Citing Article:  W06-0705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For more information on the TE system described in this section, please see (Hickl et al, 2006b) and (Harabagiu and Hickl, 2006).</S> | Reference Offset:  ['78','102'] | Reference Text:  <S sid = 78 ssid = >As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier.</S><S sid = 102 ssid = >The performance of the TE system described in Section 3 was first evaluated in the 2006 PASCAL RTE Challenge.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1114.txt | Citing Article:  W06-0705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Harabagiu and Hickl, 2006), we used TE information in order to filter answers identified by the Q/A system that were not entailed by the user's original question.</S> | Reference Offset:  ['49','133'] | Reference Text:  <S sid = 49 ssid = >Table 2: TE between AGQs and user question.</S><S sid = 133 ssid = >When TE was used to filter non-entailed answers from consideration (Method 1), the overall accuracy of the Q/A system increased by 12% over the baseline (when an EAT could be identified) and by nearly 9% (when no EAT could be identified).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1114.txt | Citing Article:  P14-1133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing.</S> | Reference Offset:  ['0','104'] | Reference Text:  <S sid = 0 ssid = >Methods For Using Textual Entailment In Open-Domain Question Answering</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1114.txt | Citing Article:  P09-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to improve QA systems' performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc.</S> | Reference Offset:  ['78','104'] | Reference Text:  <S sid = 78 ssid = >As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier.</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1114.txt | Citing Article:  P09-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006).</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Methods For Using Textual Entailment In Open-Domain Question Answering</S><S sid = 21 ssid = >In this section, we describe three different methods for integrating a textual entailment (TE) system into the architecture of an open-domain Q/A system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1114.txt | Citing Article:  P09-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of matching headline and first sentence of the document as in (Harabagiu and Hickl, 2006), we followed a different approach.</S> | Reference Offset:  ['78','104'] | Reference Text:  <S sid = 78 ssid = >As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier.</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1114.txt | Citing Article:  W10-1201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking.</S> | Reference Offset:  ['61','104'] | Reference Text:  <S sid = 61 ssid = >As described in (Hickl et al., 2006), the Preprocessing module is used to syntactically parse texts, identify the semantic dependencies of predicates, label named entities, normalize temporal and spatial expressions, resolve instances of coreference, and annotate predicates with polarity, tense, and modality information.</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1114.txt | Citing Article:  W10-1201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al, 2009), when used for filtering and ranking answers.</S> | Reference Offset:  ['78','116'] | Reference Text:  <S sid = 78 ssid = >As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier.</S><S sid = 116 ssid = >Features derived from the entailment confidence were then combined with the keyword- and relation-based features described in (Harabagiu et al., 2005a) in order to produce a final ranking of candidate answers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1114.txt | Citing Article:  P09-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the task of Question Answering, (Harabagiu and Hickl, 2006) applied a TE component to rerank candidate answers returned by a retrieval step.</S> | Reference Offset:  ['13','126'] | Reference Text:  <S sid = 13 ssid = >We believe that systems developed specifically for this task can provide current question-answering systems with valuable semantic information that can be leveraged to identify exact answers from ranked lists of candidate answers.</S><S sid = 126 ssid = >Under this approach, candidate answers were initially ranked using features derived from entailment classifications performed between (1) the original question and each candidate answer and (2) the original question and the AGQ generated from each candidate answer.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1114.txt | Citing Article:  P10-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al, 2009), (Mirkin et al, 2009).</S> | Reference Offset:  ['9','104'] | Reference Text:  <S sid = 9 ssid = >(Prager et al., 2004)).</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1114.txt | Citing Article:  C10-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This includes finding question answer pairs (Cong et al, 2008) from online forums, auto-answering queries on a technical forum (Feng et al, 2006), ranking answers (Harabagiu and Hickl, 2006) etc.</S> | Reference Offset:  ['78','104'] | Reference Text:  <S sid = 78 ssid = >As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier.</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1114.txt | Citing Article:  D09-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006).</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >Methods For Using Textual Entailment In Open-Domain Question Answering</S><S sid = 13 ssid = >We believe that systems developed specifically for this task can provide current question-answering systems with valuable semantic information that can be leveraged to identify exact answers from ranked lists of candidate answers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1114.txt | Citing Article:  N10-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al, 2006), information extraction (Romano et al, 2006), and document summarization (Lloret et al, 2008).</S> | Reference Offset:  ['9','78'] | Reference Text:  <S sid = 9 ssid = >(Prager et al., 2004)).</S><S sid = 78 ssid = >As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1114.txt | Citing Article:  C10-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006).</S> | Reference Offset:  ['38','104'] | Reference Text:  <S sid = 38 ssid = >In previous work (Harabagiu et al., 2005b), we have described techniques that can be used to automatically generate well-formed natural language questions from the text of paragraphs retrieved by a PR module.</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1114.txt | Citing Article:  P09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006).</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >Methods For Using Textual Entailment In Open-Domain Question Answering</S><S sid = 18 ssid = >Section 2 describes the three methods of using textual entailment in open-domain question answering that we have identified, while Section 3 presents the textual entailment system we have used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1114.txt | Citing Article:  S12-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009).</S> | Reference Offset:  ['11','104'] | Reference Text:  <S sid = 11 ssid = >Recently, the task of automatically recognizing one form of semantic inference – textual entailment – has received much attention from groups participating in the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005).</S><S sid = 104 ssid = >Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.</S> | Discourse Facet:  NA | Annotator: Automatic


