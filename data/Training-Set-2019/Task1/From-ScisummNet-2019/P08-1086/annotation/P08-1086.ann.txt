Citance Number: 1 | Reference Article:  P08-1086.txt | Citing Article:  N09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used.</S> | Reference Offset:  ['4','14'] | Reference Text:  <S sid = 4 ssid = >The resulting clusterings are then used in training partially class-based language models.</S><S sid = 14 ssid = >We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1086.txt | Citing Article:  P14-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features.</S> | Reference Offset:  ['65','118'] | Reference Text:  <S sid = 65 ssid = >The clusterings generated in each iteration as well as the initial clustering are stored as the set of words in each cluster, the total number of occurrences of each cluster in the training corpus, and the list of words preceeding each cluster.</S><S sid = 118 ssid = >We then added these models as additional features to the log linear model of the Arabic-English machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1086.txt | Citing Article:  N12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008).</S> | Reference Offset:  ['44','103'] | Reference Text:  <S sid = 44 ssid = >We use a predictive class bigram model as given in Eq.</S><S sid = 103 ssid = >For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (Brants et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1086.txt | Citing Article:  N12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008).</S> | Reference Offset:  ['3','77'] | Reference Text:  <S sid = 3 ssid = >We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens).</S><S sid = 77 ssid = >While the greedy non-distributed exchange algorithm is guaranteed to converge as each exchange increases the log likelihood of the assumed bigram model, this is not necessarily true for the distributed exchange algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1086.txt | Citing Article:  N12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008).</S> | Reference Offset:  ['28','140'] | Reference Text:  <S sid = 28 ssid = >This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data: The algorithm iterates over all words in the vocabulary and tentatively moves each word to each cluster.</S><S sid = 140 ssid = >We conclude that even despite the large amounts of data used to train the large word-based model in our second experiment, class-based language models are still an effective tool to ease the effects of data sparsity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1086.txt | Citing Article:  P12-3029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word.</S> | Reference Offset:  ['28','99'] | Reference Text:  <S sid = 28 ssid = >This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data: The algorithm iterates over all words in the vocabulary and tentatively moves each word to each cluster.</S><S sid = 99 ssid = >When scaling up the vocabulary and corpus sizes, the current bottleneck of our implementation is loading the current clustering into memory.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1086.txt | Citing Article:  P12-2047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word.</S> | Reference Offset:  ['28','99'] | Reference Text:  <S sid = 28 ssid = >This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data: The algorithm iterates over all words in the vocabulary and tentatively moves each word to each cluster.</S><S sid = 99 ssid = >When scaling up the vocabulary and corpus sizes, the current bottleneck of our implementation is loading the current clustering into memory.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1086.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm.</S> | Reference Offset:  ['83','114'] | Reference Text:  <S sid = 83 ssid = >Our experiments showed that this also seems to be the case for the distributed exchange algorithm.</S><S sid = 114 ssid = >The weights are trained using minimum error rate training (Och, 2003) with BLEU score as the objective function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1086.txt | Citing Article:  P12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs.</S> | Reference Offset:  ['14','138'] | Reference Text:  <S sid = 14 ssid = >We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.</S><S sid = 138 ssid = >The experiments presented show that predictive class-based models trained using the obtained word classifications can improve the quality of a state-ofthe-art machine translation system as indicated by the BLEU score in both translation tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1086.txt | Citing Article:  P12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes.</S> | Reference Offset:  ['54','110'] | Reference Text:  <S sid = 54 ssid = >Thus the algorithm scales linearly in the number of classes.</S><S sid = 110 ssid = >Given a sentence f in the source language, the machine translation problem is to automatically produce a translation eï¿½ in the target language.</S> | Discourse Facet:  NA | Annotator: Automatic


