Citance Number: 1 | Reference Article:  W09-0432.txt | Citing Article:  N10-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009).</S> | Reference Offset:  ['37','40'] | Reference Text:  <S sid = 37 ssid = >In (Koehn and Schroeder, 2007) cross-domain adaptation techniques were applied on a phrasebased SMT trained on the Europarl task, in order to translate news commentaries, from French to English.</S><S sid = 40 ssid = >Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W09-0432.txt | Citing Article:  P13-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence.</S> | Reference Offset:  ['23','41'] | Reference Text:  <S sid = 23 ssid = >Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language.</S><S sid = 41 ssid = >The adaptation of the translation and re-ordering models is performed by generating synthetic bilingual data from monolingual texts, similarly to what proposed in (Schwenk, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W09-0432.txt | Citing Article:  W11-2138.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains.</S> | Reference Offset:  ['31','45'] | Reference Text:  <S sid = 31 ssid = >Both the language and the translation models are retrained on the extracted data.</S><S sid = 45 ssid = >This paper addresses the issue of adapting an already developed phrase-based translation system in order to work properly on a different domain, for which almost no parallel data are available but only monolingual texts.1 The main components of the SMT system are the translation model, which aims at porting the content from the source to the target language, and the language model, which aims at building fluent sentences in the target language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W09-0432.txt | Citing Article:  C10-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data.</S> | Reference Offset:  ['40','72'] | Reference Text:  <S sid = 40 ssid = >Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available.</S><S sid = 72 ssid = >When more TMs Sj are available, Moses can behave in two different ways in pre-fetching the translation options.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W09-0432.txt | Citing Article:  C10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009).</S> | Reference Offset:  ['2','62'] | Reference Text:  <S sid = 2 ssid = >The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.</S><S sid = 62 ssid = >Once monolingual adaptation data is automatically translated, we can use the synthetic parallel corpus to estimate new language, translation, and re-ordering models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W09-0432.txt | Citing Article:  P12-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','153'] | Reference Text:  <S sid = 48 ssid = >This model is also learnable from parallel data.</S><S sid = 153 ssid = >Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W09-0432.txt | Citing Article:  P13-2058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007).</S> | Reference Offset:  ['7','40'] | Reference Text:  <S sid = 7 ssid = >Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.</S><S sid = 40 ssid = >Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W09-0432.txt | Citing Article:  W11-2132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses.</S> | Reference Offset:  ['57','153'] | Reference Text:  <S sid = 57 ssid = >The standard procedure of Moses for the estimation of the translation and re-ordering models from a bilingual corpus consists in three main steps: Recently, we enhanced Moses decoder to also output the word-to-word alignment between the input sentence and its translation, given that they have been added to the phrase table at training time.</S><S sid = 153 ssid = >Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W09-0432.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Further approaches to domain adaptation for SMT include adaptation using in-domain language mod els (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007).</S> | Reference Offset:  ['23','40'] | Reference Text:  <S sid = 23 ssid = >Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language.</S><S sid = 40 ssid = >Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W09-0432.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','153'] | Reference Text:  <S sid = 48 ssid = >This model is also learnable from parallel data.</S><S sid = 153 ssid = >Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W09-0432.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Three experiments involving the Twitter language model confirm Bertoldi and Federico (2009)'s findings that the language model was most helpful.</S> | Reference Offset:  ['27','45'] | Reference Text:  <S sid = 27 ssid = >In (Eck et al., 2004) adaptation is limited to the target language model (LM).</S><S sid = 45 ssid = >This paper addresses the issue of adapting an already developed phrase-based translation system in order to work properly on a different domain, for which almost no parallel data are available but only monolingual texts.1 The main components of the SMT system are the translation model, which aims at porting the content from the source to the target language, and the language model, which aims at building fluent sentences in the target language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W09-0432.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As has been observed before by Bertoldi and Federico (2009), it did not matter whether the synthetic data were used on their own or in addition to the original training data.</S> | Reference Offset:  ['64','148'] | Reference Text:  <S sid = 64 ssid = >There is another simple option which is to concatenate the synthetic parallel data with the original training data and re-build the system.</S><S sid = 148 ssid = >We proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


