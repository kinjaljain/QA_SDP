Citance Number: 1 | Reference Article:  P01-1005.txt | Citing Article:  P02-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus.</S> | Reference Offset:  ['8','35'] | Reference Text:  <S sid = 8 ssid = >While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not.</S><S sid = 35 ssid = >Each learner was trained at several cutoff points in the training corpus, i.e. the first one million words, the first five million words, and so on, until all one billion words were used for training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P01-1005.txt | Citing Article:  P02-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words.</S> | Reference Offset:  ['3','16'] | Reference Text:  <S sid = 3 ssid = >In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.</S><S sid = 16 ssid = >Confusion set disambiguation is the problem of choosing the correct use of a word, given a set of words with which it is commonly confused.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P01-1005.txt | Citing Article:  N10-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a).</S> | Reference Offset:  ['18','83'] | Reference Text:  <S sid = 18 ssid = >Numerous methods have been presented for confusable disambiguation.</S><S sid = 83 ssid = >With bagging, a variant of the original training set is constructed by randomly sampling sentences with replacement from the source training set in order to produce N new training sets of size equal to the original.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P01-1005.txt | Citing Article:  N10-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001).</S> | Reference Offset:  ['37','87'] | Reference Text:  <S sid = 37 ssid = >In Figure 1, we show learning curves for each learner, up to one billion words of training data.</S><S sid = 87 ssid = >We present the active learning algorithm we used below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P01-1005.txt | Citing Article:  W02-0814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)).</S> | Reference Offset:  ['66','116'] | Reference Text:  <S sid = 66 ssid = >Very few problems exist for which annotated data of this size is available for free.</S><S sid = 116 ssid = >We are able to attain improvements in accuracy for free using unsupervised learning, but unlike our learning curve experiments using correctly labeled data, accuracy does not continue to improve with additional data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P01-1005.txt | Citing Article:  W02-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Scaling To Very Very Large Corpora For Natural Language Disambiguation</S><S sid = 15 ssid = >Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P01-1005.txt | Citing Article:  W02-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus.</S> | Reference Offset:  ['22','113'] | Reference Text:  <S sid = 22 ssid = >Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.</S><S sid = 113 ssid = >In Table 3 we show the results from these unsupervised learning experiments for two confusion sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P01-1005.txt | Citing Article:  W02-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002).</S> | Reference Offset:  ['104','125'] | Reference Text:  <S sid = 104 ssid = >The question we want to answer is whether there is something to be gained by combining unsupervised and supervised learning when we scale up both the seed corpus and the unlabeled corpus significantly.</S><S sid = 125 ssid = >We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P01-1005.txt | Citing Article:  W03-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task.</S> | Reference Offset:  ['50','125'] | Reference Text:  <S sid = 50 ssid = >By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used.</S><S sid = 125 ssid = >We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P01-1005.txt | Citing Article:  C10-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001).</S> | Reference Offset:  ['10','14'] | Reference Text:  <S sid = 10 ssid = >The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed, and relatively small, data sets.</S><S sid = 14 ssid = >First we show learning curves for four different machine learning algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P01-1005.txt | Citing Article:  W10-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b).</S> | Reference Offset:  ['44','55'] | Reference Text:  <S sid = 44 ssid = >Figure 2 shows the size of learned representations as a function of training data size.</S><S sid = 55 ssid = >As training size increases significantly, we would expect complementarity between classifiers to decrease.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P01-1005.txt | Citing Article:  N04-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b).</S> | Reference Offset:  ['80','91'] | Reference Text:  <S sid = 80 ssid = >While active learning has been shown to work for a number of tasks, the majority of active learning experiments in natural language processing have been conducted using very small seed corpora and sets of unlabeled examples.</S><S sid = 91 ssid = >We show the results from sample selection for confusion set disambiguation in Figure 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P01-1005.txt | Citing Article:  P10-4011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001).</S> | Reference Offset:  ['82','83'] | Reference Text:  <S sid = 82 ssid = >We can use bagging (Breiman, 1996), a technique for generating a committee of classifiers, to assess the label uncertainty of a potential training instance.</S><S sid = 83 ssid = >With bagging, a variant of the original training set is constructed by randomly sampling sentences with replacement from the source training set in order to produce N new training sets of size equal to the original.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P01-1005.txt | Citing Article:  W03-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data.</S> | Reference Offset:  ['15','81'] | Reference Text:  <S sid = 15 ssid = >Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.</S><S sid = 81 ssid = >Therefore, we wish to explore situations where we have, or can afford, a nonnegligible sized training corpus (such as for part-of-speech tagging) and have access to very large amounts of unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P01-1005.txt | Citing Article:  W10-0712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001).</S> | Reference Offset:  ['12','80'] | Reference Text:  <S sid = 12 ssid = >In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation.</S><S sid = 80 ssid = >While active learning has been shown to work for a number of tasks, the majority of active learning experiments in natural language processing have been conducted using very small seed corpora and sets of unlabeled examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P01-1005.txt | Citing Article:  H05-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources.</S> | Reference Offset:  ['27','124'] | Reference Text:  <S sid = 27 ssid = >Some possibilities included modifying standard learning algorithms, exploring new learning techniques, and using more sophisticated features.</S><S sid = 124 ssid = >In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P01-1005.txt | Citing Article:  H05-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words.</S> | Reference Offset:  ['37','39'] | Reference Text:  <S sid = 37 ssid = >In Figure 1, we show learning curves for each learner, up to one billion words of training data.</S><S sid = 39 ssid = >Note that the curves appear to be log-linear even out to one billion words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P01-1005.txt | Citing Article:  H05-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner.</S> | Reference Offset:  ['15','105'] | Reference Text:  <S sid = 15 ssid = >Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.</S><S sid = 105 ssid = >We can again use a committee of bagged classifiers, this time for unsupervised learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P01-1005.txt | Citing Article:  C10-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data.</S> | Reference Offset:  ['37','91'] | Reference Text:  <S sid = 37 ssid = >In Figure 1, we show learning curves for each learner, up to one billion words of training data.</S><S sid = 91 ssid = >We show the results from sample selection for confusion set disambiguation in Figure 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P01-1005.txt | Citing Article:  W10-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods.</S> | Reference Offset:  ['47','126'] | Reference Text:  <S sid = 47 ssid = >In such cases, one could look at numerous methods for compressing data (e.g.</S><S sid = 126 ssid = >We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


