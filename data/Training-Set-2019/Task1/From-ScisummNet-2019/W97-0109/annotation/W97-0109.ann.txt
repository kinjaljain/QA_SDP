Citance Number: 1 | Reference Article:  W97-0109.txt | Citing Article:  P00-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997).</S> | Reference Offset:  ['2','75'] | Reference Text:  <S sid = 2 ssid = >We propose a new supervised learning method for PPattachment based on a semantically tagged corpus.</S><S sid = 75 ssid = >Unfortunately, at the time of writing this work, a sufficiently big corpus which was both syntactically analysed and semantically tagged did not exist.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W97-0109.txt | Citing Article:  P00-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ).</S> | Reference Offset:  ['0','181'] | Reference Text:  <S sid = 0 ssid = >Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary</S><S sid = 181 ssid = >Because the induction of the decision tree for the PP attachment is based on a supervised learning from sense-tagged examples, it was necessary to sense-disambiguate the entire training set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W97-0109.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system.</S> | Reference Offset:  ['219','221'] | Reference Text:  <S sid = 219 ssid = >This is because at the top of the decision tree all of the semantic tops of all of the content words of the given quadruple are compared with the semantic generalisations of the training examples represented through the nodes of the decision tree.</S><S sid = 221 ssid = >The decision tree therefore represents a very useful mechanism for determining the semantic level at which the decision on the PP attachment is made.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W97-0109.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet.</S> | Reference Offset:  ['212','222'] | Reference Text:  <S sid = 212 ssid = >As we have already mentioned, Collins and Brooks [C&B951 based their method on matching the testing quadruples against the set of training examples.</S><S sid = 222 ssid = >Collins and Brooks' have also demonstrated the importance of low count events in training data by an experiment where all counts less than 5 were put to zero.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W97-0109.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features.</S> | Reference Offset:  ['165','197'] | Reference Text:  <S sid = 165 ssid = >At first, all the training examples (separately for each preposition) are split into subsets which correspond to the topmost concepts of WordNet, which contains 11 topical roots for nouns and description nouns, and 337 for verbs (both nouns and verbs have hierarchical structure, although the hierarchy for verbs is shallower and wider).</S><S sid = 197 ssid = >This is because training examples with an error or with a word not found in WordNet could not fully participate on the decision tree induction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W97-0109.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997).</S> | Reference Offset:  ['212','222'] | Reference Text:  <S sid = 212 ssid = >As we have already mentioned, Collins and Brooks [C&B951 based their method on matching the testing quadruples against the set of training examples.</S><S sid = 222 ssid = >Collins and Brooks' have also demonstrated the importance of low count events in training data by an experiment where all counts less than 5 were put to zero.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W97-0109.txt | Citing Article:  W06-2603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997).</S> | Reference Offset:  ['10','182'] | Reference Text:  <S sid = 10 ssid = >In this case, we can almost certainly state that the PP is adverbial, i.e. attached to the verb.</S><S sid = 182 ssid = >This was done by the iterative algorithm described in Chapter 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W97-0109.txt | Citing Article:  I05-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','252'] | Reference Text:  <S sid = 78 ssid = >Even without any sentential context, the human brain is capable of disambiguating word senses based on circumstances or experience3.</S><S sid = 252 ssid = >At the moment, we are working on an implementation of the algorithm to work on with a wider sentential context and on its incorporation within a more complex NLP system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W97-0109.txt | Citing Article:  W04-2410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet.</S> | Reference Offset:  ['10','29'] | Reference Text:  <S sid = 10 ssid = >In this case, we can almost certainly state that the PP is adverbial, i.e. attached to the verb.</S><S sid = 29 ssid = >The current statistical state-of-the art method is the backed-off model proposed by Collins and Brooks in [C&B95] which performs with 84.5% accuracy on stand-alone quadruples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W97-0109.txt | Citing Article:  P08-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier.</S> | Reference Offset:  ['25','93'] | Reference Text:  <S sid = 25 ssid = >An iterative, unsupervised method was then used to decide between adjectival and adverbial attachment in which the decision was based on comparing the co-occurence probabilities of the given preposition with the verb and with the noun in each quadruple.</S><S sid = 93 ssid = >Every possible sense of all the related context words is evaluated and the best match.Chosen5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W97-0109.txt | Citing Article:  P08-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task.</S> | Reference Offset:  ['0','86'] | Reference Text:  <S sid = 0 ssid = >Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary</S><S sid = 86 ssid = >Similar contextual situations (these include information on the PP-attachment) are found in the training corpora and are used for the sense disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W97-0109.txt | Citing Article:  P06-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).</S> | Reference Offset:  ['26','181'] | Reference Text:  <S sid = 26 ssid = >Another promising approach is the transformation-based rule derivation presented by Brill and Resnik in [B&R941, which is a simple learning algorithm that derives a set of transformation rules.</S><S sid = 181 ssid = >Because the induction of the decision tree for the PP attachment is based on a supervised learning from sense-tagged examples, it was necessary to sense-disambiguate the entire training set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W97-0109.txt | Citing Article:  P06-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy.</S> | Reference Offset:  ['29','137'] | Reference Text:  <S sid = 29 ssid = >The current statistical state-of-the art method is the backed-off model proposed by Collins and Brooks in [C&B95] which performs with 84.5% accuracy on stand-alone quadruples.</S><S sid = 137 ssid = >Two words are similar if their semantic distance is less than 1.0 and if either their character strings are different or if one of the words has been previously disambiguated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W97-0109.txt | Citing Article:  H05-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','252'] | Reference Text:  <S sid = 78 ssid = >Even without any sentential context, the human brain is capable of disambiguating word senses based on circumstances or experience3.</S><S sid = 252 ssid = >At the moment, we are working on an implementation of the algorithm to work on with a wider sentential context and on its incorporation within a more complex NLP system.</S> | Discourse Facet:  NA | Annotator: Automatic


