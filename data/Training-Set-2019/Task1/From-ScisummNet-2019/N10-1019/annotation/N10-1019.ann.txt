Citance Number: 1 | Reference Article:  N10-1019.txt | Citing Article:  D10-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data.</S> | Reference Offset:  ['31','170'] | Reference Text:  <S sid = 31 ssid = >The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input.</S><S sid = 170 ssid = >The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N10-1019.txt | Citing Article:  D10-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers.</S> | Reference Offset:  ['0','35'] | Reference Text:  <S sid = 0 ssid = >Using Mostly Native Data to Correct Errors in Learners&rsquo; Writing</S><S sid = 35 ssid = >The metaclassifier, in contrast, is trained on a smaller set of error-annotated learner data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N10-1019.txt | Citing Article:  D10-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions.</S> | Reference Offset:  ['90','92'] | Reference Text:  <S sid = 90 ssid = >Precision and recall for the overall system are controlled by thresholding the meta-classifier class probability.</S><S sid = 92 ssid = >Precision and recall for the error-specific classifier is controlled by thresholding class probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N10-1019.txt | Citing Article:  D10-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gamon (2010) also considers missing and extraneous preposition errors.</S> | Reference Offset:  ['78','80'] | Reference Text:  <S sid = 78 ssid = >In addition, we eliminated all annotations for nonpertinent errors (i.e., non-preposition/article errors, or errors that do not involve any of the targeted prepositions), but we maintained the original (erroneous) text for these.</S><S sid = 80 ssid = >Finally, we eliminated sentences containing nested errors and immediately adjacent errors when they involve pertinent (preposition/article) errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N10-1019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b).</S> | Reference Offset:  ['19','27'] | Reference Text:  <S sid = 19 ssid = >Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008).</S><S sid = 27 ssid = >Finally, Yi et al. (2008) and Hermet et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N10-1019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier.</S> | Reference Offset:  ['31','167'] | Reference Text:  <S sid = 31 ssid = >The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input.</S><S sid = 167 ssid = >We have shown that a meta-classifier approach outperforms using a language model or a classifier alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N10-1019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010).</S> | Reference Offset:  ['26','28'] | Reference Text:  <S sid = 26 ssid = >Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision.</S><S sid = 28 ssid = >(2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N10-1019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set.</S> | Reference Offset:  ['115','126'] | Reference Text:  <S sid = 115 ssid = >Figure 1 and Figure 2 show the evaluation results of the meta-classifier for prepositions and articles, compared to the performance of the error-specific classifier and language model alone.</S><S sid = 126 ssid = >Figure 3 and Figure 4 show results obtained by using decreasing amounts of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N10-1019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','174'] | Reference Text:  <S sid = 54 ssid = >Note that the 2.5 million sentences used in the classifier training already produce 16.5 million training vectors.</S><S sid = 174 ssid = >We thank Claudia Leacock from the Butler Hill Group for detailed error analysis and the anonymous reviewers for helpful and constructive feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N10-1019.txt | Citing Article:  W12-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The heuristics are based on those used in Gamon (2010) (personal communication).</S> | Reference Offset:  ['20','26'] | Reference Text:  <S sid = 20 ssid = >Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification algorithm of choice.</S><S sid = 26 ssid = >Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N10-1019.txt | Citing Article:  W11-1410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010).</S> | Reference Offset:  ['32','36'] | Reference Text:  <S sid = 32 ssid = >Using a meta-classifier for ensemble learning has been proven effective for many machine learning problems (see e.g.</S><S sid = 36 ssid = >This allows us to address the problem of domain mismatch: We can leverage large well-formed data sets that are substantially different from real-life learner language for the primary models, and then fine-tune the output to learner English using a much smaller set of expensive and hard-to-come-by annotated learner writing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N10-1019.txt | Citing Article:  W12-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a baseline run that represents the language model approach proposed by Gamon (2010).</S> | Reference Offset:  ['127','130'] | Reference Text:  <S sid = 127 ssid = >The dotted line shows the language model baseline.</S><S sid = 130 ssid = >We can reduce the amount of training data for prepositions to 10% of the original data and still outperform the language model baseline.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N10-1019.txt | Citing Article:  W12-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010).</S> | Reference Offset:  ['135','170'] | Reference Text:  <S sid = 135 ssid = >This result can most likely be explained by the different complexity of the preposition and article tasks.</S><S sid = 170 ssid = >The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N10-1019.txt | Citing Article:  W12-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error.</S> | Reference Offset:  ['80','83'] | Reference Text:  <S sid = 80 ssid = >Finally, we eliminated sentences containing nested errors and immediately adjacent errors when they involve pertinent (preposition/article) errors.</S><S sid = 83 ssid = >(This last step eliminated 31% of the sentences annotated with preposition errors and 29% or the sentences annotated with article errors.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N10-1019.txt | Citing Article:  P13-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010).</S> | Reference Offset:  ['19','40'] | Reference Text:  <S sid = 19 ssid = >Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008).</S><S sid = 40 ssid = >Features include contextual features from a window of six tokens to the right and left, such as lexical features (word), part-ofspeech tags, and a handful of “custom features”, for example lexical head of governing VP or governed NP (as determined by part-of-speech-tag based heuristics).</S> | Discourse Facet:  NA | Annotator: Automatic


