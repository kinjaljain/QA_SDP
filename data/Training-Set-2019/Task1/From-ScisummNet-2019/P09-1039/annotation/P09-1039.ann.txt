Citance Number: 1 | Reference Article:  P09-1039.txt | Citing Article:  D11-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others.</S> | Reference Offset:  ['6','138'] | Reference Text:  <S sid = 6 ssid = >Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others.</S><S sid = 138 ssid = >model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','162'] | Reference Text:  <S sid = 40 ssid = >Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.</S><S sid = 162 ssid = >Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head).</S> | Reference Offset:  ['10','24'] | Reference Text:  <S sid = 10 ssid = >This paper presents new, concise ILP formulations for projective and non-projective dependency parsing.</S><S sid = 24 ssid = >We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) < k < max(i, j), there is a directed path in y from i to k).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','162'] | Reference Text:  <S sid = 40 ssid = >Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.</S><S sid = 162 ssid = >Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','162'] | Reference Text:  <S sid = 40 ssid = >Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.</S><S sid = 162 ssid = >Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab.</S> | Reference Offset:  ['25','88'] | Reference Text:  <S sid = 25 ssid = >A dependency tree is called projective if it only contains projective arcs.</S><S sid = 88 ssid = >12, with O(|A |· |V |) variables and constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs.</S> | Reference Offset:  ['89','155'] | Reference Text:  <S sid = 89 ssid = >Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word.</S><S sid = 155 ssid = >We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1039.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here.</S> | Reference Offset:  ['27','103'] | Reference Text:  <S sid = 27 ssid = >1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x).</S><S sid = 103 ssid = >The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3) variables and constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009).</S> | Reference Offset:  ['47','127'] | Reference Text:  <S sid = 47 ssid = >Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation.</S><S sid = 127 ssid = >baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009).</S> | Reference Offset:  ['10','47'] | Reference Text:  <S sid = 10 ssid = >This paper presents new, concise ILP formulations for projective and non-projective dependency parsing.</S><S sid = 47 ssid = >Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009).</S> | Reference Offset:  ['50','126'] | Reference Text:  <S sid = 50 ssid = >The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).</S><S sid = 126 ssid = >12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program.</S> | Reference Offset:  ['26','47'] | Reference Text:  <S sid = 26 ssid = >Fig.</S><S sid = 47 ssid = >Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009).</S> | Reference Offset:  ['29','79'] | Reference Text:  <S sid = 29 ssid = >Considering simultaneously all incidence vectors of legal dependency trees and taking the convex hull, we obtain a polyhedron that we call the arborescence polytope, denoted by Z(x).</S><S sid = 79 ssid = >9 by za ∈ B, a ∈ A.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 3: Details of the factor graph underlying the parser of Martins et al (2009).</S> | Reference Offset:  ['50','126'] | Reference Text:  <S sid = 50 ssid = >The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).</S><S sid = 126 ssid = >12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We now turn to the concise integer LP formulation of Martins et al (2009).</S> | Reference Offset:  ['0','50'] | Reference Text:  <S sid = 0 ssid = >Concise Integer Linear Programming Formulations for Dependency Parsing</S><S sid = 50 ssid = >The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs.</S> | Reference Offset:  ['50','132'] | Reference Text:  <S sid = 50 ssid = >The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).</S><S sid = 132 ssid = >Comparing with the baselines, we observe that our full model outperforms that of McDonald and Pereira (2006), and is in line with the most accurate dependency parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop.</S> | Reference Offset:  ['50','138'] | Reference Text:  <S sid = 50 ssid = >The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).</S><S sid = 138 ssid = >model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1039.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','162'] | Reference Text:  <S sid = 40 ssid = >Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.</S><S sid = 162 ssid = >Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1039.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).</S> | Reference Offset:  ['47','132'] | Reference Text:  <S sid = 47 ssid = >Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation.</S><S sid = 132 ssid = >Comparing with the baselines, we observe that our full model outperforms that of McDonald and Pereira (2006), and is in line with the most accurate dependency parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1039.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011).</S> | Reference Offset:  ['50','138'] | Reference Text:  <S sid = 50 ssid = >The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).</S><S sid = 138 ssid = >model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


