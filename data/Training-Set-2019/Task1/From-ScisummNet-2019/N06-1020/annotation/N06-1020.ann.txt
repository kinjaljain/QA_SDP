Citance Number: 1 | Reference Article:  N06-1020.txt | Citing Article:  P06-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable.</S> | Reference Offset:  ['9','41'] | Reference Text:  <S sid = 9 ssid = >Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002).</S><S sid = 41 ssid = >The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N06-1020.txt | Citing Article:  P06-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['190','191'] | Reference Text:  <S sid = 190 ssid = >This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.</S><S sid = 191 ssid = >We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N06-1020.txt | Citing Article:  W10-2105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, McClosky et al (2006) improved a statistical parser by self-training.</S> | Reference Offset:  ['24','25'] | Reference Text:  <S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S><S sid = 25 ssid = >The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N06-1020.txt | Citing Article:  D08-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006).</S> | Reference Offset:  ['24','25'] | Reference Text:  <S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S><S sid = 25 ssid = >The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N06-1020.txt | Citing Article:  D08-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >previously used for self-training of parsers (McClosky et al, 2006).</S> | Reference Offset:  ['24','25'] | Reference Text:  <S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S><S sid = 25 ssid = >The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N06-1020.txt | Citing Article:  P13-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['190','191'] | Reference Text:  <S sid = 190 ssid = >This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.</S><S sid = 191 ssid = >We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N06-1020.txt | Citing Article:  P13-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training.</S> | Reference Offset:  ['138','139'] | Reference Text:  <S sid = 138 ssid = >As Figure 2 makes clear, the relative performance of the self-trained and baseline parsers does not self-trained parser improve the parse with the highest probability vary linearly with sentence length, so we introduced binned sentence length (with each bin of length 10) as a factor.</S><S sid = 139 ssid = >Because the self-trained and baseline parsers produced equivalent output on 3,346 (66%) of the sentences, we restricted attention to the 1,693 sentences on which the self-trained and baseline parsers’ fscores differ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N06-1020.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker.</S> | Reference Offset:  ['17','24'] | Reference Text:  <S sid = 17 ssid = >It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al. (2003) report either minor improvements or significant damage from using self-training for parsing.</S><S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N06-1020.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b).</S> | Reference Offset:  ['24','59'] | Reference Text:  <S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S><S sid = 59 ssid = >Our labeled data comes from the Penn Treebank (Marcus et al., 1993) and consists of about 40,000 sentences from Wall Street Journal (WSJ) articles annotated with syntactic information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N06-1020.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training.</S> | Reference Offset:  ['77','103'] | Reference Text:  <S sid = 77 ssid = >Table 1 shows the difference in parser’s (not reranker’s) performance when trained on parser-best reranker-best sentences from NANC to WSJ training data.</S><S sid = 103 ssid = >“WSJ + NANC” represents the system trained on WSJ training (with a relative weight of 5) and 1,750k sentences from the reranker-best list of NANC.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N06-1020.txt | Citing Article:  D10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker.</S> | Reference Offset:  ['0','50'] | Reference Text:  <S sid = 0 ssid = >Effective Self-Training For Parsing</S><S sid = 50 ssid = >The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N06-1020.txt | Citing Article:  D10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features.</S> | Reference Offset:  ['76','91'] | Reference Text:  <S sid = 76 ssid = >In these cases, we will use the term reranking parser.</S><S sid = 91 ssid = >If this were the case, we would not see an improvement when evaluating a reranking parser on the same models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N06-1020.txt | Citing Article:  D10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies.</S> | Reference Offset:  ['24','25'] | Reference Text:  <S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S><S sid = 25 ssid = >The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N06-1020.txt | Citing Article:  E09-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006).</S> | Reference Offset:  ['0','24'] | Reference Text:  <S sid = 0 ssid = >Effective Self-Training For Parsing</S><S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N06-1020.txt | Citing Article:  C10-2146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging.</S> | Reference Offset:  ['18','78'] | Reference Text:  <S sid = 18 ssid = >Clark et al. (2003) applies self-training to POS-tagging and reports the same outcomes.</S><S sid = 78 ssid = >While the reranker was used to produce the reranker-best sentences, we performed this evaluation using only the first-stage parser to parse all sentences from section 22.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N06-1020.txt | Citing Article:  W10-2604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set.</S> | Reference Offset:  ['31','32'] | Reference Text:  <S sid = 31 ssid = >When one learner is confident of its predictions about the data, we apply the predicted label of the data to the training set of the other learners.</S><S sid = 32 ssid = >A variation suggested by Dasgupta et al. (2001) is to add data to the training set when multiple learners agree on the label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N06-1020.txt | Citing Article:  W11-1607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006).</S> | Reference Offset:  ['8','17'] | Reference Text:  <S sid = 8 ssid = >Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003).</S><S sid = 17 ssid = >It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al. (2003) report either minor improvements or significant damage from using self-training for parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N06-1020.txt | Citing Article:  C10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser.</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Effective Self-Training For Parsing</S><S sid = 25 ssid = >The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N06-1020.txt | Citing Article:  E09-3005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing.</S> | Reference Offset:  ['24','25'] | Reference Text:  <S sid = 24 ssid = >Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.</S><S sid = 25 ssid = >The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N06-1020.txt | Citing Article:  P13-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006).</S> | Reference Offset:  ['13','19'] | Reference Text:  <S sid = 13 ssid = >A simple method of incorporating unlabeled data into a new model is self-training.</S><S sid = 19 ssid = >One would assume that errors in the original model would be amplified in the new model.</S> | Discourse Facet:  NA | Annotator: Automatic


