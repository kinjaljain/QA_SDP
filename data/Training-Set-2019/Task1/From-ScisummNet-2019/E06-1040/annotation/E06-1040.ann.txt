Citance Number: 1 | Reference Article:  E06-1040.txt | Citing Article:  W06-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts.</S> | Reference Offset:  ['11','20'] | Reference Text:  <S sid = 11 ssid = >In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts).</S><S sid = 20 ssid = >Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1040.txt | Citing Article:  W06-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results.</S> | Reference Offset:  ['99','111'] | Reference Text:  <S sid = 99 ssid = >Pair-wise tests between results in Table 2 showed all but three differences to be significant with the likelihood of incorrectly rejecting the null hypothesis p < 0.05 (the standard threshold in NLP).</S><S sid = 111 ssid = >Pair-wise AR tests showed all differences to be significant with p < 0.05, except for the differences in BLEU, NIST and ROUGE scores for SUMTIME-Hybrid/pCRUroulette, and the difference in BLEU scores for SUMTIME-Hybrid/pCRU-2gram.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1040.txt | Citing Article:  P14-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality.</S> | Reference Offset:  ['118','143'] | Reference Text:  <S sid = 118 ssid = >If an imperfect corpus is used as the gold standard for the automatic metrics, then high correlation with human judgments is less likely, and this may explain the difference in human and automatic scores for SUMTIME-Hybrid.</S><S sid = 143 ssid = >Automatic metrics can be expected to correlate very highly with human judgments only if the reference texts used are of high quality, or rather, can be expected to be judged high quality by the human evaluators.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1040.txt | Citing Article:  W08-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006).</S> | Reference Offset:  ['39','42'] | Reference Text:  <S sid = 39 ssid = >The SUMTIME project (Reiter et al., 2005) developed an NLG system which generated textual weather forecasts from numerical forecast data.</S><S sid = 42 ssid = >SUMTIME generates very high-quality texts, in some cases forecast users believe SUMTIME texts are better than human-written texts (Reiter et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1040.txt | Citing Article:  N07-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006).</S> | Reference Offset:  ['101','148'] | Reference Text:  <S sid = 101 ssid = >Table 3 shows Pearson correlation coefficients (PCC) for the metrics and humans in Table 2.</S><S sid = 148 ssid = >We also found individual expertsâ€™ judgments are not likely to correlate highly with average expert opinion, in fact less likely than NIST scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1040.txt | Citing Article:  N07-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects.</S> | Reference Offset:  ['14','87'] | Reference Text:  <S sid = 14 ssid = >NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts.</S><S sid = 87 ssid = >Subjects completed the experiment unsupervised, at a time and place of their choosing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1040.txt | Citing Article:  W09-0604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006).</S> | Reference Offset:  ['93','131'] | Reference Text:  <S sid = 93 ssid = >Systems are ranked in order of the scores given to them by experts.</S><S sid = 131 ssid = >This behaviour is penalised by the automatic evaluation metrics, but the human evaluators do not seem to mind it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1040.txt | Citing Article:  W08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data.</S> | Reference Offset:  ['20','118'] | Reference Text:  <S sid = 20 ssid = >Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts.</S><S sid = 118 ssid = >If an imperfect corpus is used as the gold standard for the automatic metrics, then high correlation with human judgments is less likely, and this may explain the difference in human and automatic scores for SUMTIME-Hybrid.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1040.txt | Citing Article:  P09-2025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts).</S> | Reference Offset:  ['106','149'] | Reference Text:  <S sid = 106 ssid = >For example, the PCC for non-experts and experts is 0.845, but the average PCC between individual non-experts and average expert judgment is only 0.496, implying that an arbitrary non-expert is not very likely to correlate well with average expert judgments.</S><S sid = 149 ssid = >This seems to imply that if expert evaluation can only be done with one or two experts, but a high-quality reference corpus is available, then a NIST-based evaluation may produce more accurate results than an expert-based evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1040.txt | Citing Article:  N07-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale.</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >Comparing Automatic And Human Evaluation Of NLG Systems</S><S sid = 2 ssid = >We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1040.txt | Citing Article:  W09-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson's r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006).</S> | Reference Offset:  ['21','76'] | Reference Text:  <S sid = 21 ssid = >Similar evaluations have been used e.g.</S><S sid = 76 ssid = >Automatic evaluations: We used NIST2, BLEU3, and ROUGE4 to automatically evaluate the above systems and texts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1040.txt | Citing Article:  E06-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices.</S> | Reference Offset:  ['21','115'] | Reference Text:  <S sid = 21 ssid = >Similar evaluations have been used e.g.</S><S sid = 115 ssid = >These give similar results, except that BLEU-1 and BLEU-2 rank pCRU-roulette as highly as the human judges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1040.txt | Citing Article:  E06-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006).</S> | Reference Offset:  ['15','134'] | Reference Text:  <S sid = 15 ssid = >In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison.</S><S sid = 134 ssid = >Foster and Oberlander (2006), in their study of facial gestures, have also noted that humans do not mind and indeed in some cases prefer variation, whereas corpus-based evaluations give higher ratings to systems which follow corpus frequency.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1040.txt | Citing Article:  E09-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements.</S> | Reference Offset:  ['2','138'] | Reference Text:  <S sid = 2 ssid = >We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.</S><S sid = 138 ssid = >Reiter et al., 2005), and evaluations by a small number of experts may also be problematic, unless we have good reason to believe that expert opinions are highly correlated in the domain (which was certainly not the case in our weather forecast domain).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1040.txt | Citing Article:  P13-1138.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005).</S> | Reference Offset:  ['17','39'] | Reference Text:  <S sid = 17 ssid = >Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005).</S><S sid = 39 ssid = >The SUMTIME project (Reiter et al., 2005) developed an NLG system which generated textual weather forecasts from numerical forecast data.</S> | Discourse Facet:  NA | Annotator: Automatic


