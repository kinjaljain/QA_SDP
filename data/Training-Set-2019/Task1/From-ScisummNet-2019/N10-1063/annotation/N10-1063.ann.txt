Citance Number: 1 | Reference Article:  N10-1063.txt | Citing Article:  W11-1207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model.</S> | Reference Offset:  ['36','65'] | Reference Text:  <S sid = 36 ssid = >Wikipedia’s markup contains other useful indicators for parallel sentence extraction.</S><S sid = 65 ssid = >Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N10-1063.txt | Citing Article:  W11-1208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources.</S> | Reference Offset:  ['0','153'] | Reference Text:  <S sid = 0 ssid = >Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment</S><S sid = 153 ssid = >We were pleasantly surprised at the amount of parallel sentences extracted from such a varied comparable corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N10-1063.txt | Citing Article:  P13-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This methodology is similar to that of Smith et al (2010).</S> | Reference Offset:  ['20','103'] | Reference Text:  <S sid = 20 ssid = >This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996).</S><S sid = 103 ssid = >A similar source of information has been used to create seed lexicons in (Koehn and Knight, 2002) and as part of the feature space in (Haghighi et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N10-1063.txt | Citing Article:  P13-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another.</S> | Reference Offset:  ['28','52'] | Reference Text:  <S sid = 28 ssid = >Section 2 describes the multilingual resources available in Wikipedia.</S><S sid = 52 ssid = >The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N10-1063.txt | Citing Article:  P13-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data.</S> | Reference Offset:  ['149','169'] | Reference Text:  <S sid = 149 ssid = >The “Large” data condition includes all the medium data, and also includes using a broad range of available sources such as data scraped from the web (Resnik and Smith, 2003), data from the United Nations, phrase books, software documentation, and more.</S><S sid = 169 ssid = >Furthermore, adding the Wikipedia data to the large data condition still made substantial improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N10-1063.txt | Citing Article:  P13-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010).</S> | Reference Offset:  ['155','156'] | Reference Text:  <S sid = 155 ssid = >The extracted Wikipedia data is likely to make the greatest impact on broad domain test sets – indeed, initial experimentation showed little BLEU gain on in-domain test sets such as Europarl, where out-of-domain training data is unlikely to provide appropriate phrasal translations.</S><S sid = 156 ssid = >Therefore, we experimented with two broad domain test sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N10-1063.txt | Citing Article:  P13-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation.</S> | Reference Offset:  ['44','51'] | Reference Text:  <S sid = 44 ssid = >A noisy parallel corpus has documents which contain many parallel sentences in roughly the same order.</S><S sid = 51 ssid = >In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N10-1063.txt | Citing Article:  N12-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003).</S> | Reference Offset:  ['2','52'] | Reference Text:  <S sid = 2 ssid = >In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).</S><S sid = 52 ssid = >The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N10-1063.txt | Citing Article:  W11-1212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts.</S> | Reference Offset:  ['0','51'] | Reference Text:  <S sid = 0 ssid = >Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment</S><S sid = 51 ssid = >In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N10-1063.txt | Citing Article:  W11-1212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Smith et al (2010) extended these previous lines of work in several directions.</S> | Reference Offset:  ['20','108'] | Reference Text:  <S sid = 20 ssid = >This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996).</S><S sid = 108 ssid = >This feature corresponds more closely to context similarity measures used in previous work on lexicon induction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N10-1063.txt | Citing Article:  P12-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links.</S> | Reference Offset:  ['0','51'] | Reference Text:  <S sid = 0 ssid = >Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment</S><S sid = 51 ssid = >In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N10-1063.txt | Citing Article:  W11-1209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010).</S> | Reference Offset:  ['45','51'] | Reference Text:  <S sid = 45 ssid = >Comparable corpora contain topic aligned documents which are not translations of each other.</S><S sid = 51 ssid = >In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.</S> | Discourse Facet:  NA | Annotator: Automatic


