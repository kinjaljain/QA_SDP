Citance Number: 1 | Reference Article:  D10-1001.txt | Citing Article:  D10-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems.</S> | Reference Offset:  ['29','68'] | Reference Text:  <S sid = 29 ssid = >Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007).</S><S sid = 68 ssid = >This section describes the dual decomposition approach for two inference problems in NLP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D10-1001.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This method is called dual decomposition (DD) (Rush et al, 2010).</S> | Reference Offset:  ['27','29'] | Reference Text:  <S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S><S sid = 29 ssid = >Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D10-1001.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.</S> | Reference Offset:  ['27','234'] | Reference Text:  <S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S><S sid = 234 ssid = >See Koo et al. (2010) for a similar, but less aggressive step size used to solve a different task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D10-1001.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is a slight variation of the proof given by Rush et al (2010).</S> | Reference Offset:  ['110','122'] | Reference Text:  <S sid = 110 ssid = >The constraints are given in figure 2.</S><S sid = 122 ssid = >Then Proof: This theorem is a special case of Martin et al. (1990), theorem 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D10-1001.txt | Citing Article:  D11-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation.</S> | Reference Offset:  ['3','204'] | Reference Text:  <S sid = 3 ssid = >The approach provably solves a linear programming (LP) relaxation of the global inference problem.</S><S sid = 204 ssid = >The two models were again trained separately.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D10-1001.txt | Citing Article:  E12-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010).</S> | Reference Offset:  ['36','174'] | Reference Text:  <S sid = 36 ssid = >Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005).</S><S sid = 174 ssid = >This method is a heuristic, but previous work (e.g., Komodakis et al. (2007)) has shown that it is effective in practice; we use it in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D10-1001.txt | Citing Article:  E12-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To find the minimum value, we can use a subgradient method (Rush et al 2010).</S> | Reference Offset:  ['167','174'] | Reference Text:  <S sid = 167 ssid = >Instead, a standard approach is to use a subgradient method.</S><S sid = 174 ssid = >This method is a heuristic, but previous work (e.g., Komodakis et al. (2007)) has shown that it is effective in practice; we use it in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D10-1001.txt | Citing Article:  E12-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice.</S> | Reference Offset:  ['132','174'] | Reference Text:  <S sid = 132 ssid = >The original optimization problem was to find max(y,z)EQ (y · 0cfg + z · 0tag) (see Eq.</S><S sid = 174 ssid = >This method is a heuristic, but previous work (e.g., Komodakis et al. (2007)) has shown that it is effective in practice; we use it in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D10-1001.txt | Citing Article:  E12-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follows the formulation by Rush et al (2010).</S> | Reference Offset:  ['12','72'] | Reference Text:  <S sid = 12 ssid = >The structure of this paper is as follows.</S><S sid = 72 ssid = >The benefit of the formulation in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D10-1001.txt | Citing Article:  N12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010).</S> | Reference Offset:  ['110','234'] | Reference Text:  <S sid = 110 ssid = >The constraints are given in figure 2.</S><S sid = 234 ssid = >See Koo et al. (2010) for a similar, but less aggressive step size used to solve a different task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D10-1001.txt | Citing Article:  P11-1163.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area.</S> | Reference Offset:  ['27','69'] | Reference Text:  <S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S><S sid = 69 ssid = >We now describe the dual decomposition approach for integrated parsing and trigram tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D10-1001.txt | Citing Article:  P14-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Alternatively, one can employ dual decomposition (Rush et al, 2010).</S> | Reference Offset:  ['27','29'] | Reference Text:  <S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S><S sid = 29 ssid = >Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D10-1001.txt | Citing Article:  P13-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate.</S> | Reference Offset:  ['177','233'] | Reference Text:  <S sid = 177 ssid = >The complete subgradient algorithm is given in figure 4.</S><S sid = 233 ssid = >Then we defined αk = α0 * 2−7k, where 77k is the number of times that L(u(k')) > L(u(k'−1)) for k0 < k. This learning rate drops at a rate of 1/21, where t is the number of times that the dual increases from one iteration to the next.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D10-1001.txt | Citing Article:  D12-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others.</S> | Reference Offset:  ['15','27'] | Reference Text:  <S sid = 15 ssid = >However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex to implement.</S><S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D10-1001.txt | Citing Article:  P12-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010).</S> | Reference Offset:  ['79','171'] | Reference Text:  <S sid = 79 ssid = >Our second example problem is the integration of a phrase-structure parser with a higher-order dependency parser.</S><S sid = 171 ssid = >We now turn to the problem of recovering a primal solution of the LP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D10-1001.txt | Citing Article:  P11-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update).</S> | Reference Offset:  ['93','117'] | Reference Text:  <S sid = 93 ssid = >The integrated parsing problem is then to find where R = {(y, d) : y ∈ H, d ∈ D, y(i, j) = d(i, j) for all (i, j) ∈ Ifirst} This problem has a very similar structure to the problem of integrated parsing and tagging, and we can derive a similar dual decomposition algorithm.</S><S sid = 117 ssid = >7 enforce consistency between the µ(i, Y ) variables and rule variables higher in the tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D10-1001.txt | Citing Article:  P11-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations.</S> | Reference Offset:  ['193','231'] | Reference Text:  <S sid = 193 ssid = >Over 80% of the examples converge in 5 iterations or fewer; over 90% converge in 10 iterations or fewer.</S><S sid = 231 ssid = >We used the following step size in our experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D10-1001.txt | Citing Article:  P13-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','234'] | Reference Text:  <S sid = 73 ssid = >2 is that it makes explicit the idea of maximizing over all pairs (y, z) under a set of agreement constraints y(i, t) = z(i, t)—this concept will be central to the algorithms in this paper.</S><S sid = 234 ssid = >See Koo et al. (2010) for a similar, but less aggressive step size used to solve a different task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D10-1001.txt | Citing Article:  P11-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010).</S> | Reference Offset:  ['27','214'] | Reference Text:  <S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S><S sid = 214 ssid = >This paper has considered approaches for MAP inference; for closely related methods that compute approximate marginals, see Wainwright et al. (2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D10-1001.txt | Citing Article:  P11-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010).</S> | Reference Offset:  ['27','68'] | Reference Text:  <S sid = 27 ssid = >For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.</S><S sid = 68 ssid = >This section describes the dual decomposition approach for two inference problems in NLP.</S> | Discourse Facet:  NA | Annotator: Automatic


