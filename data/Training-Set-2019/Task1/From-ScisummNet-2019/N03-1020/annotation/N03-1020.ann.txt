Citance Number: 1 | Reference Article:  N03-1020.txt | Citing Article:  W03-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part.</S> | Reference Offset:  ['78','137'] | Reference Text:  <S sid = 78 ssid = >However, BLEU is a precision-based metric while the human evaluation protocol in DUC is essentially recall-based.</S><S sid = 137 ssid = >It consistently correlated highly with human assessments and had high recall and precision in significance test with manual evaluation results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N03-1020.txt | Citing Article:  P14-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003).</S> | Reference Offset:  ['0','48'] | Reference Text:  <S sid = 0 ssid = >Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics</S><S sid = 48 ssid = >The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N03-1020.txt | Citing Article:  N04-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).</S> | Reference Offset:  ['9','28'] | Reference Text:  <S sid = 9 ssid = >For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.</S><S sid = 28 ssid = >They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N03-1020.txt | Citing Article:  N04-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC.</S> | Reference Offset:  ['80','108'] | Reference Text:  <S sid = 80 ssid = >Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.</S><S sid = 108 ssid = >(2) The performance of weighted average n-gram scores is in the range between bi-gram and tri-gram co-occurrence scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N03-1020.txt | Citing Article:  N04-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.</S> | Reference Offset:  ['5','9'] | Reference Text:  <S sid = 5 ssid = >2002).</S><S sid = 9 ssid = >For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N03-1020.txt | Citing Article:  N04-4001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case.</S> | Reference Offset:  ['10','26'] | Reference Text:  <S sid = 10 ssid = >To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.</S><S sid = 26 ssid = >Only one baseline (baseline1) was created for the single document summarization task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N03-1020.txt | Citing Article:  D12-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings.</S> | Reference Offset:  ['0','48'] | Reference Text:  <S sid = 0 ssid = >Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics</S><S sid = 48 ssid = >The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N03-1020.txt | Citing Article:  C08-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations.</S> | Reference Offset:  ['11','80'] | Reference Text:  <S sid = 11 ssid = >Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996).</S><S sid = 80 ssid = >Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N03-1020.txt | Citing Article:  H05-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Rouge (Lin and Hovy, 2003) represents another such effort.</S> | Reference Offset:  ['9','28'] | Reference Text:  <S sid = 9 ssid = >For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.</S><S sid = 28 ssid = >They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N03-1020.txt | Citing Article:  P07-2047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length).</S> | Reference Offset:  ['27','74'] | Reference Text:  <S sid = 27 ssid = >To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.</S><S sid = 74 ssid = >For example, if an automatic evaluation shows there is a significant difference between run A and run B at α = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to &quot;real&quot; significance, i.e. the statistical significance in a human assessment of run A and run B?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N03-1020.txt | Citing Article:  W08-0112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks.</S> | Reference Offset:  ['50','155'] | Reference Text:  <S sid = 50 ssid = >Therefore, BLEU seems a promising automatic scoring metric for summary evaluation.</S><S sid = 155 ssid = >Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N03-1020.txt | Citing Article:  C10-2131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2.</S> | Reference Offset:  ['50','129'] | Reference Text:  <S sid = 50 ssid = >Therefore, BLEU seems a promising automatic scoring metric for summary evaluation.</S><S sid = 129 ssid = >We then discussed the IBM BLEU MT evaluation metric, its application to summary evaluation, and the difference between precisionbased BLEU translation evaluation and recall-based DUC summary evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N03-1020.txt | Citing Article:  W05-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries.</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics</S><S sid = 10 ssid = >To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N03-1020.txt | Citing Article:  W05-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries.</S> | Reference Offset:  ['0','27'] | Reference Text:  <S sid = 0 ssid = >Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics</S><S sid = 27 ssid = >To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N03-1020.txt | Citing Article:  W05-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments.</S> | Reference Offset:  ['9','49'] | Reference Text:  <S sid = 9 ssid = >For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.</S><S sid = 49 ssid = >These numbers indicate that they positively correlate at α = 0.018.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N03-1020.txt | Citing Article:  W05-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['49','157'] | Reference Text:  <S sid = 49 ssid = >These numbers indicate that they positively correlate at α = 0.018.</S><S sid = 157 ssid = >In this way the evaluation technologies can advance at the same pace as the summarization technologies improve.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N03-1020.txt | Citing Article:  W11-0503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric.</S> | Reference Offset:  ['27','29'] | Reference Text:  <S sid = 27 ssid = >To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.</S><S sid = 29 ssid = >Using SEE, the assessors compared the system's text (the peer text) to the ideal (the model text).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N03-1020.txt | Citing Article:  W11-0503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the ROUGE (Lin and Hovy, 2003) evaluation measure.</S> | Reference Offset:  ['79','153'] | Reference Text:  <S sid = 79 ssid = >We therefore prefer the metric given by equation 6 and use it in all our experiments.</S><S sid = 153 ssid = >Therefore, it would be wise to use these valuable resources, i.e. manual summaries and evaluation results, not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N03-1020.txt | Citing Article:  P10-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009.</S> | Reference Offset:  ['80','142'] | Reference Text:  <S sid = 80 ssid = >Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.</S><S sid = 142 ssid = >The reason for this might be that most of the systems participating in DUC generate summaries by sentence extraction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N03-1020.txt | Citing Article:  P10-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC.</S> | Reference Offset:  ['28','155'] | Reference Text:  <S sid = 28 ssid = >They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.</S><S sid = 155 ssid = >Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics.</S> | Discourse Facet:  NA | Annotator: Automatic


