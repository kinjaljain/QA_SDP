Citance Number: 1 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Headden III et al (2009) showed that performance could be improved by including high frequency words as well as tags in their model.</S> | Reference Offset:  ['176','193'] | Reference Text:  <S sid = 176 ssid = >We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S><S sid = 193 ssid = >Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The final base distribution over CFG-DMV rules (Psh) is inspired by the skip-head smoothing model of Headden III et al (2009).</S> | Reference Offset:  ['149','150'] | Reference Text:  <S sid = 149 ssid = >We call this EVG smoothed-skip-head.</S><S sid = 150 ssid = >As we see below, backing off by ignoring the part-ofspeech of the head H worked better than ignoring the argument position v. For L-EVG we smooth the argument part-ofspeech distribution (conditioned on the head word) with the unlexicalized EVG smoothed-skip-head model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the |w| ≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009).</S> | Reference Offset:  ['162','176'] | Reference Text:  <S sid = 162 ssid = >For the Lexicalized-EVG, we modify this procedure slightly, by first running MB smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution.</S><S sid = 176 ssid = >We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S><S sid = 193 ssid = >Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This graph indicates that the improvements in the posterior probability of the model are correlated with the evaluation, though the correlation is not as high as we might require in order to use LLH as a model selection criteria similar to Headden III et al (2009).</S> | Reference Offset:  ['10','169'] | Reference Text:  <S sid = 10 ssid = >Figure 1) where each edge indicates a head-argument relation.</S><S sid = 169 ssid = >To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate ¯θ′.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N09-1012.txt | Citing Article:  P13-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing.</S> | Reference Offset:  ['3','110'] | Reference Text:  <S sid = 3 ssid = >In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.</S><S sid = 110 ssid = >We call this the Extended Valence Grammar (EVG).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N09-1012.txt | Citing Article:  D12-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated.</S> | Reference Offset:  ['12','91'] | Reference Text:  <S sid = 12 ssid = >Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the The big dog barks Dependency Model with Valence (DMV) by Klein and Manning (2004).</S><S sid = 91 ssid = >Klein and Manning (2004) use Expectation Maximization to estimate the model parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009).</S> | Reference Offset:  ['79','91'] | Reference Text:  <S sid = 79 ssid = >The most successful recent work on dependency induction has focused on the Dependency Model with Valence (DMV) by Klein and Manning (2004).</S><S sid = 91 ssid = >Klein and Manning (2004) use Expectation Maximization to estimate the model parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An additional point of comparison is the lexicalized unsupervised parser of Headden III et al (2009), which yields the current state-of-the-art unsupervised accuracy on English at 68.8%.</S> | Reference Offset:  ['13','23'] | Reference Text:  <S sid = 13 ssid = >DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline.</S><S sid = 23 ssid = >We show that smoothing can be employed in an unsupervised fashion as well, and show that mixing DMV, EVG, and L-EVG together produces state-ofthe-art results on this task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S><S sid = 193 ssid = >Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As in the previous section, we find that the presence of linguistic rules greatly reduces this sensitivity: for HDP-DEP, the standard deviation over five randomly initialized runs with the English-specific rules is 1.5%, compared to 4.5% for the parser developed by Headden III et al. (2009) and 8.0% for DMV (Klein and Manning, 2004).</S> | Reference Offset:  ['101','161'] | Reference Text:  <S sid = 101 ssid = >They initialize using the harmonic initializer of Klein and Manning (2004).</S><S sid = 161 ssid = >Q(¯θ) is then initialized using the standard VB M-step.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rule sets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al (2009)) to benefit from their complementary strengths.</S> | Reference Offset:  ['1','176'] | Reference Text:  <S sid = 1 ssid = >Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.</S><S sid = 176 ssid = >We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N09-1012.txt | Citing Article:  W12-1911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S><S sid = 193 ssid = >Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N09-1012.txt | Citing Article:  W12-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Headden III et al (2009), by using the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved.</S> | Reference Offset:  ['40','99'] | Reference Text:  <S sid = 40 ssid = >The dependency models in this paper will be formulated as a particular kind of Probabilistic Context Free Grammar (PCFG), described below.</S><S sid = 99 ssid = >Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For better comparison with previous work we implemented three model extensions, borrowed from Headden III et al (2009).</S> | Reference Offset:  ['39','176'] | Reference Text:  <S sid = 39 ssid = >As in previous work, we restrict ourselves to projective dependency trees.</S><S sid = 176 ssid = >We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We fix λ = 1/3, which is a crude approximation to the value learned by Headden III et al. (2009).</S> | Reference Offset:  ['99','176'] | Reference Text:  <S sid = 99 ssid = >Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S><S sid = 176 ssid = >We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Headden III et al (2009) also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.</S> | Reference Offset:  ['99','176'] | Reference Text:  <S sid = 99 ssid = >Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S><S sid = 176 ssid = >We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S><S sid = 193 ssid = >Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N09-1012.txt | Citing Article:  N12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S><S sid = 193 ssid = >Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


