Citance Number: 1 | Reference Article:  P05-1057.txt | Citing Article:  H05-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005).</S> | Reference Offset:  ['21','26'] | Reference Text:  <S sid = 21 ssid = >Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model.</S><S sid = 26 ssid = >We use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1057.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Liu et al., 2005) uses a log-linear model with a greedy search.</S> | Reference Offset:  ['87','95'] | Reference Text:  <S sid = 87 ssid = >We use a greedy search algorithm to search the alignment with highest probability in the space of all possible alignments.</S><S sid = 95 ssid = >The greedy search algorithm for general loglinear models is formally described as follows: Input: e, f, eT, fT, and D Output: a The above search algorithm, however, is not efficient for our log-linear models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1057.txt | Citing Article:  P06-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005).</S> | Reference Offset:  ['0','104'] | Reference Text:  <S sid = 0 ssid = >Log-Linear Models For Word Alignment</S><S sid = 104 ssid = >The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1057.txt | Citing Article:  D07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment.</S> | Reference Offset:  ['50','131'] | Reference Text:  <S sid = 50 ssid = >The relationship between the translation model and the alignment model is given by: Although IBM models are considered more coherent than heuristic models, they have two drawbacks.</S><S sid = 131 ssid = >We have presented a framework for word alignment based on log-linear models between parallel texts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1057.txt | Citing Article:  D09-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively.</S> | Reference Offset:  ['101','104'] | Reference Text:  <S sid = 101 ssid = >We present in this section results of experiments on a parallel corpus of Chinese-English texts.</S><S sid = 104 ssid = >The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1057.txt | Citing Article:  P06-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Liu et al (2005) used a conditional log-linear model with similar features to those we have employed.</S> | Reference Offset:  ['0','125'] | Reference Text:  <S sid = 0 ssid = >Log-Linear Models For Word Alignment</S><S sid = 125 ssid = >For log-linear models, POS information and an additional dictionary are used, which is not the case for GIZA++/IBM models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1057.txt | Citing Article:  W10-2917.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','140'] | Reference Text:  <S sid = 43 ssid = >Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.</S><S sid = 140 ssid = >2004AA114010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1057.txt | Citing Article:  W10-2917.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality.</S> | Reference Offset:  ['0','56'] | Reference Text:  <S sid = 0 ssid = >Log-Linear Models For Word Alignment</S><S sid = 56 ssid = >The use of POS information for improving statistical alignment quality of the HMM-based model is described 1If there is a target word which is assigned to more than one source words, h(a, e, f) = 0. in (Toutanova et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1057.txt | Citing Article:  N06-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005).</S> | Reference Offset:  ['0','43'] | Reference Text:  <S sid = 0 ssid = >Log-Linear Models For Word Alignment</S><S sid = 43 ssid = >Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1057.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005).</S> | Reference Offset:  ['10','135'] | Reference Text:  <S sid = 10 ssid = >Statistical approaches, which depend on a set of unknown parameters that are learned from training data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996).</S><S sid = 135 ssid = >However, the search algorithm we proposed is supervised, relying on a hand-aligned bilingual corpus, while the baseline approach of IBM alignments is unsupervised.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1057.txt | Citing Article:  W07-0405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','140'] | Reference Text:  <S sid = 43 ssid = >Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.</S><S sid = 140 ssid = >2004AA114010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1057.txt | Citing Article:  H05-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Liu et al (2005) also develop a log-linear model, based on IBM Model 3.</S> | Reference Offset:  ['21','116'] | Reference Text:  <S sid = 21 ssid = >Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model.</S><S sid = 116 ssid = >Table 2 compares the results of our log-linear models with IBM Model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1057.txt | Citing Article:  D11-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A.</S> | Reference Offset:  ['0','36'] | Reference Text:  <S sid = 0 ssid = >Log-Linear Models For Word Alignment</S><S sid = 36 ssid = >An alignment a is defined as a subset of the Cartesian product of the word positions: We define the alignment problem as finding the alignment a that maximizes Pr(a  |e, f) given e and f. We directly model the probability Pr(a  |e, f).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1057.txt | Citing Article:  D11-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005).</S> | Reference Offset:  ['75','79'] | Reference Text:  <S sid = 75 ssid = >4 requires a sum over a large number of possible alignments.</S><S sid = 79 ssid = >The set of considered alignments are also called n-best list of alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1057.txt | Citing Article:  D11-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a key difference between our model and (Liu et al, 2005).</S> | Reference Offset:  ['37','47'] | Reference Text:  <S sid = 37 ssid = >An especially well-founded framework is maximum entropy (Berger et al., 1996).</S><S sid = 47 ssid = >Brown et al. (1993) proposed a series of statistical models of the translation process.</S> | Discourse Facet:  NA | Annotator: Automatic


