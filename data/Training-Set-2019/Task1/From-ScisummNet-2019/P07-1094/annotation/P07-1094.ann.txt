Citance Number: 1 | Reference Article:  P07-1094.txt | Citing Article:  D07-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known: see Besag (2004) for a tutorial introduction and Goldwater and Griffiths (2007) for an application of Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging.</S> | Reference Offset:  ['14','72'] | Reference Text:  <S sid = 14 ssid = >Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.</S><S sid = 72 ssid = >To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1094.txt | Citing Article:  P14-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models.</S> | Reference Offset:  ['20','131'] | Reference Text:  <S sid = 20 ssid = >Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).</S><S sid = 131 ssid = >Table 2 shows results for the various models and a random baseline (averaged by the various models on different sized corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1094.txt | Citing Article:  C08-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007).</S> | Reference Offset:  ['1','9'] | Reference Text:  <S sid = 1 ssid = >Unsupervised learning of linguistic structure is a difficult problem.</S><S sid = 9 ssid = >Unsupervised learning of linguistic structure is a difficult problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1094.txt | Citing Article:  W11-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference.</S> | Reference Offset:  ['61','126'] | Reference Text:  <S sid = 61 ssid = >For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.</S><S sid = 126 ssid = >In the experiments reported in the following section, we used two different versions of our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1094.txt | Citing Article:  P08-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ).</S> | Reference Offset:  ['21','24'] | Reference Text:  <S sid = 21 ssid = >More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM (Smith and Eisner, 2005).</S><S sid = 24 ssid = >Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1094.txt | Citing Article:  P10-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007).</S> | Reference Offset:  ['30','147'] | Reference Text:  <S sid = 30 ssid = >We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels.</S><S sid = 147 ssid = >We therefore introduce another evaluation measure for these experiments, a distance metric on clusterings known as variation of information (Meilˇa, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1094.txt | Citing Article:  N09-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007).</S> | Reference Offset:  ['61','82'] | Reference Text:  <S sid = 61 ssid = >For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.</S><S sid = 82 ssid = >We relax the latter assumption in same tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1094.txt | Citing Article:  N09-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here.</S> | Reference Offset:  ['70','100'] | Reference Text:  <S sid = 70 ssid = >However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estiwhere nk is the number of times k occurred in x−i.</S><S sid = 100 ssid = >Results from our model for a range of hyperparameters are presented in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1094.txt | Citing Article:  N09-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline.</S> | Reference Offset:  ['7','96'] | Reference Text:  <S sid = 7 ssid = >Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.</S><S sid = 96 ssid = >For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1094.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003).</S> | Reference Offset:  ['27','60'] | Reference Text:  <S sid = 27 ssid = >Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997).</S><S sid = 60 ssid = >In many linguistic models, including HMMs, the distributions over variables are multinomial.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1094.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007).</S> | Reference Offset:  ['88','122'] | Reference Text:  <S sid = 88 ssid = >As φ approaches 0, even a single between the variables in the model.</S><S sid = 122 ssid = >We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1094.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs.</S> | Reference Offset:  ['63','87'] | Reference Text:  <S sid = 63 ssid = >For a sequence of draws x = ... , xn) from a multinomial distribution B with observed counts ... , nK, a symmetric prior over B yields the MAP estimate Bk = When Q 1, standard MLE techniques such as EM can be used to find the MAP estimate simply by adding of size Q 1 to each of the expected counts nk at each iteration.</S><S sid = 87 ssid = >Note that, by integrating out ing the probabilities in the sampling distribution to the parameters τ and ω, we induce dependencies the power of 1 φ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1094.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','174'] | Reference Text:  <S sid = 54 ssid = >Consequently, P(t|w, �B) favors t = 1 for any sequence that does not contain exactly five heads, and assigns equal probability tot = 1 and t = 0 for any sequence that does contain exactly five heads — a counterintuitive result.</S><S sid = 174 ssid = >We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1094.txt | Citing Article:  N10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007).</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >A fully Bayesian approach to unsupervised part-of-speech tagging</S><S sid = 14 ssid = >Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1094.txt | Citing Article:  N10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing: raise the probabilities in the sampling distribution to the 1T power, where T is a temperature parameter, decrease T to wards zero, and take a single sample.</S> | Reference Offset:  ['87','98'] | Reference Text:  <S sid = 87 ssid = >Note that, by integrating out ing the probabilities in the sampling distribution to the parameters τ and ω, we induce dependencies the power of 1 φ.</S><S sid = 98 ssid = >The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1094.txt | Citing Article:  N10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model.</S> | Reference Offset:  ['31','92'] | Reference Text:  <S sid = 31 ssid = >We also evaluate using tag accuracy when possible.</S><S sid = 92 ssid = >The property of exchangeability is words for tag t.) The dictionary was constructed by crucial to the inference algorithm we describe next. listing, for each word, all tags found for that word in 747 the entire WSJ treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1094.txt | Citing Article:  W10-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information.</S> | Reference Offset:  ['20','35'] | Reference Text:  <S sid = 20 ssid = >Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).</S><S sid = 35 ssid = >In the following section, we discuss the motivation for a Bayesian approach and present our model and search procedure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1094.txt | Citing Article:  W10-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model.</S> | Reference Offset:  ['72','96'] | Reference Text:  <S sid = 72 ssid = >To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α).</S><S sid = 96 ssid = >For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1094.txt | Citing Article:  W10-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models.</S> | Reference Offset:  ['126','173'] | Reference Text:  <S sid = 126 ssid = >In the experiments reported in the following section, we used two different versions of our model.</S><S sid = 173 ssid = >For knowledgefree clustering, our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1094.txt | Citing Article:  W10-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers.</S> | Reference Offset:  ['58','78'] | Reference Text:  <S sid = 58 ssid = >By integrating over B, Equation 3 takes into account the consequences of possible variation in B.</S><S sid = 78 ssid = >The sampling distribution for ti nents, and each of the output distributions ω(t) has is given in Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


