Citance Number: 1 | Reference Article:  A94-1009.txt | Citing Article:  P06-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags.</S> | Reference Offset:  ['76','89'] | Reference Text:  <S sid = 76 ssid = >For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48.</S><S sid = 89 ssid = >For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-l3-3 from parts B to J inclusive.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A94-1009.txt | Citing Article:  P06-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94).</S> | Reference Offset:  ['45','55'] | Reference Text:  <S sid = 45 ssid = >Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible.</S><S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A94-1009.txt | Citing Article:  P00-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs.</S> | Reference Offset:  ['18','137'] | Reference Text:  <S sid = 18 ssid = >One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM).</S><S sid = 137 ssid = >Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A94-1009.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful.</S> | Reference Offset:  ['33','52'] | Reference Text:  <S sid = 33 ssid = >With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alternatives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992; Brill, 1992), DeRose (DeRose, 1988) and Kupiec (Kupiec, 1992).</S><S sid = 52 ssid = >The results suggest that a completely unconstrained initial model does not produce good quality results, and that one 'The technique was originally developed by Kupiec (Kupiec, 1989). accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A94-1009.txt | Citing Article:  C04-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S><S sid = 149 ssid = >I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A94-1009.txt | Citing Article:  W96-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle.</S> | Reference Offset:  ['10','55'] | Reference Text:  <S sid = 10 ssid = >The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.</S><S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A94-1009.txt | Citing Article:  W96-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy.</S> | Reference Offset:  ['56','137'] | Reference Text:  <S sid = 56 ssid = >We will discuss this work below.</S><S sid = 137 ssid = >Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A94-1009.txt | Citing Article:  W96-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On this point, I agree with Merialdo (1994) and Elworthy (1994).</S> | Reference Offset:  ['10','55'] | Reference Text:  <S sid = 10 ssid = >The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.</S><S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A94-1009.txt | Citing Article:  P04-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S><S sid = 149 ssid = >I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A94-1009.txt | Citing Article:  W07-2203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting.</S> | Reference Offset:  ['0','137'] | Reference Text:  <S sid = 0 ssid = >Does Baum-Welch Re-Estimation Help Taggers?</S><S sid = 137 ssid = >Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A94-1009.txt | Citing Article:  W07-2203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S><S sid = 149 ssid = >I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A94-1009.txt | Citing Article:  W07-2203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).</S> | Reference Offset:  ['49','105'] | Reference Text:  <S sid = 49 ssid = >If significant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such intervention is needed on each new text domain.</S><S sid = 105 ssid = >Secondly, training from a hand-tagged corpus (case DO+TO) always does best, even when the test data is from a different source to the training data, as it is for LOB-L.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A94-1009.txt | Citing Article:  P08-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags.</S> | Reference Offset:  ['76','89'] | Reference Text:  <S sid = 76 ssid = >For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48.</S><S sid = 89 ssid = >For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-l3-3 from parts B to J inclusive.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A94-1009.txt | Citing Article:  W09-2403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced.</S> | Reference Offset:  ['34','63'] | Reference Text:  <S sid = 34 ssid = >One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992).</S><S sid = 63 ssid = >For example, CLAWS (Garside et al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A94-1009.txt | Citing Article:  W95-0101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In [Elworthy, 1994], similar experiments were run.</S> | Reference Offset:  ['55','125'] | Reference Text:  <S sid = 55 ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S><S sid = 125 ssid = >The re-estimation was allowed to run for ten iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A94-1009.txt | Citing Article:  W95-0101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy.</S> | Reference Offset:  ['73','94'] | Reference Text:  <S sid = 73 ssid = >For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1 — a + pa. All of the accuracy figures quoted below are for ambiguous words only.</S><S sid = 94 ssid = >As an example of how these figures relate to overall accuracies, LOB-B contains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J, and the overall accuracy in the DO+TO case is hence 98.69%.</S> | Discourse Facet:  NA | Annotator: Automatic


