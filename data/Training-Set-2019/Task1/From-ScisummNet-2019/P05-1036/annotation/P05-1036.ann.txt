Citance Number: 1 | Reference Article:  P05-1036.txt | Citing Article:  E06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model.</S> | Reference Offset:  ['1','103'] | Reference Text:  <S sid = 1 ssid = >Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.</S><S sid = 103 ssid = >This supervised version compresses better than either version of the supervised noisy-channel model that lacks these rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1036.txt | Citing Article:  N07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005).</S> | Reference Offset:  ['63','127'] | Reference Text:  <S sid = 63 ssid = >We count all probabilistic context free grammar (PCFG) expansions, and then match up similar rules as unsupervised joint events.</S><S sid = 127 ssid = >Let us consider a simplified version of a K&M example, but as reinterpreted for our model: how the noisy channel model assigns a probability of the compressed tree (A) in Figure 3 given the original tree B.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1036.txt | Citing Article:  P06-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005).</S> | Reference Offset:  ['62','63'] | Reference Text:  <S sid = 62 ssid = >We create joint rules using only the first section (0.mrg) of the Penn Treebank.</S><S sid = 63 ssid = >We count all probabilistic context free grammar (PCFG) expansions, and then match up similar rules as unsupervised joint events.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1036.txt | Citing Article:  P06-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones.</S> | Reference Offset:  ['56','156'] | Reference Text:  <S sid = 56 ssid = >One of the biggest problems with this model of sentence compression is the lack of appropriate training data.</S><S sid = 156 ssid = >To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1036.txt | Citing Article:  P10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >There are several hundred rules of this type, and it is very simple to incorporate into our model.</S><S sid = 173 ssid = >We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1036.txt | Citing Article:  D07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >There are several hundred rules of this type, and it is very simple to incorporate into our model.</S><S sid = 173 ssid = >We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1036.txt | Citing Article:  D07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >There are several hundred rules of this type, and it is very simple to incorporate into our model.</S><S sid = 173 ssid = >We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1036.txt | Citing Article:  P06-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >There are several hundred rules of this type, and it is very simple to incorporate into our model.</S><S sid = 173 ssid = >We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1036.txt | Citing Article:  P06-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones.</S> | Reference Offset:  ['25','156'] | Reference Text:  <S sid = 25 ssid = >The K&M model uses parse trees for the sentences.</S><S sid = 156 ssid = >To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1036.txt | Citing Article:  P13-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality.</S> | Reference Offset:  ['78','105'] | Reference Text:  <S sid = 78 ssid = >As will be shown, this rule is not constraining enough and allows some poor compressions, but it is remarkable that any sort of compression can be achieved without training data.</S><S sid = 105 ssid = >Example 2 shows how unsupervised and semisupervised techniques can be used to improve compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1036.txt | Citing Article:  P08-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether.</S> | Reference Offset:  ['18','156'] | Reference Text:  <S sid = 18 ssid = >The K&M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.</S><S sid = 156 ssid = >To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1036.txt | Citing Article:  P08-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task.</S> | Reference Offset:  ['18','156'] | Reference Text:  <S sid = 18 ssid = >The K&M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.</S><S sid = 156 ssid = >To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1036.txt | Citing Article:  W11-1601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side.</S> | Reference Offset:  ['41','56'] | Reference Text:  <S sid = 41 ssid = >The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001).</S><S sid = 56 ssid = >One of the biggest problems with this model of sentence compression is the lack of appropriate training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1036.txt | Citing Article:  W11-1610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >There are several hundred rules of this type, and it is very simple to incorporate into our model.</S><S sid = 173 ssid = >We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1036.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data.</S> | Reference Offset:  ['0','76'] | Reference Text:  <S sid = 0 ssid = >Supervised And Unsupervised Learning For Sentence Compression</S><S sid = 76 ssid = >None of the special rules are applied.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1036.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem.</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.</S><S sid = 7 ssid = >Knight and Marcu (henceforth K&M) introduce the task of statistical sentence compression in Statistics-Based Summarization - Step One: Sentence Compression (Knight and Marcu, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1036.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label.</S> | Reference Offset:  ['76','131'] | Reference Text:  <S sid = 76 ssid = >None of the special rules are applied.</S><S sid = 131 ssid = >In a pure PCFG this would only include the label of the node.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1036.txt | Citing Article:  P13-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005).</S> | Reference Offset:  ['41','56'] | Reference Text:  <S sid = 41 ssid = >The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001).</S><S sid = 56 ssid = >One of the biggest problems with this model of sentence compression is the lack of appropriate training data.</S> | Discourse Facet:  NA | Annotator: Automatic


