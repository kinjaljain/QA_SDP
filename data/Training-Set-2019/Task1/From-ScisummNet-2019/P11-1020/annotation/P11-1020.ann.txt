Citance Number: 1 | Reference Article:  P11-1020.txt | Citing Article:  D12-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos.</S> | Reference Offset:  ['27','205'] | Reference Text:  <S sid = 27 ssid = >Examples of this kind of data include the Multiple-Translation Chinese (MTC) Corpus 2 which consists of Chinese news stories translated into English by 11 translation agencies, and literary works with multiple translations into English (e.g.</S><S sid = 205 ssid = >Asking annotators to write multiple descriptions or longer descriptions would result in more varied data but at the cost of more noise in the alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P11-1020.txt | Citing Article:  P14-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012).</S> | Reference Offset:  ['77','88'] | Reference Text:  <S sid = 77 ssid = >Many of them annotated all the available videos we had.</S><S sid = 88 ssid = >Examples of some of the descriptions collected are shown in Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P11-1020.txt | Citing Article:  D12-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011).</S> | Reference Offset:  ['11','70'] | Reference Text:  <S sid = 11 ssid = >In contrast, there are no “professional paraphrasers”, with the result that there are no readily available large corpora and no consistent standards for what constitutes a high-quality paraphrase.</S><S sid = 70 ssid = >The qualification process was done manually by the authors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P11-1020.txt | Citing Article:  S12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011).</S> | Reference Offset:  ['49','58'] | Reference Text:  <S sid = 49 ssid = >A screenshot of our annotation task is shown in Figure 1.</S><S sid = 58 ssid = >Watch and describe a short segment of a video You will be shown a segment of a video clip and asked to describe the main action/event in that segment in ONE SENTENCE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P11-1020.txt | Citing Article:  E12-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011).</S> | Reference Offset:  ['48','97'] | Reference Text:  <S sid = 48 ssid = >For each task, we asked the annotators to watch a very short video clip (usually less than 10 seconds long) and describe in one sentence the main action or event that occurred in the video clip We deployed the task on Amazon’s Mechanical Turk, with video segments selected from YouTube.</S><S sid = 97 ssid = >Even limiting the set to descriptions produced from the Tier-2 tasks, there are still 16 descriptions on average for each video, with at least 12 descriptions for over 95% of the videos.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P11-1020.txt | Citing Article:  S12-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur).</S> | Reference Offset:  ['18','29'] | Reference Text:  <S sid = 18 ssid = >In addition to describing a mechanism for collecting large-scale sentence-level paraphrases, we are also making available to the research community 85K parallel English sentences as part of the Microsoft Research Video Description Corpus 1.</S><S sid = 29 ssid = >Another method for collecting monolingual paraphrase data involves aligning semantically parallel sentences from different news articles describing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P11-1020.txt | Citing Article:  S12-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MSR video data (Chen and Dolan, 2011).</S> | Reference Offset:  ['217','220'] | Reference Text:  <S sid = 217 ssid = >Another application for our data is to apply it to computer vision tasks such as video retrieval.</S><S sid = 220 ssid = >We introduced a data collection framework that produces highly parallel data by asking different annotators to describe the same video segments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P11-1020.txt | Citing Article:  P12-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers.</S> | Reference Offset:  ['66','80'] | Reference Text:  <S sid = 66 ssid = >To ensure the quality of the annotations being produced, we used a two-tiered payment system.</S><S sid = 80 ssid = >We again used a tiered payment system to reward and retain workers who performed well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P11-1020.txt | Citing Article:  P12-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation.</S> | Reference Offset:  ['6','223'] | Reference Text:  <S sid = 6 ssid = >However, a lack of standard datasets and automatic evaluation metrics has impeded progress in the field.</S><S sid = 223 ssid = >Finally, we also introduced a new metric, PINC, to measure the lexical dissimilarity between the source sentence and the paraphrase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P11-1020.txt | Citing Article:  P13-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003).</S> | Reference Offset:  ['7','29'] | Reference Text:  <S sid = 7 ssid = >Without these resources, researchers have resorted to developing their own small, ad hoc datasets (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004; Dolan et al., 2004), and have often relied on human judgments to evaluate their results (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Bannard and Callison-Burch, 2005).</S><S sid = 29 ssid = >Another method for collecting monolingual paraphrase data involves aligning semantically parallel sentences from different news articles describing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P11-1020.txt | Citing Article:  P13-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor.</S> | Reference Offset:  ['18','89'] | Reference Text:  <S sid = 18 ssid = >In addition to describing a mechanism for collecting large-scale sentence-level paraphrases, we are also making available to the research community 85K parallel English sentences as part of the Microsoft Research Video Description Corpus 1.</S><S sid = 89 ssid = >Overall, 688 workers submitted at least one English description.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P11-1020.txt | Citing Article:  P13-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos.</S> | Reference Offset:  ['96','97'] | Reference Text:  <S sid = 96 ssid = >On average, 41 descriptions were produced for each video, with at least 27 for over 95% of the videos.</S><S sid = 97 ssid = >Even limiting the set to descriptions produced from the Tier-2 tasks, there are still 16 descriptions on average for each video, with at least 12 descriptions for over 95% of the videos.</S> | Discourse Facet:  NA | Annotator: Automatic


