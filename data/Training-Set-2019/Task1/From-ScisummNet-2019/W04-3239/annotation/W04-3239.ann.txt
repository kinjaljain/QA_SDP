Citance Number: 1 | Reference Article:  W04-3239.txt | Citing Article:  P11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection.</S> | Reference Offset:  ['1','150'] | Reference Text:  <S sid = 1 ssid = >The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.</S><S sid = 150 ssid = >When we explicitly enumerate the subtrees used in tree kernel, the number of active (non-zero) features might amount to ten thousand or more.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-3239.txt | Citing Article:  W10-2927.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['27','28'] | Reference Text:  <S sid = 27 ssid = >First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners.</S><S sid = 28 ssid = >Second, we show an implementation issue related to constructing an efficient learning algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-3239.txt | Citing Article:  W08-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['38','79'] | Reference Text:  <S sid = 38 ssid = >Decision stumps are simple classifiers, where the final decision is made by only a single hypothesis or feature.</S><S sid = 79 ssid = >The final hypothesis of SVMs with tree kernel can be given by Similarly, the final hypothesis of our boosting algorithm can be reformulated as a linear classifier: 1Strictly speaking, tree kernel uses the cardinality of each substructure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-3239.txt | Citing Article:  P06-2080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['32','50'] | Reference Text:  <S sid = 32 ssid = >The focused problem can be formalized as a general problem, called the tree classification problem.</S><S sid = 50 ssid = >This problem is formally defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-3239.txt | Citing Article:  P09-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['11','17'] | Reference Text:  <S sid = 11 ssid = >For learning algorithms to identify these topics, a text is usually represented as a bag-of-words, where a text is regarded as a multi-set (i.e., a bag) of words and the word order or syntactic relations appearing in the original text is ignored.</S><S sid = 17 ssid = >A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-3239.txt | Citing Article:  P06-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification.</S> | Reference Offset:  ['52','97'] | Reference Text:  <S sid = 52 ssid = >Given T, find the optimal rule (ˆt, ˆy) that maximizes the gain, i.e., The most naive and exhaustive method, in which we first enumerate all subtrees F and then calculate the gains for all subtrees, is usually impractical, since the number of subtrees is exponential to its size.</S><S sid = 97 ssid = >Second, sparse hypotheses are useful in practice as they provide “transparent” models with which we can analyze how the model performs or what kind of features are useful.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-3239.txt | Citing Article:  P09-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence.</S> | Reference Offset:  ['11','117'] | Reference Text:  <S sid = 11 ssid = >For learning algorithms to identify these topics, a text is usually represented as a bag-of-words, where a text is regarded as a multi-set (i.e., a bag) of words and the word order or syntactic relations appearing in the original text is ignored.</S><S sid = 117 ssid = >It is the word-based dependency tree that assumes that each word simply modifies the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-3239.txt | Citing Article:  W05-1513.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['79','168'] | Reference Text:  <S sid = 79 ssid = >The final hypothesis of SVMs with tree kernel can be given by Similarly, the final hypothesis of our boosting algorithm can be reformulated as a linear classifier: 1Strictly speaking, tree kernel uses the cardinality of each substructure.</S><S sid = 168 ssid = >The testing speed of our Boosting algorithm is much higher than that of SVMs with tree kernel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-3239.txt | Citing Article:  D09-1130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004).</S> | Reference Offset:  ['37','118'] | Reference Text:  <S sid = 37 ssid = >Figure 1 shows an example of a labeled ordered tree and its subtree and non-subtree.</S><S sid = 118 ssid = >Any subtree of this structure becomes a word n-gram.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-3239.txt | Citing Article:  D09-1130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions.</S> | Reference Offset:  ['136','141'] | Reference Text:  <S sid = 136 ssid = >This is because the number of common features between similar instances exponentially increases with size.</S><S sid = 141 ssid = >The selection of optimal hyperparameters, such as decay factors in the first approach and smoothing parameters in the second approach, is also left to as an open question.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-3239.txt | Citing Article:  D09-1130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}.</S> | Reference Offset:  ['7','37'] | Reference Text:  <S sid = 7 ssid = >Two experiments on opinion/modality classification confirm that subtree features are important.</S><S sid = 37 ssid = >Figure 1 shows an example of a labeled ordered tree and its subtree and non-subtree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-3239.txt | Citing Article:  D09-1130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['67','181'] | Reference Text:  <S sid = 67 ssid = >Rightmost extension defines a canonical search space in which one can enumerate all subtrees from a given set of trees.</S><S sid = 181 ssid = >We would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications (e.g., parse re-ranking (Collins and Duffy, 2002) and information extraction).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-3239.txt | Citing Article:  C08-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['29','59'] | Reference Text:  <S sid = 29 ssid = >We also discuss the relation between our algorithm and SVMs (Boser et al., 1992) with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002).</S><S sid = 59 ssid = >Definition 4 Rightmost Extension (Abe et al., 2002; Zaki, 2002) Let t and t' be labeled ordered trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-3239.txt | Citing Article:  P05-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['32','54'] | Reference Text:  <S sid = 32 ssid = >The focused problem can be formalized as a general problem, called the tree classification problem.</S><S sid = 54 ssid = >The method to find the optimal rule is modeled as a variant of the branch-and-bound algorithm, and is summarized in the following strategies: We will describe these steps more precisely in the following subsections.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-3239.txt | Citing Article:  D07-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT.</S> | Reference Offset:  ['0','122'] | Reference Text:  <S sid = 0 ssid = >A Boosting Algorithm For Classification Of Semi-Structured Text</S><S sid = 122 ssid = >To extend a binary classifier to a multi-class classifier, we use the one-vs-rest method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-3239.txt | Citing Article:  C10-2047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004).</S> | Reference Offset:  ['26','118'] | Reference Text:  <S sid = 26 ssid = >This paper is organized as follows.</S><S sid = 118 ssid = >Any subtree of this structure becomes a word n-gram.</S> | Discourse Facet:  NA | Annotator: Automatic


