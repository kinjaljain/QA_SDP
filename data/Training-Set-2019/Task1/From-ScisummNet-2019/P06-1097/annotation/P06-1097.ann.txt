Citance Number: 1 | Reference Article:  P06-1097.txt | Citing Article:  W10-1701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007).</S> | Reference Offset:  ['48','114'] | Reference Text:  <S sid = 48 ssid = >This is a small sampling of the kinds of knowledge sources we can use in this framework; many others have been proposed in the literature.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1097.txt | Citing Article:  W10-2917.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking.</S> | Reference Offset:  ['114','146'] | Reference Text:  <S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S><S sid = 146 ssid = >We hope that Minimum Error / Maximum Likelihood training using the EMD algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1097.txt | Citing Article:  N07-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features.</S> | Reference Offset:  ['114','117'] | Reference Text:  <S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S><S sid = 117 ssid = >All of these also used knowledge from one of the IBM Models in order to obtain competitive results with the baseline (with the exception of (Moore, 2005)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1097.txt | Citing Article:  W07-0403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998).</S> | Reference Offset:  ['24','114'] | Reference Text:  <S sid = 24 ssid = >We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998).</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1097.txt | Citing Article:  C10-2084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.</S><S sid = 149 ssid = >We would like to thank the USC Center for High Performance Computing and Communications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1097.txt | Citing Article:  W09-1804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements.</S> | Reference Offset:  ['91','114'] | Reference Text:  <S sid = 91 ssid = >Table 7 evaluates the EMD semi-supervised training algorithm.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1097.txt | Citing Article:  D07-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.</S><S sid = 149 ssid = >We would like to thank the USC Center for High Performance Computing and Communications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1097.txt | Citing Article:  D07-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006).</S> | Reference Offset:  ['27','91'] | Reference Text:  <S sid = 27 ssid = >For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4.</S><S sid = 91 ssid = >Table 7 evaluates the EMD semi-supervised training algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1097.txt | Citing Article:  D07-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.</S><S sid = 149 ssid = >We would like to thank the USC Center for High Performance Computing and Communications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1097.txt | Citing Article:  D07-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006).</S> | Reference Offset:  ['12','114'] | Reference Text:  <S sid = 12 ssid = >These steps lead to word alignments of higher accuracy which, in our case, correlate with higher MT accuracy.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1097.txt | Citing Article:  D07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model.</S> | Reference Offset:  ['91','114'] | Reference Text:  <S sid = 91 ssid = >Table 7 evaluates the EMD semi-supervised training algorithm.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1097.txt | Citing Article:  D07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b).</S> | Reference Offset:  ['0','91'] | Reference Text:  <S sid = 0 ssid = >Semi-Supervised Training For Statistical Word Alignment</S><S sid = 91 ssid = >Table 7 evaluates the EMD semi-supervised training algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1097.txt | Citing Article:  D07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3.</S> | Reference Offset:  ['57','105'] | Reference Text:  <S sid = 57 ssid = >The discriminatively reranked extended model outperforms four iterations of Model 4 in both cases with the best heuristic symmetrization, but some of the gain is lost as we are optimizing the F-measure of the 1-to-many alignments rather than the F-measure of the many-to-many alignments directly.</S><S sid = 105 ssid = >We also show the F-measure after heuristic symmetrization of the alignment test sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1097.txt | Citing Article:  D07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model.</S> | Reference Offset:  ['35','38'] | Reference Text:  <S sid = 35 ssid = >We reinterpret the five groups of parameters of Model 4 listed in the first five lines of Table 3 as sub-models of a log-linear model (see Equation 1).</S><S sid = 38 ssid = >Log-linear models are often trained to maximize entropy, but we will train our model directly on the final performance criterion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1097.txt | Citing Article:  C10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007).</S> | Reference Offset:  ['78','114'] | Reference Text:  <S sid = 78 ssid = >This approximation is called Viterbi training.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1097.txt | Citing Article:  P07-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences.</S> | Reference Offset:  ['35','38'] | Reference Text:  <S sid = 35 ssid = >We reinterpret the five groups of parameters of Model 4 listed in the first five lines of Table 3 as sub-models of a log-linear model (see Equation 1).</S><S sid = 38 ssid = >Log-linear models are often trained to maximize entropy, but we will train our model directly on the final performance criterion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1097.txt | Citing Article:  D07-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006).</S> | Reference Offset:  ['32','114'] | Reference Text:  <S sid = 32 ssid = >We use the “union”, “refined” and “intersection” heuristics defined in (Och and Ney, 2003) which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1097.txt | Citing Article:  P10-2067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models.</S> | Reference Offset:  ['37','114'] | Reference Text:  <S sid = 37 ssid = >Given a vector of these weights a, the alignment search problem, i.e. the search to return the best alignment a� of the sentences e and f according to the model, is specified by Equation 2.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1097.txt | Citing Article:  W10-0102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models.</S> | Reference Offset:  ['37','114'] | Reference Text:  <S sid = 37 ssid = >Given a vector of these weights a, the alignment search problem, i.e. the search to return the best alignment a� of the sentences e and f according to the model, is specified by Equation 2.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1097.txt | Citing Article:  W07-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.</S> | Reference Offset:  ['1','114'] | Reference Text:  <S sid = 1 ssid = >We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.</S><S sid = 114 ssid = >See (Fraser and Marcu, 2006) for further details.</S> | Discourse Facet:  NA | Annotator: Automatic


