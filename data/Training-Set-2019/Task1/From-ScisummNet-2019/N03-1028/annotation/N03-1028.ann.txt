Citance Number: 1 | Reference Article:  N03-1028.txt | Citing Article:  P03-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al, 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz et al., 1999).</S> | Reference Offset:  ['20','34'] | Reference Text:  <S sid = 20 ssid = >The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001).</S><S sid = 34 ssid = >We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N03-1028.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information.</S> | Reference Offset:  ['103','121'] | Reference Text:  <S sid = 103 ssid = >For our highest F score, we used the complete feature set, around 3.8 million in the CoNLL training set, which contains all the features whose predicate is on at least once in the training set.</S><S sid = 121 ssid = >Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N03-1028.txt | Citing Article:  W06-1670.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the perceptron performance is comparable to that of Conditional Random Field models (ShaandPereira, 2003).</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >Shallow Parsing With Conditional Random Fields</S><S sid = 3 ssid = >We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N03-1028.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker.</S> | Reference Offset:  ['109','163'] | Reference Text:  <S sid = 109 ssid = >The standard evaluation metrics for a chunker are precision P (fraction of output chunks that exactly match the reference chunks), recall R (fraction of reference chunks returned by the chunker), and their harmonic mean, the F1 score F1 = 2 * P * R/(P + R) (which we call just F score in what follows).</S><S sid = 163 ssid = >Taku Kudo provided the output of his SVM chunker for the significance test.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N03-1028.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set.</S> | Reference Offset:  ['97','103'] | Reference Text:  <S sid = 97 ssid = >Table 1 summarizes the feature set.</S><S sid = 103 ssid = >For our highest F score, we used the complete feature set, around 3.8 million in the CoNLL training set, which contains all the features whose predicate is on at least once in the training set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N03-1028.txt | Citing Article:  W04-3230.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This evaluation was also used in (Sha and Pereira, 2003).</S> | Reference Offset:  ['120','131'] | Reference Text:  <S sid = 120 ssid = >GIS, CG, and L-BFGS were used to train CRFs and MEMMs.</S><S sid = 131 ssid = >The relative slowness of iterative scaling is also documented in a recent evaluation of training methods for maximum-entropy classification (Malouf, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N03-1028.txt | Citing Article:  P08-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label.</S> | Reference Offset:  ['92','99'] | Reference Text:  <S sid = 92 ssid = >That is, the label at position i is yi = where ci is the chunk tag of word i, one of B, or I.</S><S sid = 99 ssid = >For any label y = c'c, c(y) = c is the corresponding chunk tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N03-1028.txt | Citing Article:  P05-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking (Sha and Pereira, 2003).</S> | Reference Offset:  ['3','153'] | Reference Text:  <S sid = 3 ssid = >We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.</S><S sid = 153 ssid = >In a longer version of this work we will also describe shallow parsing results for other phrase types.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N03-1028.txt | Citing Article:  W06-2918.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003).</S> | Reference Offset:  ['7','154'] | Reference Text:  <S sid = 7 ssid = >In language processing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing.</S><S sid = 154 ssid = >There is no reason why the same techniques cannot be used equally successfully for the other types or for other related tasks, such as POS tagging or named-entity recognition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N03-1028.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003).</S> | Reference Offset:  ['11','153'] | Reference Text:  <S sid = 11 ssid = >The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing.</S><S sid = 153 ssid = >In a longer version of this work we will also describe shallow parsing results for other phrase types.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N03-1028.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states.</S> | Reference Offset:  ['111','136'] | Reference Text:  <S sid = 111 ssid = >For comparisons with other reported results we use F score.</S><S sid = 136 ssid = >We believe that the superior convergence rate of preconditioned CG is due to the use of approximate second-order information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N03-1028.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set.</S> | Reference Offset:  ['114','125'] | Reference Text:  <S sid = 114 ssid = >We report F scores for comparison with previous work, but we also give statistical significance estimates using McNemar’s test for those methods that we evaluated directly.</S><S sid = 125 ssid = >Zhang et al. (2002) reported a higher F score (94.38%) with generalized winnow using additional linguistic features that were not available to us.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N03-1028.txt | Citing Article:  W10-3020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood n i log P (yi|xi) (Sha and Pereira, 2003).</S> | Reference Offset:  ['37','137'] | Reference Text:  <S sid = 37 ssid = >A CRF on (X, Y ) is specified by a vector f of local features and a corresponding weight vector .</S><S sid = 137 ssid = >This is confirmed by the performance of L-BFGS, which also uses approximate second-order information.2 Although there is no direct relationship between F scores and log-likelihood, in these experiments F score tends to follow log-likelihood.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N03-1028.txt | Citing Article:  H05-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For more information on current training methods for CRFs, see Sha and Pereira (2003).</S> | Reference Offset:  ['5','79'] | Reference Text:  <S sid = 5 ssid = >We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.</S><S sid = 79 ssid = >Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N03-1028.txt | Citing Article:  N09-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','163'] | Reference Text:  <S sid = 60 ssid = >In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian.</S><S sid = 163 ssid = >Taku Kudo provided the output of his SVM chunker for the significance test.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N03-1028.txt | Citing Article:  N10-1128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003).</S> | Reference Offset:  ['55','61'] | Reference Text:  <S sid = 55 ssid = >Instead of searching along the gradient, conjugate gradient searches along a carefully chosen linear combination of the gradient and the previous search direction.</S><S sid = 61 ssid = >However in our case the Hessian has the form</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N03-1028.txt | Citing Article:  C08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete an notations using gradient ascent iterations (Sha and Pereira, 2003).</S> | Reference Offset:  ['54','55'] | Reference Text:  <S sid = 54 ssid = >Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimization (Shewchuk, 1994).</S><S sid = 55 ssid = >Instead of searching along the gradient, conjugate gradient searches along a carefully chosen linear combination of the gradient and the previous search direction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N03-1028.txt | Citing Article:  W10-3217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chunking approach in this paper is closely similar to the work of Sha and Pereira (2003).</S> | Reference Offset:  ['121','152'] | Reference Text:  <S sid = 121 ssid = >Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features.</S><S sid = 152 ssid = >These models combine the best features of generative finite-state models and discriminative (log-)linear classifiers, and do NP chunking as well as or better than “ad hoc” classifier combinations, which were the most accurate approach until now.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N03-1028.txt | Citing Article:  W10-4113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the field of English text chunking (Sha and Pereira, 2003), the step 1, 3, and 4 have been studied sufficiently, whereas the step 2, how to select optimal feature template subset efficiently, will be the main topic of this paper.</S> | Reference Offset:  ['12','45'] | Reference Text:  <S sid = 12 ssid = >Most previous work used two main machine-learning approaches to sequence labeling.</S><S sid = 45 ssid = >)F(Y , x) can be computed efficiently using a variant of the forward-backward algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N03-1028.txt | Citing Article:  P05-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['60','163'] | Reference Text:  <S sid = 60 ssid = >In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian.</S><S sid = 163 ssid = >Taku Kudo provided the output of his SVM chunker for the significance test.</S> | Discourse Facet:  NA | Annotator: Automatic


