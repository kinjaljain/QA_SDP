Citance Number: 1 | Reference Article:  N07-1018.txt | Citing Article:  P07-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, as noted by Johnson et al (2007), this choice of beta leads to difficulties with MAP estimation.</S> | Reference Offset:  ['105','202'] | Reference Text:  <S sid = 105 ssid = >This algorithm appears fairly widely known: it was de scribed by Goodman (1998) and Finkel et al(2006)and used by Ding et al(2005), and is very simi lar to other dynamic programming algorithms forCFGs, so we only summarize it here.</S><S sid = 202 ssid = >This ability to find linguistically meaningful structure is relatively rare in our experience with unsupervised PCFG induction.We also experimented with a version of IO modi fied to perform Bayesian MAP estimation, where the Maximization step of the IO procedure is replaced with Bayesian inference using a Dirichlet prior, i.e.,where the rule probabilities ?(k) at iteration k are es timated using: ?(k)r ? max(0,E[fr|w, ?(k?1)] + ??</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N07-1018.txt | Citing Article:  P14-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Adaptor grammars are a framework for Bayesian inference of a certain class of hierarchical nonparametric models (Johnson et al, 2007b).</S> | Reference Offset:  ['0','45'] | Reference Text:  <S sid = 0 ssid = >Bayesian Inference for PCFGs via Markov Chain Monte Carlo</S><S sid = 45 ssid = >2.2 Bayesian inference for PCFGs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N07-1018.txt | Citing Article:  P14-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Adaptor Grammars are formally defined in Johnson et al (2007b), which should be consulted for technical details.</S> | Reference Offset:  ['105','133'] | Reference Text:  <S sid = 105 ssid = >This algorithm appears fairly widely known: it was de scribed by Goodman (1998) and Finkel et al(2006)and used by Ding et al(2005), and is very simi lar to other dynamic programming algorithms forCFGs, so we only summarize it here.</S><S sid = 133 ssid = >PG(t|?)PD(?|?)d? = ? A?N C(?A + fA(t)) C(?A) (3) where C was defined in Equation 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N07-1018.txt | Citing Article:  P14-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings: Johnson et al (2007b) describe a MCMC sampler and Cohen et al (2010) describe a Variational Bayes procedure.</S> | Reference Offset:  ['18','46'] | Reference Text:  <S sid = 18 ssid = >This paper presents two new MCMC algorithms for inferring the posterior distribution over parses and rule probabilities given a corpus of strings.</S><S sid = 46 ssid = >Given a corpus of strings w = (w1, . . .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N07-1018.txt | Citing Article:  P10-2057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al, 2007) as well as for ASR.</S> | Reference Offset:  ['75','105'] | Reference Text:  <S sid = 75 ssid = >Nonetheless, it is possible to define al gorithms that sample from this distribution using Markov chain Monte Carlo (MCMC).</S><S sid = 105 ssid = >This algorithm appears fairly widely known: it was de scribed by Goodman (1998) and Finkel et al(2006)and used by Ding et al(2005), and is very simi lar to other dynamic programming algorithms forCFGs, so we only summarize it here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N07-1018.txt | Citing Article:  N09-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our model is similar to the Adaptor Grammar model of Johnson et al (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar.</S> | Reference Offset:  ['21','194'] | Reference Text:  <S sid = 21 ssid = >the probabilistic model, in tegrating over the rule probabilities of the PCFG,with the goal of speeding convergence.</S><S sid = 194 ssid = >(In fact, in this grammar P(wi|?)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N07-1018.txt | Citing Article:  N09-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our model is similar in this way to the Adaptor Grammar model of Johnson et al (2007a).</S> | Reference Offset:  ['21','194'] | Reference Text:  <S sid = 21 ssid = >the probabilistic model, in tegrating over the rule probabilities of the PCFG,with the goal of speeding convergence.</S><S sid = 194 ssid = >(In fact, in this grammar P(wi|?)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N07-1018.txt | Citing Article:  N09-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given a sample, we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al, 2007a) coupled with a Monte Carlo integral (Bod, 2003).</S> | Reference Offset:  ['72','75'] | Reference Text:  <S sid = 72 ssid = >2.4 Markov chain Monte Carlo.</S><S sid = 75 ssid = >Nonetheless, it is possible to define al gorithms that sample from this distribution using Markov chain Monte Carlo (MCMC).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N07-1018.txt | Citing Article:  N09-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A natural proposal distribution, p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG approximation used in Johnson et al (2007a)).</S> | Reference Offset:  ['66','144'] | Reference Text:  <S sid = 66 ssid = >This means that the posterior distribution on ? given a set of parse trees, P(?|t, ?), is also a Dirichlet distribution.</S><S sid = 144 ssid = >Our proposal distribution is the posterior distribution over parse trees generated bythe PCFG with grammar G and production proba bilities ??, where ??</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N07-1018.txt | Citing Article:  W09-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a).</S> | Reference Offset:  ['105','108'] | Reference Text:  <S sid = 105 ssid = >This algorithm appears fairly widely known: it was de scribed by Goodman (1998) and Finkel et al(2006)and used by Ding et al(2005), and is very simi lar to other dynamic programming algorithms forCFGs, so we only summarize it here.</S><S sid = 108 ssid = >table or chart, as used in the Inside-Outside algorithm for PCFGs (Lari and Young, 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N07-1018.txt | Citing Article:  N09-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al (2007b) for a formal definition of adaptor grammars.</S> | Reference Offset:  ['27','32'] | Reference Text:  <S sid = 27 ssid = >The next section introduces the background for our paper, summarizing the key ideas behind PCFGs,Bayesian inference, and MCMC.</S><S sid = 32 ssid = >2.1 Probabilistic context-free grammars.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N07-1018.txt | Citing Article:  N09-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al (2007b).</S> | Reference Offset:  ['87','132'] | Reference Text:  <S sid = 87 ssid = >In our case, this means alternating between sampling from two distributions: P(t|?,w, ?) = n ? i=1 P(ti|wi, ?), and P(?|t,w, ?) = PD(?|f(t) + ?) = ? A?N PD(?A|fA(t) + ?A).</S><S sid = 132 ssid = >Then by inte grating over the posterior Dirichlet distributions we have: P(t|?)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N07-1018.txt | Citing Article:  N09-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johnson et al (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities, and these techniques can be used directly with adaptor grammars as well.</S> | Reference Offset:  ['45','162'] | Reference Text:  <S sid = 45 ssid = >2.2 Bayesian inference for PCFGs.</S><S sid = 162 ssid = >Bayesian inference using MCMC is a technique of generic utility, much like Expectation-Maximizationand other general inference techniques, and we be lieve that it belongs in every computational linguist?s toolbox alongside these other techniques.Inferring a PCFG to describe the syntactic structure of a natural language is an obvi ous application of grammar inference techniques,and it is well-known that PCFG inference us ing maximum-likelihood techniques such as theInside-Outside (IO) algorithm, a dynamic program ming Expectation-Maximization (EM) algorithm for PCFGs, performs extremely poorly on such tasks.We have applied the Bayesian MCMC methods de scribed here to such problems and obtain resultsvery similar to those produced using IO.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N07-1018.txt | Citing Article:  N09-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['67','215'] | Reference Text:  <S sid = 67 ssid = >Applying Bayes?</S><S sid = 215 ssid = >We attribute this to the ability of the Bayesian prior to prefer sparse grammars.We expect that these algorithms will be of inter est to the computational linguistics community both because a Bayesian approach to PCFG estimation ismore flexible than the Maximum Likelihood meth ods that currently dominate the field (c.f., the use of a prior as a bias towards sparse solutions), and because these techniques provide essential building blocks for more complex models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N07-1018.txt | Citing Article:  N09-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The adaptor grammar algorithm described in Johnson et al (2007b) repeatedly resamples parses for the sentences of the training data.</S> | Reference Offset:  ['148','208'] | Reference Text:  <S sid = 148 ssid = >from t?i as described below.</S><S sid = 208 ssid = >Thus on the next iteration ?r = 0, result ing in there being no parse whatsoever for many of the strings in the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N07-1018.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting.</S> | Reference Offset:  ['40','88'] | Reference Text:  <S sid = 40 ssid = >R is used in the derivation of t. If G does not generate t let PG(t|?)</S><S sid = 88 ssid = >Thus every two steps we generate a new sample oft and ?.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N07-1018.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This work was inspired by adaptor grammars (Johnson et al, 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree.</S> | Reference Offset:  ['116','193'] | Reference Text:  <S sid = 116 ssid = >= pS,0,n. The second step of the sampling algorithm uses the function SAMPLE, which returns a sample from PG(t|w, ?) given the PCFG (G, ?) and the insidetable pA,i,k. SAMPLE takes as arguments a non terminal A ? N and a pair of string positions 0 ? i < k ? n and returns a tree drawn from PGA(t|wi,k, ?).</S><S sid = 193 ssid = >V = 1, and every string w ? w is analysed as a single morpheme V.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N07-1018.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al, 2007b).</S> | Reference Offset:  ['45','94'] | Reference Text:  <S sid = 45 ssid = >2.2 Bayesian inference for PCFGs.</S><S sid = 94 ssid = >the Expectation step replaced by sampling t and the Maximization step replaced by sampling ?.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N07-1018.txt | Citing Article:  P09-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the monolingual setting, there is a well known tree sampling algorithm (Johnson et al,2007).</S> | Reference Offset:  ['103','105'] | Reference Text:  <S sid = 103 ssid = >,m and then setting ?j = xj/ ?m k=1 xk (Gentle, 2003).</S><S sid = 105 ssid = >This algorithm appears fairly widely known: it was de scribed by Goodman (1998) and Finkel et al(2006)and used by Ding et al(2005), and is very simi lar to other dynamic programming algorithms forCFGs, so we only summarize it here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N07-1018.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large.</S> | Reference Offset:  ['39','185'] | Reference Text:  <S sid = 39 ssid = >= ? r?R ?fr(t)r where t is generated by G and fr(t) is the number of times the production r = A ? ?</S><S sid = 185 ssid = >We experimented with a number of tech niques for speeding convergence of both the IO andHastings algorithms, and two of these were particularly effective on this problem.</S> | Discourse Facet:  NA | Annotator: Automatic


