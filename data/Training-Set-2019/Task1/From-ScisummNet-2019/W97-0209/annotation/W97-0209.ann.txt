Citance Number: 1 | Reference Article:  W97-0209.txt | Citing Article:  W98-0703.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997).</S> | Reference Offset:  ['88','92'] | Reference Text:  <S sid = 88 ssid = >In other work, Yarowsky (1993) has shown that local collocational information, including selectional constraints, can be used to great effect in sense disambiguation, though his algorithm requires supervised training.</S><S sid = 92 ssid = >More important is information beyond selectional preference, notably the wider context utilized by Yarowsky (1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W97-0209.txt | Citing Article:  E12-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al 2011), and semantic role labeling (Gildea and Jurafsky, 2002).</S> | Reference Offset:  ['0','55'] | Reference Text:  <S sid = 0 ssid = >Selectional Preference And Sense Disambiguation</S><S sid = 55 ssid = >Following Miller et al. (1994), disambiguation by random choice was used as a baseline: if a noun has one sense, use it; otherwise select at random among its senses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W97-0209.txt | Citing Article:  C00-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >I followed (Resnik, 1993)/ (Resnik, 1997) who defined selectional preference as the amount of information a verb provides about its semantic argument classes.</S> | Reference Offset:  ['19','23'] | Reference Text:  <S sid = 19 ssid = >The model defines the selectional preference strength of a predicate as: Intuitively, SR(p) measures how much information, in bits, predicate p provides about the conceptual class of its argument.</S><S sid = 23 ssid = >Formally, selectional association is defined as: This model of selectional preference has turned out to make reasonable predictions about human judgments of argument plausibility obtained by psycholinguistic methods (Resnik, 1993a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W97-0209.txt | Citing Article:  W03-1903.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997).</S> | Reference Offset:  ['3','38'] | Reference Text:  <S sid = 3 ssid = >It has long been observed that selectional constraints and word sense disambiguation are closely linked.</S><S sid = 38 ssid = >Thus, despite the absence of class annotation in the training text, it is still possible to arrive at a usable estimate of class-based probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W97-0209.txt | Citing Article:  P05-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Resnik (1997) described a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes.</S> | Reference Offset:  ['12','27'] | Reference Text:  <S sid = 12 ssid = >The basis of the approach is a probabilistic model capturing the co-occurrence behavior of predicates and conceptual classes in the taxonomy.</S><S sid = 27 ssid = >But since text corpora contain words, not classes, it is necessary to treat each occurrence of a word in an argument position as if it might represent any of the conceptual classes to which it belongs, and assign frequency counts accordingly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W97-0209.txt | Citing Article:  W05-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','96'] | Reference Text:  <S sid = 47 ssid = >For i from 1 to k, compute: C1 = fele is an ancestor of s} ai = max AR(p, c) cEc, and assign ai as the score for sense si.</S><S sid = 96 ssid = >Much of this work was conducted at Sun Microsystems Laboratories in Chelmsford, Massachusetts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W97-0209.txt | Citing Article:  W05-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These promising results enable a number of future researches: (1) larger scale experiments with different measures and semantic similarity models (e.g. (Resnik, 1997)); (2) improvement of the overall efficiency by exploring feature selection methods over the SK, and (3) the extension of the semantic similarity by a general (i.e. non binary) application of the conceptual density model.</S> | Reference Offset:  ['19','56'] | Reference Text:  <S sid = 19 ssid = >The model defines the selectional preference strength of a predicate as: Intuitively, SR(p) measures how much information, in bits, predicate p provides about the conceptual class of its argument.</S><S sid = 56 ssid = >Results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W97-0209.txt | Citing Article:  W07-0103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Techniques for automatically detecting selections preferences have been discussed in (McCarthy and Carrol, 2003) and (Resnik, 1997).</S> | Reference Offset:  ['11','61'] | Reference Text:  <S sid = 11 ssid = >The treatment of selectional preference used here is that proposed by Resnik (1993a; 1996), combining statistical and knowledge-based methods.</S><S sid = 61 ssid = >The results of the experiment show that disambiguation using automatically acquired selectional constraints leads to performance significantly better than random choice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W97-0209.txt | Citing Article:  P10-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al, 2008), textual inference (Pantel et al, 2007), word-sense disambiguation (Resnik,1997), and many more.</S> | Reference Offset:  ['63','81'] | Reference Text:  <S sid = 63 ssid = >In that respect, the most direct point of comparison is the performance of Miller et al. 's (1994) frequency heuristic — always choose the most frequent sense of a word — as evaluated using the full sense-tagged corpus, including nouns, verbs, adjectives, and adverbs.</S><S sid = 81 ssid = >As in the experiments by Cowie et al., the choice of coarser distinctions presumably accounts in part for the high accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W97-0209.txt | Citing Article:  N10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopted the association measure proposed by Resnik (1993) and successfully applied to a number of tasks in NLP including word sense disambiguation (Resnik, 1997).</S> | Reference Offset:  ['11','88'] | Reference Text:  <S sid = 11 ssid = >The treatment of selectional preference used here is that proposed by Resnik (1993a; 1996), combining statistical and knowledge-based methods.</S><S sid = 88 ssid = >In other work, Yarowsky (1993) has shown that local collocational information, including selectional constraints, can be used to great effect in sense disambiguation, though his algorithm requires supervised training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W97-0209.txt | Citing Article:  D10-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules.</S> | Reference Offset:  ['50','92'] | Reference Text:  <S sid = 50 ssid = >Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn Treebank project (Marcus et al., 1993) and parts of which have been manually sense-tagged by the WordNet group (Miller et al., 1993).</S><S sid = 92 ssid = >More important is information beyond selectional preference, notably the wider context utilized by Yarowsky (1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W97-0209.txt | Citing Article:  E09-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007).</S> | Reference Offset:  ['28','30'] | Reference Text:  <S sid = 28 ssid = >At present, this is done by distributing the &quot;credit&quot; for an observation uniformly across all the conceptual classes containing an observed argument.</S><S sid = 30 ssid = >Given the frequencies, probabilities are currently estimated using maximum likelihood; the use of word classes is itself a form of smoothing (cf.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W97-0209.txt | Citing Article:  W99-0901.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the training sets, test sets, and evaluation method described in (Resnik, 1997).</S> | Reference Offset:  ['10','53'] | Reference Text:  <S sid = 10 ssid = >In marked contrast to annotated training material for partof-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas part-of-speech tag sets tend to differ in the details); (b) sense annotation has a comparatively high error rate (Miller, personal communication, reports an upper bound for human annotators of around 90% for ambiguous cases, using a non-blind evaluation method that may make even this estimate overly optimistic); and (c) no fully automatic method provides high enough quality output to support the &quot;annotate automatically, correct manually&quot; methodology used to provide high volume annotation by data providers like the Penn 'Treebank project (Marcus et al., 1993).</S><S sid = 53 ssid = >The 100 verbs that select most strongly for their objects were identified, excluding verbs appearing only once in the training corpus; test instances of the form (verb, object, correct sense) were then extracted from the merged test corpus, including all triples where verb was one of the 100 test verbs.4 Evaluation materials were obtained in the same manner for several other surface syntactic relationships, including verb-subject (John admires), adjective-noun (tall building), modifier-head (river bank), and head-modifier (river z bank).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W97-0209.txt | Citing Article:  C04-1162.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997).</S> | Reference Offset:  ['29','61'] | Reference Text:  <S sid = 29 ssid = >Formally, given a predicate-argument relationship R (for example, the verb-object relationship), a predicate p, and a conceptual class c, where countR(p, w) is the number of times word w was observed as the argument of p with respect to R, and classes(w) is the number of taxonomic classes to which w belongs.</S><S sid = 61 ssid = >The results of the experiment show that disambiguation using automatically acquired selectional constraints leads to performance significantly better than random choice.</S> | Discourse Facet:  NA | Annotator: Automatic


