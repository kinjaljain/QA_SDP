Citance Number: 1 | Reference Article:  D08-1027.txt | Citing Article:  P09-2078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008).</S> | Reference Offset:  ['11','47'] | Reference Text:  <S sid = 11 ssid = >In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determine whether nonexpert labelers can provide reliable natural language annotations.</S><S sid = 47 ssid = >We have published the full experimental design and the data we have collected for each task online3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1027.txt | Citing Article:  N10-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort.</S> | Reference Offset:  ['7','113'] | Reference Text:  <S sid = 7 ssid = >We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.</S><S sid = 113 ssid = >More workers can be used, as described in previous sections.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1027.txt | Citing Article:  W10-0724.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations.</S> | Reference Offset:  ['161','162'] | Reference Text:  <S sid = 161 ssid = >Our evaluation of non-expert labeler data vs. expert annotations for five tasks found that for many tasks only a small number of nonexpert annotations per item are necessary to equal the performance of an expert annotator.</S><S sid = 162 ssid = >In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1027.txt | Citing Article:  N10-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008).</S> | Reference Offset:  ['34','35'] | Reference Text:  <S sid = 34 ssid = >In this section we describe Amazon Mechanical Turk and the general design of our experiments.</S><S sid = 35 ssid = >We employ the Amazon Mechanical Turk system in order to elicit annotations from non-expert labelers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1027.txt | Citing Article:  W10-3504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','165'] | Reference Text:  <S sid = 61 ssid = >We compute these ITA scores for each emotion task separately, averaging the six emotion tasks as “Avg.</S><S sid = 165 ssid = >This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1027.txt | Citing Article:  W11-0409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories.</S> | Reference Offset:  ['13','137'] | Reference Text:  <S sid = 13 ssid = >The tasks are: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.</S><S sid = 137 ssid = >(The other categorical task, word sense disambiguation, could not be improved because it already had maximum accuracy.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1027.txt | Citing Article:  N09-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels.</S> | Reference Offset:  ['4','111'] | Reference Text:  <S sid = 4 ssid = >For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers.</S><S sid = 111 ssid = >Both the overall variability, as well as the prospect of identifying high-volume but low-quality workers, suggest that controlling for individual worker quality could yield higher quality overall judgments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1027.txt | Citing Article:  S10-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion.</S> | Reference Offset:  ['8','119'] | Reference Text:  <S sid = 8 ssid = >Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms.</S><S sid = 119 ssid = >Wiebe et al. (1999) analyze linguistic annotator agreement statistics to find bias, and use a similar model to correct labels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1027.txt | Citing Article:  W10-0718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','165'] | Reference Text:  <S sid = 61 ssid = >We compute these ITA scores for each emotion task separately, averaging the six emotion tasks as “Avg.</S><S sid = 165 ssid = >This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1027.txt | Citing Article:  W10-0718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008).</S> | Reference Offset:  ['58','99'] | Reference Text:  <S sid = 58 ssid = >For each expert labeler, we computed this ITA score of the expert against the other five; we then average these ITA scores across all expert annotators to compute the average expert ITA (reported in Table 1 as “E vs. E”.</S><S sid = 99 ssid = >While an expert ITA of 0.77 was reported for the more general task involving all fourteen labels on both noun and verb events, no expert ITA numbers have been reported for this simplified temporal ordering task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1027.txt | Citing Article:  W10-0725.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008).</S> | Reference Offset:  ['35','160'] | Reference Text:  <S sid = 35 ssid = >We employ the Amazon Mechanical Turk system in order to elicit annotations from non-expert labelers.</S><S sid = 160 ssid = >We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1027.txt | Citing Article:  W10-0712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008).</S> | Reference Offset:  ['4','35'] | Reference Text:  <S sid = 4 ssid = >For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers.</S><S sid = 35 ssid = >We employ the Amazon Mechanical Turk system in order to elicit annotations from non-expert labelers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1027.txt | Citing Article:  W10-0712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry.</S> | Reference Offset:  ['8','14'] | Reference Text:  <S sid = 8 ssid = >Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms.</S><S sid = 14 ssid = >For each task, we used AMT to annotate data and measured the quality of the annotations by comparing them with the gold standard (expert) labels on the same data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1027.txt | Citing Article:  N12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments.</S> | Reference Offset:  ['14','56'] | Reference Text:  <S sid = 14 ssid = >For each task, we used AMT to annotate data and measured the quality of the annotations by comparing them with the gold standard (expert) labels on the same data.</S><S sid = 56 ssid = >We did this by comparing the interannotator agreement (ITA) of individual expert annotations to that of single non-expert and averaged non-expert annotations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1027.txt | Citing Article:  N12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008).</S> | Reference Offset:  ['5','154'] | Reference Text:  <S sid = 5 ssid = >For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.</S><S sid = 154 ssid = >For each expert annotator we train a system using only the judgments provided by that annotator, and then create a gold standard test set using the average of the responses of the remaining five labelers on that set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1027.txt | Citing Article:  W11-0702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008).</S> | Reference Offset:  ['15','103'] | Reference Text:  <S sid = 15 ssid = >Further, we compare machine learning classifiers trained on expert annotations vs. non-expert annotations.</S><S sid = 103 ssid = >This is a specific example where non-expert annotations can be used to correct expert annotations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1027.txt | Citing Article:  W10-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009).</S> | Reference Offset:  ['50','61'] | Reference Text:  <S sid = 50 ssid = >We analyze the quality of non-expert annotations on five tasks: affect recognition, word similarity, recognizing textual entailment, temporal event recognition, and word sense disambiguation.</S><S sid = 61 ssid = >We compute these ITA scores for each emotion task separately, averaging the six emotion tasks as “Avg.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1027.txt | Citing Article:  W10-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task.</S> | Reference Offset:  ['43','122'] | Reference Text:  <S sid = 43 ssid = >Annotators (variously referred to as Workers or Turkers) may then annotate the tasks of their choosing.</S><S sid = 122 ssid = >Here we take an approach based on gold standard labels, using a small amount of expert-labeled training data in order to correct for the individual biases of different non-expert annotators.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1027.txt | Citing Article:  P13-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge.</S> | Reference Offset:  ['23','85'] | Reference Text:  <S sid = 23 ssid = >Zaenen (Submitted) studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4).</S><S sid = 85 ssid = >This task replicates the recognizing textual entailment task originally proposed in the PASCAL Recognizing Textual Entailment task (Dagan et al., 2006); here for each question the annotator is presented with two sentences and given a binary choice of whether the second hypothesis sentence can be inferred from the first.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1027.txt | Citing Article:  P13-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%.</S> | Reference Offset:  ['101','147'] | Reference Text:  <S sid = 101 ssid = >As shown in Figure 5, performing simple majority voting (with random tie-breaking) over annotators results in a rapid accuracy plateau at a very high rate of 0.994 accuracy.</S><S sid = 147 ssid = >The lowest line is for the naive 50% majority voting rule.</S> | Discourse Facet:  NA | Annotator: Automatic


