Citance Number: 1 | Reference Article:  W03-1011.txt | Citing Article:  C04-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['39','108'] | Reference Text:  <S sid = 39 ssid = >Accordingly, verb v will be considered to be a feature of noun n if the probability of their cooccurrence is greater than would be expected if verbs and nouns occurred independently.</S><S sid = 108 ssid = >We would like to thank John Carroll for the use of his parser, Adam Kilgarriff and Bill Keller for valuable discussions and the UK EPSRC for its studentship to the first author.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-1011.txt | Citing Article:  C04-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement.</S> | Reference Offset:  ['13','54'] | Reference Text:  <S sid = 13 ssid = >Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.</S><S sid = 54 ssid = >Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-1011.txt | Citing Article:  D12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['39','108'] | Reference Text:  <S sid = 39 ssid = >Accordingly, verb v will be considered to be a feature of noun n if the probability of their cooccurrence is greater than would be expected if verbs and nouns occurred independently.</S><S sid = 108 ssid = >We would like to thank John Carroll for the use of his parser, Adam Kilgarriff and Bill Keller for valuable discussions and the UK EPSRC for its studentship to the first author.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-1011.txt | Citing Article:  C04-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003).</S> | Reference Offset:  ['12','13'] | Reference Text:  <S sid = 12 ssid = >Further, there is no clear way of deciding which is the best measure.</S><S sid = 13 ssid = >Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-1011.txt | Citing Article:  P11-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['39','108'] | Reference Text:  <S sid = 39 ssid = >Accordingly, verb v will be considered to be a feature of noun n if the probability of their cooccurrence is greater than would be expected if verbs and nouns occurred independently.</S><S sid = 108 ssid = >We would like to thank John Carroll for the use of his parser, Adam Kilgarriff and Bill Keller for valuable discussions and the UK EPSRC for its studentship to the first author.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-1011.txt | Citing Article:  D09-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall.</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >A General Framework For Distributional Similarity</S><S sid = 13 ssid = >Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-1011.txt | Citing Article:  P09-2018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN (u, v)=? f? FV u? FV v [w u (f)+ w v (f)]? f? FV u w u (f)+? f? FV v w v (f) where FV x is the feature vector of a word x and w x (f) is the weight of the feature f in that word? s vector, set to their point wise mutual information.</S> | Reference Offset:  ['9','13'] | Reference Text:  <S sid = 9 ssid = >Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity.</S><S sid = 13 ssid = >Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-1011.txt | Citing Article:  P09-2018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['39','108'] | Reference Text:  <S sid = 39 ssid = >Accordingly, verb v will be considered to be a feature of noun n if the probability of their cooccurrence is greater than would be expected if verbs and nouns occurred independently.</S><S sid = 108 ssid = >We would like to thank John Carroll for the use of his parser, Adam Kilgarriff and Bill Keller for valuable discussions and the UK EPSRC for its studentship to the first author.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-1011.txt | Citing Article:  P04-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb.</S> | Reference Offset:  ['13','54'] | Reference Text:  <S sid = 13 ssid = >Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.</S><S sid = 54 ssid = >Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-1011.txt | Citing Article:  P12-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel,2001), Cover (Weeds and Weir, 2003), BInc (Szpek tor and Dagan, 2008) and Berant et al (2010).</S> | Reference Offset:  ['13','54'] | Reference Text:  <S sid = 13 ssid = >Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.</S><S sid = 54 ssid = >Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


