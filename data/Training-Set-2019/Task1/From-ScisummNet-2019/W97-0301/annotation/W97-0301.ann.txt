Citance Number: 1 | Reference Article:  W97-0301.txt | Citing Article:  W12-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this section, we investigate the performance of two maximum entropy classifiers (Ratnaparkhi, 1997), one for determining whether a noun phrase has a determiner or not and the other for selecting the appropriate determiner if one is needed.</S> | Reference Offset:  ['72','96'] | Reference Text:  <S sid = 72 ssid = >The GIS procedure, as well as the maximum entropy and maximum likelihood properties of the distribution of form (1), are described in detail in (Ratnaparkhi, 1997).</S><S sid = 96 ssid = >The performance of this &quot;perfect&quot; scheme is then an upper bound on the performance of any reranking scheme that might be used to reorder the top N parses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W97-0301.txt | Citing Article:  P06-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997).</S> | Reference Offset:  ['93','110'] | Reference Text:  <S sid = 93 ssid = >Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995)2, which have the best previously published parsing accuracies on the Wall St. Journal domain.</S><S sid = 110 ssid = >The SPATTER parser is a history-based parser that uses decision tree models to guide the operations of a few tree building procedures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W97-0301.txt | Citing Article:  W06-2002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The closest relative of our framework is the maximum-entropy parser of Ratnaparkhi (Ratnaparkhi, 1997).</S> | Reference Offset:  ['72','108'] | Reference Text:  <S sid = 72 ssid = >The GIS procedure, as well as the maximum entropy and maximum likelihood properties of the distribution of form (1), are described in detail in (Ratnaparkhi, 1997).</S><S sid = 108 ssid = >Furthermore, the customized estimation framework of the bigram parser must use information that has been carefully selected for its value, whereas the maximum entropy framework ro</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W97-0301.txt | Citing Article:  W11-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Chinese parse trees are produced by a maximum entropy based parser (Ratnaparkhi, 1997).</S> | Reference Offset:  ['0','95'] | Reference Text:  <S sid = 0 ssid = >A Linear Observed Time Statistical Parser Based On Maximum Entropy Models</S><S sid = 95 ssid = >Suppose there exists a &quot;perfect&quot; reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W97-0301.txt | Citing Article:  W05-1506.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We demonstrate this by comparing our k-best lists to those in (Ratnaparkhi,1997), (Collins, 2000) and the parallel work by Charniak and Johnson (2005) in several ways, including oracle reranking and average number of found parses.</S> | Reference Offset:  ['95','117'] | Reference Text:  <S sid = 95 ssid = >Suppose there exists a &quot;perfect&quot; reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.</S><S sid = 117 ssid = >Many thanks to Mike Collins and Professor Mitch Marcus from the University of Pennsylvania for their helpful comments on this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W97-0301.txt | Citing Article:  W05-1506.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ratnaparkhi (1997) introduced the idea of oracle re ranking: suppose there exists a perfect re ranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence.</S> | Reference Offset:  ['9','95'] | Reference Text:  <S sid = 9 ssid = >Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.</S><S sid = 95 ssid = >Suppose there exists a &quot;perfect&quot; reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W97-0301.txt | Citing Article:  W05-1506.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','117'] | Reference Text:  <S sid = 43 ssid = >Otherwise, the constituent is not finished and BUILD processes the next tree in the forest, tn+1.</S><S sid = 117 ssid = >Many thanks to Mike Collins and Professor Mitch Marcus from the University of Pennsylvania for their helpful comments on this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W97-0301.txt | Citing Article:  W03-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested.</S> | Reference Offset:  ['72','103'] | Reference Text:  <S sid = 72 ssid = >The GIS procedure, as well as the maximum entropy and maximum likelihood properties of the distribution of form (1), are described in detail in (Ratnaparkhi, 1997).</S><S sid = 103 ssid = >The maximum entropy parser is a statistical shift-reduce style parser that cannot always access head-modifier pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W97-0301.txt | Citing Article:  W03-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model.</S> | Reference Offset:  ['0','72'] | Reference Text:  <S sid = 0 ssid = >A Linear Observed Time Statistical Parser Based On Maximum Entropy Models</S><S sid = 72 ssid = >The GIS procedure, as well as the maximum entropy and maximum likelihood properties of the distribution of form (1), are described in detail in (Ratnaparkhi, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W97-0301.txt | Citing Article:  W03-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The maximum entropy parser (Ratnaparkhi, 1997) parses a sentence in three phases: (1) it first tags the input sentence.</S> | Reference Offset:  ['83','114'] | Reference Text:  <S sid = 83 ssid = >All three of the passes described in section 2 are integrated in the search, i.e., when parsing a test sentence, the input to the second pass consists of K of the best distinct POS tag assignments for the input sentence.</S><S sid = 114 ssid = >The maximum entropy parser presented here achieves a parsing accuracy which exceeds the best previously published results, and parses a test sentence in linear observed time, with respect to the sentence length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W97-0301.txt | Citing Article:  P02-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The SLM was trained on 20M words of WSJ text automatically parsed using the parser in (Ratnaparkhi, 1997), binarized and enriched with headwords and NT/POS tag information as explained in Section 2.2 and Section 3.</S> | Reference Offset:  ['19','83'] | Reference Text:  <S sid = 19 ssid = >Section 5 describes experiments with the Penn Treebank and section 6 compares this paper with previously published works.</S><S sid = 83 ssid = >All three of the passes described in section 2 are integrated in the search, i.e., when parsing a test sentence, the input to the second pass consists of K of the best distinct POS tag assignments for the input sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W97-0301.txt | Citing Article:  W01-0521.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.</S> | Reference Offset:  ['72','92'] | Reference Text:  <S sid = 72 ssid = >The GIS procedure, as well as the maximum entropy and maximum likelihood properties of the distribution of form (1), are described in detail in (Ratnaparkhi, 1997).</S><S sid = 92 ssid = >Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of (Collins, 1996) and (Magerman, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W97-0301.txt | Citing Article:  W01-0521.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) dier in their details but are based on similar features.</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >A Linear Observed Time Statistical Parser Based On Maximum Entropy Models</S><S sid = 13 ssid = >The models compute the probabilities of actions based on certain syntactic characteristics, or features, of the current context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W97-0301.txt | Citing Article:  P07-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made.</S> | Reference Offset:  ['8','16'] | Reference Text:  <S sid = 8 ssid = >The observed running time of the parser on a test sentence is linear with respect to the sentence length.</S><S sid = 16 ssid = >Also, the search heuristic is very simple, and its observed running time on a test sentence is linear with respect to the sentence length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W97-0301.txt | Citing Article:  P06-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a true pipeline approach, as was done in other successful parsers, e.g. (Ratnaparkhi, 1997), in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the global structure.</S> | Reference Offset:  ['82','101'] | Reference Text:  <S sid = 82 ssid = >It should be emphasized that if K> 1, the parser does not commit to a single POS or chunk assignment for the input sentence before building constituent structure.</S><S sid = 101 ssid = >The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W97-0301.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, our parser is in many ways similar to the parser of Ratnaparkhi (1997).</S> | Reference Offset:  ['53','101'] | Reference Text:  <S sid = 53 ssid = >Finally, those features are combined under the maximum entropy framework, yielding p(a, b).</S><S sid = 101 ssid = >The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W97-0301.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','117'] | Reference Text:  <S sid = 43 ssid = >Otherwise, the constituent is not finished and BUILD processes the next tree in the forest, tn+1.</S><S sid = 117 ssid = >Many thanks to Mike Collins and Professor Mitch Marcus from the University of Pennsylvania for their helpful comments on this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W97-0301.txt | Citing Article:  N03-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the application side, (log) linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997).</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >A Linear Observed Time Statistical Parser Based On Maximum Entropy Models</S><S sid = 13 ssid = >The models compute the probabilities of actions based on certain syntactic characteristics, or features, of the current context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W97-0301.txt | Citing Article:  P10-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997).</S> | Reference Offset:  ['0','95'] | Reference Text:  <S sid = 0 ssid = >A Linear Observed Time Statistical Parser Based On Maximum Entropy Models</S><S sid = 95 ssid = >Suppose there exists a &quot;perfect&quot; reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W97-0301.txt | Citing Article:  P02-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chunks as a separate level have also been used in Collins (1996) and Ratnaparkhi (1997).</S> | Reference Offset:  ['12','93'] | Reference Text:  <S sid = 12 ssid = >The parser consists of the following three conceptually distinct parts: The maximum entropy models used here are similar in form to those in (Ratnaparkhi, 1996; Berger, Della Pietra, and Della Pietra, 1996; Lau, Rosenfeld, and Roukos, 1993).</S><S sid = 93 ssid = >Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995)2, which have the best previously published parsing accuracies on the Wall St. Journal domain.</S> | Discourse Facet:  NA | Annotator: Automatic


