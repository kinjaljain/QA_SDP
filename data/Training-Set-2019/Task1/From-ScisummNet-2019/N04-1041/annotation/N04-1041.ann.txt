Citance Number: 1 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004).</S> | Reference Offset:  ['2','97'] | Reference Text:  <S sid = 2 ssid = >The current state of the art discovers many semantic classes but fails to label their concepts.</S><S sid = 97 ssid = >In this section, we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun.</S> | Reference Offset:  ['59','69'] | Reference Text:  <S sid = 59 ssid = >Ours is a top down approach.</S><S sid = 69 ssid = >Finally, we use simple syntactic patterns to discover class names from each class' signature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC.</S> | Reference Offset:  ['0','60'] | Reference Text:  <S sid = 0 ssid = >Automatically Labeling Semantic Classes</S><S sid = 60 ssid = >We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships.</S> | Reference Offset:  ['93','179'] | Reference Text:  <S sid = 93 ssid = >The top-4 highest scoring relationships are: To name a class, we simply search for these syntactic relationships in the signature of a concept.</S><S sid = 179 ssid = >Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004).</S> | Reference Offset:  ['38','59'] | Reference Text:  <S sid = 38 ssid = >One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).</S><S sid = 59 ssid = >Ours is a top down approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1041.txt | Citing Article:  P10-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations.</S> | Reference Offset:  ['65','99'] | Reference Text:  <S sid = 65 ssid = >In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S><S sid = 99 ssid = >We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1041.txt | Citing Article:  P11-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class.</S> | Reference Offset:  ['69','172'] | Reference Text:  <S sid = 69 ssid = >Finally, we use simple syntactic patterns to discover class names from each class' signature.</S><S sid = 172 ssid = >Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1041.txt | Citing Article:  W08-1807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work.</S> | Reference Offset:  ['10','65'] | Reference Text:  <S sid = 10 ssid = >Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner.</S><S sid = 65 ssid = >In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1041.txt | Citing Article:  C10-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['57','184'] | Reference Text:  <S sid = 57 ssid = >That is, they use patterns to independently discover semantic relationships of words.</S><S sid = 184 ssid = >This research was partly supported by NSF grant #EIA-0205111.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1041.txt | Citing Article:  E09-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms.</S> | Reference Offset:  ['36','152'] | Reference Text:  <S sid = 36 ssid = >There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.</S><S sid = 152 ssid = >We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1041.txt | Citing Article:  P08-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004).</S> | Reference Offset:  ['111','121'] | Reference Text:  <S sid = 111 ssid = >Because of the low coverage of proper nouns in WordNet, only 33 of the 125 concepts we evaluated had WordNet generated labels.</S><S sid = 121 ssid = >For the 33 concepts that WordNet named, it achieved a score of 75.3% and a lenient score of 82.7%, which is high considering the simple algorithm we used to extract labels using WordNet.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1041.txt | Citing Article:  P06-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture.</S> | Reference Offset:  ['36','152'] | Reference Text:  <S sid = 36 ssid = >There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.</S><S sid = 152 ssid = >We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1041.txt | Citing Article:  P08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes.</S> | Reference Offset:  ['80','109'] | Reference Text:  <S sid = 80 ssid = >The assumption is that the best representative for a concept is a large set of very similar instances.</S><S sid = 109 ssid = >For each concept that contains at least five instances in the WordNet hierarchy, we named the concept with the most frequent common ancestor of each pair of instances.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1041.txt | Citing Article:  P08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text.</S> | Reference Offset:  ['44','172'] | Reference Text:  <S sid = 44 ssid = >Using sets of representative elements called committees, CBC discovers cluster centroids that unambiguously describe the members of a possible class.</S><S sid = 172 ssid = >Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1041.txt | Citing Article:  C10-2110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels.</S> | Reference Offset:  ['69','160'] | Reference Text:  <S sid = 69 ssid = >Finally, we use simple syntactic patterns to discover class names from each class' signature.</S><S sid = 160 ssid = >Given a question such as &quot;What color ...&quot;, the likelihood of a correct answer being present in a retrieved passage is greatly increased if we know the set of all possible colors and index them in the document collection appropriately.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1041.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun.</S> | Reference Offset:  ['59','69'] | Reference Text:  <S sid = 59 ssid = >Ours is a top down approach.</S><S sid = 69 ssid = >Finally, we use simple syntactic patterns to discover class names from each class' signature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1041.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004).</S> | Reference Offset:  ['65','84'] | Reference Text:  <S sid = 65 ssid = >In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S><S sid = 84 ssid = >We therefore multiply mief with the following discounting factor: n m where n is the number of words and N = cef × By averaging the feature vectors of the committee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1041.txt | Citing Article:  P05-2025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster.</S> | Reference Offset:  ['0','64'] | Reference Text:  <S sid = 0 ssid = >Automatically Labeling Semantic Classes</S><S sid = 64 ssid = >The input to our labeling algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1041.txt | Citing Article:  W09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she.</S> | Reference Offset:  ['9','11'] | Reference Text:  <S sid = 9 ssid = >For example, WordNet includes a rare sense of computer that means `the person who computes'.</S><S sid = 11 ssid = >Also, the words dog, computer and company all have a sense that is a hyponym of person.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1041.txt | Citing Article:  C10-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation.</S> | Reference Offset:  ['77','89'] | Reference Text:  <S sid = 77 ssid = >We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: Following (Pantel and Lin 2002), we construct a committee for each semantic class.</S><S sid = 89 ssid = >The two columns of numbers indicate the frequency and mutual information score for each feature respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


