Citance Number: 1 | Reference Article:  W03-0407.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003).</S> | Reference Offset:  ['89','121'] | Reference Text:  <S sid = 89 ssid = >Towards the end of the co-training run, more material is being selected for C&C than TNT.</S><S sid = 121 ssid = >The results are shown in Table 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-0407.txt | Citing Article:  P05-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)).</S> | Reference Offset:  ['8','124'] | Reference Text:  <S sid = 8 ssid = >In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data.</S><S sid = 124 ssid = >Although bootstrapping from unlabelled data is particularly valuable when only small amounts of training material are available, it is also interesting to see if selftraining or co-training can improve state of the art POS taggers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-0407.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance.</S> | Reference Offset:  ['120','131'] | Reference Text:  <S sid = 120 ssid = >Both taggers were initialised with either 500 or 50 seed sentences, and agreement-based co-training was applied, using a cache size of 500 sentences.</S><S sid = 131 ssid = >These results show no significant improvement using either self-training or co-training with very large seed datasets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-0407.txt | Citing Article:  P10-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >Bootstrapping POS-Taggers Using Unlabelled Data</S><S sid = 8 ssid = >In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-0407.txt | Citing Article:  N09-2054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases.</S> | Reference Offset:  ['24','99'] | Reference Text:  <S sid = 24 ssid = >However, our experiments are relevant for languages for which there is little or no annotated data.</S><S sid = 99 ssid = >We see that naive co-training improves as the cache size increases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-0407.txt | Citing Article:  N06-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes.</S> | Reference Offset:  ['37','41'] | Reference Text:  <S sid = 37 ssid = >The ME tagger, which we refer to as C&C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003).</S><S sid = 41 ssid = >TNT is very fast for both training and tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-0407.txt | Citing Article:  C10-2146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003).</S> | Reference Offset:  ['12','117'] | Reference Text:  <S sid = 12 ssid = >There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers.</S><S sid = 117 ssid = >Co-training using this much larger amount of unlabelled material did improve our previously mentioned results, but not by a large margin.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-0407.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training.</S> | Reference Offset:  ['17','22'] | Reference Text:  <S sid = 17 ssid = >Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy.</S><S sid = 22 ssid = >Using standard sections of the WSJ Penn Treebank as seed data, we have been unable to improve the performance of the taggers using selftraining or co-training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-0407.txt | Citing Article:  H05-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001).</S> | Reference Offset:  ['7','145'] | Reference Text:  <S sid = 7 ssid = >Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).</S><S sid = 145 ssid = >The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-0407.txt | Citing Article:  P10-3016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training.</S> | Reference Offset:  ['7','37'] | Reference Text:  <S sid = 7 ssid = >Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).</S><S sid = 37 ssid = >The ME tagger, which we refer to as C&C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W03-0407.txt | Citing Article:  W11-0223.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round.</S> | Reference Offset:  ['85','128'] | Reference Text:  <S sid = 85 ssid = >Figure 3 shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.</S><S sid = 128 ssid = >In each round of self-training or naive co-training 10% of the cache was randomly selected and added to the labelled training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W03-0407.txt | Citing Article:  D10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available.</S> | Reference Offset:  ['7','35'] | Reference Text:  <S sid = 7 ssid = >Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).</S><S sid = 35 ssid = >In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W03-0407.txt | Citing Article:  W11-0412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round.</S> | Reference Offset:  ['85','128'] | Reference Text:  <S sid = 85 ssid = >Figure 3 shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.</S><S sid = 128 ssid = >In each round of self-training or naive co-training 10% of the cache was randomly selected and added to the labelled training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W03-0407.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).</S><S sid = 11 ssid = >We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W03-0407.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost.</S> | Reference Offset:  ['100','137'] | Reference Text:  <S sid = 100 ssid = >For a large cache, the performance levels for naive co-training are very similar to those produced by our agreement-based co-training method.</S><S sid = 137 ssid = >This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W03-0407.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round.</S> | Reference Offset:  ['85','109'] | Reference Text:  <S sid = 85 ssid = >Figure 3 shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.</S><S sid = 109 ssid = >We also performed a number of experiments using much more unlabelled training material than before.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W03-0407.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations.</S> | Reference Offset:  ['32','121'] | Reference Text:  <S sid = 32 ssid = >Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold.</S><S sid = 121 ssid = >The results are shown in Table 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W03-0407.txt | Citing Article:  P09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).</S><S sid = 11 ssid = >We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W03-0407.txt | Citing Article:  E12-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner.</S> | Reference Offset:  ['10','27'] | Reference Text:  <S sid = 10 ssid = >In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data.</S><S sid = 27 ssid = >Given two (or more) “views” (as described in Blum and Mitchell (1998)) of a classification task, co-training can be informally described as follows: The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s).</S> | Discourse Facet:  NA | Annotator: Automatic


