Citance Number: 1 | Reference Article:  P04-1061.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 180 ssid = >This work also benefited from an enormous amount of useful feedback, from many audiences and individuals.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-1061.txt | Citing Article:  P14-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al, 2012), have not addressed filler-gap comprehension.</S> | Reference Offset:  ['0','109'] | Reference Text:  <S sid = 0 ssid = >Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency</S><S sid = 109 ssid = >This dependency induction model is reasonably successful.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-1061.txt | Citing Article:  W09-0103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the field of language acquisition computational linguists such as Klein and Manning (2004) have studied the unsupervised acquisition of syntactic structure, while linguists such as Boersma and Hayes (2001), Gold smith (2001), Pater (2008) and Albright and Hayes (2003) are developing probabilistic models of the acquisition of phonology and/or morphology, and Frank et al (2007) experimentally tests the predictions of a Bayesian model of lexical acquisition.</S> | Reference Offset:  ['6','122'] | Reference Text:  <S sid = 6 ssid = >Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003).</S><S sid = 122 ssid = >Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-1061.txt | Citing Article:  P13-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 180 ssid = >This work also benefited from an enormous amount of useful feedback, from many audiences and individuals.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-1061.txt | Citing Article:  P13-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the standard generative Dependency Model with Valence (Klein and Manning, 2004).</S> | Reference Offset:  ['1','66'] | Reference Text:  <S sid = 1 ssid = >present a generative model for the learning of dependency structures.</S><S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004).</S> | Reference Offset:  ['66','138'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 138 ssid = >For the DMV, it is already a model over these structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009).</S> | Reference Offset:  ['138','147'] | Reference Text:  <S sid = 138 ssid = >For the DMV, it is already a model over these structures.</S><S sid = 147 ssid = >In figure 6, we give the behavior of the CCM constituency model and the DMV dependency model on both constituency and dependency induction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We encode more detailed valence information than Klein and Manning (2004) and condition child generation on parent valence.</S> | Reference Offset:  ['11','66'] | Reference Text:  <S sid = 11 ssid = >First, most state-ofthe-art supervised parsers make use of specific lexical information in addition to word-class level information â€“ perhaps lexical information could be a useful source of information for unsupervised methods.</S><S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The resulting simplified model closely resembles DMV (Klein and Manning, 2004), except that it 1) explicitly generate words x rather than only part of-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution.</S> | Reference Offset:  ['66','138'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 138 ssid = >For the DMV, it is already a model over these structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 180 ssid = >This work also benefited from an enormous amount of useful feedback, from many audiences and individuals.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 180 ssid = >This work also benefited from an enormous amount of useful feedback, from many audiences and individuals.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-1061.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','180'] | Reference Text:  <S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S><S sid = 180 ssid = >This work also benefited from an enormous amount of useful feedback, from many audiences and individuals.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-1061.txt | Citing Article:  P09-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, following (Klein and Manning, 2004) we strip out punctuation from the sentences.</S> | Reference Offset:  ['23','144'] | Reference Text:  <S sid = 23 ssid = >WSJ10 is the subset of sentences which contained 10 words or less after the removal of punctuation.</S><S sid = 144 ssid = >To avoid this bias, we alter the DMV in the following ways.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-1061.txt | Citing Article:  P09-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The maximum unsupervised accuracy it achieved on the Bulgarian data is 47.6% with initialization from Klein and Manning (2004) and this result is not stable.</S> | Reference Offset:  ['10','52'] | Reference Text:  <S sid = 10 ssid = >Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models (Clark, 2001; Klein and Manning, 2002), but there are compelling reasons to reconsider unsupervised dependency parsing.</S><S sid = 52 ssid = >As a result, their learned grammars were of very poor quality and had high variance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P04-1061.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the generative models of section 5, f has the form of a dependency model with valence (Klein and Manning, 2004).</S> | Reference Offset:  ['1','66'] | Reference Text:  <S sid = 1 ssid = >present a generative model for the learning of dependency structures.</S><S sid = 66 ssid = >We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P04-1061.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well.</S> | Reference Offset:  ['34','120'] | Reference Text:  <S sid = 34 ssid = >Dependencies are seen as ordered (head, dependent) pairs of words, but the score of a dependency can optionally condition on other characteristics of the structure, most often the direction of the dependency (whether the arrow points left or right).</S><S sid = 120 ssid = >Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; SchÂ¨utze, 1995; Clark, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P04-1061.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags.</S> | Reference Offset:  ['23','168'] | Reference Text:  <S sid = 23 ssid = >WSJ10 is the subset of sentences which contained 10 words or less after the removal of punctuation.</S><S sid = 168 ssid = >Again, if we modify the gold standard so as to make determiners the head of NPs, then this model with distributional tags scores 50.6% on directed and 64.8% on undirected dependency accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P04-1061.txt | Citing Article:  P13-2017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005).</S> | Reference Offset:  ['0','122'] | Reference Text:  <S sid = 0 ssid = >Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency</S><S sid = 122 ssid = >Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P04-1061.txt | Citing Article:  D07-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step.</S> | Reference Offset:  ['3','10'] | Reference Text:  <S sid = 3 ssid = >The product model outperforms both components on their respective evaluation metrics, giving the best published figures for undependency parsing constituency parsing.</S><S sid = 10 ssid = >Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models (Clark, 2001; Klein and Manning, 2002), but there are compelling reasons to reconsider unsupervised dependency parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P04-1061.txt | Citing Article:  W12-1911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al, 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated.</S> | Reference Offset:  ['78','138'] | Reference Text:  <S sid = 78 ssid = >If a stop is generated, no more arguments are generated for the current head to the current side.</S><S sid = 138 ssid = >For the DMV, it is already a model over these structures.</S> | Discourse Facet:  NA | Annotator: Automatic


