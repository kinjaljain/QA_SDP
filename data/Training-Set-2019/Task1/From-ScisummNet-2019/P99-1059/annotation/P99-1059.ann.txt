Citance Number: 1 | Reference Article:  P99-1059.txt | Citing Article:  W01-1404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999).</S> | Reference Offset:  ['40','58'] | Reference Text:  <S sid = 40 ssid = >Standard context-free parsing algorithms are inefficient in such a case.</S><S sid = 58 ssid = >Among items of the same width, those of type .L should be considered last.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P99-1059.txt | Citing Article:  W01-1404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999).</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars</S><S sid = 18 ssid = >The reader is assumed to be familiar with context-free grammars.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P99-1059.txt | Citing Article:  P14-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999).</S> | Reference Offset:  ['99','101'] | Reference Text:  <S sid = 99 ssid = >We omit the formal proof that G and H admit isomorphic derivations and hence generate the same languages, observing only that if (x, y) = (bib2 • • • bi,b3+1- • • bk) E L(Ha)— a condition used in defining La above—then A[a] [bi] • • • B3[MaB3+1[bi+11 • • • Bk[bk], for any A, B1, .</S><S sid = 101 ssid = >In general, G has p = 0(IVD13) = 0(t3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P99-1059.txt | Citing Article:  P14-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999).</S> | Reference Offset:  ['42','55'] | Reference Text:  <S sid = 42 ssid = >For a bilexical grammar, the worst case is IPI = VD I 3 ' I VT12, which is large for a large vocabulary VT. We may improve the analysis somewhat by observing that when parsing d1 • • • dn, the CKY algorithm only considers nonterminals of the form A[di]; by restricting to the relevant productions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)• We observe that in practical applications we always have n < IVTI• Let us then restrict our analysis to the (infinite) set of input instances of the parsing problem that satisfy relation n < WTI.</S><S sid = 55 ssid = >The algorithm makes a single pass through the possible items, setting the bit for each if it can be derived using any rule in (b) from items whose bits are already set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P99-1059.txt | Citing Article:  P09-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m).</S> | Reference Offset:  ['15','106'] | Reference Text:  <S sid = 15 ssid = >But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S><S sid = 106 ssid = >8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P99-1059.txt | Citing Article:  P09-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al.</S> | Reference Offset:  ['43','106'] | Reference Text:  <S sid = 43 ssid = >With this assumption, the asymptotic time complexity of the CKY algorithm becomes 0(n5 • IVD13).</S><S sid = 106 ssid = >8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P99-1059.txt | Citing Article:  P13-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable.</S> | Reference Offset:  ['17','51'] | Reference Text:  <S sid = 17 ssid = >We use dynamic programming to assemble such subderivations into a full parse.</S><S sid = 51 ssid = >This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P99-1059.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity.</S> | Reference Offset:  ['110','120'] | Reference Text:  <S sid = 110 ssid = >Formally, a flip state is one that allows entry on a —> transition and that either allows exit on a transition or is a final state.</S><S sid = 120 ssid = >A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P99-1059.txt | Citing Article:  D08-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.</S> | Reference Offset:  ['40','103'] | Reference Text:  <S sid = 40 ssid = >Standard context-free parsing algorithms are inefficient in such a case.</S><S sid = 103 ssid = >If the HAs in H happen to be deterministic, then in each binary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = 0(t2), so the parser will operate in time 0(n4t2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999).</S> | Reference Offset:  ['111','120'] | Reference Text:  <S sid = 111 ssid = >We are concerned here with head automaton grammars H such that every Ha is split.</S><S sid = 120 ssid = >A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On this point, see (Eisner and Satta, 1999, and footnote 6).</S> | Reference Offset:  ['15','131'] | Reference Text:  <S sid = 15 ssid = >But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S><S sid = 131 ssid = >For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies.</S> | Reference Offset:  ['29','123'] | Reference Text:  <S sid = 29 ssid = >We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT.</S><S sid = 123 ssid = >For a practical speedup, add h\j as an antecedent to the MID rule (and fill in the parse table from right to left).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999).</S> | Reference Offset:  ['0','83'] | Reference Text:  <S sid = 0 ssid = >Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars</S><S sid = 83 ssid = >7 Head automaton grammars in time 0(n4) In this section we show that a length-n string generated by a head automaton grammar (Alshawi, 1996) can be parsed in time 0(n4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P99-1059.txt | Citing Article:  E09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003).</S> | Reference Offset:  ['1','12'] | Reference Text:  <S sid = 1 ssid = >stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.</S><S sid = 12 ssid = >Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P99-1059.txt | Citing Article:  N06-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider.</S> | Reference Offset:  ['15','40'] | Reference Text:  <S sid = 15 ssid = >But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S><S sid = 40 ssid = >Standard context-free parsing algorithms are inefficient in such a case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P99-1059.txt | Citing Article:  H05-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models.</S> | Reference Offset:  ['2','135'] | Reference Text:  <S sid = 2 ssid = >We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.</S><S sid = 135 ssid = >Our dynamic programming techniques for cheaply attaching head information to derivations can also be exploited in parsing formalisms other than rewriting systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P99-1059.txt | Citing Article:  N09-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999).</S> | Reference Offset:  ['84','131'] | Reference Text:  <S sid = 84 ssid = >We do this by providing a translation from head automaton grammars to bilexical CFGs.4 This result improves on the head-automaton parsing algorithm given by Alshawi, which is analogous to the CKY algorithm on bilexical CFGs and is likewise 0(n5) in practice (see §3).</S><S sid = 131 ssid = >For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P99-1059.txt | Citing Article:  D08-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999).</S> | Reference Offset:  ['111','120'] | Reference Text:  <S sid = 111 ssid = >We are concerned here with head automaton grammars H such that every Ha is split.</S><S sid = 120 ssid = >A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P99-1059.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states.</S> | Reference Offset:  ['17','51'] | Reference Text:  <S sid = 17 ssid = >We use dynamic programming to assemble such subderivations into a full parse.</S><S sid = 51 ssid = >This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P99-1059.txt | Citing Article:  W06-2929.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999).</S> | Reference Offset:  ['2','15'] | Reference Text:  <S sid = 2 ssid = >We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.</S><S sid = 15 ssid = >But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S> | Discourse Facet:  NA | Annotator: Automatic


