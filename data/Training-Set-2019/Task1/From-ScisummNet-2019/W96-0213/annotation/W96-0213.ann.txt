Citance Number: 1 | Reference Article:  W96-0213.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the raw Penn Treebank data contains many inconsistencies in its annotations (cf. Ratnaparkhi, 1996), a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence.</S> | Reference Offset:  ['38','49'] | Reference Text:  <S sid = 38 ssid = >The test corpus is tagged one sentence at a time.</S><S sid = 49 ssid = >The feature set and search algorithm were tested and debugged only on the Training and Development sets, and the official test result on the unseen Test set is presented in the conclusion of the paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W96-0213.txt | Citing Article:  P14-2131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For both tree banks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996).</S> | Reference Offset:  ['20','65'] | Reference Text:  <S sid = 20 ssid = >The parameters {p, ai , • .. , } are then chosen to maximize the likelihood of the training data using p: This model also can be interpreted under the Maximum Entropy formalism, in which the goal is to maximize the entropy of a distribution subject to certain constraints.</S><S sid = 65 ssid = >Using the set of 29 difficult words, the model performs at 96.49% accuracy on the Development Set, an insignificant improvement from the baseline accuracy of 96.43%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We started with a maximum entropy based tagger that uses features very similar to the ones proposed in Ratnaparkhi (1996).</S> | Reference Offset:  ['0','81'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Model For Part-Of-Speech Tagging</S><S sid = 81 ssid = >The Maximum Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ratnaparkhi (1996: 134) suggests use of an approximation summing over the training data, which does not sum over possible tags:& quot; h E f j= 2 P (~) p (ti lhi) f j (hi, ti )i=1 However, we believe this passage is in error: such an estimate is ineffective in the iterative scaling algorithm.</S> | Reference Offset:  ['28','30'] | Reference Text:  <S sid = 28 ssid = >The specific word and tag context available to a feature is given in the following definition of a history hi: If the above feature exists in the feature set of the model, its corresponding model parameter will contribute towards the joint probability p(hi,ti) when wi ends with &quot;ing&quot; and when ti =VBG1.</S><S sid = 30 ssid = >The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &quot;templates&quot; given in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi (1996).</S> | Reference Offset:  ['22','30'] | Reference Text:  <S sid = 22 ssid = >Thus the constraints force the model to match its feature expectations with those observed in the training data.</S><S sid = 30 ssid = >The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &quot;templates&quot; given in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They are a subset of the features used in Ratnaparkhi (1996).</S> | Reference Offset:  ['12','95'] | Reference Text:  <S sid = 12 ssid = >This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus.</S><S sid = 95 ssid = >8(Brill, 1994) uses prefix/suffix additions and deletions, which are not used in this paper. unlike MaxEnt, cannot be used as a probabilistic component in a larger model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The feature templates in Ratnaparkhi (1996) that were left out were the ones that look at the previous word, the word two positions before the current, and the word two positions after the current.</S> | Reference Offset:  ['27','63'] | Reference Text:  <S sid = 27 ssid = >A feature, given (h,t), may activate on any word or tag in the history h, and must encode any information that might help predict t, such as the spelling of the current word, or the identity of the previous two tags.</S><S sid = 63 ssid = >The features which ask about previous tags and surrounding words now additionally ask about the identity of the current word, e.g., a specialized feature for the word about in Table 3 could be: Table 8 shows the results of an experiment in which specialized features are constructed for &quot;difficult&quot; words, and are added to the baseline feature set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Model Overall Unknown Word Accuracy Accuracy Baseline, 96.72% 84.5% J Ratnaparkhi 96.63% 85.56% (1996) Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi (1996: 142) for Convenience.</S> | Reference Offset:  ['65','91'] | Reference Text:  <S sid = 65 ssid = >Using the set of 29 difficult words, the model performs at 96.49% accuracy on the Development Set, an insignificant improvement from the baseline accuracy of 96.43%.</S><S sid = 91 ssid = >TBL is a non-statistical approach to POS tagging which also uses a rich feature representation, and performs at a total word accuracy of 96.5% and an unknown word accuracy of 85%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This may stem from the differences between the two models &apos; feature templates, thresholds, and approximations of the expected values for the features, as discussed in the beginning of the section, or may just reflect differences in the choice of training and test sets (which are not precisely specified in Ratnaparkhi (1996)).</S> | Reference Offset:  ['30','49'] | Reference Text:  <S sid = 30 ssid = >The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &quot;templates&quot; given in Table 1.</S><S sid = 49 ssid = >The feature set and search algorithm were tested and debugged only on the Training and Development sets, and the official test result on the unseen Test set is presented in the conclusion of the paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One conclusion that we can draw is that at present the additional word features used in Ratnaparkhi (1996) looking at words more than one position away from the current do not appear to be helping the overall performance of the models.</S> | Reference Offset:  ['56','94'] | Reference Text:  <S sid = 56 ssid = >The Maximum Entropy model allows arbitrary binary-valued features on the context, so it can use additional specialized, i.e., word-specific, features to correctly tag the &quot;residue&quot; that the baseline features cannot model.</S><S sid = 94 ssid = >However, since TBL is non-statistical, it does not provide probability distributions and 7 (Brill, 1994) looks at words ±3 away from the current, whereas the feature set in this paper uses a window of ±2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W96-0213.txt | Citing Article:  W00-1308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some are the result of inconsistency in labeling in the training data (Ratnaparkhi 1996), which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context.</S> | Reference Offset:  ['0','68'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Model For Part-Of-Speech Tagging</S><S sid = 68 ssid = >The lack of improvement implies that either the feature set is still impoverished, or that the training data is inconsistent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W96-0213.txt | Citing Article:  P07-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e.,? =2 in the equation above).</S> | Reference Offset:  ['27','44'] | Reference Text:  <S sid = 27 ssid = >A feature, given (h,t), may activate on any word or tag in the history h, and must encode any information that might help predict t, such as the spelling of the current word, or the identity of the previous two tags.</S><S sid = 44 ssid = >If the Tag Dictionary is in effect, the search procedure, for known words, generates only tags given by the dictionary entry, while for unknown words, generates all tags in the tag set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W96-0213.txt | Citing Article:  E12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy).</S> | Reference Offset:  ['65','91'] | Reference Text:  <S sid = 65 ssid = >Using the set of 29 difficult words, the model performs at 96.49% accuracy on the Development Set, an insignificant improvement from the baseline accuracy of 96.43%.</S><S sid = 91 ssid = >TBL is a non-statistical approach to POS tagging which also uses a rich feature representation, and performs at a total word accuracy of 96.5% and an unknown word accuracy of 85%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W96-0213.txt | Citing Article:  E12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Feature templates as in (Ratnaparkhi, 1996),.</S> | Reference Offset:  ['28','30'] | Reference Text:  <S sid = 28 ssid = >The specific word and tag context available to a feature is given in the following definition of a history hi: If the above feature exists in the feature set of the model, its corresponding model parameter will contribute towards the joint probability p(hi,ti) when wi ends with &quot;ing&quot; and when ti =VBG1.</S><S sid = 30 ssid = >The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &quot;templates&quot; given in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W96-0213.txt | Citing Article:  N03-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best result known to us is achieved by Toutanova [2002] by enriching the feature representation of the MaxEnt approach [Ratnaparkhi, 1996].</S> | Reference Offset:  ['91','93'] | Reference Text:  <S sid = 91 ssid = >TBL is a non-statistical approach to POS tagging which also uses a rich feature representation, and performs at a total word accuracy of 96.5% and an unknown word accuracy of 85%.</S><S sid = 93 ssid = >The TBL representation of the surrounding word context is almost the same7 and the TBL representation of unknown words is a superset8 of the unknown word representation in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W96-0213.txt | Citing Article:  W03-0806.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, implementing an efficient version of the MXPOST POS tagger (Ratnaparkhi, 1996) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component.</S> | Reference Offset:  ['0','86'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Model For Part-Of-Speech Tagging</S><S sid = 86 ssid = >A POS tagger is one component in the SDT based statistical parsing system described in (Jelinek et al., 1994, Magerman, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W96-0213.txt | Citing Article:  I08-4024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi (1996) which was applied for English POS tagging.</S> | Reference Offset:  ['14','47'] | Reference Text:  <S sid = 14 ssid = >Lastly, the results in this paper are compared to those from previous work on POS tagging.</S><S sid = 47 ssid = >The search is described below:</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W96-0213.txt | Citing Article:  P02-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have explained elsewhere (Clark, 2002) how suitable features can be defined in terms of the  word ,pos-tag pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi (1996).</S> | Reference Offset:  ['12','56'] | Reference Text:  <S sid = 12 ssid = >This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus.</S><S sid = 56 ssid = >The Maximum Entropy model allows arbitrary binary-valued features on the context, so it can use additional specialized, i.e., word-specific, features to correctly tag the &quot;residue&quot; that the baseline features cannot model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W96-0213.txt | Citing Article:  H05-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given the parallel corpus, we tagged the English words with a publicly available maximum entropy tagger (Ratnaparkhi, 1996), and we used an implementation of the IBM translation model (Al Onaizan et al, 1999) to align the words.</S> | Reference Offset:  ['0','11'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Model For Part-Of-Speech Tagging</S><S sid = 11 ssid = >Previous uses of this model include language modeling(Lau et al., 1993), machine translation(Berger et al., 1996), prepositional phrase attachment(Ratnaparkhi et al., 1994), and word morphology(Della Pietra et al., 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W96-0213.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The C&C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two super tags.</S> | Reference Offset:  ['27','63'] | Reference Text:  <S sid = 27 ssid = >A feature, given (h,t), may activate on any word or tag in the history h, and must encode any information that might help predict t, such as the spelling of the current word, or the identity of the previous two tags.</S><S sid = 63 ssid = >The features which ask about previous tags and surrounding words now additionally ask about the identity of the current word, e.g., a specialized feature for the word about in Table 3 could be: Table 8 shows the results of an experiment in which specialized features are constructed for &quot;difficult&quot; words, and are added to the baseline feature set.</S> | Discourse Facet:  NA | Annotator: Automatic


