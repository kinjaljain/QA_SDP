Citance Number: 1 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word.</S> | Reference Offset:  ['3','143'] | Reference Text:  <S sid = 3 ssid = >This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.</S><S sid = 143 ssid = >The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings.</S> | Reference Offset:  ['0','134'] | Reference Text:  <S sid = 0 ssid = >Multi-Prototype Vector-Space Models of Word Meaning</S><S sid = 134 ssid = >Cluster similarity metrics: Besides AvgSim and MaxSim, there are many similarity metrics over mixture models, e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to.</S> | Reference Offset:  ['26','49'] | Reference Text:  <S sid = 26 ssid = >The results demonstrate the superiority of a clustered approach over both traditional prototype and exemplar-based vector-space models.</S><S sid = 49 ssid = >Figure 1 gives an overview of this process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010).</S> | Reference Offset:  ['60','84'] | Reference Text:  <S sid = 60 ssid = >All results reported in this paper use cosine similarity, 1 We compare across two different feature functions tf-idf weighting and X2 weighting, chosen due to their ubiquity in the literature (Agirre et al., 2009; Curran, 2004).</S><S sid = 84 ssid = >The exemplar approach yields significantly higher correlation than the single prototype approach in all cases except Gigaword with tf-idf features (p < 0.05).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N10-1013.txt | Citing Article:  P14-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010).</S> | Reference Offset:  ['0','113'] | Reference Text:  <S sid = 0 ssid = >Multi-Prototype Vector-Space Models of Word Meaning</S><S sid = 113 ssid = >Also, once again, the performance of the multi-prototype approach is better for homonyms than polysemes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words.</S> | Reference Offset:  ['18','143'] | Reference Text:  <S sid = 18 ssid = >The set of vectors for a word is determined by unsupervised word sense discovery (WSD) (Sch¨utze, 1998), which clusters the contexts in which a word appears.</S><S sid = 143 ssid = >The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word.</S> | Reference Offset:  ['0','123'] | Reference Text:  <S sid = 0 ssid = >Multi-Prototype Vector-Space Models of Word Meaning</S><S sid = 123 ssid = >This notion is evaluated empirically by computing the correlation between the predicted similarity using the contextual multi-prototype method and human similarity judgements for different usages of the same word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included.</S> | Reference Offset:  ['76','128'] | Reference Text:  <S sid = 76 ssid = >Figure 2 plots Spearman’s p on WordSim-353 against the number of clusters (K) for Wikipedia and Gigaword corpora, using pruned tf-idf and k2 features.2 In general pruned tf-idf features yield higher correlation than k2 features.</S><S sid = 128 ssid = >However we have not yet evaluated its performance when using more powerful feature representations such those based on Latent or Explicit Semantic Analysis (Deerwester et al., 1990; Gabrilovich and Markovitch, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance.</S> | Reference Offset:  ['85','91'] | Reference Text:  <S sid = 85 ssid = >Furthermore, it performs significantly worse 2(Feature pruning) We find that results using tf-idf features are extremely sensitive to feature pruning while x2 features are more robust.</S><S sid = 91 ssid = >Squares indicate performance when combining across clusterings. than combined multi-prototype for tf-idf features, and does not differ significantly for x2 features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5).</S> | Reference Offset:  ['56','106'] | Reference Text:  <S sid = 56 ssid = >Based on preliminary experiments comparing various clustering methods, we found movMF gave the best results.</S><S sid = 106 ssid = >However, given the right number of clusters, it also produces better results for polysemous words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a).</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >Multi-Prototype Vector-Space Models of Word Meaning</S><S sid = 20 ssid = >This paper shows how they can be combined to create an improved vector-space model of lexical semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N10-1013.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category.</S> | Reference Offset:  ['38','124'] | Reference Text:  <S sid = 38 ssid = >Such models have been widely studied in the Psychology literature (Griffiths et al., 2007; Love et al., 2004; Rosseel, 2002).</S><S sid = 124 ssid = >The Usage Similarity (USim) data set collected in Erk et al. (2009) provides such similarity scores from human raters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N10-1013.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers.</S> | Reference Offset:  ['19','143'] | Reference Text:  <S sid = 19 ssid = >In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.</S><S sid = 143 ssid = >The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N10-1013.txt | Citing Article:  N12-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors.</S> | Reference Offset:  ['35','47'] | Reference Text:  <S sid = 35 ssid = >Occurrences are clustered and cluster centroids are used as prototype vectors.</S><S sid = 47 ssid = >Our approach is similar to standard vector-space models of word meaning, with the addition of a perword-type clustering step: Occurrences for a specific word type are collected from the corpus and clustered using any appropriate method (§3.1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N10-1013.txt | Citing Article:  E12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses.</S> | Reference Offset:  ['51','99'] | Reference Text:  <S sid = 51 ssid = >This approach is commonly employed in unsupervised word sense discovery; however, we do not assume that clusters correspond to traditional word senses.</S><S sid = 99 ssid = >They are grouped into homonyms (words with very distinct senses) and polysemes (words with related senses).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N10-1013.txt | Citing Article:  P13-1171.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010).</S> | Reference Offset:  ['93','123'] | Reference Text:  <S sid = 93 ssid = >We next evaluated the multi-prototype approach on its ability to determine the most closely related words for a given target word (using the Wikipedia corpus with tf-idf features).</S><S sid = 123 ssid = >This notion is evaluated empirically by computing the correlation between the predicted similarity using the contextual multi-prototype method and human similarity judgements for different usages of the same word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N10-1013.txt | Citing Article:  W11-1310.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built.</S> | Reference Offset:  ['21','35'] | Reference Text:  <S sid = 21 ssid = >First, a word’s contexts are clustered to produce groups of similar context vectors.</S><S sid = 35 ssid = >Occurrences are clustered and cluster centroids are used as prototype vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N10-1013.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >However, movMF introduces an additional per-cluster concentration parameter controlling its semantic breadth, allowing it to more accurately model non-uniformities in the distribution of cluster sizes.</S><S sid = 149 ssid = >Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N10-1013.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice.</S> | Reference Offset:  ['19','104'] | Reference Text:  <S sid = 19 ssid = >In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.</S><S sid = 104 ssid = >Clustering more accurately identifies homonyms’ clearly distinct senses and produces prototypes that better capture the different uses of these words.</S> | Discourse Facet:  NA | Annotator: Automatic


