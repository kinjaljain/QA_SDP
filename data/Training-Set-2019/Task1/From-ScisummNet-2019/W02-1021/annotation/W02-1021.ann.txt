Citance Number: 1 | Reference Article:  W02-1021.txt | Citing Article:  P03-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002).</S> | Reference Offset:  ['0','22'] | Reference Text:  <S sid = 0 ssid = >Generation Of Word Graphs In Statistical Machine Translation</S><S sid = 22 ssid = >In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-1021.txt | Citing Article:  P14-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes.</S> | Reference Offset:  ['66','67'] | Reference Text:  <S sid = 66 ssid = >The word graph density is computed as the total number of word graph edges divided by the number of reference sentence words â€” analogously to the word graph density in speech recognition.</S><S sid = 67 ssid = >The effect of pruning on the graph error rate is shown in Table 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-1021.txt | Citing Article:  D08-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details).</S> | Reference Offset:  ['55','61'] | Reference Text:  <S sid = 55 ssid = >As rest cost estimation, we use the probabilities determined in a backward pass as follows: for each node in the graph, we calculate the probability of a best path from this node to the goal node, i.e. the highest probability for completing a partial hypothesis.</S><S sid = 61 ssid = >The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-1021.txt | Citing Article:  D11-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice.</S> | Reference Offset:  ['50','52'] | Reference Text:  <S sid = 50 ssid = >This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model.</S><S sid = 52 ssid = >The results of the comparison of the one-pass and the two-pass search are given in Section 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-1021.txt | Citing Article:  N07-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002).</S> | Reference Offset:  ['73','75'] | Reference Text:  <S sid = 73 ssid = >We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.</S><S sid = 75 ssid = >The quality of the hypotheses contained in a word graph is better than of those in an N-best list.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-1021.txt | Citing Article:  W05-0836.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002).</S> | Reference Offset:  ['0','75'] | Reference Text:  <S sid = 0 ssid = >Generation Of Word Graphs In Statistical Machine Translation</S><S sid = 75 ssid = >The quality of the hypotheses contained in a word graph is better than of those in an N-best list.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-1021.txt | Citing Article:  C04-1168.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The machine translation system is a graph based decoder (Ueffing et al, 2002).</S> | Reference Offset:  ['0','22'] | Reference Text:  <S sid = 0 ssid = >Generation Of Word Graphs In Statistical Machine Translation</S><S sid = 22 ssid = >In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-1021.txt | Citing Article:  P06-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002).</S> | Reference Offset:  ['40','44'] | Reference Text:  <S sid = 40 ssid = >', which is then chosen by the single best search.</S><S sid = 44 ssid = >The pruning is based on the beam search concept also used in the single best search: we determine the probability of the best sentence hypothesis in the word graph.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-1021.txt | Citing Article:  P06-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002).</S> | Reference Offset:  ['51','63'] | Reference Text:  <S sid = 51 ssid = >The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S><S sid = 63 ssid = >The calculation of the graph error rate is performed by a dynamic programming based algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-1021.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The only publication, we are aware of, is (Ueffing et al, 2002).</S> | Reference Offset:  ['49','51'] | Reference Text:  <S sid = 49 ssid = >Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).</S><S sid = 51 ssid = >The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-1021.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['28','77'] | Reference Text:  <S sid = 28 ssid = >If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.</S><S sid = 77 ssid = >For the future, we plan the application of refined translation and language models for rescoring on word graphs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-1021.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002).</S> | Reference Offset:  ['12','32'] | Reference Text:  <S sid = 12 ssid = >After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.</S><S sid = 32 ssid = >After the pruning in beam search, all hypotheses that are no longer active do not have to be kept in the bookkeeping structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-1021.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation.</S> | Reference Offset:  ['51','56'] | Reference Text:  <S sid = 51 ssid = >The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S><S sid = 56 ssid = >This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-1021.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The algorithm in (Ueffing et al, 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity.</S> | Reference Offset:  ['51','56'] | Reference Text:  <S sid = 51 ssid = >The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S><S sid = 56 ssid = >This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-1021.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002).</S> | Reference Offset:  ['43','74'] | Reference Text:  <S sid = 43 ssid = >Then we apply word graph pruning with a threshold t < 1 and study the change of graph error rate (see Section 5).</S><S sid = 74 ssid = >Experiments have shown that the graph error rate significantly decreases for rising word graph densities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-1021.txt | Citing Article:  C10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one.</S> | Reference Offset:  ['45','46'] | Reference Text:  <S sid = 45 ssid = >All hypotheses in the graph which probability is lower than this maximum probability multiplied with the pruning threshold are discarded.</S><S sid = 46 ssid = >If the pruning threshold t is zero, the word graph is not pruned at all, and if t = 1, we retain only the sentence with maximum probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-1021.txt | Citing Article:  C04-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002).</S> | Reference Offset:  ['49','61'] | Reference Text:  <S sid = 49 ssid = >Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).</S><S sid = 61 ssid = >The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-1021.txt | Citing Article:  D08-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list.</S> | Reference Offset:  ['19','22'] | Reference Text:  <S sid = 19 ssid = >One efficient way to store the different alternatives is a word graph.</S><S sid = 22 ssid = >In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-1021.txt | Citing Article:  P03-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002).</S> | Reference Offset:  ['44','51'] | Reference Text:  <S sid = 44 ssid = >The pruning is based on the beam search concept also used in the single best search: we determine the probability of the best sentence hypothesis in the word graph.</S><S sid = 51 ssid = >The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-1021.txt | Citing Article:  D08-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For a description on how to generate lattices, see (Ueffing et al, 2002).</S> | Reference Offset:  ['28','51'] | Reference Text:  <S sid = 28 ssid = >If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.</S><S sid = 51 ssid = >The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


