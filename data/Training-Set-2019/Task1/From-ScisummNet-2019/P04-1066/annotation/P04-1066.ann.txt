Citance Number: 1 | Reference Article:  P04-1066.txt | Citing Article:  P13-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-1066.txt | Citing Article:  N10-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-1066.txt | Citing Article:  P11-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported.</S> | Reference Offset:  ['2','121'] | Reference Text:  <S sid = 2 ssid = >We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.</S><S sid = 121 ssid = >With it, the combined model has substantially lower error than the heuristic model without re-estimation; without it, for any number of EM iterations, the combined model has higher error than the heuristic model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-1066.txt | Citing Article:  I08-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.</S> | Reference Offset:  ['20','92'] | Reference Text:  <S sid = 20 ssid = >Since Model 1, like many other word-alignment models, requires each target word to be generated by exactly one source word (including the null word), an alignment a can be represented by a vector a1,... , am, where each aj is the sentence position of the source word generating tj according to the alignment.</S><S sid = 92 ssid = >We cannot make use of LLR scores because the null word occurs in every source sentence, and any word occuring in every source sentence will have an LLR score of 0 with every target word, since p(t|s) = p(t) in that case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-1066.txt | Citing Article:  P08-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-1066.txt | Citing Article:  D10-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1.</S> | Reference Offset:  ['0','62'] | Reference Text:  <S sid = 0 ssid = >Improving IBM Word Alignment Model 1</S><S sid = 62 ssid = >We address the lack of sufficient alignments of target words to the null source word by adding extra null words to each source sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-1066.txt | Citing Article:  N06-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions.</S> | Reference Offset:  ['29','85'] | Reference Text:  <S sid = 29 ssid = >The first of these nonstructural problems with Model 1, as standardly trained, is that rare words in the source language tend to act as “garbage collectors” (Brown et al., 1993b; Och and Ney, 2004), aligning to too many words in the target language.</S><S sid = 85 ssid = >This is exactly the property needed to prevent rare source words from becoming garbage collectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-1066.txt | Citing Article:  N07-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution.</S> | Reference Offset:  ['82','135'] | Reference Text:  <S sid = 82 ssid = >The simplest approach would be to divide each LLR score by the sum of the scores for the source word of the pair, which would produce a normalized conditional probability distribution for each source word.</S><S sid = 135 ssid = >We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30%, simply by changing the way its parameters are estimated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-1066.txt | Citing Article:  W10-4217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above).</S> | Reference Offset:  ['32','110'] | Reference Text:  <S sid = 32 ssid = >Suppose the frequent source word has the translation present in the target sentence only 10% of the time in our training data, and thus has an estimated translation probability of around 0.1 for this target word.</S><S sid = 110 ssid = >The columns of the table present (in order) a description of the model being tested, the AER on the trial data, the AER on the test data, test data recall, and test data precision, followed by the optimal values on the trial data for the LLR exponent, the initial (heuristic model) null-word weight, the nullword weight used in EM re-estimation, the add-n parameter value used in EM re-estimation, and the number of iterations of EM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-1066.txt | Citing Article:  W10-4217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2.</S> | Reference Offset:  ['86','92'] | Reference Text:  <S sid = 86 ssid = >To maintain this property, for each source word we compute the sum of the LLR scores over all target words, but we then divide every LLR score by the single largest of these sums.</S><S sid = 92 ssid = >We cannot make use of LLR scores because the null word occurs in every source sentence, and any word occuring in every source sentence will have an LLR score of 0 with every target word, since p(t|s) = p(t) in that case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-1066.txt | Citing Article:  W10-4217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-1066.txt | Citing Article:  D12-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question.</S> | Reference Offset:  ['0','19'] | Reference Text:  <S sid = 0 ssid = >Improving IBM Word Alignment Model 1</S><S sid = 19 ssid = >We may also be interested in the question of what is the most likely alignment of a source sentence and a target sentence, given an instance of Model 1; where, by an alignment, we mean a specification of which source words generated which target words according to the generative model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-1066.txt | Citing Article:  D12-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004).</S> | Reference Offset:  ['72','94'] | Reference Text:  <S sid = 72 ssid = >But this is only the case if we want to obtain the parameter values at convergence, and we have strong reasons to believe that these values do not produce the most accurate sentence alignments.</S><S sid = 94 ssid = >Hence we initialize the distribution for the null word to be the unigram distribution of target words, so that frequent function words will receive a higher probability of aligning to the null word than rare words, which tend to be content words that do have a translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-1066.txt | Citing Article:  P06-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P04-1066.txt | Citing Article:  P06-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P04-1066.txt | Citing Article:  P06-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P04-1066.txt | Citing Article:  C10-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P04-1066.txt | Citing Article:  W06-1607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance.</S> | Reference Offset:  ['61','135'] | Reference Text:  <S sid = 61 ssid = >It should certainly be better than smoothing with a unigram distribution, since we especially want to benefit from smoothing the translation probabilities for the rarest words, and smoothing with a unigram distribution would assume that rare words are more likely to translate to frequent words than to other rare words, which seems counterintuitive.</S><S sid = 135 ssid = >We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30%, simply by changing the way its parameters are estimated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P04-1066.txt | Citing Article:  P12-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.</S><S sid = 144 ssid = >Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P04-1066.txt | Citing Article:  P12-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010).</S> | Reference Offset:  ['5','12'] | Reference Text:  <S sid = 5 ssid = >Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003).</S><S sid = 12 ssid = >This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1.</S> | Discourse Facet:  NA | Annotator: Automatic


