Citance Number: 1 | Reference Article:  P96-1021.txt | Citing Article:  P98-2230.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis pace can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion transduction model.</S> | Reference Offset:  ['77','112'] | Reference Text:  <S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S><S sid = 112 ssid = >The second possibility is to use a stochastic bracketing transduction grammar (SBTG) in the channel model, replacing the translation model altogether.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P96-1021.txt | Citing Article:  P98-2230.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation.</S> | Reference Offset:  ['24','181'] | Reference Text:  <S sid = 24 ssid = >6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT.</S><S sid = 181 ssid = >We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P96-1021.txt | Citing Article:  P98-2230.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Subsequently, a method was developed to use a special case of the ITGR the aforementioned BTGR for the translation task itself (Wu, 1996).</S> | Reference Offset:  ['58','77'] | Reference Text:  <S sid = 58 ssid = >For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995).</S><S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P96-1021.txt | Citing Article:  P98-2230.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wu (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation.</S> | Reference Offset:  ['40','81'] | Reference Text:  <S sid = 40 ssid = >Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English.</S><S sid = 81 ssid = >An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus generates two strings, one on the Chinese stream and one on the English stream.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P96-1021.txt | Citing Article:  W12-3128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To tackle the problem of glue rules, He (2010) extended the HPB model by using bracketing transduction grammar (Wu, 1996) instead of the monotone glue rules, and trained an extra classifier for glue rules to predict reorderings of neighboring phrases.</S> | Reference Offset:  ['77','112'] | Reference Text:  <S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S><S sid = 112 ssid = >The second possibility is to use a stochastic bracketing transduction grammar (SBTG) in the channel model, replacing the translation model altogether.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P96-1021.txt | Citing Article:  W05-1507.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram language model.</S> | Reference Offset:  ['0','75'] | Reference Text:  <S sid = 0 ssid = >A Polynomial-Time Algorithm For Statistical Machine Translation</S><S sid = 75 ssid = >This greatly reduces the search space and makes possible a polynomial-time optimization algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P96-1021.txt | Citing Article:  W06-1627.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection.</S> | Reference Offset:  ['124','131'] | Reference Text:  <S sid = 124 ssid = >As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967).</S><S sid = 131 ssid = >The algorithm is given below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P96-1021.txt | Citing Article:  N06-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To do bigram-integrated decoding, we need to augment each chart item (X,i, j) with two target-language boundary words u and v to produce a bigram-item like u ··· v Xi j, following the dynamic programming algorithm of Wu (1996).</S> | Reference Offset:  ['124','149'] | Reference Text:  <S sid = 124 ssid = >As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967).</S><S sid = 149 ssid = >First, we are not merely parsing, but translating with a bigram language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P96-1021.txt | Citing Article:  N06-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >NP (1) VPP-VP (2), NP (1) VPP-VP (2) VPP-VP? VP (1) PP (2), PP (2) VP (1) In this case m-gram integrated decoding can bedone in O (|w|3+4 (m? 1)) time which is much lower order polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG.</S> | Reference Offset:  ['0','75'] | Reference Text:  <S sid = 0 ssid = >A Polynomial-Time Algorithm For Statistical Machine Translation</S><S sid = 75 ssid = >This greatly reduces the search space and makes possible a polynomial-time optimization algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P96-1021.txt | Citing Article:  P03-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation.</S> | Reference Offset:  ['0','181'] | Reference Text:  <S sid = 0 ssid = >A Polynomial-Time Algorithm For Statistical Machine Translation</S><S sid = 181 ssid = >We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P96-1021.txt | Citing Article:  P03-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used.</S> | Reference Offset:  ['42','56'] | Reference Text:  <S sid = 42 ssid = >In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below.</S><S sid = 56 ssid = >Note there are no explicit linguistic grammars in the IBM channel model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P96-1021.txt | Citing Article:  N09-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996).</S> | Reference Offset:  ['88','106'] | Reference Text:  <S sid = 88 ssid = >BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation.</S><S sid = 106 ssid = >But the structural constraints of the BTG can improve search efficiency, even without differentiated constituent categories.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P96-1021.txt | Citing Article:  P06-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.</S> | Reference Offset:  ['77','112'] | Reference Text:  <S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S><S sid = 112 ssid = >The second possibility is to use a stochastic bracketing transduction grammar (SBTG) in the channel model, replacing the translation model altogether.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P96-1021.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Wu (1996) or Melamed (2004) for a detailed exposition.</S> | Reference Offset:  ['77','153'] | Reference Text:  <S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S><S sid = 153 ssid = >The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P96-1021.txt | Citing Article:  P06-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reordering restrictions for word-based SMT decoders were introduced by (Berger et al, 1996) and (Wu, 1996).</S> | Reference Offset:  ['39','43'] | Reference Text:  <S sid = 39 ssid = >The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process.</S><S sid = 43 ssid = >Estimation of the parameters has been described elsewhere (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P96-1021.txt | Citing Article:  P06-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Wu,1996) propose using contiguity restrictions on the reordering.</S> | Reference Offset:  ['47','77'] | Reference Text:  <S sid = 47 ssid = >This price is paid for the robustness that is obtained by using very flexible language and translation models.</S><S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P96-1021.txt | Citing Article:  E09-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996).</S> | Reference Offset:  ['29','77'] | Reference Text:  <S sid = 29 ssid = >This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models.</S><S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P96-1021.txt | Citing Article:  P05-3025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002).</S> | Reference Offset:  ['77','83'] | Reference Text:  <S sid = 77 ssid = >Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d).</S><S sid = 83 ssid = >With each production of the grammar is associated either a straight orientation or an inverted orientation, respectively denoted as follows:</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P96-1021.txt | Citing Article:  W07-2216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework ,including information extraction (Miller et al, 2000) and syntax-based machine translation (Wu, 1996).</S> | Reference Offset:  ['138','148'] | Reference Text:  <S sid = 138 ssid = >In contrast, the ID/LP work was directed at parsing a single language with free word-order.</S><S sid = 148 ssid = >Our work differs from the ID/LP work in several important respects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P96-1021.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above.</S> | Reference Offset:  ['16','173'] | Reference Text:  <S sid = 16 ssid = >For comparison, the accuracies from the A*-based systems are also shown.</S><S sid = 173 ssid = >For comparison, the accuracies from the A*-based systems are also shown.</S> | Discourse Facet:  NA | Annotator: Automatic


