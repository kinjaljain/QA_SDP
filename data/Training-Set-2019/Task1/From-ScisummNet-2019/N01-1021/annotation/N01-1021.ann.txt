Citance Number: 1 | Reference Article:  N01-1021.txt | Citing Article:  P10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which correlates with increased reading difficulty.</S> | Reference Offset:  ['0','42'] | Reference Text:  <S sid = 0 ssid = >A Probabilistic Earley Parser As A Psycholinguistic Model</S><S sid = 42 ssid = >The computation of prefix probabilities takes advantage of the design of the Earley parser (Earley, 1970) which by itself is not probabilistic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N01-1021.txt | Citing Article:  P10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word's prefix probability against this word's prefix probability.</S> | Reference Offset:  ['38','92'] | Reference Text:  <S sid = 38 ssid = >Stolcke’s algorithm solves this problem by computing, at each word of an input string, the prefix probability.</S><S sid = 92 ssid = >Scaling this number by taking its log gives the surprisal, and defines a word-based measure of cognitive effort in terms of the prefix-based one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N01-1021.txt | Citing Article:  P10-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity.</S> | Reference Offset:  ['26','92'] | Reference Text:  <S sid = 26 ssid = >Traditional language models used for speech are ngram models, in which n − 1 words of history serve as the basis for predicting the nth word.</S><S sid = 92 ssid = >Scaling this number by taking its log gives the surprisal, and defines a word-based measure of cognitive effort in terms of the prefix-based one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N01-1021.txt | Citing Article:  P10-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability.</S> | Reference Offset:  ['40','55'] | Reference Text:  <S sid = 40 ssid = >If the grammar is consistent (the probabilities of all derivations sum to 1.0) then subtracting the prefix probability from 1.0 gives the total probability of all the analyses the parser has disconfirmed.</S><S sid = 55 ssid = >Stolcke’s innovation, as regards prefix probabilities is to add two additional pieces of information to each state: α, the forward, or prefix probability, and y the “inside” probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N01-1021.txt | Citing Article:  W10-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory.</S> | Reference Offset:  ['30','83'] | Reference Text:  <S sid = 30 ssid = >The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g.</S><S sid = 83 ssid = >The probabilistic Earley parser computes all parses of its input, so as a psycholinguistic theory it is a total parallelism theory.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N01-1021.txt | Citing Article:  W10-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hale (2001) pointed out that the ratio of the prefix probabilities.</S> | Reference Offset:  ['55','130'] | Reference Text:  <S sid = 55 ssid = >Stolcke’s innovation, as regards prefix probabilities is to add two additional pieces of information to each state: α, the forward, or prefix probability, and y the “inside” probability.</S><S sid = 130 ssid = >However, the garden path effect is still observable at “resigned” where the prefix probability ratio is nearly 10 times greater than at either of the nouns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N01-1021.txt | Citing Article:  P08-2002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hale (2001) suggests this quantity as an index of psycholinguistic difficulty.</S> | Reference Offset:  ['0','75'] | Reference Text:  <S sid = 0 ssid = >A Probabilistic Earley Parser As A Psycholinguistic Model</S><S sid = 75 ssid = >Psycholinguistic theories vary regarding the amount bandwidth they attribute to the human sentence processing mechanism.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N01-1021.txt | Citing Article:  P10-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar.</S> | Reference Offset:  ['24','30'] | Reference Text:  <S sid = 24 ssid = >This is typically done using conditional probabilities of the form the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1.</S><S sid = 30 ssid = >The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N01-1021.txt | Citing Article:  P10-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences.</S> | Reference Offset:  ['36','90'] | Reference Text:  <S sid = 36 ssid = >Even if a PCFG is consistent, it would appear to have another drawback: it only assigns probabilities to complete sentences of its language.</S><S sid = 90 ssid = >If there were separate processing costs distinct from the optimization costs postulated in the grammar, then strong competence is violated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N01-1021.txt | Citing Article:  P06-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and the subject/object relative asymmetry.</S> | Reference Offset:  ['4','135'] | Reference Text:  <S sid = 4 ssid = >Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.</S><S sid = 135 ssid = >Grammar (3) generates both subject and object relative clauses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N01-1021.txt | Citing Article:  W10-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t1 and word t.</S> | Reference Offset:  ['81','88'] | Reference Text:  <S sid = 81 ssid = >Finally, there is the possibility of total parallelism, in which the entire set of trees compatible with the input is maintained somehow from word to word.</S><S sid = 88 ssid = >This prefix-based linking hypothesis can be turned into one that generates predictions about word-byword reading times by comparing the total effort expended before some word to the total effort after: in particular, take the comparison to be a ratio.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N01-1021.txt | Citing Article:  D12-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well 356 known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena.</S> | Reference Offset:  ['4','13'] | Reference Text:  <S sid = 4 ssid = >Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.</S><S sid = 13 ssid = >Principle 3 Sentence processing is eager.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N01-1021.txt | Citing Article:  W11-0612.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The surprisal (Hale, 2001) at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser.</S> | Reference Offset:  ['20','24'] | Reference Text:  <S sid = 20 ssid = >In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen.</S><S sid = 24 ssid = >This is typically done using conditional probabilities of the form the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N01-1021.txt | Citing Article:  P11-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence.</S> | Reference Offset:  ['7','82'] | Reference Text:  <S sid = 7 ssid = >Principle 1 The relation between the parser and grammar is one of strong competence.</S><S sid = 82 ssid = >On such a theory, garden-pathing cannot be explained by reanalysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N01-1021.txt | Citing Article:  P06-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states (Hale, 2001).</S> | Reference Offset:  ['46','51'] | Reference Text:  <S sid = 46 ssid = >A state is a record that specifies An Earley parser has three main functions, predict, scan and complete, each of which can enter new states into the chart.</S><S sid = 51 ssid = >Finally, complete propagates this change throughout the chart.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N01-1021.txt | Citing Article:  P06-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This conclusion is strengthened when we turn to consider the performance of the parser on the standard Penn Treebank test set: the Within model showed a small increase in F-score over the PCFG baseline, while the copy model showed no such advantage.5All the models we proposed offer a broad coverage account of human parsing, not just a limited model on a hand-selected set of examples, such as the models proposed by Jurafsky (1996) and Hale (2001) (but see Crocker and Brants 2000).</S> | Reference Offset:  ['0','6'] | Reference Text:  <S sid = 0 ssid = >A Probabilistic Earley Parser As A Psycholinguistic Model</S><S sid = 6 ssid = >The answer to be proposed here observes three principles.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N01-1021.txt | Citing Article:  W10-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although this does not match the interpretation of the experimental results followed in this paper, it leaves open the possibility that the feature could model the data according to a different measure of parsing difficulty, such as surprisal (Hale, 2001).</S> | Reference Offset:  ['67','116'] | Reference Text:  <S sid = 67 ssid = >Scanning does not alter any probabilities.</S><S sid = 116 ssid = >To examine this possibility, consider now a different example sentence, this time from the language of grammar (2).</S> | Discourse Facet:  NA | Annotator: Automatic


