Citance Number: 1 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)).</S> | Reference Offset:  ['41','42'] | Reference Text:  <S sid = 41 ssid = >We used the following features (several of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction): The word being tagged, the previous word, and the next word.</S><S sid = 42 ssid = >The previous tag, and the previous two tags (bigram and trigram features).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b).</S> | Reference Offset:  ['163','179'] | Reference Text:  <S sid = 163 ssid = >We applied the voted perceptron and boosting algorithms to the data described in section 2.3.</S><S sid = 179 ssid = >See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.</S> | Reference Offset:  ['1','178'] | Reference Text:  <S sid = 1 ssid = >This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.</S><S sid = 178 ssid = >(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','194'] | Reference Text:  <S sid = 48 ssid = >For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S><S sid = 194 ssid = >Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1062.txt | Citing Article:  P13-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','194'] | Reference Text:  <S sid = 48 ssid = >For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S><S sid = 194 ssid = >Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1062.txt | Citing Article:  I08-4025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a).</S> | Reference Offset:  ['5','179'] | Reference Text:  <S sid = 5 ssid = >The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.</S><S sid = 179 ssid = >See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1062.txt | Citing Article:  I08-4025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead.</S> | Reference Offset:  ['3','156'] | Reference Text:  <S sid = 3 ssid = >The second approach uses the voted perceptron algorithm.</S><S sid = 156 ssid = >(Freund & Schapire 1999) describe a refinement of the perceptron, the voted perceptron.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1062.txt | Citing Article:  I08-4025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007).</S> | Reference Offset:  ['89','142'] | Reference Text:  <S sid = 89 ssid = >This will generate a feature string for each of the entities in a candidate, this time using the values rather than .</S><S sid = 142 ssid = >The boosting algorithm chooses the feature/update pair which is optimal in terms of minimizing the loss function, i.e., and then makes the update .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1062.txt | Citing Article:  N10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features.</S> | Reference Offset:  ['84','117'] | Reference Text:  <S sid = 84 ssid = >We will use “feature templates” to describe the features that we used.</S><S sid = 117 ssid = >We define We assume a set of additional features, for .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1062.txt | Citing Article:  N10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','194'] | Reference Text:  <S sid = 48 ssid = >For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S><S sid = 194 ssid = >Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1062.txt | Citing Article:  W03-0435.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach has been used earlier by (Collins, 2002).</S> | Reference Offset:  ['3','177'] | Reference Text:  <S sid = 3 ssid = >The second approach uses the voted perceptron algorithm.</S><S sid = 177 ssid = >Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1062.txt | Citing Article:  P09-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a).</S> | Reference Offset:  ['0','163'] | Reference Text:  <S sid = 0 ssid = >Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron</S><S sid = 163 ssid = >We applied the voted perceptron and boosting algorithms to the data described in section 2.3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1062.txt | Citing Article:  D08-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The detailed algorithm can be found in (Collins, 2002).</S> | Reference Offset:  ['126','179'] | Reference Text:  <S sid = 126 ssid = >The first algorithm we consider is the boosting algorithm for ranking described in (Collins 2000).</S><S sid = 179 ssid = >See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1062.txt | Citing Article:  W06-3607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features.</S> | Reference Offset:  ['36','40'] | Reference Text:  <S sid = 36 ssid = >As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et.</S><S sid = 40 ssid = >Thus the maximumentropy tagger we used represents a serious baseline for the task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1062.txt | Citing Article:  N09-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','194'] | Reference Text:  <S sid = 48 ssid = >For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S><S sid = 194 ssid = >Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1062.txt | Citing Article:  N09-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','194'] | Reference Text:  <S sid = 48 ssid = >For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S><S sid = 194 ssid = >Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1062.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (2002) includes a number of interesting contextual predicates for NER.</S> | Reference Offset:  ['65','105'] | Reference Text:  <S sid = 65 ssid = >Conceptually, the candidate is represented by a large number of features for where is the number of distinct feature strings in training data.</S><S sid = 105 ssid = >Also define to be the number of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1062.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes.</S> | Reference Offset:  ['86','105'] | Reference Text:  <S sid = 86 ssid = >We take the entity to span words inclusive in the candidate. is seen from words to inclusive in a segmentation.</S><S sid = 105 ssid = >Also define to be the number of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1062.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002).</S> | Reference Offset:  ['178','187'] | Reference Text:  <S sid = 178 ssid = >(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.</S><S sid = 187 ssid = >This example also illustrates why this approach is unlikely to improve the performance of the maximum-entropy tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1062.txt | Citing Article:  D11-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b).</S> | Reference Offset:  ['49','78'] | Reference Text:  <S sid = 49 ssid = >The word with each character mapped to its type, but repeated consecutive character types are not repeated in the mapped string.</S><S sid = 78 ssid = >Each character in the word is mapped to its , but repeated consecutive character types are not repeated in the mapped string.</S> | Discourse Facet:  NA | Annotator: Automatic


