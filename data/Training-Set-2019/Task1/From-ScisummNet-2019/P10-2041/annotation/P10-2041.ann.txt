Citance Number: 1 | Reference Article:  P10-2041.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010).</S> | Reference Offset:  ['70','80'] | Reference Text:  <S sid = 70 ssid = >The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.</S><S sid = 80 ssid = >The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P10-2041.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010).</S> | Reference Offset:  ['23','54'] | Reference Text:  <S sid = 23 ssid = >However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S><S sid = 54 ssid = >We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P10-2041.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010).</S> | Reference Offset:  ['23','80'] | Reference Text:  <S sid = 23 ssid = >However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S><S sid = 80 ssid = >The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P10-2041.txt | Citing Article:  P14-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010).</S> | Reference Offset:  ['13','36'] | Reference Text:  <S sid = 13 ssid = >We are aware of two comparable previous approaches.</S><S sid = 36 ssid = >We estimated this effect on a 1000-sentence sample of our experimental data described below, and found the correlation between sentence log probability difference and sentence length to be r = −0.92, while the cross-entropy difference was almost uncorrelated with sentence length (r = 0.04).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P10-2041.txt | Citing Article:  P14-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data.</S> | Reference Offset:  ['42','48'] | Reference Text:  <S sid = 42 ssid = >We used the text from 1999 through 2008 as in-domain training data, and we used the first 2000 sentences from January 2009 as test data.</S><S sid = 48 ssid = >To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P10-2041.txt | Citing Article:  W12-3148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT.</S> | Reference Offset:  ['14','60'] | Reference Text:  <S sid = 14 ssid = >Lin et al. (1997) and Gao et al.</S><S sid = 60 ssid = >As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P10-2041.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010).</S> | Reference Offset:  ['3','80'] | Reference Text:  <S sid = 3 ssid = >We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.</S><S sid = 80 ssid = >The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P10-2041.txt | Citing Article:  W12-3149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data.</S> | Reference Offset:  ['23','48'] | Reference Text:  <S sid = 23 ssid = >However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S><S sid = 48 ssid = >To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).</S> | Reference Offset:  ['14','61'] | Reference Text:  <S sid = 14 ssid = >Lin et al. (1997) and Gao et al.</S><S sid = 61 ssid = >(2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.</S> | Reference Offset:  ['22','54'] | Reference Text:  <S sid = 22 ssid = >Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).</S><S sid = 54 ssid = >We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus.</S> | Reference Offset:  ['18','49'] | Reference Text:  <S sid = 18 ssid = >Klakow (2000) estimates a unigram language model from the entire non-domain-specific corpus to be selected from, and scores each candidate text segment from that corpus by the change in the log likelihood of the in-domain data according to the unigram model, if that segment were removed from the corpus used to estimate the unigram model.</S><S sid = 49 ssid = >To make these language models comparable, and to show the feasibility of optimizing the fit to the in-domain data without training a model on the entire Gigaword corpus, we trained the Gigaword language model for data selection on a random sample of the Gigaword corpus of a similar size to that of the Europarl training data: 1,874,051 sentences, 48,459,945 tokens.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010).</S> | Reference Offset:  ['66','77'] | Reference Text:  <S sid = 66 ssid = >The test set perplexity for the language model trained on the full Gigaword corpus is 135.</S><S sid = 77 ssid = >This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel.</S> | Reference Offset:  ['23','54'] | Reference Text:  <S sid = 23 ssid = >However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S><S sid = 54 ssid = >We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P10-2041.txt | Citing Article:  W12-3140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010).</S> | Reference Offset:  ['40','43'] | Reference Text:  <S sid = 40 ssid = >For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).</S><S sid = 43 ssid = >For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P10-2041.txt | Citing Article:  P11-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010).</S> | Reference Offset:  ['0','81'] | Reference Text:  <S sid = 0 ssid = >Intelligent Selection of Language Model Training Data</S><S sid = 81 ssid = >This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P10-2041.txt | Citing Article:  W12-3137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010).</S> | Reference Offset:  ['40','43'] | Reference Text:  <S sid = 40 ssid = >For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).</S><S sid = 43 ssid = >For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P10-2041.txt | Citing Article:  P13-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010).</S> | Reference Offset:  ['14','60'] | Reference Text:  <S sid = 14 ssid = >Lin et al. (1997) and Gao et al.</S><S sid = 60 ssid = >As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P10-2041.txt | Citing Article:  P13-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010).</S> | Reference Offset:  ['32','39'] | Reference Text:  <S sid = 32 ssid = >Equivalently, we can work in the log domain with the quantity log(P(s|I)) − log(P(s|N)).</S><S sid = 39 ssid = >We have empirically evaluated our proposed method for selecting data from a non-domainspecific source to model text in a specific domain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P10-2041.txt | Citing Article:  P13-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy).</S> | Reference Offset:  ['22','70'] | Reference Text:  <S sid = 22 ssid = >Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).</S><S sid = 70 ssid = >The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P10-2041.txt | Citing Article:  D12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010).</S> | Reference Offset:  ['5','10'] | Reference Text:  <S sid = 5 ssid = >It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.</S><S sid = 10 ssid = >In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.</S> | Discourse Facet:  NA | Annotator: Automatic


