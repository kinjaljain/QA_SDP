Citance Number: 1 | Reference Article:  P05-1015.txt | Citing Article:  P06-2081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This can help explain why, as Pang and Lee (2005) note, one person's four-star review is another's two-star.</S> | Reference Offset:  ['27','51'] | Reference Text:  <S sid = 27 ssid = >As Table 1 shows, both subjects performed perfectly when the rating separation was at least 3 “notches” in the original scale (we define a notch as a half star in a four- or five-star scheme and 10 points in a 100-point scheme).</S><S sid = 51 ssid = >Even though Eric Lurio uses a 5 star system, his grading is very relaxed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1015.txt | Citing Article:  P06-2081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','171'] | Reference Text:  <S sid = 53 ssid = >feature space to a metric space.If we choose from a family of sufficiently “gradual” functions, then similar items necessarily receive similar labels.</S><S sid = 171 ssid = >Koppel and Schler (2005) independently found in a three-class study that thresholding a positive/negative classifier trained only on clearly positive or clearly negative examples did not yield large improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1015.txt | Citing Article:  P14-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http: //help.sentiment140.com/for-students.</S> | Reference Offset:  ['35','81'] | Reference Text:  <S sid = 35 ssid = >Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002).</S><S sid = 81 ssid = >To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1015.txt | Citing Article:  P10-2049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is a huge body of work on OM in movie reviews which was sparked by the dataset from Pang and Lee (2005).</S> | Reference Offset:  ['35','36'] | Reference Text:  <S sid = 35 ssid = >Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002).</S><S sid = 36 ssid = >We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1015.txt | Citing Article:  P11-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews.</S> | Reference Offset:  ['35','154'] | Reference Text:  <S sid = 35 ssid = >Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002).</S><S sid = 154 ssid = >We are also interested in framing multi-class but non-scale-based categorization problems as metric labeling tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1015.txt | Citing Article:  D11-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','171'] | Reference Text:  <S sid = 53 ssid = >feature space to a metric space.If we choose from a family of sufficiently “gradual” functions, then similar items necessarily receive similar labels.</S><S sid = 171 ssid = >Koppel and Schler (2005) independently found in a three-class study that thresholding a positive/negative classifier trained only on clearly positive or clearly negative examples did not yield large improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1015.txt | Citing Article:  D11-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al, 2005).</S> | Reference Offset:  ['56','138'] | Reference Text:  <S sid = 56 ssid = >Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods.</S><S sid = 138 ssid = >Q: Your datasets had many labeled reviews and only one author each.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1015.txt | Citing Article:  P11-4022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The optimal POS bi-tags have been derived experimentally by using top 10% features on information gain based-pruning classifier on polarity dataset by (Pang and Lee, 2005).</S> | Reference Offset:  ['63','128'] | Reference Text:  <S sid = 63 ssid = >In a sense, we are using explicit item and label similarity information to increasingly penalize the initial classifier as it assigns more divergent labels to similar items.</S><S sid = 128 ssid = >Q: What about using PSP as one of the features for input to a standard classifier?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1015.txt | Citing Article:  D08-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Cascaded models for sentiment classification were studied by (Pang and Lee, 2005).</S> | Reference Offset:  ['69','153'] | Reference Text:  <S sid = 69 ssid = >Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004).</S><S sid = 153 ssid = >Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1015.txt | Citing Article:  N10-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This indicates there is some difficulty distinguishing between the fine-grained categories we specified, but high agreement at a coarser level, which advocates using a ranking approach for evaluation (see also Pang and Lee 2005).</S> | Reference Offset:  ['12','22'] | Reference Text:  <S sid = 12 ssid = >Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa.</S><S sid = 22 ssid = >Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g., what an understated author intends as high praise may seem lukewarm), author inconsistency at assigning fine-grained ratings, and For data, we first collected Internet movie reviews in English from four authors, removing explicit rating indicators from each document’s text automatically.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1015.txt | Citing Article:  D12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','171'] | Reference Text:  <S sid = 53 ssid = >feature space to a metric space.If we choose from a family of sufficiently “gradual” functions, then similar items necessarily receive similar labels.</S><S sid = 171 ssid = >Koppel and Schler (2005) independently found in a three-class study that thresholding a positive/negative classifier trained only on clearly positive or clearly negative examples did not yield large improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1015.txt | Citing Article:  P07-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem.</S> | Reference Offset:  ['12','69'] | Reference Text:  <S sid = 12 ssid = >Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa.</S><S sid = 69 ssid = >Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1015.txt | Citing Article:  C10-2088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Machine learning techniques have been proposed for sentiment classification (Pang et al., 2002; Mullen and Collier, 2004) based on annotated samples from experts, but they have limited performance especially when estimating ratings of a multi-point scale (Pang and Lee, 2005).</S> | Reference Offset:  ['1','35'] | Reference Text:  <S sid = 1 ssid = >address the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”).</S><S sid = 35 ssid = >Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1015.txt | Citing Article:  C10-2088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We then adopt the machine learning method proposed in (Pangand Lee, 2005) and the Bayesian Network classifier (Russell and Norvig, 2002) for feature rating estimation.</S> | Reference Offset:  ['35','59'] | Reference Text:  <S sid = 35 ssid = >Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002).</S><S sid = 59 ssid = >But we can also think of our task as a metric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random fields, to explicitly encode our desideratum.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1015.txt | Citing Article:  C10-2088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In 2005, Pang and Lee extended their earlier work in (Pang and Lee, 2004) to determine a reviewer's evaluation with respect to multi scales (Pang and Lee, 2005).</S> | Reference Offset:  ['1','69'] | Reference Text:  <S sid = 1 ssid = >address the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”).</S><S sid = 69 ssid = >Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1015.txt | Citing Article:  C10-2088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the approach of Pangand Lee (Pang and Lee, 2005) described in Section 2 for feature rating estimation.</S> | Reference Offset:  ['71','145'] | Reference Text:  <S sid = 71 ssid = >We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3.</S><S sid = 145 ssid = >A few that we tested are described in the Appendix, and we propose some others in the next section.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1015.txt | Citing Article:  P12-2018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >RT-s: Short movie reviews dataset containing one sentence per review (Pang and Lee, 2005).</S> | Reference Offset:  ['36','81'] | Reference Text:  <S sid = 36 ssid = >We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews.</S><S sid = 81 ssid = >To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1015.txt | Citing Article:  P11-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We might attempt to exploit these dependencies in a manner similar to Pang and Lee (2005) to improve three-way classification.</S> | Reference Offset:  ['4','105'] | Reference Text:  <S sid = 4 ssid = >Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier’s output in an explicit attempt to ensure that similar items receive similar labels.</S><S sid = 105 ssid = >Thus, we can in fact effectively exploit similarities in the three-class case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1015.txt | Citing Article:  W10-0216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Data: The dataset consists of snippets from Rotten Tomatoes (Pang and Lee, 2005).</S> | Reference Offset:  ['46','81'] | Reference Text:  <S sid = 46 ssid = >Training consists of building, for each label, an SVM binary classifier distinguishing labelfrom “not-”.</S><S sid = 81 ssid = >To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1015.txt | Citing Article:  W10-0216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This dataset was created and used by Pang and Lee (2005) to train a classifier for identifying positive sentences in a full length review.</S> | Reference Offset:  ['81','82'] | Reference Text:  <S sid = 81 ssid = >To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes.</S><S sid = 82 ssid = >Then, we trained a Naive Bayes classifier on this data set and applied it to our scale dataset to identify the positive sentences (recall that objective sentences were already removed).</S> | Discourse Facet:  NA | Annotator: Automatic


