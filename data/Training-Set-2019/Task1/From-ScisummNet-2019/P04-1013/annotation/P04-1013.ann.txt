Citance Number: 1 | Reference Article:  P04-1013.txt | Citing Article:  P05-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.</S> | Reference Offset:  ['7','154'] | Reference Text:  <S sid = 7 ssid = >Klein and Manning (2002) argue that these results show a pattern where discriminative probability models are inferior to generative probability models, but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria.</S><S sid = 154 ssid = >The other models he investigates conflate changes in the probability models with changes in the training criteria, and the discriminative probability models do worse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-1013.txt | Citing Article:  P14-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson (2004), and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al.</S> | Reference Offset:  ['61','77'] | Reference Text:  <S sid = 61 ssid = >At each position i, representations from earlier in the sequence are combined with features of the new position i to produce a vector of real valued features which represent the prefix ending at i.</S><S sid = 77 ssid = >The input features for these loglinear models are the real-valued vectors computed by h(d1,..., di−1) and l(yield(di,..., dm)), as explained in more detail in (Henderson, 2003b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-1013.txt | Citing Article:  P14-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, these parsers are among the best in the literature, with a test performance of 90.7 F1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)).</S> | Reference Offset:  ['10','166'] | Reference Text:  <S sid = 10 ssid = >The parser which uses this approach outperforms both a generative model and a discriminative model, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).</S><S sid = 166 ssid = >Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-1013.txt | Citing Article:  P07-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made.</S> | Reference Offset:  ['21','56'] | Reference Text:  <S sid = 21 ssid = >As with many previous statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), we use a history-based model of parsing.</S><S sid = 56 ssid = >Both the parse history d1,..., di−1 and the lookahead string yield(di,..., dm) grow with the length of the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-1013.txt | Citing Article:  P07-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the discriminative training techniques successfully applied in (Henderson, 2004) to the feed-forward neural network model can be directly applied to the mean field model proposed in this paper.</S> | Reference Offset:  ['79','167'] | Reference Text:  <S sid = 79 ssid = >Training is applied to this full neural network, as described in the next section.</S><S sid = 167 ssid = >This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-1013.txt | Citing Article:  P06-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other techniques are also possible; Henderson (2004) uses neural networks to induce latent left-corner parser states.</S> | Reference Offset:  ['0','57'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training Of A Neural Network Statistical Parser</S><S sid = 57 ssid = >In order to apply standard probability estimation methods, we use neural networks to induce finite representations of both these sequences, which we will denote h(d1,..., di−1) and l(yield(di,..., dm)), respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-1013.txt | Citing Article:  P08-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','169'] | Reference Text:  <S sid = 53 ssid = >In contrast, the parameters of the generative model only include words which are either already incorporated into the structure, or are the immediate next word to be incorporated.</S><S sid = 169 ssid = >This approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing, which have parameterizations which depart substantially from the successful previous generative models of parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-1013.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','169'] | Reference Text:  <S sid = 53 ssid = >In contrast, the parameters of the generative model only include words which are either already incorporated into the structure, or are the immediate next word to be incorporated.</S><S sid = 169 ssid = >This approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing, which have parameterizations which depart substantially from the successful previous generative models of parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-1013.txt | Citing Article:  P08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al, 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent.</S> | Reference Offset:  ['80','81'] | Reference Text:  <S sid = 80 ssid = >As with many other machine learning methods, training a Simple Synchrony Network involves first defining an appropriate learning criteria and then performing some form of gradient descent learning to search for the optimum values of the network’s parameters according to this criteria.</S><S sid = 81 ssid = >In all the parsing models investigated here, we use the on-line version of Backpropagation to perform the gradient descent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-1013.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model.</S> | Reference Offset:  ['19','28'] | Reference Text:  <S sid = 19 ssid = >This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a).</S><S sid = 28 ssid = >Parsing a constituent starts by pushing the leftmost word w of the constituent onto the stack with a shift(w) action.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-1013.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, as with any generative model, it may be easy to improve the parser's accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even with out introduction of any additional linguistic features.</S> | Reference Offset:  ['19','138'] | Reference Text:  <S sid = 19 ssid = >This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a).</S><S sid = 138 ssid = >Standard measures of accuracy are shown in table 1.8 The largest accuracy difference is between the parser with the discriminative probability model (DSSN-Freq>5) and those with the generative probability model, despite the larger vocabulary of the former.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-1013.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We would expect further improvement of ISBN results if we applied discriminative retraining (Henderson, 2004) or reranking with data-defined kernels (Henderson and Titov, 2005), even without introduction of any additional features.</S> | Reference Offset:  ['19','77'] | Reference Text:  <S sid = 19 ssid = >This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a).</S><S sid = 77 ssid = >The input features for these loglinear models are the real-valued vectors computed by h(d1,..., di−1) and l(yield(di,..., dm)), as explained in more detail in (Henderson, 2003b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-1013.txt | Citing Article:  P06-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Henderson (2004) finds that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of his generative model.</S> | Reference Offset:  ['4','161'] | Reference Text:  <S sid = 4 ssid = >We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.</S><S sid = 161 ssid = >Both these methods are limited to reranking the output of another parser, while our trained parser can be used alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-1013.txt | Citing Article:  P06-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','169'] | Reference Text:  <S sid = 53 ssid = >In contrast, the parameters of the generative model only include words which are either already incorporated into the structure, or are the immediate next word to be incorporated.</S><S sid = 169 ssid = >This approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing, which have parameterizations which depart substantially from the successful previous generative models of parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P04-1013.txt | Citing Article:  D07-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, as with any generative model, it should be easy to improve the parser's accuracy with discriminative reranking, such as discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without the introduction of any additional linguistic features.</S> | Reference Offset:  ['19','138'] | Reference Text:  <S sid = 19 ssid = >This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a).</S><S sid = 138 ssid = >Standard measures of accuracy are shown in table 1.8 The largest accuracy difference is between the parser with the discriminative probability model (DSSN-Freq>5) and those with the generative probability model, despite the larger vocabulary of the former.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P04-1013.txt | Citing Article:  P13-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains.</S> | Reference Offset:  ['11','59'] | Reference Text:  <S sid = 11 ssid = >To compare these different approaches, we use a neural network architecture called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001) to estimate the parameters of the probability models.</S><S sid = 59 ssid = >Of the previous work on using neural networks for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks.</S> | Discourse Facet:  NA | Annotator: Automatic


