Citance Number: 1 | Reference Article:  N04-1043.txt | Citing Article:  W04-2603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Miller et al (2004) describe a relevant technique for the latter.</S> | Reference Offset:  ['3','24'] | Reference Text:  <S sid = 3 ssid = >We evaluate the technique for named-entity tagging.</S><S sid = 24 ssid = >However, we also hoped that a third technique, active learning (Cohn et al., 1996; McCallum and Nigam, 1998), would be particularly effective when used in conjunction with hierarchical word clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1043.txt | Citing Article:  W06-1615.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004).</S> | Reference Offset:  ['103','115'] | Reference Text:  <S sid = 103 ssid = >Immediately, with only 5,000 words of training, the discriminative model significantly outperforms the HMM.</S><S sid = 115 ssid = >The work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Lin et al., 2003; Boschee et al, 2002; Collins and Singer, 1999; Yarowsky, 1995) that all focuses on reducing annotation requirements through a combination of (a) seed examples, (b) large unannotated corpora, and (c) training example selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1043.txt | Citing Article:  P08-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004).</S> | Reference Offset:  ['72','121'] | Reference Text:  <S sid = 72 ssid = >Clusters of various granularity are specified by prefixes of the bit strings.</S><S sid = 121 ssid = >Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1043.txt | Citing Article:  P08-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.</S> | Reference Offset:  ['44','72'] | Reference Text:  <S sid = 44 ssid = >Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990).</S><S sid = 72 ssid = >Clusters of various granularity are specified by prefixes of the bit strings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1043.txt | Citing Article:  P08-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing.</S> | Reference Offset:  ['2','75'] | Reference Text:  <S sid = 2 ssid = >Active learning is used to select training examples.</S><S sid = 75 ssid = >We used 4 different prefix lengths: 8 bit, 12 bit, 16 bit, and 20 bit.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1043.txt | Citing Article:  P08-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach.</S> | Reference Offset:  ['57','108'] | Reference Text:  <S sid = 57 ssid = >To implement discriminative training, we followed the averaged perceptron approach of (Collins, 2002).</S><S sid = 108 ssid = >Figure 3 shows (a) discriminative tagger performance without cluster features, (b) the same tagger using active learning, (c) the discriminative tagger with cluster features, and (d) the discriminative tagger with cluster features using active learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1043.txt | Citing Article:  P11-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER.</S> | Reference Offset:  ['15','44'] | Reference Text:  <S sid = 15 ssid = >First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).</S><S sid = 44 ssid = >Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1043.txt | Citing Article:  D11-1133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training).</S> | Reference Offset:  ['50','104'] | Reference Text:  <S sid = 50 ssid = >All experimental results given in this paper are with the Spatter clustering algorithm.</S><S sid = 104 ssid = >With 50,000 words of training, performance for the discriminative model exceeds 90F, a level not reached by the HMM until it has observed 150,000 words of training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1043.txt | Citing Article:  P08-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','128'] | Reference Text:  <S sid = 63 ssid = >Instead, we simply iterated for 5 epochs in all cases, regardless of the training set size or number of features used.</S><S sid = 128 ssid = >At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1043.txt | Citing Article:  P11-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010).</S> | Reference Offset:  ['0','44'] | Reference Text:  <S sid = 0 ssid = >Name Tagging With Word Clusters And Discriminative Training</S><S sid = 44 ssid = >Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1043.txt | Citing Article:  W06-0206.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).</S> | Reference Offset:  ['59','115'] | Reference Text:  <S sid = 59 ssid = >First, the method performed nearly as well as the currently best global discriminative model (Sha and Pereira, 2003), as evaluated on one of the few tasks for which there are any published results (noun phrase chunking).</S><S sid = 115 ssid = >The work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Lin et al., 2003; Boschee et al, 2002; Collins and Singer, 1999; Yarowsky, 1995) that all focuses on reducing annotation requirements through a combination of (a) seed examples, (b) large unannotated corpora, and (c) training example selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1043.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008).</S> | Reference Offset:  ['3','15'] | Reference Text:  <S sid = 3 ssid = >We evaluate the technique for named-entity tagging.</S><S sid = 15 ssid = >First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1043.txt | Citing Article:  P09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992).</S> | Reference Offset:  ['15','44'] | Reference Text:  <S sid = 15 ssid = >First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).</S><S sid = 44 ssid = >Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1043.txt | Citing Article:  C10-2137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Name Tagging With Word Clusters And Discriminative Training</S><S sid = 1 ssid = >We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1043.txt | Citing Article:  D11-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008).</S> | Reference Offset:  ['3','15'] | Reference Text:  <S sid = 3 ssid = >We evaluate the technique for named-entity tagging.</S><S sid = 15 ssid = >First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1043.txt | Citing Article:  D11-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008).</S> | Reference Offset:  ['3','15'] | Reference Text:  <S sid = 3 ssid = >We evaluate the technique for named-entity tagging.</S><S sid = 15 ssid = >First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1043.txt | Citing Article:  W12-1914.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','128'] | Reference Text:  <S sid = 63 ssid = >Instead, we simply iterated for 5 epochs in all cases, regardless of the training set size or number of features used.</S><S sid = 128 ssid = >At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1043.txt | Citing Article:  N09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node.</S> | Reference Offset:  ['51','52'] | Reference Text:  <S sid = 51 ssid = >The result of running the clustering algorithm is a binary tree, where each word occupies a single leaf node, and where each leaf node contains a single word.</S><S sid = 52 ssid = >The root node defines a cluster containing the entire vocabulary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1043.txt | Citing Article:  N09-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain.</S> | Reference Offset:  ['43','121'] | Reference Text:  <S sid = 43 ssid = >Armed with modern discriminative training methods, it seemed reasonable to us to revisit hierarchical clustering.</S><S sid = 121 ssid = >Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1043.txt | Citing Article:  P08-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data.</S> | Reference Offset:  ['69','95'] | Reference Text:  <S sid = 69 ssid = >Our model uses a total of 19 classes of features.</S><S sid = 95 ssid = >For this experiment, we used all of the features described in Section 3 except word cluster features.</S> | Discourse Facet:  NA | Annotator: Automatic


