Citance Number: 1 | Reference Article:  E03-1008.txt | Citing Article:  N03-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have explored using different settings for the seed set size (Steedman et al, 2003).</S> | Reference Offset:  ['16','143'] | Reference Text:  <S sid = 16 ssid = >Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.</S><S sid = 143 ssid = >We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E03-1008.txt | Citing Article:  D12-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup.</S> | Reference Offset:  ['6','90'] | Reference Text:  <S sid = 6 ssid = >Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.</S><S sid = 90 ssid = >The results are shown in Figure 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','150'] | Reference Text:  <S sid = 55 ssid = >Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.</S><S sid = 150 ssid = >We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','150'] | Reference Text:  <S sid = 55 ssid = >Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.</S><S sid = 150 ssid = >We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets.</S> | Reference Offset:  ['59','107'] | Reference Text:  <S sid = 59 ssid = >Second, we do not require the two parsers to have the same training sets.</S><S sid = 107 ssid = >As the amount of seed data increases, coverage becomes less of a problem, and the co-training advantage is diminished.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data.</S> | Reference Offset:  ['79','115'] | Reference Text:  <S sid = 79 ssid = >Therefore we used two sizes of seed data: 500 and 1,000 sentences, to see if cotraining could improve parser performance using these small amounts of labelled seed data.</S><S sid = 115 ssid = >1,000 annotated sentences from the Brown section of the Penn Treebank were used as the seed data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data.</S> | Reference Offset:  ['79','145'] | Reference Text:  <S sid = 79 ssid = >Therefore we used two sizes of seed data: 500 and 1,000 sentences, to see if cotraining could improve parser performance using these small amounts of labelled seed data.</S><S sid = 145 ssid = >Co-training performance may improve if we consider co-training using sub-parses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere).</S> | Reference Offset:  ['11','87'] | Reference Text:  <S sid = 11 ssid = >Previous work in co-training statistical parsers (Sarkar, 2001) used two components of a single parsing framework (that is, a parser and a supertagger for that parser).</S><S sid = 87 ssid = >Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E03-1008.txt | Citing Article:  P07-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','150'] | Reference Text:  <S sid = 55 ssid = >Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.</S><S sid = 150 ssid = >We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E03-1008.txt | Citing Article:  P06-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains.</S> | Reference Offset:  ['25','70'] | Reference Text:  <S sid = 25 ssid = >To apply the theory of co-training to parsing, we need to ensure that each parser is capable of learning the parsing task alone and that the two parsers have different views.</S><S sid = 70 ssid = >The domains over which the two models operate are quite distinct.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E03-1008.txt | Citing Article:  W08-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006).</S> | Reference Offset:  ['86','131'] | Reference Text:  <S sid = 86 ssid = >Self-training experiments were conducted in which each parser was retrained on its own output.</S><S sid = 131 ssid = >The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E03-1008.txt | Citing Article:  W07-2204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003).</S> | Reference Offset:  ['86','87'] | Reference Text:  <S sid = 86 ssid = >Self-training experiments were conducted in which each parser was retrained on its own output.</S><S sid = 87 ssid = >Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E03-1008.txt | Citing Article:  W09-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results.</S> | Reference Offset:  ['15','126'] | Reference Text:  <S sid = 15 ssid = >In Section 4 we show that co-training outperforms self-training, and that co-training is most beneficial when the seed set of manually created parses is small.</S><S sid = 126 ssid = >While the amount of improvement in performance is less than the previous case, co-training provides an additional boost to the parsing performance, to 78.7%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E03-1008.txt | Citing Article:  W09-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration.</S> | Reference Offset:  ['84','97'] | Reference Text:  <S sid = 84 ssid = >Since we need to parse all sentences in section 0 at each iteration, in the experiments reported in this paper we only evaluated one of the parsers, the Collins-CFG parser, at each iteration.</S><S sid = 97 ssid = >During each co-training round, the LTAG parser parsed 30 sentences, and the 20 labelled sentences with the highest scores (according to the LTAG joint probability) were added to the training data of the Collins-CFG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E03-1008.txt | Citing Article:  N06-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing.</S> | Reference Offset:  ['89','93'] | Reference Text:  <S sid = 89 ssid = >Self-training was used by Charniak (1997), where a modest gain was reported after re-training his parser on 30 million words.</S><S sid = 93 ssid = >During each round of self-training, 30 sentences were parsed by each parser, and each parser was retrained upon the 20 self-labelled sentences which it scored most highly (each parser using its own joint probability (equation 1) as the score).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E03-1008.txt | Citing Article:  N06-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','150'] | Reference Text:  <S sid = 55 ssid = >Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.</S><S sid = 150 ssid = >We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E03-1008.txt | Citing Article:  P06-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bootstrapping was applied to syntax learning by Steedman et al (2003).</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Bootstrapping Statistical Parsers From Small Datasets</S><S sid = 14 ssid = >Section 3 considers how co-training applied to training statistical parsers can be made computationally viable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E03-1008.txt | Citing Article:  D09-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.</S> | Reference Offset:  ['12','38'] | Reference Text:  <S sid = 12 ssid = >In contrast, this paper considers co-training two diverse statistical parsers: the Collins lexicalized PCFG parser and a Lexicalized Tree Adjoining Grammar (LTAG) parser.</S><S sid = 38 ssid = >That is, a subset of those sentences labelled by the LTAG parser is added to the training set of the Collins PCFG parser, and vice versa.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E03-1008.txt | Citing Article:  W03-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','150'] | Reference Text:  <S sid = 55 ssid = >Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.</S><S sid = 150 ssid = >We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E03-1008.txt | Citing Article:  W03-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training.</S> | Reference Offset:  ['5','61'] | Reference Text:  <S sid = 5 ssid = >In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data.</S><S sid = 61 ssid = >In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.</S> | Discourse Facet:  NA | Annotator: Automatic


