Citance Number: 1 | Reference Article:  W08-0309.txt | Citing Article:  D08-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al, 2008).</S> | Reference Offset:  ['5','53'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 53 ssid = >The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W08-0309.txt | Citing Article:  W12-3106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks.</S> | Reference Offset:  ['5','221'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 221 ssid = >This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W08-0309.txt | Citing Article:  E09-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on SMT (Callison-Burch et al, 2008), called newstest 2008 in the following.</S> | Reference Offset:  ['5','22'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 22 ssid = >We refer to this as the News test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W08-0309.txt | Citing Article:  E09-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al, 2008).</S> | Reference Offset:  ['32','221'] | Reference Text:  <S sid = 32 ssid = >For instance the Edinburgh French-English system has a BLEU score of 26.8 on the part that was originally Spanish, but a score of on 9.7 on the part that was originally Hungarian.</S><S sid = 221 ssid = >This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W08-0309.txt | Citing Article:  W09-0422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >PC Translator this year and also in Callison-Burch et al (2008).</S> | Reference Offset:  ['5','29'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 29 ssid = >For instance, is it easier, when the machine translation system translates in the same direction as the human translator?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W08-0309.txt | Citing Article:  W09-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).</S> | Reference Offset:  ['5','19'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 19 ssid = >2 Overview of the shared translation task The shared translation task consisted of 10 language pairs: English to German, German to English, English to Spanish, Spanish to English, English to French, French to English, English to Czech, Czech to English, Hungarian to English, and German to Spanish.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W08-0309.txt | Citing Article:  W09-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >RWTH participated in this shared task with the two most promising metrics according to the previous experiments ,i.e. POSBLEU and POSF, and the detailed results can be found in (Callison-Burch et al, 2008).</S> | Reference Offset:  ['5','17'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 17 ssid = >Section 6 presents the results of the evaluation task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W08-0309.txt | Citing Article:  W11-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks.</S> | Reference Offset:  ['5','19'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 19 ssid = >2 Overview of the shared translation task The shared translation task consisted of 10 language pairs: English to German, German to English, English to Spanish, Spanish to English, English to French, French to English, English to Czech, Czech to English, Hungarian to English, and German to Spanish.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W08-0309.txt | Citing Article:  W11-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The system level evaluation procedure follows WMT08 (Callison-Burch et al., 2008), which ranked each system submitted on WMT08 in three types of tasks: Rank: Human judges candidate sentence ranking order of quality.</S> | Reference Offset:  ['3','102'] | Reference Text:  <S sid = 3 ssid = >We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.</S><S sid = 102 ssid = >The order of the types of evaluation were randomized.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W08-0309.txt | Citing Article:  W11-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >test set in WMT08 (Callison-Burch et al, 2008).</S> | Reference Offset:  ['5','8'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 8 ssid = >This out-of-domain test set contrasts with the in-domain Europarl test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W08-0309.txt | Citing Article:  E12-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For German, Spanish and Czech we use the news test sets proposed in (Callison-Burch et al 2010), for French and Italian the news test sets presented in (Callison-Burch et al 2008), for Arabic, Farsi and Turkish, sets of 2,000 news sentences extracted from the Arabic-English and English-Persian datasets and the SE-Times corpus.</S> | Reference Offset:  ['19','99'] | Reference Text:  <S sid = 19 ssid = >2 Overview of the shared translation task The shared translation task consisted of 10 language pairs: English to German, German to English, English to Spanish, Spanish to English, English to French, French to English, English to Czech, Czech to English, Hungarian to English, and German to Spanish.</S><S sid = 99 ssid = >It was selected as few as 5% of the time for the EnglishSpanish News task to as many as 12.5% for the Czech-English News task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W08-0309.txt | Citing Article:  W09-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al, 2008).</S> | Reference Offset:  ['5','29'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 29 ssid = >For instance, is it easier, when the machine translation system translates in the same direction as the human translator?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W08-0309.txt | Citing Article:  D09-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Callison-Burch et al (2008), we assigned a score to each of the 11 MT systems based on how of ten its translations were judged to be better than or equal to any other system.</S> | Reference Offset:  ['148','149'] | Reference Text:  <S sid = 148 ssid = >We assigned a ranking to the systems for each of the three types of manual evaluation based on: • The percent of time that the sentences it produced were judged to be better than or equal to the translations of any other system.</S><S sid = 149 ssid = >• The percent of time that its constituent translations were judged to be better than or equal to the translations of any other system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W08-0309.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Detailed token and type statistics can be found in Callison-Burch et al (2008).</S> | Reference Offset:  ['5','221'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 221 ssid = >This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W08-0309.txt | Citing Article:  W11-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the model is tuned with mert (Bertoldi, et al) 5) the official test set from ACL WMT 2008 (Callison-Burch et al, 2008), consisting of 2000 sentences, is used as test set.</S> | Reference Offset:  ['5','123'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 123 ssid = >Like the other system combination entrants, it was tuned on the Europarl test set and tested on the News test set, using systems that submitted entries to both tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W08-0309.txt | Citing Article:  P11-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al, 2008, 2010), assessing the performance of the propose devaluation metric at the sentence level using ranking preference consistency, which also known as Kendall's rank correlation coefficient, to evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking.</S> | Reference Offset:  ['147','198'] | Reference Text:  <S sid = 147 ssid = >To measure the correlation of the automatic metrics with the human judgments of translation quality at the system-level we used Spearman’s rank correlation coefficient p. We converted the raw scores assigned each system into ranks.</S><S sid = 198 ssid = >The average correlation coefficient between the constituent-based judgments with the sentence ranking judgments is only p = 0.51.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W08-0309.txt | Citing Article:  P09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al, 2008).</S> | Reference Offset:  ['169','228'] | Reference Text:  <S sid = 169 ssid = >Because the systems generally assign real numbers as scores, we excluded pairs that the human annotators ranked as ties.</S><S sid = 228 ssid = >In addition, we evaluated seven commercial rule-based MT systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W08-0309.txt | Citing Article:  P09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For details, see Callison-Burch et al (2008).</S> | Reference Offset:  ['5','221'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 221 ssid = >This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W08-0309.txt | Citing Article:  W11-2105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >only ,i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al, 2008).</S> | Reference Offset:  ['84','149'] | Reference Text:  <S sid = 84 ssid = >The same is done for sentence and each of the system translations.</S><S sid = 149 ssid = >• The percent of time that its constituent translations were judged to be better than or equal to the translations of any other system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W08-0309.txt | Citing Article:  P09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al, 2008).</S> | Reference Offset:  ['5','53'] | Reference Text:  <S sid = 5 ssid = >This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).</S><S sid = 53 ssid = >The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


