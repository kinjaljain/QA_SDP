Citance Number: 1 | Reference Article:  N01-1025.txt | Citing Article:  N03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a support vector machine (SVM) based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process.</S> | Reference Offset:  ['0','189'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 189 ssid = >In this paper, we introduce a uniform framework for chunking task based on Support Vector Machines (SVMs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N01-1025.txt | Citing Article:  N03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the chunker yamcha (Kudo and Matsumoto,2001), which is based on support vector machines (Vapnik, 1998).</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 15 ssid = >New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) have been proposed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N01-1025.txt | Citing Article:  N06-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the chunking task, the bagged model also outperforms the models of Kudo and Matsumoto (2001).</S> | Reference Offset:  ['176','190'] | Reference Text:  <S sid = 176 ssid = >As far as accuracies are concerned, our model outperforms Tjong Kim Sangâ€™s model.</S><S sid = 190 ssid = >Experimental results on WSJ corpus show that our method outperforms other conventional machine learning frameworks such MBL and Maximum Entropy Models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N01-1025.txt | Citing Article:  N06-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['95','192'] | Reference Text:  <S sid = 95 ssid = >The technique can be regarded as a sort of Dynamic Programming (DP) matching, in which the best answer is searched by maximizing the total certainty score for the combination of tags.</S><S sid = 192 ssid = >In addition, we achieve higher accuracy by applying weighted voting of 8-SVM based systems which are trained using distinct chunk representations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N01-1025.txt | Citing Article:  C04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the chunker YamCha (Kudo and Matsumoto, 2001).</S> | Reference Offset:  ['20','131'] | Reference Text:  <S sid = 20 ssid = >(Joachims, 1998; Taira and Haruno, 1999; Kudo and Matsumoto, 2000a).</S><S sid = 131 ssid = >Although we can use models with IOBES-F or IOBES-B representations for the committees for the weighted voting, we do not use them in our voting experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N01-1025.txt | Citing Article:  W06-2404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We employ the technique of Support Vector Machines (SVMs) (Vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (Kudo and Matsumoto, 2001) and named entity chunking (Mayfield et al, 2003).</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 7 ssid = >Various NLP tasks can be seen as a chunking task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N01-1025.txt | Citing Article:  W06-2404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we use an SVMs-based chunking tool YamCha (Kudo and Matsumoto, 2001).</S> | Reference Offset:  ['151','189'] | Reference Text:  <S sid = 151 ssid = >Table 2 shows results of our SVMs based chunking with individual chunk representations.</S><S sid = 189 ssid = >In this paper, we introduce a uniform framework for chunking task based on Support Vector Machines (SVMs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N01-1025.txt | Citing Article:  P03-2039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a Support Vector Machines-basedchunker, YamCha (Kudo and Matsumoto, 2001), to extract unknown words from the output of the morphological analysis.</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 21 ssid = >In this paper, we apply Support Vector Machines to the chunking task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N01-1025.txt | Citing Article:  D09-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001).</S> | Reference Offset:  ['7','182'] | Reference Text:  <S sid = 7 ssid = >Various NLP tasks can be seen as a chunking task.</S><S sid = 182 ssid = >Incorporating variable context length model In our experiments, we simply use the socalled fixed context length model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N01-1025.txt | Citing Article:  P06-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As regards the basic feature set for Chunking, we followed (Kudo and Matsumoto, 2001), which is the same feature set that provided the best result in CoNLL-2000.</S> | Reference Offset:  ['143','155'] | Reference Text:  <S sid = 143 ssid = >Chunking data set (chunking) This data set was used for CoNLL-2000 shared task(Tjong Kim Sang and Buchholz, 2000).</S><S sid = 155 ssid = >Table 4 shows the precision, recall and of the best result for each data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N01-1025.txt | Citing Article:  P06-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With Chunking, (Kudo and Matsumoto, 2001) reported the best F-score of 93.91 with the voting of several models trained by Support Vec tor Machine in the same experimental settings and with the same feature set.</S> | Reference Offset:  ['178','190'] | Reference Text:  <S sid = 178 ssid = >By combining weighted voting schemes, we achieve accuracy of 93.91.</S><S sid = 190 ssid = >Experimental results on WSJ corpus show that our method outperforms other conventional machine learning frameworks such MBL and Maximum Entropy Models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N01-1025.txt | Citing Article:  D09-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001).</S> | Reference Offset:  ['22','192'] | Reference Text:  <S sid = 22 ssid = >In addition, in order to achieve higher accuracy, we apply weighted voting of 8 SVM-based systems which are trained using distinct chunk representations.</S><S sid = 192 ssid = >In addition, we achieve higher accuracy by applying weighted voting of 8-SVM based systems which are trained using distinct chunk representations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N01-1025.txt | Citing Article:  I08-4033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004).</S> | Reference Offset:  ['11','102'] | Reference Text:  <S sid = 11 ssid = >Various machine learning approaches have been proposed for chunking (Ramshaw and Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000).</S><S sid = 102 ssid = >Tjong Kim Sang et al. report that they achieve higher accuracy by applying weighted voting of systems which are trained using distinct chunk representations and different machine learning algorithms, such as MBL, ME and IGTree(Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N01-1025.txt | Citing Article:  P05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See, for example, NPchunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001).</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 21 ssid = >In this paper, we apply Support Vector Machines to the chunking task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N01-1025.txt | Citing Article:  P06-2042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used YamCha (Kudo and Matsumoto, 2001) as a text chunker, which is based on Support Vector Machine (SVM).</S> | Reference Offset:  ['0','189'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 189 ssid = >In this paper, we introduce a uniform framework for chunking task based on Support Vector Machines (SVMs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N01-1025.txt | Citing Article:  W03-1720.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, we introduce a Support Vector Machine (below SVM)-based chunker (Kudo and Matsumoto, 2001) to cover the errors made by the segmenter.</S> | Reference Offset:  ['0','189'] | Reference Text:  <S sid = 0 ssid = >Chunking With Support Vector Machines</S><S sid = 189 ssid = >In this paper, we introduce a uniform framework for chunking task based on Support Vector Machines (SVMs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N01-1025.txt | Citing Article:  W10-4141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wu et al (2007, 2008) presented an automatic chunk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 (Kudo and Matsumoto, 2001) chunk representation structures with either left-to-right or right-to left directions.</S> | Reference Offset:  ['92','93'] | Reference Text:  <S sid = 92 ssid = >In addition, we can reverse the parsing direction (from right to left) by using two chunk tags which appear to the r.h.s. of the current token ( ).</S><S sid = 93 ssid = >In this paper, we call the method which parses from left to right as forward parsing, and the method which parses from right to left as backward parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N01-1025.txt | Citing Article:  H05-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sha and Pereira (2003) reported the Kudo and Matsumoto (2001) performance on the NP-Chunking task to be 94.39 and to be the best reported result on this task.</S> | Reference Offset:  ['7','21'] | Reference Text:  <S sid = 7 ssid = >Various NLP tasks can be seen as a chunking task.</S><S sid = 21 ssid = >In this paper, we apply Support Vector Machines to the chunking task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N01-1025.txt | Citing Article:  I08-5008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.</S> | Reference Offset:  ['143','177'] | Reference Text:  <S sid = 143 ssid = >Chunking data set (chunking) This data set was used for CoNLL-2000 shared task(Tjong Kim Sang and Buchholz, 2000).</S><S sid = 177 ssid = >In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N01-1025.txt | Citing Article:  C02-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.</S> | Reference Offset:  ['143','177'] | Reference Text:  <S sid = 143 ssid = >Chunking data set (chunking) This data set was used for CoNLL-2000 shared task(Tjong Kim Sang and Buchholz, 2000).</S><S sid = 177 ssid = >In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5.</S> | Discourse Facet:  NA | Annotator: Automatic


