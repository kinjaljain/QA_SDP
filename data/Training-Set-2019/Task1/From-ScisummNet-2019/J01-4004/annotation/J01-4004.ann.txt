Citance Number: 1 | Reference Article:  J01-4004.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0).</S> | Reference Offset:  ['306','307'] | Reference Text:  <S sid = 306 ssid = >The work of Aone and Bennett (1995), McCarthy and Lehnert (1995), Fisher et al. (1995), and McCarthy (1996) employed decision tree learning.</S><S sid = 307 ssid = >The RESOLVE system is presented in McCarthy and Lehnert (1995), Fisher et al. (1995), and McCarthy (1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J01-4004.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain.</S> | Reference Offset:  ['228','307'] | Reference Text:  <S sid = 228 ssid = >The RESOLVE system is described in three papers: McCarthy and Lehnert (1995), Fisher et al. (1995), and McCarthy (1996).</S><S sid = 307 ssid = >The RESOLVE system is presented in McCarthy and Lehnert (1995), Fisher et al. (1995), and McCarthy (1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J01-4004.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Soon et al (2001) use twelve features (see Table 1).</S> | Reference Offset:  ['196','241'] | Reference Text:  <S sid = 196 ssid = >Table 3 and Table 4 show the results of the experiment.</S><S sid = 241 ssid = >RESOLVE makes use of 39 features, considerably more than our system's 12 features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J01-4004.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data.</S> | Reference Offset:  ['204','330'] | Reference Text:  <S sid = 204 ssid = >Since these features are highly informative, we were curious to see how much they contribute to our MUC-6 and MUC-7 results of 62.6% and 60.4%, respectively.</S><S sid = 330 ssid = >We evaluated our approach on common data sets, namely, the MUC-6 and MUC-7 coreference corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J01-4004.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs.</S> | Reference Offset:  ['193','313'] | Reference Text:  <S sid = 193 ssid = >One factor that affects the performance of a machine learning approach is the set of features used.</S><S sid = 313 ssid = >The features used were slightly changed for this domain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J01-4004.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001).</S> | Reference Offset:  ['220','240'] | Reference Text:  <S sid = 220 ssid = >However, its F-measure is comparable to that of STR_MATCH.</S><S sid = 240 ssid = >For MUC-7, there is no drop in F-measure; for MUC-6, the F-measure dropped slightly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J01-4004.txt | Citing Article:  P14-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While the second best system (Bjorkelund and Farkas, 2012) followed the widely used baseline of Soon et al (2001), the winning system (Fernandes et al, 2012) proposed the use of a tree representation.</S> | Reference Offset:  ['134','312'] | Reference Text:  <S sid = 134 ssid = >C5 is a commonly used decision tree learning algorithm and thus it may be considered as a baseline method against which other learning algorithms can be compared.</S><S sid = 312 ssid = >Fisher et al. (1995) adapted RESOLVE to work in MUC-6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J01-4004.txt | Citing Article:  P14-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3).</S> | Reference Offset:  ['143','145'] | Reference Text:  <S sid = 143 ssid = >For each j, the algorithm considers every markable i before j as a potential antecedent.</S><S sid = 145 ssid = >A coreferring antecedent is found if the classifier returns true.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J01-4004.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set.</S> | Reference Offset:  ['188','312'] | Reference Text:  <S sid = 188 ssid = >Our result is encouraging since it indicates that a learning approach using relatively shallow features can achieve scores comparable to those of systems built using nonlearning approaches.</S><S sid = 312 ssid = >Fisher et al. (1995) adapted RESOLVE to work in MUC-6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J01-4004.txt | Citing Article:  N06-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instances are created following Soon et al (2001).</S> | Reference Offset:  ['247','312'] | Reference Text:  <S sid = 247 ssid = >The following two subsections describe the errors in more detail.</S><S sid = 312 ssid = >Fisher et al. (1995) adapted RESOLVE to work in MUC-6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J01-4004.txt | Citing Article:  N06-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Ng & Cardie (2002), our baseline system reimplements the Soon et al (2001) system.</S> | Reference Offset:  ['211','247'] | Reference Text:  <S sid = 211 ssid = >Other baseline systems that are used are ONE_CHAIN, ONE_WRD, and HD_WRD (Cardie and Wagstaff 1999).</S><S sid = 247 ssid = >The following two subsections describe the errors in more detail.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J01-4004.txt | Citing Article:  N06-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE).</S> | Reference Offset:  ['53','206'] | Reference Text:  <S sid = 53 ssid = >The distance feature has different effects on different noun phrases.</S><S sid = 206 ssid = >In terms of absolute Fmeasure, the difference between using these three features and using all features is 2.3% for MUC-6 and 1% for MUC-7; in other words, the other nine features contribute just 2.3% and 1% more for each of the MUC years.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J01-4004.txt | Citing Article:  S10-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference.</S> | Reference Offset:  ['32','232'] | Reference Text:  <S sid = 32 ssid = >We also implemented a module that recognizes MUC-style named entities, that is, organization, person, location, date, time, money, and percent.</S><S sid = 232 ssid = >On the other hand, the markables extracted by our system include nested noun phrases, MUC-style named entity types (money, percent, date, etc.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J01-4004.txt | Citing Article:  S10-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set.</S> | Reference Offset:  ['242','312'] | Reference Text:  <S sid = 242 ssid = >RESOLVE's feature set includes the two highly informative features, ALIAS and STR_MATCH.</S><S sid = 312 ssid = >Fisher et al. (1995) adapted RESOLVE to work in MUC-6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J01-4004.txt | Citing Article:  P11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier.</S> | Reference Offset:  ['145','312'] | Reference Text:  <S sid = 145 ssid = >A coreferring antecedent is found if the classifier returns true.</S><S sid = 312 ssid = >Fisher et al. (1995) adapted RESOLVE to work in MUC-6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J01-4004.txt | Citing Article:  C10-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a).</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >A Machine Learning Approach To Coreference Resolution Of Noun Phrases</S><S sid = 20 ssid = >We adopt a corpus-based, machine learning approach to noun phrase coreference resolution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J01-4004.txt | Citing Article:  D08-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al, 2001).</S> | Reference Offset:  ['0','57'] | Reference Text:  <S sid = 0 ssid = >A Machine Learning Approach To Coreference Resolution Of Noun Phrases</S><S sid = 57 ssid = >Such knowledge sources are commonly used for the task of determining coreference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J01-4004.txt | Citing Article:  D08-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold.</S> | Reference Offset:  ['80','156'] | Reference Text:  <S sid = 80 ssid = >Since WordNet orders the senses of a noun by their frequency, this is equivalent to choosing the most frequent sense as the semantic class for each noun.</S><S sid = 156 ssid = >We use the same method to generate coreference chains for both MUC-6 and MUC7, except for the following.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J01-4004.txt | Citing Article:  D08-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001).</S> | Reference Offset:  ['50','55'] | Reference Text:  <S sid = 50 ssid = >One important factor is the distance between the two markables.</S><S sid = 55 ssid = >We include the distance feature so that the learning algorithm can best decide the distribution for different classes of noun phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J01-4004.txt | Citing Article:  P10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001).</S> | Reference Offset:  ['196','205'] | Reference Text:  <S sid = 196 ssid = >Table 3 and Table 4 show the results of the experiment.</S><S sid = 205 ssid = >Systems ALIAS_STR and ALIAS_STR_APPOS in Table 3 and Table 4 show the results of the experiment.</S> | Discourse Facet:  NA | Annotator: Automatic


