Citance Number: 1 | Reference Article:  P05-1077.txt | Citing Article:  P14-2081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010).</S> | Reference Offset:  ['4','197'] | Reference Text:  <S sid = 4 ssid = >In the last decade, the field of Natural Language Processing (NLP), has seen a surge in the use of corpus motivated techniques.</S><S sid = 197 ssid = >However, most language analysis tools are too infeasible to run on the scale of the web.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1077.txt | Citing Article:  W11-2504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['64','205'] | Reference Text:  <S sid = 64 ssid = >This algorithm was further improved by Charikar (2002).</S><S sid = 205 ssid = >We wish to thank USC Center for High Performance Computing and Communications (HPCC) for helping us use their cluster computers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1077.txt | Citing Article:  P08-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time.</S> | Reference Offset:  ['36','84'] | Reference Text:  <S sid = 36 ssid = >Using fast search algorithm to find nearest neighbors.</S><S sid = 84 ssid = >B of its closest neighbors in the sorted list.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1077.txt | Citing Article:  P12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005).</S> | Reference Offset:  ['129','133'] | Reference Text:  <S sid = 129 ssid = >We use the context words as features of the noun vector.</S><S sid = 133 ssid = >We do not use grammatical features in the web corpus since parsing is generally not easily web scalable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1077.txt | Citing Article:  P06-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005).</S> | Reference Offset:  ['66','120'] | Reference Text:  <S sid = 66 ssid = >The algorithm given in Charikar (2002) is described to find the nearest neighbor for a given vector.</S><S sid = 120 ssid = >We eliminate duplicate and near duplicate documents by using the algorithm described by Kolcz et al. (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1077.txt | Citing Article:  P07-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['64','205'] | Reference Text:  <S sid = 64 ssid = >This algorithm was further improved by Charikar (2002).</S><S sid = 205 ssid = >We wish to thank USC Center for High Performance Computing and Communications (HPCC) for helping us use their cluster computers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002).</S> | Reference Offset:  ['188','190'] | Reference Text:  <S sid = 188 ssid = >We use the same 6GB corpus that was used for training by Pantel and Lin (2002) so that the results are comparable.</S><S sid = 190 ssid = >We then compare this output to the one provided by the system of Pantel and Lin (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations.</S> | Reference Offset:  ['28','104'] | Reference Text:  <S sid = 28 ssid = >Interestingly, cosine similarity is widely used in NLP for various applications such as clustering.</S><S sid = 104 ssid = >But with the rapidly growing amount of raw text available on the web, one could improve clustering performance by carefully harnessing its power.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005).</S> | Reference Offset:  ['138','139'] | Reference Text:  <S sid = 138 ssid = >Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989).</S><S sid = 139 ssid = >We first construct a frequency count vector C(e) = (ce1, ce2, ..., cek), where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in context f. We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miek) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: where n is the number of words and N = En Em j=1 cij is the total frequency count of all i=1 features of all words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly.</S> | Reference Offset:  ['64','152'] | Reference Text:  <S sid = 64 ssid = >This algorithm was further improved by Charikar (2002).</S><S sid = 152 ssid = >These similarity lists are cut off at a threshold of 0.15.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1077.txt | Citing Article:  D11-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently.</S> | Reference Offset:  ['11','20'] | Reference Text:  <S sid = 11 ssid = >In doing so, we are going to explore the literature and techniques of randomized algorithms.</S><S sid = 20 ssid = >LSH functions are generally based on randomized algorithms and are probabilistic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1077.txt | Citing Article:  N10-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005).</S> | Reference Offset:  ['29','144'] | Reference Text:  <S sid = 29 ssid = >In this paper, we perform high speed similarity list creation for nouns collected from a huge web corpus.</S><S sid = 144 ssid = >The web corpus is used to show that our framework is webscalable, while the newspaper corpus is used to compare the output of our system with the similarity lists output by an existing system, which are calculated using the traditional formula as given in equation 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1077.txt | Citing Article:  C10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This baseline system follows the design of previous work (Ravichandran et al, 2005).</S> | Reference Offset:  ['70','88'] | Reference Text:  <S sid = 70 ssid = >In the previous section, we introduced the theory for calculation of fast cosine similarity.</S><S sid = 88 ssid = >Why does the fast hamming distance algorithm work?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1077.txt | Citing Article:  C10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We followed the notation of the original paper (Ravichandran et al, 2005) here.</S> | Reference Offset:  ['8','96'] | Reference Text:  <S sid = 8 ssid = >In this paper, we investigate the first two avenues.</S><S sid = 96 ssid = >This is a huge saving from the original O(n2k) algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1077.txt | Citing Article:  C10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ravichandran et al (2005) applied LSH to the task of noun clustering.</S> | Reference Offset:  ['92','132'] | Reference Text:  <S sid = 92 ssid = >However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.</S><S sid = 132 ssid = >For each noun we take the grammatical context of the noun as identified by Minipar5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets.</S> | Reference Offset:  ['15','185'] | Reference Text:  <S sid = 15 ssid = >With large amounts of data, say n in the order of millions or even billions, having an n2k algorithm would be very infeasible.</S><S sid = 185 ssid = >The experiment was infeasible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work.</S> | Reference Offset:  ['88','102'] | Reference Text:  <S sid = 88 ssid = >Why does the fast hamming distance algorithm work?</S><S sid = 102 ssid = >However, to unleash the real power of clustering one has to work with large amounts of text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005).</S> | Reference Offset:  ['12','187'] | Reference Text:  <S sid = 12 ssid = >All clustering algorithms make use of some distance similarity (e.g., cosine similarity) to measure pair wise distance between sets of vectors.</S><S sid = 187 ssid = >For evaluating the quality of our final similarity lists, we use the system developed by Pantel and Lin (2002) as gold standard on a much smaller data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005).</S> | Reference Offset:  ['92','173'] | Reference Text:  <S sid = 92 ssid = >However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.</S><S sid = 173 ssid = >Number of bit index random permutations functions q; 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1077.txt | Citing Article:  N09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies.</S> | Reference Offset:  ['60','120'] | Reference Text:  <S sid = 60 ssid = >Thus, the above theorem, converts the problem of finding cosine distance between two vectors to the problem of finding hamming distance between their bit streams (as given by equation 4).</S><S sid = 120 ssid = >We eliminate duplicate and near duplicate documents by using the algorithm described by Kolcz et al. (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


