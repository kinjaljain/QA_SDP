Citance Number: 1 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented.</S> | Reference Offset:  ['6','132'] | Reference Text:  <S sid = 6 ssid = >However, relatively few documents have keywords assigned, and therefore finding methods to automate the assignment is desirable.</S><S sid = 132 ssid = >The results are presented next.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented.</S> | Reference Offset:  ['0','132'] | Reference Text:  <S sid = 0 ssid = >Improved Automatic Keyword Extraction Given More Linguistic Knowledge</S><S sid = 132 ssid = >The results are presented next.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a).</S> | Reference Offset:  ['1','109'] | Reference Text:  <S sid = 1 ssid = >In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.</S><S sid = 109 ssid = >The machine learning approach used for the experiments is that of rule induction, i.e., the model that is constructed from the given examples, consists of a set of rules2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords.</S> | Reference Offset:  ['84','173'] | Reference Text:  <S sid = 84 ssid = >In the next set of experiments a partial parserl was used to select all NP-chunks from the documents.</S><S sid = 173 ssid = >The total number of manually assigned terms present in the abstracts is 3 816, and the mean is 7.63 terms per document. tion approaches, extracting NP-chunks gives a better precision, while extracting all words or sequences of words matching any of a set of POS tag patterns gives a higher recall compared to extracting ngrams.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency.</S> | Reference Offset:  ['65','127'] | Reference Text:  <S sid = 65 ssid = >For all experiments the same training, validation, and test sets were used.</S><S sid = 127 ssid = >When calculating the recall, the value for the total number of manually assigned keywords present in the documents is used, independent of the number actually present in the different representations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)).</S> | Reference Offset:  ['129','146'] | Reference Text:  <S sid = 129 ssid = >Several runs were made for each representation, with the goal to maximise the performance as evaluated on the validation set: first the weights of the positive examples were adjusted, as the data set is unbalanced.</S><S sid = 146 ssid = >For the other three runs a single classifier had the best performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-1028.txt | Citing Article:  N04-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword.</S> | Reference Offset:  ['10','15'] | Reference Text:  <S sid = 10 ssid = >In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).</S><S sid = 15 ssid = >The trained model is subsequently applied to documents for which no keywords are assigned: each defined term from these documents is classified either as a keyword or a non-keyword; or—if a probabilistic model is used—the probability of the defined term being a keyword is given.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-1028.txt | Citing Article:  C08-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)).</S> | Reference Offset:  ['30','46'] | Reference Text:  <S sid = 30 ssid = >There are two drawbacks in common with the approaches proposed by Turney (2000) and Frank et al. (1999).</S><S sid = 46 ssid = >As opposed to Turney (2000) and Frank et al. (1999), who experiment with keyword extraction from full-length texts, this work concerns keyword extraction from abstracts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-1028.txt | Citing Article:  C08-2021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others.</S> | Reference Offset:  ['13','119'] | Reference Text:  <S sid = 13 ssid = >Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term.</S><S sid = 119 ssid = >In other words, the within-document frequency, the collection frequency, and the proportion of the document preceding the first appearance for each potential term were calculated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-1028.txt | Citing Article:  C08-2021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003).</S> | Reference Offset:  ['13','97'] | Reference Text:  <S sid = 13 ssid = >Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term.</S><S sid = 97 ssid = >These were Within-document frequency Collection frequency Relative position of the first occurrence (the proportion of the document preceding the first occurrence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W03-1028.txt | Citing Article:  P07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003).</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >Improved Automatic Keyword Extraction Given More Linguistic Knowledge</S><S sid = 2 ssid = >The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W03-1028.txt | Citing Article:  S10-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others.</S> | Reference Offset:  ['13','119'] | Reference Text:  <S sid = 13 ssid = >Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term.</S><S sid = 119 ssid = >In other words, the within-document frequency, the collection frequency, and the proportion of the document preceding the first appearance for each potential term were calculated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W03-1028.txt | Citing Article:  S10-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003).</S> | Reference Offset:  ['13','97'] | Reference Text:  <S sid = 13 ssid = >Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term.</S><S sid = 97 ssid = >These were Within-document frequency Collection frequency Relative position of the first occurrence (the proportion of the document preceding the first occurrence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W03-1028.txt | Citing Article:  C08-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More linguistic knowledge has been explored by Hulth (2003).</S> | Reference Offset:  ['0','49'] | Reference Text:  <S sid = 0 ssid = >Improved Automatic Keyword Extraction Given More Linguistic Knowledge</S><S sid = 49 ssid = >As the results were poor, two alternatives to extracting n-grams as the potential terms were explored.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W03-1028.txt | Citing Article:  S10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002.</S> | Reference Offset:  ['57','58'] | Reference Text:  <S sid = 57 ssid = >The collection used for the experiments described in this paper consists of 2 000 abstracts in English, with their corresponding title and keywords from the Inspec database.</S><S sid = 58 ssid = >The abstracts are from the years 1998 to 2002, from journal papers, and from the disciplines Computers and Control, and Information Technology.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W03-1028.txt | Citing Article:  W11-0316.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As shown in (Hulth, 2003), most key phrases are noun phrases.</S> | Reference Offset:  ['50','82'] | Reference Text:  <S sid = 50 ssid = >The first approach was to extract all noun phrases in the documents as judged by an NP-chunker.</S><S sid = 82 ssid = >When inspecting manually assigned keywords, the vast majority turn out to be nouns or noun phrases with adjectives, and as discussed in Section 2, the research on term extraction focuses on noun patterns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W03-1028.txt | Citing Article:  P08-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003).</S> | Reference Offset:  ['10','46'] | Reference Text:  <S sid = 10 ssid = >In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).</S><S sid = 46 ssid = >As opposed to Turney (2000) and Frank et al. (1999), who experiment with keyword extraction from full-length texts, this work concerns keyword extraction from abstracts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W03-1028.txt | Citing Article:  P08-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results.</S> | Reference Offset:  ['1','10'] | Reference Text:  <S sid = 1 ssid = >In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.</S><S sid = 10 ssid = >In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W03-1028.txt | Citing Article:  C10-2042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b).</S> | Reference Offset:  ['20','96'] | Reference Text:  <S sid = 20 ssid = >Part of the same training and test material is later used by Frank et al. (1999) for evaluating their algorithm in relation to Turney’s algorithm.</S><S sid = 96 ssid = >or mass) Initially, the same features that Frank et al. (1999) used for their domain-independent experiments were used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W03-1028.txt | Citing Article:  C10-2042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall.</S> | Reference Offset:  ['177','185'] | Reference Text:  <S sid = 177 ssid = >Using phrases means that the length of the potential terms is not restricted to something arbitrary, rather the terms are treat as the units they are.</S><S sid = 185 ssid = >In this paper I have not touched upon the more intricate aspects of evaluation, but simply treated the manually assigned keywords as the gold standard.</S> | Discourse Facet:  NA | Annotator: Automatic


