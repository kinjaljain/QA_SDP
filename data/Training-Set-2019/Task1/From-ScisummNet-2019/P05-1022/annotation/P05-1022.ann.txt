Citance Number: 1 | Reference Article:  P05-1022.txt | Citing Article:  P12-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing.</S> | Reference Offset:  ['5','170'] | Reference Text:  <S sid = 5 ssid = >We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.</S><S sid = 170 ssid = >We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1022.txt | Citing Article:  W12-3117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions.</S> | Reference Offset:  ['5','172'] | Reference Text:  <S sid = 5 ssid = >We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.</S><S sid = 172 ssid = >The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1022.txt | Citing Article:  P12-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011).</S> | Reference Offset:  ['0','5'] | Reference Text:  <S sid = 0 ssid = >Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking</S><S sid = 5 ssid = >We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1022.txt | Citing Article:  N06-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','177'] | Reference Text:  <S sid = 44 ssid = >Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.</S><S sid = 177 ssid = >Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1022.txt | Citing Article:  W12-0503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','177'] | Reference Text:  <S sid = 44 ssid = >Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.</S><S sid = 177 ssid = >Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1022.txt | Citing Article:  D11-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005).</S> | Reference Offset:  ['5','158'] | Reference Text:  <S sid = 5 ssid = >We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.</S><S sid = 158 ssid = >Because there may not be a unique best parse for each sentence (i.e., |Y+(s) |> 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1022.txt | Citing Article:  W08-1705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories.</S> | Reference Offset:  ['59','74'] | Reference Text:  <S sid = 59 ssid = >By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states.</S><S sid = 74 ssid = >The unpruned edges are then exhaustively evaluated according to the fine-grained probabilistic model; in effect, each coarse-grained dynamic programming state is split into one or more fine-grained dynamic programming states.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1022.txt | Citing Article:  D10-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','177'] | Reference Text:  <S sid = 44 ssid = >Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.</S><S sid = 177 ssid = >Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1022.txt | Citing Article:  W06-2929.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes.</S> | Reference Offset:  ['43','167'] | Reference Text:  <S sid = 43 ssid = >Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n−1th-best parses.</S><S sid = 167 ssid = >The n-best parser’s most probable parses are already of state-of-the-art quality, but the reranker further improves the f-score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1022.txt | Citing Article:  P08-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively.</S> | Reference Offset:  ['162','172'] | Reference Text:  <S sid = 162 ssid = >We evaluated the performance of our reranking parser using the standard PARSEVAL metrics.</S><S sid = 172 ssid = >The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1022.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','177'] | Reference Text:  <S sid = 44 ssid = >Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.</S><S sid = 177 ssid = >Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1022.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser.</S> | Reference Offset:  ['68','101'] | Reference Text:  <S sid = 68 ssid = >Instead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar.</S><S sid = 101 ssid = >From the 1-best result we see that the base accuracy of the parser is 89.7%.1 2-best and 10-best show dramatic oracle-rate improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1022.txt | Citing Article:  W11-1901.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','177'] | Reference Text:  <S sid = 44 ssid = >Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.</S><S sid = 177 ssid = >Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1022.txt | Citing Article:  P11-1163.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged.</S> | Reference Offset:  ['16','113'] | Reference Text:  <S sid = 16 ssid = >So a feature extractor maps each y to a vector of feature values f(y) = (f1(y), ..., fm(y)).</S><S sid = 113 ssid = >Feature schema are often parameterized in various ways.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1022.txt | Citing Article:  P11-1163.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times.</S> | Reference Offset:  ['106','108'] | Reference Text:  <S sid = 106 ssid = >This section describes how each parse y is mapped to a feature vector f(y) = (f1(y), ... , fm(y)).</S><S sid = 108 ssid = >The first feature f1(y) = log p(y) is the logarithm of the parse probability p according to the n-best parser model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1022.txt | Citing Article:  D11-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system.</S> | Reference Offset:  ['7','58'] | Reference Text:  <S sid = 7 ssid = >The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser’s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art.</S><S sid = 58 ssid = >There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1022.txt | Citing Article:  W10-1757.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005).</S> | Reference Offset:  ['5','158'] | Reference Text:  <S sid = 5 ssid = >We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.</S><S sid = 158 ssid = >Because there may not be a unique best parse for each sentence (i.e., |Y+(s) |> 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1022.txt | Citing Article:  P07-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005).</S> | Reference Offset:  ['58','100'] | Reference Text:  <S sid = 58 ssid = >There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser.</S><S sid = 100 ssid = >(The results are for all sentences of section 23 of the WSJ tree-bank of length ≤ 100.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1022.txt | Citing Article:  N07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions.</S> | Reference Offset:  ['36','68'] | Reference Text:  <S sid = 36 ssid = >Indeed, Collins’s n-best list of parses for section 24 of the Penn tree-bank has some sentences with only a single parse, because the n-best parser could not find any parses.</S><S sid = 68 ssid = >Instead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1022.txt | Citing Article:  D08-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies.</S> | Reference Offset:  ['88','166'] | Reference Text:  <S sid = 88 ssid = >Table 1 shows how the number of fine-grained dynamic programming states increases as a function of sentence length for the sentences in section 24 of the Treebank.</S><S sid = 166 ssid = >The results are presented in Table 3.</S> | Discourse Facet:  NA | Annotator: Automatic


