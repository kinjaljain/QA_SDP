Citance Number: 1 | Reference Article:  C02-1054.txt | Citing Article:  W03-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this task, we use a named entity recognizer (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Efficient Support Vector Classifiers For Named Entity Recognition</S><S sid = 1 ssid = >Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person, organization, and date.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C02-1054.txt | Citing Article:  D11-1144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers.</S> | Reference Offset:  ['52','124'] | Reference Text:  <S sid = 52 ssid = >We sim ply replaced the ME model with SVM classifiers.The above datasets are processed by a morphological analyzer ChaSen 2.2.12.</S><S sid = 124 ssid = >Therefore, the classifiers are very slow.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C02-1054.txt | Citing Article:  W11-0207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Several approaches used classifiers such as decision trees or SVMs (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['90','265'] | Reference Text:  <S sid = 90 ssid = >There are a few approaches to extend SVMs to cover t -class problems.</S><S sid = 265 ssid = >Since it was too slow, we made SVMs faster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C02-1054.txt | Citing Article:  W05-0610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Isozaki and Kazawa (2002) compared three commonly used methods for named entity recognition the SVM with quadratic kernel, maximal entropy method, and a rule based learning system, and showed that the SVM-based system performed better than the other two.</S> | Reference Offset:  ['38','110'] | Reference Text:  <S sid = 38 ssid = >1.2 SVM-based NE recognition.</S><S sid = 110 ssid = >?SVM?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C02-1054.txt | Citing Article:  C04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Since we used Isozaki's methods (Isozaki and Kazawa, 2002), the run-time complexity is not a problem.) Kudo and Matsumoto (2002) proposed an SVM-based Dependency Analyzer for Japanese sentences.</S> | Reference Offset:  ['39','259'] | Reference Text:  <S sid = 39 ssid = >As far as we know, the first SVM-based NE system was proposed by Yamada et al (2001) for Japanese.His system is an extension of Kudo?s chunking sys tem (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.</S><S sid = 259 ssid = >We can also reduce the run-time complexity of a multi-class problem by cascading SVMs in the form of a binary tree (Schwenker, 2001) or a directacyclic graph (Platt et al, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C02-1054.txt | Citing Article:  P08-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In natural language applications, the size |SV| tends to be very large (Isozaki and Kazawa, 2002), often above 10,000.</S> | Reference Offset:  ['129','130'] | Reference Text:  <S sid = 129 ssid = >Since SVMs learn combinations of features,  tends to be very large.</S><S sid = 130 ssid = >This tendencywill hold for other tasks of natural language pro cessing, too.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C02-1054.txt | Citing Article:  P08-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kernel Expansion (Isozaki and Kazawa, 2002) is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function y (x) =sgn (w xd+ b).</S> | Reference Offset:  ['28','172'] | Reference Text:  <S sid = 28 ssid = >The goal is to find a decision func tion that accurately predicts for unseen  . A non-linear SVM classifier gives a decision function ( ) * sign ,+-) for an input vector  where +-) .* / 0 21)3 546879: !6; Here, () *=!$# means  is a member of a cer tain class and () $* # means  is not a mem ber.</S><S sid = 172 ssid = >(eXpand the Quadratic Kernel).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C02-1054.txt | Citing Article:  P08-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, even the sparse-representation version of w tends to be very large: (Isozaki and Kazawa, 2002) report that some of their second degree expanded NER models were more than 80 times slower to load than the original models (and 224 times faster to classify). This approach obviously does not scale well, both to tasks with more features and to larger degree kernels. PKE Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003).</S> | Reference Offset:  ['266','267'] | Reference Text:  <S sid = 266 ssid = >The improved classifier is 21 times faster than TinySVMand 102 times faster than SVM-Light.</S><S sid = 267 ssid = >The im proved training program is 2.3 times faster than TinySVM and 3.5 times faster than SVM-Light.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C02-1054.txt | Citing Article:  P04-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Support vector machines (e.g., Vapnik (1995), Joachims (1998)) are a different kind of kernel method that, unlike KPCA methods, have al ready gained high popularity for NLP applications (e.g., Takamura and Matsumoto (2001), Isozaki and Kazawa (2002), Mayfield et al (2003)) including the word sense disambiguation task (e.g., Cabezas et al (2001)).</S> | Reference Offset:  ['258','262'] | Reference Text:  <S sid = 258 ssid = >Downs et al (Downs et al, 2001) try to reduce the number of support vectors by using linear dependence.</S><S sid = 262 ssid = >For this kind of prob lem, probability-based kernels are studied for more theoretically well-founded methods (Jaakkola and Haussler, 1998; Tsuda et al, 2001; Shimodaira et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C02-1054.txt | Citing Article:  P03-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, an SVM-based NE-chunker run sat a rate of only 85byte/sec, while previous rule based system can process several kilobytes per second (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['13','38'] | Reference Text:  <S sid = 13 ssid = >The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001)) can process several kilobytes in a second.</S><S sid = 38 ssid = >1.2 SVM-based NE recognition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C02-1054.txt | Citing Article:  P03-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Isozaki et al propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['0','172'] | Reference Text:  <S sid = 0 ssid = >Efficient Support Vector Classifiers For Named Entity Recognition</S><S sid = 172 ssid = >(eXpand the Quadratic Kernel).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C02-1054.txt | Citing Article:  W10-0213.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case.</S> | Reference Offset:  ['130','244'] | Reference Text:  <S sid = 130 ssid = >This tendencywill hold for other tasks of natural language pro cessing, too.</S><S sid = 244 ssid = >The above methods can also be applied to othertasks in natural language processing such as chunk ing and POS tagging because the quadratic kernels give good results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C02-1054.txt | Citing Article:  P03-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Ikehara et al, 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['39','258'] | Reference Text:  <S sid = 39 ssid = >As far as we know, the first SVM-based NE system was proposed by Yamada et al (2001) for Japanese.His system is an extension of Kudo?s chunking sys tem (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.</S><S sid = 258 ssid = >Downs et al (Downs et al, 2001) try to reduce the number of support vectors by using linear dependence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C02-1054.txt | Citing Article:  W10-2416.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Isozaki and Kazawa (2002) studied the use of SVM instead.</S> | Reference Offset:  ['110','252'] | Reference Text:  <S sid = 110 ssid = >?SVM?</S><S sid = 252 ssid = >instead of support vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C02-1054.txt | Citing Article:  N03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Isozaki (Isozaki and Kazawa,2002) controls the parameters of a statistical morphological analyzer so as to produce more fine-grained output.</S> | Reference Offset:  ['52','58'] | Reference Text:  <S sid = 52 ssid = >We sim ply replaced the ME model with SVM classifiers.The above datasets are processed by a morphological analyzer ChaSen 2.2.12.</S><S sid = 58 ssid = >See Isozaki (2001) for details.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C02-1054.txt | Citing Article:  N03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Isozaki (Isozaki and Kazawa, 2002) introduces the thesaurus NTT Goi Taikei (Ikehara et al, 1999) to augment the Table 5: The depth of redundant analysis and the extraction accuracy Pair Wise Method Depth of morph.</S> | Reference Offset:  ['59','168'] | Reference Text:  <S sid = 59 ssid = >Now, Japanese NE recognition is solved by theclassification of words (Sekine et al, 1998; Borth wick, 1999; Uchimoto et al, 2000).</S><S sid = 168 ssid = >Z?? B@] for every non-zero pair G?Z??</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C02-1054.txt | Citing Article:  P06-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['6','20'] | Reference Text:  <S sid = 6 ssid = >We also present an SVM-based feature selec tion method and an efficient training method.</S><S sid = 20 ssid = >In this paper, we present a method that makes the NE system substantially faster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C02-1054.txt | Citing Article:  D09-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq.</S> | Reference Offset:  ['131','172'] | Reference Text:  <S sid = 131 ssid = >Here, we focus on the quadratic kernel BG * I#!?G ?</S><S sid = 172 ssid = >(eXpand the Quadratic Kernel).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C02-1054.txt | Citing Article:  C10-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['92','272'] | Reference Text:  <S sid = 92 ssid = >approach.</S><S sid = 272 ssid = >We also thank Shigeru Katagiri and Ken-ichiro Ishii for their support.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C02-1054.txt | Citing Article:  W03-1208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Words, chunks and their relations in the texts were analyzed by CaboCha (Kudoand Matsumoto, 2002), and named entities were analyzed by the SVM-based NE tagger (Isozaki and Kazawa, 2002).</S> | Reference Offset:  ['38','110'] | Reference Text:  <S sid = 38 ssid = >1.2 SVM-based NE recognition.</S><S sid = 110 ssid = >?SVM?</S> | Discourse Facet:  NA | Annotator: Automatic


