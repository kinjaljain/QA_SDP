Citance Number: 1 | Reference Article:  P08-1024.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable.</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Latent Variable Model for Statistical Machine Translation</S><S sid = 16 ssid = >For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1024.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['174','175'] | Reference Text:  <S sid = 174 ssid = >The use of richer, more linguistic grammars (e.g., Galley et al. (2004)) may also improve the system.</S><S sid = 175 ssid = >The authors acknowledge the support of the EPSRC (Blunsom & Osborne, grant EP/D074959/1; Cohn, grant GR/T04557/01).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1024.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance.</S> | Reference Offset:  ['136','149'] | Reference Text:  <S sid = 136 ssid = >Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.</S><S sid = 149 ssid = >Translation comparison Having demonstrated that accounting for derivational ambiguity leads to improvements for our discriminative model, we now place the performance of our system in the context of the standard approach to hierarchical translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1024.txt | Citing Article:  D09-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only.</S> | Reference Offset:  ['97','155'] | Reference Text:  <S sid = 97 ssid = >This algorithm is similar to the methods for decoding with a SCFG intersected with an n-gram language model, which require language model contexts to be stored in each chart cell.</S><S sid = 155 ssid = >Additionally we show the scores achieved by MERT training the full set of features for Hiero, with and without a language model.8 We provide these results for reference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1024.txt | Citing Article:  D09-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)).</S> | Reference Offset:  ['42','90'] | Reference Text:  <S sid = 42 ssid = >Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler.</S><S sid = 90 ssid = >Instead we discard the unreachable portion of the training sample (24% in our experiments).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1024.txt | Citing Article:  W11-2130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart.</S> | Reference Offset:  ['80','158'] | Reference Text:  <S sid = 80 ssid = >Again, we use the inside-outside algorithm to find the ‘reference’ feature expectations from this chart.</S><S sid = 158 ssid = >This is encouraging as our model was trained to optimise likelihood rather than BLEU, yet it is still competitive on that metric.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1024.txt | Citing Article:  W09-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008).</S> | Reference Offset:  ['14','57'] | Reference Text:  <S sid = 14 ssid = >Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation.</S><S sid = 57 ssid = >Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1024.txt | Citing Article:  C10-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008).</S> | Reference Offset:  ['0','6'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Latent Variable Model for Statistical Machine Translation</S><S sid = 6 ssid = >Statistical machine translation (SMT) has seen a resurgence in popularity in recent years, with progress being driven by a move to phrase-based and syntax-inspired approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1024.txt | Citing Article:  D11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008).</S> | Reference Offset:  ['67','72'] | Reference Text:  <S sid = 67 ssid = >Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007).</S><S sid = 72 ssid = >This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1024.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Latent Variable Model for Statistical Machine Translation</S><S sid = 3 ssid = >We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1024.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(6) as max-derivation decoding, which are first termed by Blunsom et al (2008).</S> | Reference Offset:  ['136','173'] | Reference Text:  <S sid = 136 ssid = >Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.</S><S sid = 173 ssid = >To do so would require integrating a language model feature into the max-translation decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1024.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly.</S> | Reference Offset:  ['136','173'] | Reference Text:  <S sid = 136 ssid = >Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.</S><S sid = 173 ssid = >To do so would require integrating a language model feature into the max-translation decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1024.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space.</S> | Reference Offset:  ['23','43'] | Reference Text:  <S sid = 23 ssid = >(Koehn et al., 2003).</S><S sid = 43 ssid = >Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1024.txt | Citing Article:  W11-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008).</S> | Reference Offset:  ['48','61'] | Reference Text:  <S sid = 48 ssid = >The model can learn from many of these derivations and thereby learn from all these translation fragments.</S><S sid = 61 ssid = >The conditional probability of a derivation, d, for a target translation, e, conditioned on the source, f, is given by: where Hk(d, e, f) = rEd Here k ranges over the model’s features, and Λ = {Ak} are the model parameters (weights for their corresponding features).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1024.txt | Citing Article:  W11-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Blunsom et al (2008) for more information.</S> | Reference Offset:  ['23','39'] | Reference Text:  <S sid = 23 ssid = >(Koehn et al., 2003).</S><S sid = 39 ssid = >To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1024.txt | Citing Article:  E09-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008).</S> | Reference Offset:  ['125','154'] | Reference Text:  <S sid = 125 ssid = >In decoding we can search for the maximum probability derivation, which is the standard practice in SMT, or for the maximum probability translation which is what we actually want from our model, i.e. the best translation.</S><S sid = 154 ssid = >As well as both modelling the same distribution, when our model is trained with a single parameter per-rule these systems have the same parameter space, differing only in the manner of estimation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1024.txt | Citing Article:  D09-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008).</S> | Reference Offset:  ['0','11'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Latent Variable Model for Statistical Machine Translation</S><S sid = 11 ssid = >We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1024.txt | Citing Article:  P10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.</S> | Reference Offset:  ['3','65'] | Reference Text:  <S sid = 3 ssid = >We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.</S><S sid = 65 ssid = >The distribution is globally normalised by the partition function, ZA(f), which sums out the numerator in (1) for every derivation (and therefore every translation) of f: Given (1), the conditional probability of a target translation given the source is the sum over all of its derivations: where O(e, f) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single ‘best’ derivation using a Viterbi or beam search algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1024.txt | Citing Article:  C10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008).</S> | Reference Offset:  ['21','72'] | Reference Text:  <S sid = 21 ssid = >This model maximises the conditional likelihood of the data, p(e|f), where e and f are the English and foreign sentences, respectively.</S><S sid = 72 ssid = >This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1024.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008).</S> | Reference Offset:  ['57','62'] | Reference Text:  <S sid = 57 ssid = >Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)).</S><S sid = 62 ssid = >The feature functions Hk are predefined real-valued functions over the source and target sentences, and can include overlapping and non-independent features of the data.</S> | Discourse Facet:  NA | Annotator: Automatic


