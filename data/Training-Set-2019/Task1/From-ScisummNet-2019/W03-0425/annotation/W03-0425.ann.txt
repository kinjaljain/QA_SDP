Citance Number: 1 | Reference Article:  W03-0425.txt | Citing Article:  W03-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation.</S> | Reference Offset:  ['3','34'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 34 ssid = >A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C|w, Ci) = S (C, Ci), where S is the Kronecker operator (S (x, y) := (x = y?1 : 0)) – each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest number of votes wins.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-0425.txt | Citing Article:  W03-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b).</S> | Reference Offset:  ['3','59'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 59 ssid = >For the German task, the improvement yielded by classifier combination is smaller.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-0425.txt | Citing Article:  P05-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['29','60'] | Reference Text:  <S sid = 29 ssid = >In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.</S><S sid = 60 ssid = >As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-0425.txt | Citing Article:  D07-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['29','60'] | Reference Text:  <S sid = 29 ssid = >In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.</S><S sid = 60 ssid = >As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-0425.txt | Citing Article:  W06-2919.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['29','60'] | Reference Text:  <S sid = 29 ssid = >In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.</S><S sid = 60 ssid = >As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-0425.txt | Citing Article:  P06-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc.</S> | Reference Offset:  ['3','34'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 34 ssid = >A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C|w, Ci) = S (C, Ci), where S is the Kronecker operator (S (x, y) := (x = y?1 : 0)) – each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest number of votes wins.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-0425.txt | Citing Article:  P08-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['29','60'] | Reference Text:  <S sid = 29 ssid = >In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.</S><S sid = 60 ssid = >As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-0425.txt | Citing Article:  W05-0611.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003).</S> | Reference Offset:  ['3','14'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 14 ssid = >The model weights are trained using the improved iterative scaling algorithm (Berger et al., 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-0425.txt | Citing Article:  W06-2202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['29','60'] | Reference Text:  <S sid = 29 ssid = >In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.</S><S sid = 60 ssid = >As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-0425.txt | Citing Article:  D10-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005).</S> | Reference Offset:  ['3','14'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 14 ssid = >The model weights are trained using the improved iterative scaling algorithm (Berger et al., 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W03-0425.txt | Citing Article:  D10-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods.</S> | Reference Offset:  ['1','3'] | Reference Text:  <S sid = 1 ssid = >This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.</S><S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W03-0425.txt | Citing Article:  W05-0610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003).</S> | Reference Offset:  ['3','28'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 28 ssid = >To facilitate comparison with other classifiers for this task, most reported results 3 The method of retaining only the boundaries and reclassifying the entities was shown to improve the performance of 11 of the 12 systems participating in the CoNLL-2002 shared tasks, in both languages (Florian, 2002b). are obtained by using features exclusively extracted from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W03-0425.txt | Citing Article:  I08-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003).</S> | Reference Offset:  ['3','7'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 7 ssid = >RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Florian et al (2003) employed the same technique in a combination of learners.</S> | Reference Offset:  ['3','34'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 34 ssid = >A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C|w, Ci) = S (C, Ci), where S is the Kronecker operator (S (x, y) := (x = y?1 : 0)) – each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest number of votes wins.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each.</S> | Reference Offset:  ['1','3'] | Reference Text:  <S sid = 1 ssid = >This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.</S><S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best.</S> | Reference Offset:  ['3','33'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 33 ssid = >Table 2 presents the combination results, for different ways of estimating the interpolation parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003).</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition Through Classifier Combination</S><S sid = 2 ssid = >When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003).</S> | Reference Offset:  ['0','51'] | Reference Text:  <S sid = 0 ssid = >Named Entity Recognition Through Classifier Combination</S><S sid = 51 ssid = >German poses a completely different problem for named entity recognition: the data is considerably sparser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate.</S> | Reference Offset:  ['3','58'] | Reference Text:  <S sid = 3 ssid = >This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).</S><S sid = 58 ssid = >In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W03-0425.txt | Citing Article:  W03-0419.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Florian et al (2003) have also obtained the highest F1 rate for the German data.</S> | Reference Offset:  ['53','55'] | Reference Text:  <S sid = 53 ssid = >We note that the numbers are roughly twice as large for the development data in German as they are for English.</S><S sid = 55 ssid = >Also, specifically for the German data, traditional approaches which utilize capitalization do not work as well as in English, because all nouns are capitalized in German.</S> | Discourse Facet:  NA | Annotator: Automatic


