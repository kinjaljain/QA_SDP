Citance Number: 1 | Reference Article:  A97-1029.txt | Citing Article:  W97-0312.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bikel et al (Bikel et al, 1997) report on Nymble, an HMM-based name tagging system operating in English and Spanish.</S> | Reference Offset:  ['4','106'] | Reference Text:  <S sid = 4 ssid = >More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993).</S><S sid = 106 ssid = >We report the results for English and for Spanish and then the results of a set of experiments to determine the impact of the training set size on the algorithm's performance in both English and Spanish.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A97-1029.txt | Citing Article:  W03-0432.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al, 1997).</S> | Reference Offset:  ['47','96'] | Reference Text:  <S sid = 47 ssid = >The word feature is the one part of this model which is language-dependent.</S><S sid = 96 ssid = >We then expanded the feature set to its current state in order to capture more subtleties related mostly to numbers; due to increased performance (although not entirely dramatic) on every test, we kept the enlarged feature set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A97-1029.txt | Citing Article:  P01-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nymble (Bikel et al, 1997) uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Nymble: A High-Performance Learning Name-Finder</S><S sid = 1 ssid = >This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A97-1029.txt | Citing Article:  P05-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Bikelet al, 1997) are other examples of the use of HMMs.</S> | Reference Offset:  ['31','97'] | Reference Text:  <S sid = 31 ssid = >Within each of the name-class states, we use a statistical bigram language model, with the usual one-word-per-state emission.</S><S sid = 97 ssid = >Contrary to our expectations (which were based on our experience with English), Spanish contained many examples of lower-case words in organization and location names.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A97-1029.txt | Citing Article:  P02-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al (1997), McCallum et al (2000) and Laerty et al (2001).</S> | Reference Offset:  ['49','82'] | Reference Text:  <S sid = 49 ssid = >In order to generate the first word, we must make a transition from one name-class to another, as well as calculate the likelihood of that word.</S><S sid = 82 ssid = >Unlike a traditional HMM, the probability of generating a particular word is 1 for each word-state inside each of the name-class states.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A97-1029.txt | Citing Article:  A00-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al 1997).</S> | Reference Offset:  ['1','4'] | Reference Text:  <S sid = 1 ssid = >This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.</S><S sid = 4 ssid = >More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A97-1029.txt | Citing Article:  P06-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The HMM tagger generally follows the Nymble model (Bikel et al 1997), and uses best-first search to generate N-Best hypotheses for each input sentence.</S> | Reference Offset:  ['7','114'] | Reference Text:  <S sid = 7 ssid = >To our knowledge, Nymble out-performs the best published results of any other learning name-finder.</S><S sid = 114 ssid = >Table 4.1 illustrates Nymble's performance as compared to the best reported scores for each category.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A97-1029.txt | Citing Article:  P03-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The base system is an HMM based tagger, similar to (Bikel et al, 1997).</S> | Reference Offset:  ['5','111'] | Reference Text:  <S sid = 5 ssid = >We would now propose that HMM's have successfully been applied to the problem of name-finding.</S><S sid = 111 ssid = >To our knowledge, our learned name-finding system has achieved a higher F-measure than any other learned system when compared to state-of-the-art manual (rule-based) systems on similar data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A97-1029.txt | Citing Article:  P03-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The alternative to true casing text is to destroy case information in the training material SNORIFY procedure in (Bikel et al, 1997).</S> | Reference Offset:  ['121','130'] | Reference Text:  <S sid = 121 ssid = >Given this maximum size of training available to us, we successfully divided the training material in half until we were using only one eighth of the original training set size or a training set of 50,000 words for the smallest experiment.</S><S sid = 130 ssid = >We also measured the performance of the system with half the training data or slightly more than 100,000 words of text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A97-1029.txt | Citing Article:  I05-3006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The typical machine learning approaches for English NE are transformation-based learning [Aberdeen et al 1995], hidden Markov model [Bikel et al. 1997], maximum entropy model [Borthwick, 1999], support vector machine learning [Eunji Yi et al 2004], unsupervised model [Collins et al 1999] and etc.</S> | Reference Offset:  ['17','20'] | Reference Text:  <S sid = 17 ssid = >(2.2) Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995).</S><S sid = 20 ssid = >In addition to these finitestate pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A97-1029.txt | Citing Article:  P06-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al, 1997), the dictionary HMM model (Kouetal., 2005) and Maximum Entropy Markov Mod els (MEMMs) (Finkel et al, 2004).</S> | Reference Offset:  ['3','17'] | Reference Text:  <S sid = 3 ssid = >In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems.</S><S sid = 17 ssid = >(2.2) Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A97-1029.txt | Citing Article:  A00-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These include rule-based systems [Krupka 1998], Hidden Markov Models (HMM) [Bikel et al 1997] and Maximum Entropy Models (MaxEnt) [Borthwick 1998].</S> | Reference Offset:  ['3','10'] | Reference Text:  <S sid = 3 ssid = >In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems.</S><S sid = 10 ssid = >We will describe the various models employed, the methods for training these models and the method for &quot;decoding&quot; on test data (the term &quot;decoding&quot; borrowed from the speech recognition community, since one goal of traversing an HMM is to recover the hidden state sequence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A97-1029.txt | Citing Article:  A00-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >constrained HMM Our original HMM is similar to the Nymble [Bikel et al 1997] system that is based on bigram statistics.</S> | Reference Offset:  ['5','6'] | Reference Text:  <S sid = 5 ssid = >We would now propose that HMM's have successfully been applied to the problem of name-finding.</S><S sid = 6 ssid = >We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &quot;Nymble&quot;.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A97-1029.txt | Citing Article:  W02-0813.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, an automatic named entity tagger (Bikel et al, 1997) was run on the sentences to map proper nouns to a small set of semantic classes.</S> | Reference Offset:  ['6','92'] | Reference Text:  <S sid = 6 ssid = >We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &quot;Nymble&quot;.</S><S sid = 92 ssid = >Although the part-of-speech tagger used capitalization to help it determine proper-noun tags, this feature was only implicit in the model, and then only after two levels of back-off!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A97-1029.txt | Citing Article:  W04-0705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines.</S> | Reference Offset:  ['4','20'] | Reference Text:  <S sid = 4 ssid = >More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993).</S><S sid = 20 ssid = >In addition to these finitestate pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A97-1029.txt | Citing Article:  P06-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another related work is (Bikel et al, 1997) which used HMMs as part of its modelling for the name finding problem in information extraction.</S> | Reference Offset:  ['5','21'] | Reference Text:  <S sid = 5 ssid = >We would now propose that HMM's have successfully been applied to the problem of name-finding.</S><S sid = 21 ssid = >The atomic elements of information extraction— indeed, of language as a whole—could be considered the who, where, when and how much in a sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A97-1029.txt | Citing Article:  W03-0428.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al, 1997, Bikel et al, 1997).</S> | Reference Offset:  ['40','45'] | Reference Text:  <S sid = 40 ssid = >Also, most of the word features are used to distinguish types of numbers, which are language-independent.2 The rationale for having such features is clear: in Roman languages, capitalization gives good evidence of names.3 This section describes the model formally, discussing the transition probabilities to the wordstates, which &quot;generate&quot; the words of each name-class.</S><S sid = 45 ssid = >The rest of the features distinguish types of capitalization and all other words (such as punctuation marks, which are separate tokens).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  A97-1029.txt | Citing Article:  W06-0206.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al 1997).</S> | Reference Offset:  ['6','117'] | Reference Text:  <S sid = 6 ssid = >We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &quot;Nymble&quot;.</S><S sid = 117 ssid = >More generally how does performance vary as the training set size is increased or decreased?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  A97-1029.txt | Citing Article:  N04-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As hidden Markov models have been used both for name finding (Bikel et al (1997)) and tokenization (Cutting et al.</S> | Reference Offset:  ['3','17'] | Reference Text:  <S sid = 3 ssid = >In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems.</S><S sid = 17 ssid = >(2.2) Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  A97-1029.txt | Citing Article:  C02-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, an automatic named entity tagger (Bikel et al, 1997) was run on the sentences to map proper nouns to a small set of semantic classes.</S> | Reference Offset:  ['6','92'] | Reference Text:  <S sid = 6 ssid = >We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &quot;Nymble&quot;.</S><S sid = 92 ssid = >Although the part-of-speech tagger used capitalization to help it determine proper-noun tags, this feature was only implicit in the model, and then only after two levels of back-off!</S> | Discourse Facet:  NA | Annotator: Automatic


