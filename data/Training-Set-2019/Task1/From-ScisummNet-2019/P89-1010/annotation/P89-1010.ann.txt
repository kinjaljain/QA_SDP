Citance Number: 1 | Reference Article:  P89-1010.txt | Citing Article:  H89-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word).</S> | Reference Offset:  ['64','101'] | Reference Text:  <S sid = 64 ssid = >In addition to identifying semantic relations of the doctor/nurse variety, we believe the association ratio can also be used to search for interesting lexico-syntactic relationships between verbs and typical arguments/adjuncts.</S><S sid = 101 ssid = >The association ratios (above) show that association norms apply to function words as well as content words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P89-1010.txt | Citing Article:  N12-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.</S> | Reference Offset:  ['0','41'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 41 ssid = >Technically, the association ratio is different from mutual information in two respects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P89-1010.txt | Citing Article:  H05-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998).</S> | Reference Offset:  ['0','41'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 41 ssid = >Technically, the association ratio is different from mutual information in two respects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P89-1010.txt | Citing Article:  H05-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora.</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 25 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P89-1010.txt | Citing Article:  P06-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We encode the semantic compatibility between a noun and its parse tree parent (and grammatical relationship with the parent) using mutual information (MI) (Church and Hanks, 1989).</S> | Reference Offset:  ['0','31'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 31 ssid = >If there is no interesting relationship between x and y, then P(x,y) P(x) P(y), and thus, 1(x,y)2-- 0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P89-1010.txt | Citing Article:  W06-1665.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','165'] | Reference Text:  <S sid = 61 ssid = >(In fact, the pair (a, doctors) above, appears significantly less often than chance.</S><S sid = 165 ssid = >give them the money to save the dogs[ANIMAL] from being destroyed(DESTRUC11 , program intended to save the giant birds[ANIMAL] from extinction[DESTRUCT] ,</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P89-1010.txt | Citing Article:  D09-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We weigh each context f using point wise mutual information (Church and Hanks 1989).</S> | Reference Offset:  ['0','91'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 91 ssid = >There are concordancing programs (see Figure 1 at the end of this paper), which are basically KWIC (key word in context [Aho, Kernighan, and Weinberger (1988), p. 122]) indexes with additional features such as the ability to extend the context, sort leftwards as well as rightwards, and so on.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P89-1010.txt | Citing Article:  C96-2099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Z (2) d:: l j= i (d -1 )dmax: max distance used wl: the i-th letter in the sentence w g (d): a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis (Church and Hanks, 1989).</S> | Reference Offset:  ['48','99'] | Reference Text:  <S sid = 48 ssid = >For example, given the sentence, &quot;Library workers were prohibited from saving books from this heap of ruins,&quot; which appeared in an AP story on April 1, 1988, f(prohibited) = 1 and f(prohibited, front) = 2.</S><S sid = 99 ssid = >For example, although it is easy to see from the concordance selection in Figure 1 that the word &quot;to&quot; often comes before &quot;save&quot; and the word &quot;the&quot; often comes after &quot;save,&quot; it is hard to say from examination of a concordance alone whether either or both of these co-occurrences have any significance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P89-1010.txt | Citing Article:  W02-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness be tween words.</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 7 ssid = >It is common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their co-occurrence with other words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P89-1010.txt | Citing Article:  W10-0307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >I also experimented with the co-occurence frequency c (s, w) and point-wise mutual information (Church and Hanks, 1989) as similarity functions.</S> | Reference Offset:  ['0','41'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 41 ssid = >Technically, the association ratio is different from mutual information in two respects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P89-1010.txt | Citing Article:  H05-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Beginning with (Church and Hanks, 1989), numerous authors have used the point wise mutual in formation between pairs of words to analyze word co-locations and associations.</S> | Reference Offset:  ['63','145'] | Reference Text:  <S sid = 63 ssid = >Although the psycholinguistic literature documents the significance of noun/noun word associations such as doctor/nurse in considerable detail, relatively little is said about associations among verbs, function words, adjectives, and other non-nouns.</S><S sid = 145 ssid = >In other words, they use two words to triangulate in on a word sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P89-1010.txt | Citing Article:  S10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Point-wise mutual information (PMI, Church and Hanks (1989)) is used to capture the semantic relatedness of the candidate to the topic of the document.</S> | Reference Offset:  ['0','41'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 41 ssid = >Technically, the association ratio is different from mutual information in two respects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P89-1010.txt | Citing Article:  C10-2144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Early approaches to identifying MWEs concentrated on their collocational behavior (Church and Hanks, 1989).</S> | Reference Offset:  ['77','115'] | Reference Text:  <S sid = 77 ssid = >As Sinclair suggests, the approach is well suited for identifying phrasal verbs.</S><S sid = 115 ssid = >Lexicographers have tended to use concordances impressionistically; semantic theorist, AI-ers, and others have concentrated on a few interesting examples, e.g., &quot;bachelor,&quot; and have not given much thought to how the results might be scaled up.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P89-1010.txt | Citing Article:  P06-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically, the measure Snom (vt ,vh) is derived from point-wise mutual information (Church and Hanks, 1989): Snom (vt ,vh)= log p (vt, vh|nom) p (vt) p (vh|pers) (3) where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs.</S> | Reference Offset:  ['113','130'] | Reference Text:  <S sid = 113 ssid = >Having established the relative importance of &quot;save ... from&quot;, and having noted that the two words are rarely adjacent, we would now like to speed up the labor-intensive task of categorizing the concordance lines.</S><S sid = 130 ssid = >In fact, this pattern probably subsumes most of the occurrences of the &quot;save [ANIMAL)&quot; pattern noticed in Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P89-1010.txt | Citing Article:  W06-1204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003).</S> | Reference Offset:  ['0','41'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 41 ssid = >Technically, the association ratio is different from mutual information in two respects.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P89-1010.txt | Citing Article:  W06-1204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006).</S> | Reference Offset:  ['0','80'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 80 ssid = >(Part of speech notation is borrowed from [Francis and Kucera (1982)]; in = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P89-1010.txt | Citing Article:  N04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989).</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 25 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P89-1010.txt | Citing Article:  I05-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Church and Hanks (1989) proposed a measure of association called Mutual Information.</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 25 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P89-1010.txt | Citing Article:  W09-1706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have used POINTWISE MUTUAL INFORMATION (PMI, Church and Hanks (1989)) to account for the differences in information value between the several headwords and attributes.</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 25 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P89-1010.txt | Citing Article:  D11-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The explanation for such a behavior is: since we are not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989).</S> | Reference Offset:  ['51','52'] | Reference Text:  <S sid = 51 ssid = >In contrast, when 1(x,y)=-' 0, the pairs less interesting.</S><S sid = 52 ssid = >(As a very rough rule of thumb, we have observed that pairs with 1(x,y)> 3 tend to be interesting, and pairs with smaller f(x,y) are generally not.</S> | Discourse Facet:  NA | Annotator: Automatic


