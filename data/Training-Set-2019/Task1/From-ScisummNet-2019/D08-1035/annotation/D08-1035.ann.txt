Citance Number: 1 | Reference Article:  D08-1035.txt | Citing Article:  P13-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008).</S> | Reference Offset:  ['5','115'] | Reference Text:  <S sid = 5 ssid = >The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.</S><S sid = 115 ssid = >To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1035.txt | Citing Article:  N09-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations.</S> | Reference Offset:  ['24','104'] | Reference Text:  <S sid = 24 ssid = >We consider two approaches to handling the language models: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial distribution (also known as the multivariate Polya distribution).</S><S sid = 104 ssid = >The Dirichlet compound multinomial integrates over language models, but we must still set the prior θ0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1035.txt | Citing Article:  N09-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix.</S> | Reference Offset:  ['42','103'] | Reference Text:  <S sid = 42 ssid = >Both of these last two systems use dynamic programming to search the space of segmentations.</S><S sid = 103 ssid = >These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1035.txt | Citing Article:  N09-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries.</S> | Reference Offset:  ['157','158'] | Reference Text:  <S sid = 157 ssid = >For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author.</S><S sid = 158 ssid = >This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1035.txt | Citing Article:  N09-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008).</S> | Reference Offset:  ['167','189'] | Reference Text:  <S sid = 167 ssid = >This system is referred to as BAYESSEG in Table 1.</S><S sid = 189 ssid = >BAYESSEG-CUE is the Bayesian system with cue phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1035.txt | Citing Article:  P10-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents.</S> | Reference Offset:  ['64','80'] | Reference Text:  <S sid = 64 ssid = >For the task of segmenting documents, we are interested only in the segment indices, and would prefer not to have to search in the space of language models as well.</S><S sid = 80 ssid = >The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1035.txt | Citing Article:  P10-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure.</S> | Reference Offset:  ['71','103'] | Reference Text:  <S sid = 71 ssid = >This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006).</S><S sid = 103 ssid = >These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1035.txt | Citing Article:  P10-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008).</S> | Reference Offset:  ['8','70'] | Reference Text:  <S sid = 8 ssid = >Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments.</S><S sid = 70 ssid = >Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1035.txt | Citing Article:  P10-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story.</S> | Reference Offset:  ['182','195'] | Reference Text:  <S sid = 182 ssid = >Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).</S><S sid = 195 ssid = >On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1035.txt | Citing Article:  N10-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook.</S> | Reference Offset:  ['82','198'] | Reference Text:  <S sid = 82 ssid = >Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework.</S><S sid = 198 ssid = >Cue phrases improve performance on the meeting corpus, but not on the textbook corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1035.txt | Citing Article:  P12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008).</S> | Reference Offset:  ['43','113'] | Reference Text:  <S sid = 43 ssid = >An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006).</S><S sid = 113 ssid = >Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1035.txt | Citing Article:  N12-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations.</S> | Reference Offset:  ['179','182'] | Reference Text:  <S sid = 179 ssid = >In all cases, we use the publicly available executables provided by the authors.</S><S sid = 182 ssid = >Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1035.txt | Citing Article:  D11-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008).</S> | Reference Offset:  ['40','191'] | Reference Text:  <S sid = 40 ssid = >Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.</S><S sid = 191 ssid = >Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1035.txt | Citing Article:  D11-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model.</S> | Reference Offset:  ['21','114'] | Reference Text:  <S sid = 21 ssid = >More formally, we treat the words in each sentence as draws from a language model associated with the topic segment.</S><S sid = 114 ssid = >The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1035.txt | Citing Article:  D11-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008).</S> | Reference Offset:  ['40','191'] | Reference Text:  <S sid = 40 ssid = >Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.</S><S sid = 191 ssid = >Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1035.txt | Citing Article:  P13-1167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['68','217'] | Reference Text:  <S sid = 68 ssid = >Let nj be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt — θj, then the posterior distribution for θj is Dirichlet with vector parameter nj +θ0 (Bernardo and Smith, 2000).</S><S sid = 217 ssid = >Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1035.txt | Citing Article:  P13-1167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011).</S> | Reference Offset:  ['164','191'] | Reference Text:  <S sid = 164 ssid = >We use the evaluation source code provided by Malioutov and Barzilay (2006).</S><S sid = 191 ssid = >Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


