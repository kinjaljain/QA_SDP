Citance Number: 1 | Reference Article:  P06-1055.txt | Citing Article:  P14-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005).</S> | Reference Offset:  ['69','112'] | Reference Text:  <S sid = 69 ssid = >Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).</S><S sid = 112 ssid = >Matsuzaki et al. (2005) discuss two approximations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1055.txt | Citing Article:  P14-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','204'] | Reference Text:  <S sid = 63 ssid = >In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.</S><S sid = 204 ssid = >It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1055.txt | Citing Article:  W10-1408.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps.</S> | Reference Offset:  ['46','69'] | Reference Text:  <S sid = 46 ssid = >For instance, Matsuzaki et al. (2005) start by annotating their grammar with the identity of the parent and sibling, which are observed (i.e. not latent), before adding latent annotations.4 If these manual annotations are good, they reduce the search space for EM by constraining it to a smaller region.</S><S sid = 69 ssid = >Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1055.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods.</S> | Reference Offset:  ['11','173'] | Reference Text:  <S sid = 11 ssid = >In this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as NP, VP, etc.) but split based on the likelihood of the training trees.</S><S sid = 173 ssid = >For example, the whdeterminers (WDT) split into one class for that and another for which, while the wh-adverbs align by reference type: event-based how and why vs. entity-based when and where.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1055.txt | Citing Article:  D07-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006).</S> | Reference Offset:  ['7','16'] | Reference Text:  <S sid = 7 ssid = >Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005).</S><S sid = 16 ssid = >Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of Klein and Manning (2003)â€™s manual grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1055.txt | Citing Article:  D07-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs.</S> | Reference Offset:  ['14','203'] | Reference Text:  <S sid = 14 ssid = >Recently, Matsuzaki et al. (2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols.</S><S sid = 203 ssid = >While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1055.txt | Citing Article:  D07-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006).</S> | Reference Offset:  ['89','91'] | Reference Text:  <S sid = 89 ssid = >Therefore the inside score of A is: Since A can be produced as A1 or A2 by its parents, its outside score is: Replacing these quantities in (2) gives us the likelihood Pn(w, T) where these two annotations and their corresponding rules have been merged, around only node n. We approximate the overall loss in data likelihood due to merging A1 and A2 everywhere in all sentences wi by the product of this loss for each local change: This expression is an approximation because it neglects interactions between instances of a symbol at multiple places in the same tree.</S><S sid = 91 ssid = >We refer to the operation of splitting annotations and re-merging some them based on likelihood loss as a split-merge (SM) cycle.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1055.txt | Citing Article:  N10-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','204'] | Reference Text:  <S sid = 63 ssid = >In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.</S><S sid = 204 ssid = >It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1055.txt | Citing Article:  P08-2026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)).</S> | Reference Offset:  ['121','201'] | Reference Text:  <S sid = 121 ssid = >Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al. (2005).</S><S sid = 201 ssid = >As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1055.txt | Citing Article:  N12-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','204'] | Reference Text:  <S sid = 63 ssid = >In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.</S><S sid = 204 ssid = >It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1055.txt | Citing Article:  W10-1405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement.</S> | Reference Offset:  ['20','127'] | Reference Text:  <S sid = 20 ssid = >However, we use a more sophisticated split-and-merge approach that allocates subsymbols adaptively where they are most effective, like a linguist would.</S><S sid = 127 ssid = >Parameter smoothing leads to even better accuracy for grammars with high complexity. producing a packed forest representation of the posterior symbol probabilities for each span.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1055.txt | Citing Article:  W12-3904.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','204'] | Reference Text:  <S sid = 63 ssid = >In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.</S><S sid = 204 ssid = >It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1055.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005).</S> | Reference Offset:  ['69','112'] | Reference Text:  <S sid = 69 ssid = >Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).</S><S sid = 112 ssid = >Matsuzaki et al. (2005) discuss two approximations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1055.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006).</S> | Reference Offset:  ['112','121'] | Reference Text:  <S sid = 112 ssid = >Matsuzaki et al. (2005) discuss two approximations.</S><S sid = 121 ssid = >Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1055.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006).</S> | Reference Offset:  ['34','69'] | Reference Text:  <S sid = 34 ssid = >Our experiments are based on a completely unannotated X-bar style grammar, obtained directly from the Penn Treebank by the binarization procedure shown in Figure 1.</S><S sid = 69 ssid = >Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1055.txt | Citing Article:  P10-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree.</S> | Reference Offset:  ['17','201'] | Reference Text:  <S sid = 17 ssid = >Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars.</S><S sid = 201 ssid = >As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1055.txt | Citing Article:  P11-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank.</S> | Reference Offset:  ['55','141'] | Reference Text:  <S sid = 55 ssid = >The results are shown in Figure 3.</S><S sid = 141 ssid = >For example, after 4 SM cycles, the Fl scores of the 4 trained grammars have a variance of only 0.024, which is tiny compared to the deviation of 0.43 obtained by Matsuzaki et al. (2005)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1055.txt | Citing Article:  P13-2110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006).</S> | Reference Offset:  ['53','123'] | Reference Text:  <S sid = 53 ssid = >Beginning with this baseline grammar, we repeatedly split and re-train the grammar.</S><S sid = 123 ssid = >As a result, the percentage of complete matches with the max-rule parser is typically higher than with the Viterbi parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1055.txt | Citing Article:  C10-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','204'] | Reference Text:  <S sid = 63 ssid = >In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.</S><S sid = 204 ssid = >It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1055.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate.</S> | Reference Offset:  ['69','112'] | Reference Text:  <S sid = 69 ssid = >Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).</S><S sid = 112 ssid = >Matsuzaki et al. (2005) discuss two approximations.</S> | Discourse Facet:  NA | Annotator: Automatic


