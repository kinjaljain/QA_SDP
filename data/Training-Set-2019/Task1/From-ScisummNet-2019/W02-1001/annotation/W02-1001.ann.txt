Citance Number: 1 | Reference Article:  W02-1001.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)).</S> | Reference Offset:  ['23','26'] | Reference Text:  <S sid = 23 ssid = >Proceedings of the Conference on Empirical Methods in Natural of tags and each tag/word pair have associated parameters.</S><S sid = 26 ssid = >In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-1001.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b).</S> | Reference Offset:  ['10','15'] | Reference Text:  <S sid = 10 ssid = >These algorithms have been shown by (Freund  Schapire 99) to be competitive with modern learning algorithms such as support vector machines; however, theyhave previously been applied mainly to classi cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems.</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-1001.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms</S><S sid = 14 ssid = >Although we concentrate on taggingproblems in this paper, the theoretical frame work and algorithm described in section 3 ofthis paper should be applicable to a wide va riety of models where Viterbi-style algorithmscan be used for decoding: examples are Proba bilistic Context-Free Grammars, or ME models for parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-1001.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks.</S> | Reference Offset:  ['10','17'] | Reference Text:  <S sid = 10 ssid = >These algorithms have been shown by (Freund  Schapire 99) to be competitive with modern learning algorithms such as support vector machines; however, theyhave previously been applied mainly to classi cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems.</S><S sid = 17 ssid = >2.1 HMM Taggers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-1001.txt | Citing Article:  P14-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action.</S> | Reference Offset:  ['15','26'] | Reference Text:  <S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S><S sid = 26 ssid = >In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-1001.txt | Citing Article:  H05-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002).</S> | Reference Offset:  ['8','15'] | Reference Text:  <S sid = 8 ssid = >(Laerty et al 2001) give experimental results suggesting that CRFs can per form signi cantly better than ME models.In this paper we describe parameter estima tion algorithms which are natural alternatives toCRFs.</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-1001.txt | Citing Article:  H05-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['12','26'] | Reference Text:  <S sid = 12 ssid = >We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S><S sid = 26 ssid = >In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-1001.txt | Citing Article:  H05-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A perceptron algorithm gives 97.11% (Collins, 2002).</S> | Reference Offset:  ['12','15'] | Reference Text:  <S sid = 12 ssid = >We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-1001.txt | Citing Article:  H05-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['12','26'] | Reference Text:  <S sid = 12 ssid = >We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S><S sid = 26 ssid = >In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-1001.txt | Citing Article:  P12-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['12','26'] | Reference Text:  <S sid = 12 ssid = >We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S><S sid = 26 ssid = >In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-1001.txt | Citing Article:  P12-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008).</S> | Reference Offset:  ['9','15'] | Reference Text:  <S sid = 9 ssid = >The algorithms are based on the percep tron algorithm (Rosenblatt 58), and the voted or averaged versions of the perceptron described in (Freund  Schapire 99).</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-1001.txt | Citing Article:  N10-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We trained this model using the averaged perceptron algorithm (Collins, 2002).</S> | Reference Offset:  ['9','15'] | Reference Text:  <S sid = 9 ssid = >The algorithms are based on the percep tron algorithm (Rosenblatt 58), and the voted or averaged versions of the perceptron described in (Freund  Schapire 99).</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-1001.txt | Citing Article:  C08-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the perceptron algorithm (Collins, 2002) to train the re-ranker.</S> | Reference Offset:  ['12','15'] | Reference Text:  <S sid = 12 ssid = >We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-1001.txt | Citing Article:  C08-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['12','26'] | Reference Text:  <S sid = 12 ssid = >We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S><S sid = 26 ssid = >In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-1001.txt | Citing Article:  C08-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (2002)'s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x X to outputs y Y.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-1001.txt | Citing Article:  N03-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-1001.txt | Citing Article:  N03-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002).</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-1001.txt | Citing Article:  N03-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-1001.txt | Citing Article:  N03-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably.</S> | Reference Offset:  ['15','21'] | Reference Text:  <S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S><S sid = 21 ssid = >Language Processing (EMNLP), Philadelphia, July 2002, pp.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-1001.txt | Citing Article:  N03-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding.</S> | Reference Offset:  ['5','15'] | Reference Text:  <S sid = 5 ssid = >Maximum-entropy (ME) models are justi ably a very popular choice for tagging problems in Natural Language Processing: for example see (Ratnaparkhi 96) for their use on part-of-speech tagging, and (McCallum et al 2000) for their use on a FAQ segmentation task.</S><S sid = 15 ssid = >See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S> | Discourse Facet:  NA | Annotator: Automatic


