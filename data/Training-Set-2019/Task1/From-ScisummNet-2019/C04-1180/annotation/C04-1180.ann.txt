Citance Number: 1 | Reference Article:  C04-1180.txt | Citing Article:  W04-3215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004).</S> | Reference Offset:  ['2','12'] | Reference Text:  <S sid = 2 ssid = >Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.</S><S sid = 12 ssid = >Andthird, there exist highly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1180.txt | Citing Article:  P13-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >CCG-based syntactic parsing (Bos et al, 2004).</S> | Reference Offset:  ['14','18'] | Reference Text:  <S sid = 14 ssid = >The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).</S><S sid = 18 ssid = >Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1180.txt | Citing Article:  D07-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 1 ssid = >This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1180.txt | Citing Article:  P06-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004).</S> | Reference Offset:  ['14','18'] | Reference Text:  <S sid = 14 ssid = >The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).</S><S sid = 18 ssid = >Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1180.txt | Citing Article:  C10-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation.</S> | Reference Offset:  ['0','12'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 12 ssid = >Andthird, there exist highly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1180.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 14 ssid = >The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1180.txt | Citing Article:  W07-1210.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bos et al (2004) derive semantic interpretations from a wide-coverage categorial grammar. There are several differences between this and RASP-RMRS, but the most important arise from the differences between CCG and RASP.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 1 ssid = >This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1180.txt | Citing Article:  P10-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steed man, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al, 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 1 ssid = >This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1180.txt | Citing Article:  C10-2168.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004).</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 2 ssid = >Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1180.txt | Citing Article:  W09-2602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 1 ssid = >This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C04-1180.txt | Citing Article:  W06-1604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics.</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 13 ssid = >The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C04-1180.txt | Citing Article:  D10-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Additionally, Bos et al (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 1 ssid = >This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C04-1180.txt | Citing Article:  W06-1807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004).</S> | Reference Offset:  ['14','18'] | Reference Text:  <S sid = 14 ssid = >The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).</S><S sid = 18 ssid = >Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C04-1180.txt | Citing Article:  W06-1807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004).</S> | Reference Offset:  ['6','13'] | Reference Text:  <S sid = 6 ssid = >However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.</S><S sid = 13 ssid = >The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C04-1180.txt | Citing Article:  D11-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 14 ssid = >The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C04-1180.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 1 ssid = >This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C04-1180.txt | Citing Article:  P06-2091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004).</S> | Reference Offset:  ['0','17'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 17 ssid = >Briscoe and Carroll (2002) gen erate underspecified semantic representations fromtheir robust parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C04-1180.txt | Citing Article:  P06-2091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004).</S> | Reference Offset:  ['15','18'] | Reference Text:  <S sid = 15 ssid = >The use of the ?-calculusis integral to our method.</S><S sid = 18 ssid = >Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C04-1180.txt | Citing Article:  P06-2091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004).</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 18 ssid = >Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C04-1180.txt | Citing Article:  P06-2091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences.</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Wide-Coverage Semantic Representations From A CCG Parser</S><S sid = 7 ssid = >There are a number of ad vantages to using CCG for this task.</S> | Discourse Facet:  NA | Annotator: Automatic


