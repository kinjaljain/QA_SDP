Citance Number: 1 | Reference Article:  P06-1066.txt | Citing Article:  W07-0706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus.</S> | Reference Offset:  ['15','205'] | Reference Text:  <S sid = 15 ssid = >In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.</S><S sid = 205 ssid = >On the contrary, the MaxEnt-based reordering model is not limited by this constraint since it is based on features of phrase, not phrase itself.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1066.txt | Citing Article:  P11-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006).</S> | Reference Offset:  ['15','63'] | Reference Text:  <S sid = 15 ssid = >In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.</S><S sid = 63 ssid = >The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1066.txt | Citing Article:  P11-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006).</S> | Reference Offset:  ['185','218'] | Reference Text:  <S sid = 185 ssid = >The flat and distortion reordering models (Row 3 and 4) show similar performance with Pharaoh.</S><S sid = 218 ssid = >It can be said that their decoder did not violate the ITG constraints but not that it observed the ITG.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1066.txt | Citing Article:  D12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences.</S> | Reference Offset:  ['5','31'] | Reference Text:  <S sid = 5 ssid = >Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently.</S><S sid = 31 ssid = >Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1066.txt | Citing Article:  D12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding.</S> | Reference Offset:  ['198','205'] | Reference Text:  <S sid = 198 ssid = >In this paper we presented a MaxEnt-based phrase reordering model for SMT.</S><S sid = 205 ssid = >On the contrary, the MaxEnt-based reordering model is not limited by this constraint since it is based on features of phrase, not phrase itself.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1066.txt | Citing Article:  D09-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent.</S> | Reference Offset:  ['12','31'] | Reference Text:  <S sid = 12 ssid = >Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either.</S><S sid = 31 ssid = >Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1066.txt | Citing Article:  D09-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases.</S> | Reference Offset:  ['8','15'] | Reference Text:  <S sid = 8 ssid = >Many systems use very simple models to reorder phrases 1.</S><S sid = 15 ssid = >In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1066.txt | Citing Article:  P14-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006).</S> | Reference Offset:  ['35','144'] | Reference Text:  <S sid = 35 ssid = >Finally, a maximum entropy classifier will be trained on the features.</S><S sid = 144 ssid = >Why do we use the first words as features?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1066.txt | Citing Article:  P14-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006).</S> | Reference Offset:  ['31','35'] | Reference Text:  <S sid = 31 ssid = >Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.</S><S sid = 35 ssid = >Finally, a maximum entropy classifier will be trained on the features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1066.txt | Citing Article:  C10-2025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1.</S> | Reference Offset:  ['0','30'] | Reference Text:  <S sid = 0 ssid = >Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation</S><S sid = 30 ssid = >The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1066.txt | Citing Article:  C10-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG).</S> | Reference Offset:  ['26','63'] | Reference Text:  <S sid = 26 ssid = >In this paper, we build a maximum entropy based classification model as the reordering model.</S><S sid = 63 ssid = >The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1066.txt | Citing Article:  C10-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule.</S> | Reference Offset:  ['43','68'] | Reference Text:  <S sid = 43 ssid = >Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order.</S><S sid = 68 ssid = >Given a source sentence c, firstly we initiate the chart with phrases from phrase translation table by applying the lexical rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1066.txt | Citing Article:  P12-3022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['56','225'] | Reference Text:  <S sid = 56 ssid = >So the phrasal reordering is totally dependent on the language model.</S><S sid = 225 ssid = >Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1066.txt | Citing Article:  P12-3022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['56','225'] | Reference Text:  <S sid = 56 ssid = >So the phrasal reordering is totally dependent on the language model.</S><S sid = 225 ssid = >Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1066.txt | Citing Article:  P09-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['56','225'] | Reference Text:  <S sid = 56 ssid = >So the phrasal reordering is totally dependent on the language model.</S><S sid = 225 ssid = >Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1066.txt | Citing Article:  W11-2150.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder.</S> | Reference Offset:  ['2','214'] | Reference Text:  <S sid = 2 ssid = >The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.</S><S sid = 214 ssid = >Beyond the MaxEnt-based reordering model, another feature deserving attention in our system is the CKY style decoder which observes the ITG.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1066.txt | Citing Article:  D11-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['56','225'] | Reference Text:  <S sid = 56 ssid = >So the phrasal reordering is totally dependent on the language model.</S><S sid = 225 ssid = >Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1066.txt | Citing Article:  P11-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['56','225'] | Reference Text:  <S sid = 56 ssid = >So the phrasal reordering is totally dependent on the language model.</S><S sid = 225 ssid = >Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1066.txt | Citing Article:  P11-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006).</S> | Reference Offset:  ['198','202'] | Reference Text:  <S sid = 198 ssid = >In this paper we presented a MaxEnt-based phrase reordering model for SMT.</S><S sid = 202 ssid = >Traditional distortion/flat-based SMT translation systems are good for learning phrase translation pairs, but learn nothing for phrasal reorderings from real-world data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1066.txt | Citing Article:  P11-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006).</S> | Reference Offset:  ['43','60'] | Reference Text:  <S sid = 43 ssid = >Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order.</S><S sid = 60 ssid = >The third one is a flat reordering model, which assigns probabilities for the straight and inverted order.</S> | Discourse Facet:  NA | Annotator: Automatic


