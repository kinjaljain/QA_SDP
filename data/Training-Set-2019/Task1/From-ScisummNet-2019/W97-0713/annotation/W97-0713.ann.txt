Citance Number: 1 | Reference Article:  W97-0713.txt | Citing Article:  W98-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, if we want to select the most important parts of a text, sentences might prove again to be too large segments (Marcu, 1997a; Teufel and Moens, 1998): in some cases, only one of the clauses that make up a sentence should be selected for summarization.</S> | Reference Offset:  ['1','10'] | Reference Text:  <S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S><S sid = 10 ssid = >In order to determine whether there exists any correspondence between what readers believe to be important and the nuclei of the RS-trees, we selected, from each of the five texts, the set of textual units that were labeled as &quot;very important&quot; by a majority of the judges For example, for text (1), we selected units 4 and 12, ic, 11% of the units Overall, the judges selected 36 units as being very important, which is approximately 22% of the units in a text The percentages of important units for the five texts were 11,36,35, 17, and 22 respectively We took the maximal scores computed for each textual unit from the RS-trees built by each analyst and selected a percentage of units that matched the percentage of important units selected by the judges In the cases in which there were ties, we selected a percentage of units that was closest to the one computed for the judges For example, we selected units 4 and 12, which represented the most important 11% of units as induced from the RS-tree btult by the first analyst However, we selected only unit 4, which represented 6% of the most important units as induced from the RS-tree built by the second analyst The reason for selecting only unit 4 for the second analyst was that units 1011, and 12 have the same score — 4 (see table 1) If we had selected units 10, 11 and 12 as well, we would have ended up selecting 22% of the units in text (1), which is farther from 11 than 6 Hence, we determined for each text the set of important units as labeled by judges and as derived from the RS-trees of those texts We calculated for each text the recall and precision of the important units derived from the RS-trees, with respect to the units labeled important by the judges The overall recall and precision was the same for both analysts 56% recall and 66% precision In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were both 25 7%, a = 0 059 In summarizing text, it is often useful to consider not only clauses, but full sentences To account for this, we considered to be important all the textual units that pertained to a sentence that was characterized by at least one important textual unit For example, we labeled as important textual units 1 to 4 in text (I), because they make up a full sentence and because unit 4 was labeled as Important For the adjusted data, we determined again the percentages of important units for the five texts and we re-calculated the mall and precision for both analysts the recall was 69% and 66% and the precision 82% and 75% respectively In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were 38 4%, a = 0 048 These results confirm that there exists a strong correlation between the nuclei of the RS-trees that pertain to a text and what readers perceive as being important in that text Given the values of recall and precision that we obtained, it Is plausible that an adequate computational treatment of discourse theories would provide most of what is needed for selecting accurately the important units in a text However, the results also suggest that RST by itself is not enough if one wants to strive for perfection • The above results not only provide strong evidence that discourse theories can be used effectively for text summarization, but also enable one to derive strategies that an automatic summarizer aught follow For example, the Spearman correlation coefficient between the judges and the first analyst, the one who did not follow the paragraph structure, was lower than the one between the judges and the second analyst It follows that most human judges are inclined to use the paragraph breaks as valuable sources of information when they interpret discourse If the atm of a summarization program is to MIMIC human behavior, it seems adequate for the program to take advantage of the paragraph structure of the texts that it analyzes Currently, the rank assignment for each textual unit in an RS-tree is done entirely on the basis of the maximal depth in the tree where that unit is salient (Marcu, 1996) Our data seem to support the fact that there exists a correlation also between the types of relations that are used to connect various textual units and the importance of those units in a text We plan to design other experiments that can provide clearcut evidence on the nature of this correlation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W97-0713.txt | Citing Article:  W98-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['13','14'] | Reference Text:  <S sid = 13 ssid = >We described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text The experiment suggests that discourse-based methods can account for detemmimg the most important units in a text with a recall and precision as high as 70% We showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system, the Microsoft Office97 summarizer The discourse-based summanzation program that we propose outperforms both the baseline and the commercial summarizer (see table 3) However, since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses, it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations</S><S sid = 14 ssid = >the invaluable help he gave me during every stage of this work and to Marilyn Mantel, David Mitchell, Kevin Schlueter, and Melanie Baliko for their advice on expenmental design and statistics I am also grateful to Marzena Makuta for her help with the RST analyses and to my colleagues and friends who volunteered to act as judges in the experiments described here This reasearch was supported by the Natural Sciences and Engineenng Research Council of Canada</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W97-0713.txt | Citing Article:  W98-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['13','14'] | Reference Text:  <S sid = 13 ssid = >We described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text The experiment suggests that discourse-based methods can account for detemmimg the most important units in a text with a recall and precision as high as 70% We showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system, the Microsoft Office97 summarizer The discourse-based summanzation program that we propose outperforms both the baseline and the commercial summarizer (see table 3) However, since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses, it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations</S><S sid = 14 ssid = >the invaluable help he gave me during every stage of this work and to Marilyn Mantel, David Mitchell, Kevin Schlueter, and Melanie Baliko for their advice on expenmental design and statistics I am also grateful to Marzena Makuta for her help with the RST analyses and to my colleagues and friends who volunteered to act as judges in the experiments described here This reasearch was supported by the Natural Sciences and Engineenng Research Council of Canada</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W97-0713.txt | Citing Article:  W98-0301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['13','14'] | Reference Text:  <S sid = 13 ssid = >We described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text The experiment suggests that discourse-based methods can account for detemmimg the most important units in a text with a recall and precision as high as 70% We showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system, the Microsoft Office97 summarizer The discourse-based summanzation program that we propose outperforms both the baseline and the commercial summarizer (see table 3) However, since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses, it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations</S><S sid = 14 ssid = >the invaluable help he gave me during every stage of this work and to Marilyn Mantel, David Mitchell, Kevin Schlueter, and Melanie Baliko for their advice on expenmental design and statistics I am also grateful to Marzena Makuta for her help with the RST analyses and to my colleagues and friends who volunteered to act as judges in the experiments described here This reasearch was supported by the Natural Sciences and Engineenng Research Council of Canada</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W97-0713.txt | Citing Article:  W12-0515.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This kind of approach has been very popular in summarization; however the difficulty of this task often requires more complex representations, and different kinds of models to learn relevance in text have been proposed, such as discourse-based (Marcu, 1997) or network-based (Salton et al, 1997) models and many others.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >From Discourse Structures To Text Summaries</S><S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W97-0713.txt | Citing Article:  W00-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al, 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997).</S> | Reference Offset:  ['1','2'] | Reference Text:  <S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S><S sid = 2 ssid = >The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;convincing&quot; samples of the output In very few cases, the direct output of a summarization program is compared with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use baps words such as &quot;greatest&quot; and &quot;significant&quot; or indicator phrases such as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important sentences use stigma words such as &quot;hardly&quot; and &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, 1971), (vi) important sentences and concepts are the highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W97-0713.txt | Citing Article:  C04-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu, 1997), while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (Hatzivassiloglou et al, 2001).</S> | Reference Offset:  ['10','11'] | Reference Text:  <S sid = 10 ssid = >In order to determine whether there exists any correspondence between what readers believe to be important and the nuclei of the RS-trees, we selected, from each of the five texts, the set of textual units that were labeled as &quot;very important&quot; by a majority of the judges For example, for text (1), we selected units 4 and 12, ic, 11% of the units Overall, the judges selected 36 units as being very important, which is approximately 22% of the units in a text The percentages of important units for the five texts were 11,36,35, 17, and 22 respectively We took the maximal scores computed for each textual unit from the RS-trees built by each analyst and selected a percentage of units that matched the percentage of important units selected by the judges In the cases in which there were ties, we selected a percentage of units that was closest to the one computed for the judges For example, we selected units 4 and 12, which represented the most important 11% of units as induced from the RS-tree btult by the first analyst However, we selected only unit 4, which represented 6% of the most important units as induced from the RS-tree built by the second analyst The reason for selecting only unit 4 for the second analyst was that units 1011, and 12 have the same score — 4 (see table 1) If we had selected units 10, 11 and 12 as well, we would have ended up selecting 22% of the units in text (1), which is farther from 11 than 6 Hence, we determined for each text the set of important units as labeled by judges and as derived from the RS-trees of those texts We calculated for each text the recall and precision of the important units derived from the RS-trees, with respect to the units labeled important by the judges The overall recall and precision was the same for both analysts 56% recall and 66% precision In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were both 25 7%, a = 0 059 In summarizing text, it is often useful to consider not only clauses, but full sentences To account for this, we considered to be important all the textual units that pertained to a sentence that was characterized by at least one important textual unit For example, we labeled as important textual units 1 to 4 in text (I), because they make up a full sentence and because unit 4 was labeled as Important For the adjusted data, we determined again the percentages of important units for the five texts and we re-calculated the mall and precision for both analysts the recall was 69% and 66% and the precision 82% and 75% respectively In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were 38 4%, a = 0 048 These results confirm that there exists a strong correlation between the nuclei of the RS-trees that pertain to a text and what readers perceive as being important in that text Given the values of recall and precision that we obtained, it Is plausible that an adequate computational treatment of discourse theories would provide most of what is needed for selecting accurately the important units in a text However, the results also suggest that RST by itself is not enough if one wants to strive for perfection • The above results not only provide strong evidence that discourse theories can be used effectively for text summarization, but also enable one to derive strategies that an automatic summarizer aught follow For example, the Spearman correlation coefficient between the judges and the first analyst, the one who did not follow the paragraph structure, was lower than the one between the judges and the second analyst It follows that most human judges are inclined to use the paragraph breaks as valuable sources of information when they interpret discourse If the atm of a summarization program is to MIMIC human behavior, it seems adequate for the program to take advantage of the paragraph structure of the texts that it analyzes Currently, the rank assignment for each textual unit in an RS-tree is done entirely on the basis of the maximal depth in the tree where that unit is salient (Marcu, 1996) Our data seem to support the fact that there exists a correlation also between the types of relations that are used to connect various textual units and the importance of those units in a text We plan to design other experiments that can provide clearcut evidence on the nature of this correlation</S><S sid = 11 ssid = >Our summarization program relies on a rhetorical parser that builds RS-trees for unrestricted texts The mathematical foundations of the rhetorical parsing algonthm rely on a firse-Order formalization of valid text structures (Marcu, 1997b) The assumptions of the formalization are the following 1 The elementary units of complex text structures are non-overlapping spans of text 2 Rhetorical, coherence, and cohesive relations hold between textual units of various sizes 3 Relations can be partitioned into two classes paratactic and hypotactic Paratactic relations are those that hold between spans of equal importance Hypotactic relations are those that hold between a span that is essential for the writer's purpose, i e, a nucleus, and a span that increases the understanding of the nucleus but is not essential for the writer's purpose, i a, a satellite 4 The abstract structure of most texts is a binary, tree-like structure 5 If a relation holds between two textual spans of the tree structure of a text, that relation also holds between the most important units of the constituent subspans The most important units of a textual span are determined recursively they correspond to the most important units of the unmediate subspans when the relation that holds between these subspans is paratactic, and to the most important units of the nucleus subspan when the relation that holds between the immediate subspans is hypotacuc The rhetoncal parsing algorithm, which is outlined. in figure 1, is based on a comprehensive corpus analysis of more than 450 discourse markers and 7900 text fragments (see (Marcu, 1997b) for details) When given a text, the rhetorical parser determines first the discourse markers and the elementary units that make up that text The parser uses then the information derived from the corpus analysis in order to hypothesize rhetoncal relations among the elementary units In the end, the parser applies a constraint-satisfaction procedure to detemune the text structures that are valid If more than one valid structure is found, the parser chooses one that is the &quot;best&quot; according to a given metric The details of the algorithms that INPUT a text T I Determine the set D of all discourse markers in T and the set UT of elementary textual units in T 2 Hypothesize a set of relations R between the elements of UT 3 Determine the set ValTrees of all valid RS-trees of T that can be built using relations from R 4 Determine the &quot;best&quot; RS-tree in ValTrees on the basis of a metric that assigns higher weights to the trees that are more skewed to the right are used by the rethoncal parser are discussed at length in (Marcu, 1997a, Marcu, 1997b) When the rhetorical parser takes text (I) as input, it produces the RS-tree in figure 2 The convention that we use is that nuclei are surrounded by solid boxes and satellites by dotted boxes, the links between a node and a subordinate nucleus or nuclei are represented by solid arrows, and the links between a node and a subordinate satellite by dotted Imes The nodes with only one satellite denote occurrences of parenthetical information for example, textual unit 2 is labeled as parenthetical to the textual unit that results from juxtaposing 1 and 3 The numbers associated with each leaf correspond to the numencal labels in text (1) The numbers associated with each internal node correspond to the salient units of that node and are explicitly represented in the RS-tree By inspecting the RS-tree in figure 2, one can nonce that the trees that are built by the program do not have the same granularity as the trees constructed by the analysts For example, the program treats units 13,14, and 15 as one elementary unit However, as we argue in (Marcie, 1997b), the corpus analysis on which our parser is built supports the observation that, in most cases, the global structure of the RS-tree is not affected by the inability of the rhetorical parser to uncover all clauses in a text — most of the clauses that are not uncovered are nuclei of Mir relations The summarization program takes the RS-tree produced by the rhetorical parser and selects the textual units that are most salient in that text If the aim of the program is to produce just a very short summary, only the salient units associated with the internal nodes found closer to the root are selected The longer the summary one wants to generate, the farther the selected salient units will be from the root In fact, one can see that the RS-trees built by the rhetoncal parser induce a partial order on the Importance of the textual units For text (1), the most important unit is 4 The textual units that are salient in the nodes found one level below represent the next level of importance (in this case, unit 12— unit 4 was already accounted for) The next level contains units 5,6, 16, and 18, and so on To evaluate our program, we associated with each textual unit in the RS-trees built by the rhetorical parser a score in the same way we did for the RS-trees built by the analysts For example, the RS-tree in figure 2 has a depth of 6 Because unit 4 is salient for the root, it gets a score of 6 Units 5,6 are salient for an internal node found two levels below the root therefore, their score is 4 Unit 9 is salient for a leaf found five levels below the root therefore, its score is 1 Table 1 presents the scores associated by our summarization program to each unit in text (1) We used the importance scores assigned by our program to compute statistics similar to those discussed in the previous section When the program selected only the textual units with the highest scores, in percentages that were equal to those of the judges, the recall was 53% and the precision was 50% When the program selected the full sentences that were associated with the most unportant units, in percentages that were equal to those of the judges, the recall was 66% and the precision 68% The lower recall and precision scores associated with clauses seem to be caused primarily by the difference in granularity with respect to the way the texts were broken into subunits the program does not recover all minimal textual units, and as a consequence, its assignment of Importance scores is coarser When full sentences are considered, the judges and the program work at the same level of granularity, and as a consequence, the summarization results improve significantly</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W97-0713.txt | Citing Article:  W04-2607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We intend to investigate any potential linkages between the word groups in the texts and other theories that provide pre-determined structures of text, such as Rhetorical Structure Theory (Marcu, 1997).</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >From Discourse Structures To Text Summaries</S><S sid = 10 ssid = >In order to determine whether there exists any correspondence between what readers believe to be important and the nuclei of the RS-trees, we selected, from each of the five texts, the set of textual units that were labeled as &quot;very important&quot; by a majority of the judges For example, for text (1), we selected units 4 and 12, ic, 11% of the units Overall, the judges selected 36 units as being very important, which is approximately 22% of the units in a text The percentages of important units for the five texts were 11,36,35, 17, and 22 respectively We took the maximal scores computed for each textual unit from the RS-trees built by each analyst and selected a percentage of units that matched the percentage of important units selected by the judges In the cases in which there were ties, we selected a percentage of units that was closest to the one computed for the judges For example, we selected units 4 and 12, which represented the most important 11% of units as induced from the RS-tree btult by the first analyst However, we selected only unit 4, which represented 6% of the most important units as induced from the RS-tree built by the second analyst The reason for selecting only unit 4 for the second analyst was that units 1011, and 12 have the same score — 4 (see table 1) If we had selected units 10, 11 and 12 as well, we would have ended up selecting 22% of the units in text (1), which is farther from 11 than 6 Hence, we determined for each text the set of important units as labeled by judges and as derived from the RS-trees of those texts We calculated for each text the recall and precision of the important units derived from the RS-trees, with respect to the units labeled important by the judges The overall recall and precision was the same for both analysts 56% recall and 66% precision In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were both 25 7%, a = 0 059 In summarizing text, it is often useful to consider not only clauses, but full sentences To account for this, we considered to be important all the textual units that pertained to a sentence that was characterized by at least one important textual unit For example, we labeled as important textual units 1 to 4 in text (I), because they make up a full sentence and because unit 4 was labeled as Important For the adjusted data, we determined again the percentages of important units for the five texts and we re-calculated the mall and precision for both analysts the recall was 69% and 66% and the precision 82% and 75% respectively In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were 38 4%, a = 0 048 These results confirm that there exists a strong correlation between the nuclei of the RS-trees that pertain to a text and what readers perceive as being important in that text Given the values of recall and precision that we obtained, it Is plausible that an adequate computational treatment of discourse theories would provide most of what is needed for selecting accurately the important units in a text However, the results also suggest that RST by itself is not enough if one wants to strive for perfection • The above results not only provide strong evidence that discourse theories can be used effectively for text summarization, but also enable one to derive strategies that an automatic summarizer aught follow For example, the Spearman correlation coefficient between the judges and the first analyst, the one who did not follow the paragraph structure, was lower than the one between the judges and the second analyst It follows that most human judges are inclined to use the paragraph breaks as valuable sources of information when they interpret discourse If the atm of a summarization program is to MIMIC human behavior, it seems adequate for the program to take advantage of the paragraph structure of the texts that it analyzes Currently, the rank assignment for each textual unit in an RS-tree is done entirely on the basis of the maximal depth in the tree where that unit is salient (Marcu, 1996) Our data seem to support the fact that there exists a correlation also between the types of relations that are used to connect various textual units and the importance of those units in a text We plan to design other experiments that can provide clearcut evidence on the nature of this correlation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W97-0713.txt | Citing Article:  W02-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >RST can be used in sentence selection for single document summarization [Marcu, 1997].</S> | Reference Offset:  ['1','2'] | Reference Text:  <S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S><S sid = 2 ssid = >The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;convincing&quot; samples of the output In very few cases, the direct output of a summarization program is compared with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use baps words such as &quot;greatest&quot; and &quot;significant&quot; or indicator phrases such as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important sentences use stigma words such as &quot;hardly&quot; and &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, 1971), (vi) important sentences and concepts are the highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W97-0713.txt | Citing Article:  W00-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ono et al (1994), T'sou et al (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST).</S> | Reference Offset:  ['1','12'] | Reference Text:  <S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S><S sid = 12 ssid = >We are not aware of any RST-based summarization program for English However, Ono et al (1994) discuss a summarization program for Japanese whose minimal textual units are sentences Due to the differences between English and Japanese, it was impossible for us to compare Ono's summarizer with ours Fundamental differences concerning the assumptions that underlie One's work and ours are discussed at length in (Marcu, 1997b) We were able to obtain only one other program that summarizes English text — the one included in the Microsoft Office97 package We run the Microsoft summanzation program on the five texts from Scientific American and selected the same percentages of textual units as those considered important by the judges When we selected percentages of text that corresponded only to the clauses considered important by the judges, the Microsoft program recalled 28% of the units, with a precision of 26% When we selected percentages of text that corresponded to sentences considered important by the judges, the Microsoft program recalled 41% of the units, with a precision of 39% Al] Microsoft figures are only slightly above those that correspond to the baseline algontlims that select important units randomly It follows that our program outperforms significantly the one found in the Office97 package We are not aware of any other summarization program that can build summaries with granularity as fine as a clause (as our program can)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W97-0713.txt | Citing Article:  W00-1206.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ono et al (1994), T'sou et al (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST, Mann and Thompson 1986).</S> | Reference Offset:  ['3','12'] | Reference Text:  <S sid = 3 ssid = >Researchers in computational linguistics (Mann and Thompson, 1988, Matthiessen and Thompson, 1988, Sparck Jones, 1993) have long speculated that the nuclei that pertain to a rhetorical structure tree (RS-tree) (Mann and Thompson, 1988) constitute an adequate summanzation of the text for which that RS-tree was built However, to our knowledge, there was no experiment to confirm how valid this speculation really is In what follows, we describe an experiment that shows that there exists a strong correlation between the nuclei of the RS-tree of a text and what readers perceive to be the most important units in a text We know from the results reported in the psychological literature on summarization (Johnson, 1970, Chou Hare and Borchardt, 1984, Sherrard, 1989) that there exists a certain degree of disagreement between readers with respect to the importance that they assign to various textual units and that the disagreement is dependent on the quality of the text and the comprehension and summarization skills of the readers (Winograd.</S><S sid = 12 ssid = >We are not aware of any RST-based summarization program for English However, Ono et al (1994) discuss a summarization program for Japanese whose minimal textual units are sentences Due to the differences between English and Japanese, it was impossible for us to compare Ono's summarizer with ours Fundamental differences concerning the assumptions that underlie One's work and ours are discussed at length in (Marcu, 1997b) We were able to obtain only one other program that summarizes English text — the one included in the Microsoft Office97 package We run the Microsoft summanzation program on the five texts from Scientific American and selected the same percentages of textual units as those considered important by the judges When we selected percentages of text that corresponded only to the clauses considered important by the judges, the Microsoft program recalled 28% of the units, with a precision of 26% When we selected percentages of text that corresponded to sentences considered important by the judges, the Microsoft program recalled 41% of the units, with a precision of 39% Al] Microsoft figures are only slightly above those that correspond to the baseline algontlims that select important units randomly It follows that our program outperforms significantly the one found in the Office97 package We are not aware of any other summarization program that can build summaries with granularity as fine as a clause (as our program can)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W97-0713.txt | Citing Article:  P00-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al, 1973), probabilistic measures for token salience (Salton et al, 1997), and the use of implicit discourse structure (Marcu,1997).</S> | Reference Offset:  ['1','2'] | Reference Text:  <S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S><S sid = 2 ssid = >The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;convincing&quot; samples of the output In very few cases, the direct output of a summarization program is compared with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use baps words such as &quot;greatest&quot; and &quot;significant&quot; or indicator phrases such as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important sentences use stigma words such as &quot;hardly&quot; and &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, 1971), (vi) important sentences and concepts are the highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W97-0713.txt | Citing Article:  W06-3401.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Theories such as RST have been popular for sometime as a way of describing the multi-levelled rhetorical relations that exist in text, with relevant applications such as automatic summarization (Marcu, 1997) and natural language generation (Knott and Dale, 1996).</S> | Reference Offset:  ['1','11'] | Reference Text:  <S sid = 1 ssid = >We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view</S><S sid = 11 ssid = >Our summarization program relies on a rhetorical parser that builds RS-trees for unrestricted texts The mathematical foundations of the rhetorical parsing algonthm rely on a firse-Order formalization of valid text structures (Marcu, 1997b) The assumptions of the formalization are the following 1 The elementary units of complex text structures are non-overlapping spans of text 2 Rhetorical, coherence, and cohesive relations hold between textual units of various sizes 3 Relations can be partitioned into two classes paratactic and hypotactic Paratactic relations are those that hold between spans of equal importance Hypotactic relations are those that hold between a span that is essential for the writer's purpose, i e, a nucleus, and a span that increases the understanding of the nucleus but is not essential for the writer's purpose, i a, a satellite 4 The abstract structure of most texts is a binary, tree-like structure 5 If a relation holds between two textual spans of the tree structure of a text, that relation also holds between the most important units of the constituent subspans The most important units of a textual span are determined recursively they correspond to the most important units of the unmediate subspans when the relation that holds between these subspans is paratactic, and to the most important units of the nucleus subspan when the relation that holds between the immediate subspans is hypotacuc The rhetoncal parsing algorithm, which is outlined. in figure 1, is based on a comprehensive corpus analysis of more than 450 discourse markers and 7900 text fragments (see (Marcu, 1997b) for details) When given a text, the rhetorical parser determines first the discourse markers and the elementary units that make up that text The parser uses then the information derived from the corpus analysis in order to hypothesize rhetoncal relations among the elementary units In the end, the parser applies a constraint-satisfaction procedure to detemune the text structures that are valid If more than one valid structure is found, the parser chooses one that is the &quot;best&quot; according to a given metric The details of the algorithms that INPUT a text T I Determine the set D of all discourse markers in T and the set UT of elementary textual units in T 2 Hypothesize a set of relations R between the elements of UT 3 Determine the set ValTrees of all valid RS-trees of T that can be built using relations from R 4 Determine the &quot;best&quot; RS-tree in ValTrees on the basis of a metric that assigns higher weights to the trees that are more skewed to the right are used by the rethoncal parser are discussed at length in (Marcu, 1997a, Marcu, 1997b) When the rhetorical parser takes text (I) as input, it produces the RS-tree in figure 2 The convention that we use is that nuclei are surrounded by solid boxes and satellites by dotted boxes, the links between a node and a subordinate nucleus or nuclei are represented by solid arrows, and the links between a node and a subordinate satellite by dotted Imes The nodes with only one satellite denote occurrences of parenthetical information for example, textual unit 2 is labeled as parenthetical to the textual unit that results from juxtaposing 1 and 3 The numbers associated with each leaf correspond to the numencal labels in text (1) The numbers associated with each internal node correspond to the salient units of that node and are explicitly represented in the RS-tree By inspecting the RS-tree in figure 2, one can nonce that the trees that are built by the program do not have the same granularity as the trees constructed by the analysts For example, the program treats units 13,14, and 15 as one elementary unit However, as we argue in (Marcie, 1997b), the corpus analysis on which our parser is built supports the observation that, in most cases, the global structure of the RS-tree is not affected by the inability of the rhetorical parser to uncover all clauses in a text — most of the clauses that are not uncovered are nuclei of Mir relations The summarization program takes the RS-tree produced by the rhetorical parser and selects the textual units that are most salient in that text If the aim of the program is to produce just a very short summary, only the salient units associated with the internal nodes found closer to the root are selected The longer the summary one wants to generate, the farther the selected salient units will be from the root In fact, one can see that the RS-trees built by the rhetoncal parser induce a partial order on the Importance of the textual units For text (1), the most important unit is 4 The textual units that are salient in the nodes found one level below represent the next level of importance (in this case, unit 12— unit 4 was already accounted for) The next level contains units 5,6, 16, and 18, and so on To evaluate our program, we associated with each textual unit in the RS-trees built by the rhetorical parser a score in the same way we did for the RS-trees built by the analysts For example, the RS-tree in figure 2 has a depth of 6 Because unit 4 is salient for the root, it gets a score of 6 Units 5,6 are salient for an internal node found two levels below the root therefore, their score is 4 Unit 9 is salient for a leaf found five levels below the root therefore, its score is 1 Table 1 presents the scores associated by our summarization program to each unit in text (1) We used the importance scores assigned by our program to compute statistics similar to those discussed in the previous section When the program selected only the textual units with the highest scores, in percentages that were equal to those of the judges, the recall was 53% and the precision was 50% When the program selected the full sentences that were associated with the most unportant units, in percentages that were equal to those of the judges, the recall was 66% and the precision 68% The lower recall and precision scores associated with clauses seem to be caused primarily by the difference in granularity with respect to the way the texts were broken into subunits the program does not recover all minimal textual units, and as a consequence, its assignment of Importance scores is coarser When full sentences are considered, the judges and the program work at the same level of granularity, and as a consequence, the summarization results improve significantly</S> | Discourse Facet:  NA | Annotator: Automatic


