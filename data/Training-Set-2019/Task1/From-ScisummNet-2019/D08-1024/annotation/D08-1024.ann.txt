Citance Number: 1 | Reference Article:  D08-1024.txt | Citing Article:  N09-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment.</S> | Reference Offset:  ['19','118'] | Reference Text:  <S sid = 19 ssid = >First, we generalize Marton and Resnik’s (2008) soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model.</S><S sid = 118 ssid = >We then define coarse- and fine-grained versions of the structural distortion model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1024.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU.</S> | Reference Offset:  ['39','42'] | Reference Text:  <S sid = 39 ssid = >Sentence-level approximations to BLEU exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLEU computations in the context of a set O of previously-translated sentences, following Watanabe et al. (2007).</S><S sid = 42 ssid = >These counts are sufficient to calculate a BLEU score, which we write as BLEU(c(e)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1024.txt | Citing Article:  P11-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length.</S> | Reference Offset:  ['111','163'] | Reference Text:  <S sid = 111 ssid = >In a phrase-based model, reordering is performed both within phrase pairs and by the phrasereordering model.</S><S sid = 163 ssid = >Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1024.txt | Citing Article:  D12-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008).</S> | Reference Offset:  ['2','15'] | Reference Text:  <S sid = 2 ssid = >Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.</S><S sid = 15 ssid = >This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1024.txt | Citing Article:  P10-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008).</S> | Reference Offset:  ['23','94'] | Reference Text:  <S sid = 23 ssid = >The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007).</S><S sid = 94 ssid = >Marton and Resnik optimized their features’ weights using MERT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1024.txt | Citing Article:  P13-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008).</S> | Reference Offset:  ['141','145'] | Reference Text:  <S sid = 141 ssid = >First of all, we find that MIRA is competitive with MERT when both use the baseline feature set.</S><S sid = 145 ssid = >When training with MERT, the coarse-grained pair of syntax features yields a small improvement, but the finegrained syntax features do not yield any further improvement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1024.txt | Citing Article:  P13-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008).</S> | Reference Offset:  ['9','14'] | Reference Text:  <S sid = 9 ssid = >One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information.</S><S sid = 14 ssid = >There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1024.txt | Citing Article:  P13-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008).</S> | Reference Offset:  ['2','87'] | Reference Text:  <S sid = 2 ssid = >Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.</S><S sid = 87 ssid = >The first features we explore are based on a line of research introduced by Chiang (2005) and improved on by Marton and Resnik (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1024.txt | Citing Article:  W11-2113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.</S><S sid = 168 ssid = >This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1024.txt | Citing Article:  W11-2130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score.</S> | Reference Offset:  ['14','54'] | Reference Text:  <S sid = 14 ssid = >There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).</S><S sid = 54 ssid = >Here, we introduce a new oracle-translation selection method, formulating the intuition behind local updating as an optimization problem: Instead of choosing the highest-BLEU translation from an n-best list, we choose the translation that maximizes a combination of (approximate) BLEU and the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1024.txt | Citing Article:  P12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.</S> | Reference Offset:  ['14','91'] | Reference Text:  <S sid = 14 ssid = >There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).</S><S sid = 91 ssid = >However, this more syntactically aware model, when tested in Chinese-English translation, did not improve translation performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1024.txt | Citing Article:  P12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008).</S> | Reference Offset:  ['2','157'] | Reference Text:  <S sid = 2 ssid = >Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.</S><S sid = 157 ssid = >Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1024.txt | Citing Article:  E12-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe.</S> | Reference Offset:  ['14','23'] | Reference Text:  <S sid = 14 ssid = >There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).</S><S sid = 23 ssid = >The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1024.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008).</S> | Reference Offset:  ['2','156'] | Reference Text:  <S sid = 2 ssid = >Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.</S><S sid = 156 ssid = >In this paper, we have brought together two existing lines of work: the training method of Watanabe et al. (2007), and the models of Chiang (2005) and Marton and Resnik (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1024.txt | Citing Article:  N12-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008).</S> | Reference Offset:  ['14','15'] | Reference Text:  <S sid = 14 ssid = >There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).</S><S sid = 15 ssid = >This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1024.txt | Citing Article:  D09-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008).</S> | Reference Offset:  ['0','7'] | Reference Text:  <S sid = 0 ssid = >Online Large-Margin Training of Syntactic and Structural Translation Features</S><S sid = 7 ssid = >Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1024.txt | Citing Article:  P13-1156.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment.</S> | Reference Offset:  ['92','126'] | Reference Text:  <S sid = 92 ssid = >Recently, Marton and Resnik (2008) revisited the idea of constituency features, and succeeded in showing that finer-grained soft syntactic constraints yield substantial improvements in BLEU score for both Chinese-English and Arabic-English translation.</S><S sid = 126 ssid = >We now describe our experiments to test MIRA and our features, the soft-syntactic constraints and the structural distortion features, on an Arabic-English translation task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1024.txt | Citing Article:  P10-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account.</S> | Reference Offset:  ['65','66'] | Reference Text:  <S sid = 65 ssid = >This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions.</S><S sid = 66 ssid = >Since our loss function cannot be so decomposed, we select: The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1024.txt | Citing Article:  P12-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008).</S> | Reference Offset:  ['2','14'] | Reference Text:  <S sid = 2 ssid = >Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.</S><S sid = 14 ssid = >There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1024.txt | Citing Article:  D09-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.</S><S sid = 168 ssid = >This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.</S> | Discourse Facet:  NA | Annotator: Automatic


