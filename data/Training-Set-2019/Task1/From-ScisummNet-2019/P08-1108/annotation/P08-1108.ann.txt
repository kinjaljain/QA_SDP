Citance Number: 1 | Reference Article:  P08-1108.txt | Citing Article:  D08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models.</S> | Reference Offset:  ['19','146'] | Reference Text:  <S sid = 19 ssid = >Both models have been used to achieve state-of-the-art accuracy for a wide range of languages, as shown in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), but McDonald and Nivre (2007) showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models.</S><S sid = 146 ssid = >The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1108.txt | Citing Article:  D10-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.</S><S sid = 160 ssid = >Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1108.txt | Citing Article:  W10-1404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers.</S> | Reference Offset:  ['3','158'] | Reference Text:  <S sid = 3 ssid = >By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.</S><S sid = 158 ssid = >Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1108.txt | Citing Article:  W10-1404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first.</S> | Reference Offset:  ['9','30'] | Reference Text:  <S sid = 9 ssid = >Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007).</S><S sid = 30 ssid = >An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1108.txt | Citing Article:  W09-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008).</S> | Reference Offset:  ['0','143'] | Reference Text:  <S sid = 0 ssid = >Integrating Graph-Based and Transition-Based Dependency Parsers</S><S sid = 143 ssid = >Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006), who report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1108.txt | Citing Article:  W10-3007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008).</S> | Reference Offset:  ['124','128'] | Reference Text:  <S sid = 124 ssid = >Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt.</S><S sid = 128 ssid = >But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1108.txt | Citing Article:  C10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.</S><S sid = 160 ssid = >Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1108.txt | Citing Article:  C10-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers.</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Integrating Graph-Based and Transition-Based Dependency Parsers</S><S sid = 9 ssid = >Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1108.txt | Citing Article:  C10-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising.</S> | Reference Offset:  ['30','127'] | Reference Text:  <S sid = 30 ssid = >An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a).</S><S sid = 127 ssid = >As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1108.txt | Citing Article:  P11-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.</S><S sid = 160 ssid = >Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1108.txt | Citing Article:  P12-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser.</S> | Reference Offset:  ['127','136'] | Reference Text:  <S sid = 127 ssid = >As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007).</S><S sid = 136 ssid = >But where MST is good, MSTMalt is often significantly better.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1108.txt | Citing Article:  P09-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers.</S> | Reference Offset:  ['68','150'] | Reference Text:  <S sid = 68 ssid = >To train a guided version BC of base model B with guide model C and training set T, the guided model is trained, not on the original training set T, but on a version of T that has been parsed with the guide model C under a cross-validation scheme (to avoid overlap with training data for C).</S><S sid = 150 ssid = >Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1108.txt | Citing Article:  E09-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism.</S> | Reference Offset:  ['39','138'] | Reference Text:  <S sid = 39 ssid = >To learn a scoring function on transitions, these systems rely on discriminative learning methods, such as memory-based learning or support vector machines, using a strictly local learning procedure where only single transitions are scored (not complete transition sequences).</S><S sid = 138 ssid = >Although both Malt and MST use discriminative algorithms, Malt uses a batch learning algorithm (SVM) and MST uses an online learning algorithm (MIRA).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1108.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008).</S> | Reference Offset:  ['101','128'] | Reference Text:  <S sid = 101 ssid = >For example, one could start with a Malt model, use it to train a guided MSTMalt model, then use that as the guide to train a MaltMSTM.,t model, etc.</S><S sid = 128 ssid = >But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1108.txt | Citing Article:  P11-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008).</S> | Reference Offset:  ['149','150'] | Reference Text:  <S sid = 149 ssid = >(2004), who trained classifiers on auxiliary data to guide named entity classifiers.</S><S sid = 150 ssid = >Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1108.txt | Citing Article:  P09-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008).</S> | Reference Offset:  ['14','127'] | Reference Text:  <S sid = 14 ssid = >Theoretically, these approaches are very different.</S><S sid = 127 ssid = >As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1108.txt | Citing Article:  P09-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.</S><S sid = 160 ssid = >Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1108.txt | Citing Article:  P09-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.</S> | Reference Offset:  ['124','128'] | Reference Text:  <S sid = 124 ssid = >Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt.</S><S sid = 128 ssid = >But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1108.txt | Citing Article:  N10-1115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.</S><S sid = 160 ssid = >Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1108.txt | Citing Article:  W10-2927.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser.</S> | Reference Offset:  ['124','128'] | Reference Text:  <S sid = 124 ssid = >Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt.</S><S sid = 128 ssid = >But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.</S> | Discourse Facet:  NA | Annotator: Automatic


