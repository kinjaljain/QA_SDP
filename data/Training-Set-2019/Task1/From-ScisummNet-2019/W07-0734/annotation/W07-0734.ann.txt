Citance Number: 1 | Reference Article:  W07-0734.txt | Citing Article:  W07-0718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['48','59'] | Reference Text:  <S sid = 48 ssid = >METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules.</S><S sid = 59 ssid = >For Spanish, German and French we used the evaluation data provided by the shared task at last year’s WMT workshop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W07-0734.txt | Citing Article:  P13-2073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W07-0734.txt | Citing Article:  W10-0102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data.</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W07-0734.txt | Citing Article:  W10-1760.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.</S><S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W07-0734.txt | Citing Article:  W09-2301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W07-0734.txt | Citing Article:  W08-0329.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The translation quality is measured by three MT evaluation metrics: TER (Snover et al, 2006), BLEU (Papineni et al, 2002), and METEOR (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W07-0734.txt | Citing Article:  W08-0324.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006).</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W07-0734.txt | Citing Article:  P11-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.</S><S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W07-0734.txt | Citing Article:  W08-1808.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task.</S> | Reference Offset:  ['12','13'] | Reference Text:  <S sid = 12 ssid = >METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level.</S><S sid = 13 ssid = >Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W07-0734.txt | Citing Article:  P09-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1: Feature sets for regression learning can easily retrain the learner under different conditions, therefore enabling our method to be applied to sentence-level translation selection fro many sets of translation systems without any additional human work.</S> | Reference Offset:  ['61','75'] | Reference Text:  <S sid = 61 ssid = >Some, but not all, of these data sets have multiple human judgments per translation hypothesis.</S><S sid = 75 ssid = >Recall, however, is still given more weight than precision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W07-0734.txt | Citing Article:  P10-2067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality.</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W07-0734.txt | Citing Article:  W10-0710.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments</S><S sid = 1 ssid = >an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W07-0734.txt | Citing Article:  N10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007).</S> | Reference Offset:  ['40','57'] | Reference Text:  <S sid = 40 ssid = >In the latest release, we tuned these parameters to optimize correlation with human judgments based on more extensive experimentation, as reported in section 4.</S><S sid = 57 ssid = >We suspected that parameters that were optimized to maximize correlation with human judgments for English would not necessarily be optimal for other languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W07-0734.txt | Citing Article:  N10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008).</S> | Reference Offset:  ['78','81'] | Reference Text:  <S sid = 78 ssid = >When optimizing correlation with the sum of adequacy and fluency, optimal values fall in between the values found for adequacy and fluency.</S><S sid = 81 ssid = >Resulting parameters are shown in Table 4.3.2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W07-0734.txt | Citing Article:  N10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al, 2002) with a maximum N gram length of 4, TER (Snover et al, 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter).</S> | Reference Offset:  ['11','13'] | Reference Text:  <S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S><S sid = 13 ssid = >Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W07-0734.txt | Citing Article:  W11-2203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010).</S> | Reference Offset:  ['0','85'] | Reference Text:  <S sid = 0 ssid = >METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments</S><S sid = 85 ssid = >In this paper we described newly developed language-specific instances of the METEOR metric and the process of optimizing metric parameters for different human measures of translation quality and for different languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W07-0734.txt | Citing Article:  D08-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures.</S> | Reference Offset:  ['11','18'] | Reference Text:  <S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S><S sid = 18 ssid = >METEOR evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W07-0734.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12.</S> | Reference Offset:  ['7','11'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 11 ssid = >METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W07-0734.txt | Citing Article:  P08-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in Open CCG using a corpus-derived grammar.</S> | Reference Offset:  ['7','21'] | Reference Text:  <S sid = 7 ssid = >The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).</S><S sid = 21 ssid = >An alignment is mapping between words, such that every word in each string maps to at most one word in the other string.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W07-0734.txt | Citing Article:  W11-1604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available.</S> | Reference Offset:  ['54','58'] | Reference Text:  <S sid = 54 ssid = >Human judgments come in the form of “adequacy” and “fluency&quot; quantitative scores.</S><S sid = 58 ssid = >For English, we used the NIST 2003 Arabic-toEnglish MT evaluation data for training and the NIST 2004 Arabic-to-English evaluation data for testing.</S> | Discourse Facet:  NA | Annotator: Automatic


