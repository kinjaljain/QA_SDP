Citance Number: 1 | Reference Article:  C04-1059.txt | Citing Article:  W07-0717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus.</S> | Reference Offset:  ['12','35'] | Reference Text:  <S sid = 12 ssid = >P(t) is the target language model and P(s|t) is the translation model.</S><S sid = 35 ssid = >Given a baseline statistical machine translation system, the language model adaptation is done in several steps shown as follows: ? Generate a set of initial translation hypotheses H = {h1 ?hn} for source sentences s, using either the baseline MT system with the background language model or only the translation model ? Use H to build query ? Use query to retrieve relevant sentences from the large corpus ? Build specific language models from retrieved sentences ? Interpolate the specific language model with the background language ? Re-translate sentences s with adapted language model Figure-1: Adaptation Algorithm The specific language model )|( hwP iA and the general background model )|( hwP iB are combined using linear interpolation: )|()1()|()|(?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1059.txt | Citing Article:  P13-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Language Model Adaptation For Statistical Machine Translation Via Structured Query Models</S><S sid = 1 ssid = >We explore unsupervised language model adaptation techniques for Statistical Machine Translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1059.txt | Citing Article:  D10-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >individual target hypotheses (Zhao et al, 2004).</S> | Reference Offset:  ['10','12'] | Reference Text:  <S sid = 10 ssid = >Statistical machine translation is based on the noisy channel model, where the translation hypothesis is searched over the space defined by a translation model and a target language (Brown et al, 1993).</S><S sid = 12 ssid = >P(t) is the target language model and P(s|t) is the translation model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1059.txt | Citing Article:  W11-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model.</S> | Reference Offset:  ['22','35'] | Reference Text:  <S sid = 22 ssid = >Our approach can be characterized as unsupervised data augmentation by retrieval of relevant documents from large monolingual corpora, and interpolation of the specific language model, build from the retrieved data, with a background language model.</S><S sid = 35 ssid = >Given a baseline statistical machine translation system, the language model adaptation is done in several steps shown as follows: ? Generate a set of initial translation hypotheses H = {h1 ?hn} for source sentences s, using either the baseline MT system with the background language model or only the translation model ? Use H to build query ? Use query to retrieve relevant sentences from the large corpus ? Build specific language models from retrieved sentences ? Interpolate the specific language model with the background language ? Re-translate sentences s with adapted language model Figure-1: Adaptation Algorithm The specific language model )|( hwP iA and the general background model )|( hwP iB are combined using linear interpolation: )|()1()|()|(?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1059.txt | Citing Article:  W10-1759.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora.</S> | Reference Offset:  ['2','125'] | Reference Text:  <S sid = 2 ssid = >The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.</S><S sid = 125 ssid = >For each of the 4 corpora different numbers of similar sentences (1, 10, 100, and 1000) were retrieved to build specific language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1059.txt | Citing Article:  P11-2078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection.</S> | Reference Offset:  ['2','25'] | Reference Text:  <S sid = 2 ssid = >The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.</S><S sid = 25 ssid = >Then these translations hypotheses are reformulated as queries to retrieve similar sentences from a very large text collection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1059.txt | Citing Article:  W09-0432.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Refinements of this approach are described in (Zhao et al., 2004).</S> | Reference Offset:  ['46','79'] | Reference Text:  <S sid = 46 ssid = >We follow (Eck, et al, 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories.</S><S sid = 79 ssid = >After extraction of the information, structured query models are proposed using the structured query language, described in the Section 3.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1059.txt | Citing Article:  D12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004).</S> | Reference Offset:  ['60','169'] | Reference Text:  <S sid = 60 ssid = >But it can miss some informative translation words, which could lead to better-adapted language models.</S><S sid = 169 ssid = >The oracle experiment suggests that better initial translations lead to better language models and thereby better 2nd iteration translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1059.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This adaptation technique was first proposed by Zhao et al (2004).</S> | Reference Offset:  ['18','168'] | Reference Text:  <S sid = 18 ssid = >Maximum entropy, minimum discrimination adaptation (Chen, et. al., 1998).</S><S sid = 168 ssid = >This result indicates that there is room for further improvements using this language model adaptation technique.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1059.txt | Citing Article:  D07-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation.</S> | Reference Offset:  ['18','46'] | Reference Text:  <S sid = 18 ssid = >Maximum entropy, minimum discrimination adaptation (Chen, et. al., 1998).</S><S sid = 46 ssid = >We follow (Eck, et al, 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories.</S> | Discourse Facet:  NA | Annotator: Automatic


