Citance Number: 1 | Reference Article:  N09-1036.txt | Citing Article:  P10-2042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000).</S> | Reference Offset:  ['28','161'] | Reference Text:  <S sid = 28 ssid = >Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learners must solve.</S><S sid = 161 ssid = >As Figure 1 shows, table label resampling produces parses with higher posterior probability, and Table 1 shows that table label resampling makes a significant difference in the word segmentation f-score of the collocation and collocation-syllable adaptor grammars.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N09-1036.txt | Citing Article:  P14-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johnson and Goldwater (2009) showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations.</S> | Reference Offset:  ['69','71'] | Reference Text:  <S sid = 69 ssid = >While adaptor grammars cannot express the Goldwater et al. bigram model, they can get much the same effect by directly modelling collocations (Johnson, 2008).</S><S sid = 71 ssid = >Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N09-1036.txt | Citing Article:  P14-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure (5-21), as prior work has found that this yields the highest word segmentation token f-score (Johnson and Goldwater, 2009).</S> | Reference Offset:  ['76','122'] | Reference Text:  <S sid = 76 ssid = >Here we present an adaptor grammar that models collocations together with these phonotactic constraints.</S><S sid = 122 ssid = >We evaluated the word token f-score of the most frequent marginal word segmentation, and compared that to average of the word token f-score for the 800 samples, which is also reported in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N09-1036.txt | Citing Article:  P14-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from Johnson and Goldwater (2009).</S> | Reference Offset:  ['69','73'] | Reference Text:  <S sid = 69 ssid = >While adaptor grammars cannot express the Goldwater et al. bigram model, they can get much the same effect by directly modelling collocations (Johnson, 2008).</S><S sid = 73 ssid = >Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N09-1036.txt | Citing Article:  E12-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the standard hyperparameters values α = 1.0, β = 0.01 and τ = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009).</S> | Reference Offset:  ['88','104'] | Reference Text:  <S sid = 88 ssid = >First, we can integrate out θ, and second, we can infer values for the adaptor hyperparameters using sampling.</S><S sid = 104 ssid = >We use a slice sampler here because it does not require a proposal distribution (Neal, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N09-1036.txt | Citing Article:  S12-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009).</S> | Reference Offset:  ['100','134'] | Reference Text:  <S sid = 100 ssid = >After each Gibbs sweep through the parse trees t we resampled each of the adaptor parameters from the posterior distribution of the parameter using a slice sampler 10 times.</S><S sid = 134 ssid = >This is easy to do in the context of Gibbs sampling, since this distribution is a minor variant of the distribution P(ti I t−i, wi) used during Gibbs sampling itself.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N09-1036.txt | Citing Article:  P13-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009).</S> | Reference Offset:  ['11','15'] | Reference Text:  <S sid = 11 ssid = >Dirichlet Processes and their associated predictive distributions, Chinese Restaurant Processes, are one kind of nonparametric Bayesian model that has received considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006).</S><S sid = 15 ssid = >Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N09-1036.txt | Citing Article:  D10-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009).</S> | Reference Offset:  ['100','131'] | Reference Text:  <S sid = 100 ssid = >After each Gibbs sweep through the parse trees t we resampled each of the adaptor parameters from the posterior distribution of the parameter using a slice sampler 10 times.</S><S sid = 131 ssid = >This means that in batch initialization each initial parse tree is randomly generated without any adaptation at all.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N09-1036.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','167'] | Reference Text:  <S sid = 52 ssid = >In the “Chinese Restaurant” metaphor samples produced by the adaptor are viewed as “customers” and zn is the index of the “table” that the nth customer is seated at.</S><S sid = 167 ssid = >This research was funded by NSF awards 0544127 and 0631667 to Mark Johnson.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N09-1036.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is the same set-up used by Liang and Klein (2009), Goldwater et al (2006), and Johnson and Goldwater (2009).</S> | Reference Offset:  ['15','111'] | Reference Text:  <S sid = 15 ssid = >Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).</S><S sid = 111 ssid = >The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N09-1036.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009).</S> | Reference Offset:  ['15','96'] | Reference Text:  <S sid = 15 ssid = >Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).</S><S sid = 96 ssid = >However, it is possible to perform Bayesian inference by putting a prior on them and sampling their values.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N09-1036.txt | Citing Article:  P12-2017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While in principle, increasing the number of rejuvenation steps and particles will make this gap smaller and smaller, we believe the existence of the gap to be interesting in its own right, suggesting a general difference in learning behaviour between batch and incremental learners, especially given the similar results in Johnson and Goldwater (2009).</S> | Reference Offset:  ['110','140'] | Reference Text:  <S sid = 110 ssid = >That paper leaves unspecified a number of implementation details, which we show can make a crucial difference to segmentation accuracy.</S><S sid = 140 ssid = >It is interesting that incremental initialization results in more accurate word segmentation, even though the trees it produces have lower posterior probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N09-1036.txt | Citing Article:  P12-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing.</S> | Reference Offset:  ['15','27'] | Reference Text:  <S sid = 15 ssid = >Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).</S><S sid = 27 ssid = >This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N09-1036.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem.</S> | Reference Offset:  ['15','162'] | Reference Text:  <S sid = 15 ssid = >Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).</S><S sid = 162 ssid = >This paper has examined adaptor grammar inference procedures and their effect on the word segmentation problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N09-1036.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the standard Brent corpus (Brent and Cartwright,1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task.</S> | Reference Offset:  ['26','29'] | Reference Text:  <S sid = 26 ssid = >Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus.</S><S sid = 29 ssid = >Elman (1990) introduced an idealized version of this task, and Brent and Cartwright (1996) presented a version of it where the data consists of unsegmented phonemic representations of the sentences in the Bernstein-Ratner corpus of child-directed speech (Bernstein-Ratner, 1987).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N09-1036.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >GUnigram and GSyllable can be found in Johnson and Goldwater (2009).</S> | Reference Offset:  ['15','73'] | Reference Text:  <S sid = 15 ssid = >Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).</S><S sid = 73 ssid = >Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N09-1036.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in.</S> | Reference Offset:  ['119','160'] | Reference Text:  <S sid = 119 ssid = >For each setting in the experiments described in Table 1 we ran 8 samplers for 2,000 iterations (i.e., passes through the training data), and kept the sample parse trees from every 10th iteration after iteration 1000, resulting in 800 sample parses for every sentence.</S><S sid = 160 ssid = >On the other hand, the collocation-syllable adaptor grammar involves a rich hierarchical structure, and in fact without table label resampling our sampler did not burn in or mix within 2,000 iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N09-1036.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al, 2009).</S> | Reference Offset:  ['27','111'] | Reference Text:  <S sid = 27 ssid = >This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars.</S><S sid = 111 ssid = >The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N09-1036.txt | Citing Article:  D10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language.</S> | Reference Offset:  ['18','19'] | Reference Text:  <S sid = 18 ssid = >Given an appropriate adaptor grammar, the features learned by adaptor grammars can correspond to linguistic units such as words, syllables and collocations.</S><S sid = 19 ssid = >Different adaptor grammars encode different assumptions about the structure of these units and how they relate to each other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N09-1036.txt | Citing Article:  W10-2912.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.) provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).</S> | Reference Offset:  ['34','64'] | Reference Text:  <S sid = 34 ssid = >But because it assumes that the probability of a word is determined purely by multiplying together the probability of its individual phonemes, it has no way to encode the fact that certain strings of phonemes (the words of the language) have much higher probabilities than other strings containing the same phonemes.</S><S sid = 64 ssid = >While the Word adaptor will generally prefer to reuse Words that have been used elsewhere in the corpus, it is always possible to generate a fresh Word using the CFG rules expanding Word into a string of Phonemes.</S> | Discourse Facet:  NA | Annotator: Automatic


