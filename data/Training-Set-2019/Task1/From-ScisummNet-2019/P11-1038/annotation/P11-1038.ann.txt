Citance Number: 1 | Reference Article:  P11-1038.txt | Citing Article:  W12-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms.</S> | Reference Offset:  ['19','49'] | Reference Text:  <S sid = 19 ssid = >Our objective is to restore ill-formed words to their canonical lexical forms in standard English.</S><S sid = 49 ssid = >We define the task of text normalisation to be a mapping from “ill-formed” OOV lexical items to their standard lexical forms, focusing exclusively on English for the purposes of this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P11-1038.txt | Citing Article:  P13-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity.</S> | Reference Offset:  ['3','208'] | Reference Text:  <S sid = 3 ssid = >Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.</S><S sid = 208 ssid = >We found that most illformed words are based on morphophonemic variation and proposed a cascaded method to detect and normalise ill-formed words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P11-1038.txt | Citing Article:  P13-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >w2wN: The output of the word-to-word normalization of Han and Baldwin (2011).</S> | Reference Offset:  ['4','199'] | Reference Text:  <S sid = 4 ssid = >Both word similarity and context are then exploited to select the most probable correction candidate for the word.</S><S sid = 199 ssid = >We found several limitations in our proposed approach by analysing the output of our method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P11-1038.txt | Citing Article:  P13-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011).</S> | Reference Offset:  ['153','161'] | Reference Text:  <S sid = 153 ssid = >This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus.</S><S sid = 161 ssid = >We evaluate detection performance by token-level precision, recall and F-score (Q = 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P11-1038.txt | Citing Article:  P14-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011).</S> | Reference Offset:  ['11','212'] | Reference Text:  <S sid = 11 ssid = >If there were some way of preprocessing the message to produce a more canonical lexical rendering, we would expect the quality of the parser to improve appreciably.</S><S sid = 212 ssid = >First, we plan to improve our ill-formed word detection classifier by introducing an OOV word whitelist.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P11-1038.txt | Citing Article:  P12-3006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity.</S> | Reference Offset:  ['3','208'] | Reference Text:  <S sid = 3 ssid = >Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.</S><S sid = 208 ssid = >We found that most illformed words are based on morphophonemic variation and proposed a cascaded method to detect and normalise ill-formed words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised.</S> | Reference Offset:  ['74','159'] | Reference Text:  <S sid = 74 ssid = >We identified 254 token instances of lexical normalisation, and broke them down into categories, as listed in Table 1.</S><S sid = 159 ssid = >This step is crucial to further normalisation, because if correct OOV words are identified as ill-formed, the candidate selection step can never be correct.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011).</S> | Reference Offset:  ['43','104'] | Reference Text:  <S sid = 43 ssid = >For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup.</S><S sid = 104 ssid = >To the best of our knowledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al., 2003; Sun et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available.</S> | Reference Offset:  ['12','49'] | Reference Text:  <S sid = 12 ssid = >Our aim in this paper is this task of lexical normalisation of noisy English text, with a particular focus on Twitter and SMS messages.</S><S sid = 49 ssid = >We define the task of text normalisation to be a mapping from “ill-formed” OOV lexical items to their standard lexical forms, focusing exclusively on English for the purposes of this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011).</S> | Reference Offset:  ['86','106'] | Reference Text:  <S sid = 86 ssid = >In confusion set generation, we generate a set of IV normalisation candidates for each OOV word type based on morphophonemic variation.</S><S sid = 106 ssid = >The most direct source of evidence is IV words around an OOV word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011).</S> | Reference Offset:  ['14','163'] | Reference Text:  <S sid = 14 ssid = >The message normalisation task is challenging.</S><S sid = 163 ssid = >For candidate selection, we once again evaluate using token-level precision, recall and F-score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs.</S> | Reference Offset:  ['155','198'] | Reference Text:  <S sid = 155 ssid = >In addition to comparing our method with competitor methods, we also study the contribution of different feature groups.</S><S sid = 198 ssid = >The best F-score is achieved when combining dictionary lookup, word similarity and context support (“DL+WS+CS”), in which ill-formed words are first looked up in the slang dictionary, and only if no match is found do we apply our normalisation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries.</S> | Reference Offset:  ['79','156'] | Reference Text:  <S sid = 79 ssid = >“Slang” refers to instances of Internet slang (e.g. lol “laugh out loud”), as found in a slang dictionary (see Section 3.1).</S><S sid = 156 ssid = >We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm).</S> | Reference Offset:  ['18','199'] | Reference Text:  <S sid = 18 ssid = >In addition, the detection of ill-formed words is difficult due to noisy context.</S><S sid = 199 ssid = >We found several limitations in our proposed approach by analysing the output of our method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P11-1038.txt | Citing Article:  D12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors.</S> | Reference Offset:  ['168','172'] | Reference Text:  <S sid = 168 ssid = >First, higher detection threshold values (td) give better precision but lower recall.</S><S sid = 172 ssid = >The lower precision for the Blog corpus appears to be due to the text not being as clean as NYT, introducing parser errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P11-1038.txt | Citing Article:  E12-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u).</S> | Reference Offset:  ['180','213'] | Reference Text:  <S sid = 180 ssid = >In our annotation, the annotators only normalised ill-formed word if they had high confidence of how to normalise, as with talkin “talking”.</S><S sid = 213 ssid = >Furthermore, we intend to alleviate noisy contexts with a bootstrapping approach, in which ill-formed words with high confidence and no ambiguity will be replaced by their standard forms, and fed into the normalisation model as new training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P11-1038.txt | Citing Article:  E12-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011).</S> | Reference Offset:  ['37','203'] | Reference Text:  <S sid = 37 ssid = >Statistical machine translation (SMT) has been proposed as a means of context-sensitive text normalisation, by treating the ill-formed text as the source language, and the standard form as the target language.</S><S sid = 203 ssid = >For such cases, the method falls back to context-independent normalisation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P11-1038.txt | Citing Article:  P12-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%.</S> | Reference Offset:  ['102','131'] | Reference Text:  <S sid = 102 ssid = >If we truncate the ranking to the top 10% of candidates, the recall drops back to 84% with a 90% reduction in candidates.</S><S sid = 131 ssid = >We separately introduce a threshold td E 11, 2, ...,10} on the number of positive predictions returned by the detection classifier over the set of normalisation candidates for a given OOV token: the token is considered to be ill-formed iff td or more candidates are positively classified, i.e. predicted to be correct candidates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P11-1038.txt | Citing Article:  P12-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity.</S> | Reference Offset:  ['3','26'] | Reference Text:  <S sid = 3 ssid = >Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.</S><S sid = 26 ssid = >Then, all candidates are ranked according to a list of features generated from noisy context and similarity between ill-formed words and candidates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P11-1038.txt | Citing Article:  P12-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['79','214'] | Reference Text:  <S sid = 79 ssid = >“Slang” refers to instances of Internet slang (e.g. lol “laugh out loud”), as found in a slang dictionary (see Section 3.1).</S><S sid = 214 ssid = >NICTA is funded by the Australian government as represented by Department of Broadband, Communication and Digital Economy, and the Australian Research Council through the ICT centre of Excellence programme.</S> | Discourse Facet:  NA | Annotator: Automatic


