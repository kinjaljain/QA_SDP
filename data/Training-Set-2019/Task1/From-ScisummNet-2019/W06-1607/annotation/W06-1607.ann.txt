Citance Number: 1 | Reference Article:  W06-1607.txt | Citing Article:  W07-0717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al, 2006), and binary features to indicate phrase pair presence within different components.</S> | Reference Offset:  ['21','24'] | Reference Text:  <S sid = 21 ssid = >We propose various ways of dealing with these special features of the phrasetable smoothing problem, and give evaluations of their performance within a phrase-based SMT system.</S><S sid = 24 ssid = >The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-1607.txt | Citing Article:  P14-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also use the fix-discount method in Foster et al (2006) for phrase table smoothing.</S> | Reference Offset:  ['27','110'] | Reference Text:  <S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S><S sid = 110 ssid = >For the black-box techniques, the smoothed phrase table replaced the original relative-frequency (RF) phrase table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-1607.txt | Citing Article:  W07-0724.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, we used several types of phrase table smoothing in the WMT 2007 system because this proved helpful on other translation tasks: relative frequency estimates, Kneser-Neyand Zens-Ney-smoothed probabilities (Foster et al, 2006).</S> | Reference Offset:  ['88','110'] | Reference Text:  <S sid = 88 ssid = >(Zens and Ney, 2004) obtain p(sj|ti) from smoothed relative-frequency estimates in a wordaligned corpus.</S><S sid = 110 ssid = >For the black-box techniques, the smoothed phrase table replaced the original relative-frequency (RF) phrase table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-1607.txt | Citing Article:  N07-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a version from Foster et al (2006), modified from (Koehn et al,2003), which is an average of pairwise word translation probabilities.</S> | Reference Offset:  ['27','51'] | Reference Text:  <S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S><S sid = 51 ssid = >This is the traditional approach for glass-box smoothing (Koehn et al., 2003; Zens and Ney, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-1607.txt | Citing Article:  W10-1715.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Foster et al (2006) applied ideas from language model smoothing to the translation model.</S> | Reference Offset:  ['24','27'] | Reference Text:  <S sid = 24 ssid = >The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).</S><S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-1607.txt | Citing Article:  D07-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006).</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Phrasetable Smoothing For Statistical Machine Translation</S><S sid = 9 ssid = >In this paper, we provide the first systematic study of smoothing methods for phrase-based SMT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-1607.txt | Citing Article:  P07-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al (2006)).</S> | Reference Offset:  ['51','82'] | Reference Text:  <S sid = 51 ssid = >This is the traditional approach for glass-box smoothing (Koehn et al., 2003; Zens and Ney, 2004).</S><S sid = 82 ssid = >For both unigram and Kneser-Ney smoothing distributions, we used a discounting coefficient derived by (Ney et al., 1994) on the basis of a leaveone-out analysis: D = n1/(n1 + 2n2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-1607.txt | Citing Article:  W10-1761.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al, 2006).</S> | Reference Offset:  ['15','122'] | Reference Text:  <S sid = 15 ssid = >Traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities (Koehn et al., 2003), eg, p(˜s|˜t) = c(˜s, ˜t)/ Es˜ c(˜s, ˜t) (since the estimation problems for p(˜s|˜t) and p(˜t|˜s) are symmetrical, we will usually refer only to p(˜s|˜t) for brevity).</S><S sid = 122 ssid = >Using more discounting coefficients does not appear to help.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-1607.txt | Citing Article:  D07-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The phrase translation model probabilities are smoothed according to one of several techniques as described in (Foster et al, 2006) and identified in the discussion below.</S> | Reference Offset:  ['24','27'] | Reference Text:  <S sid = 24 ssid = >The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).</S><S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-1607.txt | Citing Article:  D07-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, a number of different phrase table smoothing algorithms were used: no smoothing, Good-Turing smoothing, Kneser-Ney 3 parameter smoothing and the log linear mixture involving two features called Zens-Ney (Foster et al, 2006).</S> | Reference Offset:  ['24','39'] | Reference Text:  <S sid = 24 ssid = >The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).</S><S sid = 39 ssid = >In this paper, we give experimental results for phrasetable smoothing techniques analogous to Good-Turing, Fixed-Discount, Kneser-Ney, and Modified Kneser-Ney LM smoothing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-1607.txt | Citing Article:  D07-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is plausible since a similar effect (a decrease in the benefit of smoothing) has been noted with phrase table smoothing (Foster et al, 2006).</S> | Reference Offset:  ['110','111'] | Reference Text:  <S sid = 110 ssid = >For the black-box techniques, the smoothed phrase table replaced the original relative-frequency (RF) phrase table.</S><S sid = 111 ssid = >For the glass-box techniques, a phrase table (either the original RF phrase table or its replacement after black-box smoothing) was interpolated in loglinear fashion with the smoothing glass-box distribution, with weights set to maximize BLEU on a development corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-1607.txt | Citing Article:  W10-1719.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, we applied phrase table smoothing as described in Foster et al (2006).</S> | Reference Offset:  ['27','110'] | Reference Text:  <S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S><S sid = 110 ssid = >For the black-box techniques, the smoothed phrase table replaced the original relative-frequency (RF) phrase table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-1607.txt | Citing Article:  N07-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The phrase based translation model is similar to that of Koehn, with the exception that phrase probability estimates P (s? |t?) are smoothed using the Good-Turing technique (Foster et al, 2006).</S> | Reference Offset:  ['24','27'] | Reference Text:  <S sid = 24 ssid = >The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).</S><S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-1607.txt | Citing Article:  C10-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >From this point of view, TMG can also be seen as a TM smoothing technique based on multiple TMs instead of single one such as Foster et al (2006).</S> | Reference Offset:  ['20','78'] | Reference Text:  <S sid = 20 ssid = >Phrasetable smoothing differs from ngram LM smoothing in the following ways: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 53–61, Sydney, July 2006. c�2006 Association for Computational Linguistics c(˜s, ˜t) = 0.1 However, probability mass is To model p(t, a|s), we use a standard loglinear reserved for the set of unseen translations, implying that probability mass is subtracted from the seen translations.</S><S sid = 78 ssid = >Our solution to this problem is based on the technique described in (Church s� where n1+(*, ˜t) is the number of phrases s˜ for which c(˜s, ˜t) > 0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-1607.txt | Citing Article:  C10-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some literatures have paid attention to this issue as well, such as Foster et al (2006) and Mylonakis and Simaan (2008).</S> | Reference Offset:  ['27','37'] | Reference Text:  <S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S><S sid = 37 ssid = >The black-box approach, which is the simpler of the two, has received little attention in the SMT literature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-1607.txt | Citing Article:  D12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This can happen, if we define features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al 2006).</S> | Reference Offset:  ['13','106'] | Reference Text:  <S sid = 13 ssid = >We define a phrasetable as a set of source phrases (ngrams) s˜ and their translations ˜t, along with associated translation probabilities p(˜s|˜t) and ˜t|˜s).</S><S sid = 106 ssid = >As described above, (Zens and Ney, 2004) and (Koehn et al., 2005) use two different variants of glass-box smoothing (which they call “lexical smoothing”) over the phrasetable, and combine the resulting estimates with pure relativefrequency ones in a loglinear model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-1607.txt | Citing Article:  W09-0413.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting as described in Foster et al (2006).</S> | Reference Offset:  ['88','110'] | Reference Text:  <S sid = 88 ssid = >(Zens and Ney, 2004) obtain p(sj|ti) from smoothed relative-frequency estimates in a wordaligned corpus.</S><S sid = 110 ssid = >For the black-box techniques, the smoothed phrase table replaced the original relative-frequency (RF) phrase table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-1607.txt | Citing Article:  N12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Smoothing is obviously one possibility (Foster et al, 2006).</S> | Reference Offset:  ['42','58'] | Reference Text:  <S sid = 42 ssid = >The other possibility, which is similar in spirit to ngram LM lower-order estimates, is to combine estimates made by replacing words in t˜ with wildcards, as proposed in section 3.4.</S><S sid = 58 ssid = >Obviously, this gives the glass-box techniques an advantage when the different smoothing techniques are compared using BLEU!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-1607.txt | Citing Article:  W12-3158.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Absolute discounting is a popular smoothing method for relative frequencies (Foster et al, 2006).</S> | Reference Offset:  ['31','123'] | Reference Text:  <S sid = 31 ssid = >There are two main ingredients in all such recipes: some probability distribution that is smoother than relative frequencies (ie, that has fewer parameters and is thus less complex) and some technique for combining that distribution with relative frequency estimates.</S><S sid = 123 ssid = >Smoothing relative frequencies with an additional Zens-Ney phrasetable gives about the same gain as KneserNey smoothing on its own.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-1607.txt | Citing Article:  D11-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The phrase translation probabilities p (e1|f )and p (f |e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al, 2006).</S> | Reference Offset:  ['15','27'] | Reference Text:  <S sid = 15 ssid = >Traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities (Koehn et al., 2003), eg, p(˜s|˜t) = c(˜s, ˜t)/ Es˜ c(˜s, ˜t) (since the estimation problems for p(˜s|˜t) and p(˜t|˜s) are symmetrical, we will usually refer only to p(˜s|˜t) for brevity).</S><S sid = 27 ssid = >To derive the joint counts c(˜s, ˜t) from which p(˜s|˜t) and p(˜t|˜s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


