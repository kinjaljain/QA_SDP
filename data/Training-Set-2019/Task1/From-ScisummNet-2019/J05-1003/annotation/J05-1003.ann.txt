Citance Number: 1 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance.</S> | Reference Offset:  ['0','213'] | Reference Text:  <S sid = 0 ssid = >Discriminative Reranking For Natural Language Parsing</S><S sid = 213 ssid = >A baseline statistical parser is used to generate N-best output both for its training set and for test data sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees.</S> | Reference Offset:  ['7','462'] | Reference Text:  <S sid = 7 ssid = >The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the model.</S><S sid = 462 ssid = >These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question): For this reason we describe these approaches as “Joint log-linear models.”The probability of a tree xi,j is Here Z is the (infinite) set of possible trees, and the denominator cannot be calculated explicitly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our work, we included all features described in (Collins and Koo, 2005).</S> | Reference Offset:  ['356','459'] | Reference Text:  <S sid = 356 ssid = >The following types of features were included in the model.</S><S sid = 459 ssid = >Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish.</S> | Reference Offset:  ['425','427'] | Reference Text:  <S sid = 425 ssid = >5.4.3 Efficiency Gains.</S><S sid = 427 ssid = >In this section we explore the empirical gains in efficiency seen on the parsing data sets in this article.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence.</S> | Reference Offset:  ['335','350'] | Reference Text:  <S sid = 335 ssid = >For example, the iterative scaling procedure described above must be applied for a number of features.</S><S sid = 350 ssid = >The number of such parses varies sentence by sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J05-1003.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule.</S> | Reference Offset:  ['359','360'] | Reference Text:  <S sid = 359 ssid = >Two-level rules.</S><S sid = 360 ssid = >Same as Rules, but also including the entire rule above the rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J05-1003.txt | Citing Article:  D11-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser.</S> | Reference Offset:  ['247','358'] | Reference Text:  <S sid = 247 ssid = >The ranking error rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best parse: where again, gpÄ is one if p is true, zero otherwise.</S><S sid = 358 ssid = >Note that the output of our baseline parser produces syntactic trees with headword annotations (see Collins [1999]) for a description of the rules used to find headwords).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J05-1003.txt | Citing Article:  W07-1202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is also work on discriminative models for parse reranking (Collins and Koo, 2005).</S> | Reference Offset:  ['0','123'] | Reference Text:  <S sid = 0 ssid = >Discriminative Reranking For Natural Language Parsing</S><S sid = 123 ssid = >This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J05-1003.txt | Citing Article:  W08-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on.</S> | Reference Offset:  ['0','40'] | Reference Text:  <S sid = 0 ssid = >Discriminative Reranking For Natural Language Parsing</S><S sid = 40 ssid = >Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J05-1003.txt | Citing Article:  W08-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005).</S> | Reference Offset:  ['123','290'] | Reference Text:  <S sid = 123 ssid = >This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.</S><S sid = 290 ssid = >We now describe the form of these updates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences.</S> | Reference Offset:  ['69','407'] | Reference Text:  <S sid = 69 ssid = >They are important for a few reasons.</S><S sid = 407 ssid = >For example, a score of 101.5 indicates a 1.5% increase in this score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005).</S> | Reference Offset:  ['274','346'] | Reference Text:  <S sid = 274 ssid = >In general, a separate set of instances is used in cross-validation to choose the stopping point, that is, to decide on the number of features in the model.</S><S sid = 346 ssid = >The remaining 4,000 sentences were used as development data and to cross-validate the number of rounds (features) in the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007).</S> | Reference Offset:  ['17','469'] | Reference Text:  <S sid = 17 ssid = >We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).</S><S sid = 469 ssid = >Johnson et al. (1999) and Riezler et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features.</S> | Reference Offset:  ['415','451'] | Reference Text:  <S sid = 415 ssid = >Note that the ExpLoss results are very slightly different from the original results published in Collins (2000).</S><S sid = 451 ssid = >The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005).</S> | Reference Offset:  ['406','448'] | Reference Text:  <S sid = 406 ssid = >The accuracy shown is the performance relative to the baseline method of using the probability from the generative model alone in ranking parses, where the measure in equation (21) is used to measure performance.</S><S sid = 448 ssid = >It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J05-1003.txt | Citing Article:  W08-0130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values.</S> | Reference Offset:  ['264','278'] | Reference Text:  <S sid = 264 ssid = >We use Si,j to refer to this weight.</S><S sid = 278 ssid = >Assuming we greedily pick a single feature with some weight to update the model, and given that the current parameter settings are ¯a, the optimal feature/weight pair (k*, d*) is Note that this is essentially the idea behind the “boosting”approach to feature selection introduced in section 3.3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J05-1003.txt | Citing Article:  P11-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005).</S> | Reference Offset:  ['55','454'] | Reference Text:  <S sid = 55 ssid = >We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models.</S><S sid = 454 ssid = >The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J05-1003.txt | Citing Article:  P08-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005).</S> | Reference Offset:  ['66','360'] | Reference Text:  <S sid = 66 ssid = >The reranking models in this article were originally introduced in Collins (2000).</S><S sid = 360 ssid = >Same as Rules, but also including the entire rule above the rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J05-1003.txt | Citing Article:  D07-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used.</S> | Reference Offset:  ['0','460'] | Reference Text:  <S sid = 0 ssid = >Discriminative Reranking For Natural Language Parsing</S><S sid = 460 ssid = >Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J05-1003.txt | Citing Article:  W12-1623.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN).</S> | Reference Offset:  ['75','396'] | Reference Text:  <S sid = 75 ssid = >We define GEN(x)ÎY to be the set of candidates for a given input x.</S><S sid = 396 ssid = >The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method.</S> | Discourse Facet:  NA | Annotator: Automatic


