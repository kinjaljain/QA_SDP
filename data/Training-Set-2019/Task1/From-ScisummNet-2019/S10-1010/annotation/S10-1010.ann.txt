Citance Number: 1 | Reference Article:  S10-1010.txt | Citing Article:  P14-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013).</S> | Reference Offset:  ['5','50'] | Reference Text:  <S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S><S sid = 50 ssid = >The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a hand-built gold standard of annotated texts using the TimeML markup scheme.4 However, all event annotation was reviewed to make sure that the annotation complied with the latest guidelines and all temporal relations were added according to the Tempeval-2 relation tasks, using the specified relation types.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  S10-1010.txt | Citing Article:  P14-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3).</S> | Reference Offset:  ['31','44'] | Reference Text:  <S sid = 31 ssid = >F. Determine the temporal relation between two events where one event syntactically dominates the other event.</S><S sid = 44 ssid = >All corpora include event and timex annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  S10-1010.txt | Citing Article:  P13-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish.</S> | Reference Offset:  ['5','58'] | Reference Text:  <S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S><S sid = 58 ssid = >The distribution over the six languages was very uneven: sixteen systems for English, two for Spanish and one for English and Spanish.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  S10-1010.txt | Citing Article:  E12-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence.</S> | Reference Offset:  ['34','81'] | Reference Text:  <S sid = 34 ssid = >Task participants could choose to either do all tasks, focus on the time expression task, focus on the event task, or focus on the four temporal relation tasks.</S><S sid = 81 ssid = >However, we had expected that for TempEval-2 the systems would score better on task C since we added the restriction that the event and time expression had to be syntactically adjacent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  S10-1010.txt | Citing Article:  E12-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art.</S> | Reference Offset:  ['5','77'] | Reference Text:  <S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S><S sid = 77 ssid = >A comparison with the Tempeval-1 results from Semeval-2007 may be of interest.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  S10-1010.txt | Citing Article:  N12-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010).</S> | Reference Offset:  ['0','5'] | Reference Text:  <S sid = 0 ssid = >SemEval-2010 Task 13: TempEval-2</S><S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  S10-1010.txt | Citing Article:  N12-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010).</S> | Reference Offset:  ['0','5'] | Reference Text:  <S sid = 0 ssid = >SemEval-2010 Task 13: TempEval-2</S><S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  S10-1010.txt | Citing Article:  N12-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010).</S> | Reference Offset:  ['4','5'] | Reference Text:  <S sid = 4 ssid = >However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate.</S><S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  S10-1010.txt | Citing Article:  W11-0418.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010).</S> | Reference Offset:  ['5','50'] | Reference Text:  <S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S><S sid = 50 ssid = >The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a hand-built gold standard of annotated texts using the TimeML markup scheme.4 However, all event annotation was reviewed to make sure that the annotation complied with the latest guidelines and all temporal relations were added according to the Tempeval-2 relation tasks, using the specified relation types.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  S10-1010.txt | Citing Article:  E12-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem.</S> | Reference Offset:  ['5','77'] | Reference Text:  <S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S><S sid = 77 ssid = >A comparison with the Tempeval-1 results from Semeval-2007 may be of interest.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  S10-1010.txt | Citing Article:  E12-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical.</S> | Reference Offset:  ['5','80'] | Reference Text:  <S sid = 5 ssid = >The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.</S><S sid = 80 ssid = >The results are very similar except for task D, but if we take a away the one outlier (the NCSUjoint score of 0.21) then the average becomes 0.78 with a standard deviation of 0.05.</S> | Discourse Facet:  NA | Annotator: Automatic


