Citance Number: 1 | Reference Article:  D07-1090.txt | Citing Article:  D07-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >5-gram word language models in English are trained on a variety of monolingual corpora (Brants et al, 2007).</S> | Reference Offset:  ['9','161'] | Reference Text:  <S sid = 9 ssid = >We focus on n-gram language models, which are trained on unlabeled monolingual text.</S><S sid = 161 ssid = >We trained 5-gram language models on amounts of text varying from 13 million to 2 trillion tokens.The data is divided into four sets; language mod els are trained for each set separately4 . For eachtraining data size, we report the size of the result ing language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al, 2002) obtainedby the machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1090.txt | Citing Article:  P14-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brantset al, 2007).</S> | Reference Offset:  ['8','29'] | Reference Text:  <S sid = 8 ssid = >One or more feature func tions may be of the form h(e, f) = h(e), in which case it is referred to as a language model.</S><S sid = 29 ssid = >Models The topic of large, distributed language models is relatively new.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1090.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To scale LMs to larger corpora with higher-order dependencies, researchers have considered alternative parameterizations such as class-based models (Brown et al, 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represent ion schemes such as suffix arrays (Emami et al, 2007), Golomb Coding (Church et al, 2007) and distributed language models that scale more readily (Brants et al, 2007).</S> | Reference Offset:  ['30','33'] | Reference Text:  <S sid = 30 ssid = >Recently a two-pass approach hasbeen proposed (Zhang et al, 2006), wherein a lower order n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.</S><S sid = 33 ssid = >More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1090.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['62','200'] | Reference Text:  <S sid = 62 ssid = >5.1 Vocabulary Generation.</S><S sid = 200 ssid = >It is measured on test data T = w|T |1 : PP (T ) = e ? 1|T | |T |</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1090.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here we choose to work with stupid back off smoothing (Brants et al, 2007) since this is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney.</S> | Reference Offset:  ['4','162'] | Reference Text:  <S sid = 4 ssid = >We in troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.</S><S sid = 162 ssid = >For smaller train ing sizes, we have also computed test-set perplexityusing Kneser-Ney Smoothing, and report it for com parison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1090.txt | Citing Article:  P08-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work (Brants et al, 2007) has shown it to be appropriate to large-scale language modeling.</S> | Reference Offset:  ['30','33'] | Reference Text:  <S sid = 30 ssid = >Recently a two-pass approach hasbeen proposed (Zhang et al, 2006), wherein a lower order n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.</S><S sid = 33 ssid = >More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1090.txt | Citing Article:  D11-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Brants et al (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points.</S> | Reference Offset:  ['11','161'] | Reference Text:  <S sid = 11 ssid = >Questions that arise in this context include: (1) How might one build a language model that allows scaling to very large amounts of training data?</S><S sid = 161 ssid = >We trained 5-gram language models on amounts of text varying from 13 million to 2 trillion tokens.The data is divided into four sets; language mod els are trained for each set separately4 . For eachtraining data size, we report the size of the result ing language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al, 2002) obtainedby the machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1090.txt | Citing Article:  D11-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Brants et al (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data.</S> | Reference Offset:  ['188','196'] | Reference Text:  <S sid = 188 ssid = >The web data set has the smallest relative increase.</S><S sid = 196 ssid = >The es timated runtime for that is approximately one week on 1500 machines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1090.txt | Citing Article:  E09-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We build sentence specific zero-cutoff stupid-back off (Brants et al., 2007) 5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore each 10000-best list.</S> | Reference Offset:  ['9','30'] | Reference Text:  <S sid = 9 ssid = >We focus on n-gram language models, which are trained on unlabeled monolingual text.</S><S sid = 30 ssid = >Recently a two-pass approach hasbeen proposed (Zhang et al, 2006), wherein a lower order n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D07-1090.txt | Citing Article:  S12-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al, 2007).</S> | Reference Offset:  ['12','33'] | Reference Text:  <S sid = 12 ssid = >(2) How much does translation performance improve as the size of the language model increases?</S><S sid = 33 ssid = >More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D07-1090.txt | Citing Article:  W12-2706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We aim to allow the estimation of large scale distributed models, similar in size to the ones in Brantset al (2007).</S> | Reference Offset:  ['33','175'] | Reference Text:  <S sid = 33 ssid = >More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).</S><S sid = 175 ssid = >7.2 Size of the Language Models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D07-1090.txt | Citing Article:  W12-2706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In relation to language models, Brants et al (2007) recently proposed a distributed MapReduce infrastructure to build Ngram language models having up to 300 billion n-grams.</S> | Reference Offset:  ['2','191'] | Reference Text:  <S sid = 2 ssid = >A distributed infrastruc ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams.</S><S sid = 191 ssid = >300 billion n-grams.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D07-1090.txt | Citing Article:  P10-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.</S> | Reference Offset:  ['0','35'] | Reference Text:  <S sid = 0 ssid = >Large Language Models in Machine Translation</S><S sid = 35 ssid = >The difference is that they integrate the distributed language model into their machine translation decoder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D07-1090.txt | Citing Article:  E09-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al, 2007).</S> | Reference Offset:  ['4','163'] | Reference Text:  <S sid = 4 ssid = >We in troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.</S><S sid = 163 ssid = >7.1 Data Sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D07-1090.txt | Citing Article:  E12-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al 2011), query logs (Chen et al 2007), and large n-gram corpora (Brants et al., 2007) are employed.</S> | Reference Offset:  ['30','33'] | Reference Text:  <S sid = 30 ssid = >Recently a two-pass approach hasbeen proposed (Zhang et al, 2006), wherein a lower order n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.</S><S sid = 33 ssid = >More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D07-1090.txt | Citing Article:  D09-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al, 2007).</S> | Reference Offset:  ['32','37'] | Reference Text:  <S sid = 32 ssid = >The amount of data used was 3 billion words.</S><S sid = 37 ssid = >The largest amount of data used in the experiments is 4 billion words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D07-1090.txt | Citing Article:  P09-2086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We implemented an N -gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline (Brants et al, 2007).</S> | Reference Offset:  ['30','119'] | Reference Text:  <S sid = 30 ssid = >Recently a two-pass approach hasbeen proposed (Zhang et al, 2006), wherein a lower order n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.</S><S sid = 119 ssid = >This can be done similarly to the n-gram counting using a MapReduce (Step 0 in Table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D07-1090.txt | Citing Article:  P13-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In machine translation, improved language models have resulted in significant improvements in translation performance (Brants et al., 2007).</S> | Reference Offset:  ['0','33'] | Reference Text:  <S sid = 0 ssid = >Large Language Models in Machine Translation</S><S sid = 33 ssid = >More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D07-1090.txt | Citing Article:  D12-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid back off (Brants et al 2007).</S> | Reference Offset:  ['4','130'] | Reference Text:  <S sid = 4 ssid = >We in troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.</S><S sid = 130 ssid = >The largest models reported here with Kneser-Ney Smoothing were trained on 31 billion tokens.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D07-1090.txt | Citing Article:  P12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al, 2007).</S> | Reference Offset:  ['9','161'] | Reference Text:  <S sid = 9 ssid = >We focus on n-gram language models, which are trained on unlabeled monolingual text.</S><S sid = 161 ssid = >We trained 5-gram language models on amounts of text varying from 13 million to 2 trillion tokens.The data is divided into four sets; language mod els are trained for each set separately4 . For eachtraining data size, we report the size of the result ing language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al, 2002) obtainedby the machine translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


