Citance Number: 1 | Reference Article:  E06-1051.txt | Citing Article:  E12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004).</S> | Reference Offset:  ['57','154'] | Reference Text:  <S sid = 57 ssid = >Potentially any kernel function can work with any kernel-based algorithm.</S><S sid = 154 ssid = >Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1051.txt | Citing Article:  E12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings.</S> | Reference Offset:  ['86','121'] | Reference Text:  <S sid = 86 ssid = >Finally, the Shallow Linguistic kernel It follows directly from the explicit construction of the feature space and from closure properties of kernels that KSL is a valid kernel.</S><S sid = 121 ssid = >Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1051.txt | Citing Article:  E12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006).</S> | Reference Offset:  ['0','154'] | Reference Text:  <S sid = 0 ssid = >Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature</S><S sid = 154 ssid = >Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1051.txt | Citing Article:  W11-0201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0 : X → F, and then use a linear algorithm for discovering nonlinear patterns.</S><S sid = 172 ssid = >Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1051.txt | Citing Article:  W08-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0 : X → F, and then use a linear algorithm for discovering nonlinear patterns.</S><S sid = 172 ssid = >Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1051.txt | Citing Article:  W08-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006).</S> | Reference Offset:  ['123','124'] | Reference Text:  <S sid = 123 ssid = >As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifiFinally, Figure 5 shows the learning curve of the combined kernel KSL using the OARD evaluation methodology.</S><S sid = 124 ssid = >The curve reaches a plateau with around 100 Medline abstracts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1051.txt | Citing Article:  W08-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0 : X → F, and then use a linear algorithm for discovering nonlinear patterns.</S><S sid = 172 ssid = >Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1051.txt | Citing Article:  W08-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD).</S> | Reference Offset:  ['111','122'] | Reference Text:  <S sid = 111 ssid = >On the other hand, if we use the OARD methodology, only one occurrence for each interaction has to be extracted to maximize the score.</S><S sid = 122 ssid = >We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1051.txt | Citing Article:  W08-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006).</S> | Reference Offset:  ['122','123'] | Reference Text:  <S sid = 122 ssid = >We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)).</S><S sid = 123 ssid = >As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifiFinally, Figure 5 shows the learning curve of the combined kernel KSL using the OARD evaluation methodology.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1051.txt | Citing Article:  C10-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%.</S> | Reference Offset:  ['90','120'] | Reference Text:  <S sid = 90 ssid = >The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).</S><S sid = 120 ssid = >All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1051.txt | Citing Article:  C10-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0 : X → F, and then use a linear algorithm for discovering nonlinear patterns.</S><S sid = 172 ssid = >Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1051.txt | Citing Article:  C10-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.</S> | Reference Offset:  ['81','85'] | Reference Text:  <S sid = 81 ssid = >PoS The PoS tag of the token.</S><S sid = 85 ssid = >Notice that KLC differs substantially from KGC as it considers the ordering of the tokens and the feature space is enriched with PoS, lemma and orthographic features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1051.txt | Citing Article:  C08-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006).</S> | Reference Offset:  ['1','4'] | Reference Text:  <S sid = 1 ssid = >We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.</S><S sid = 4 ssid = >The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1051.txt | Citing Article:  C08-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Giuliano et al (2006) use no syntactic information.</S> | Reference Offset:  ['30','158'] | Reference Text:  <S sid = 30 ssid = >Surprisingly, it outperforms most of the systems based on syntactic or semantic information, even when this information is manually annotated (i.e. the LLL challenge).</S><S sid = 158 ssid = >They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1051.txt | Citing Article:  C08-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['53','172'] | Reference Text:  <S sid = 53 ssid = >The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0 : X → F, and then use a linear algorithm for discovering nonlinear patterns.</S><S sid = 172 ssid = >Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E06-1051.txt | Citing Article:  C08-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall.</S> | Reference Offset:  ['47','100'] | Reference Text:  <S sid = 47 ssid = >This allows us to make the classification task simpler without loosing information.</S><S sid = 100 ssid = >The first subset does not include coreferences, while the second one includes simple cases of coreference, mainly appositions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E06-1051.txt | Citing Article:  N09-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)).</S> | Reference Offset:  ['50','90'] | Reference Text:  <S sid = 50 ssid = >For the protein-protein interaction task (AImed) we use the correct entities provided by the manual annotation.</S><S sid = 90 ssid = >The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E06-1051.txt | Citing Article:  N09-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation.</S> | Reference Offset:  ['120','132'] | Reference Text:  <S sid = 120 ssid = >All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).</S><S sid = 132 ssid = >Given that we are interested here in the contribution of each kernel, we evaluated the experiments by 10-fold cross-validation on the whole training set avoiding the submission process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E06-1051.txt | Citing Article:  W07-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006).</S> | Reference Offset:  ['0','11'] | Reference Text:  <S sid = 0 ssid = >Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature</S><S sid = 11 ssid = >In particular, we explore a kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E06-1051.txt | Citing Article:  W07-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts.</S> | Reference Offset:  ['66','120'] | Reference Text:  <S sid = 66 ssid = >In (Bunescu and Mooney, 2005b), the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns: Fore-Between: tokens before and between the two candidate interacting entities.</S><S sid = 120 ssid = >All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


