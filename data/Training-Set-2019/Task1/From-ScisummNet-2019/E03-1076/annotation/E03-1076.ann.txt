Citance Number: 1 | Reference Article:  E03-1076.txt | Citing Article:  P03-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus.</S> | Reference Offset:  ['95','133'] | Reference Text:  <S sid = 95 ssid = >We then obtain statistics on the parts-ofspeech of words in the corpus.</S><S sid = 133 ssid = >We introduced various methods to split compound words into parts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E03-1076.txt | Citing Article:  P13-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010).</S> | Reference Offset:  ['6','102'] | Reference Text:  <S sid = 6 ssid = >Compounding of words is common in a number of languages (German, Dutch, Finnish, Greek, etc.).</S><S sid = 102 ssid = >An evaluation of full sentences is expected to show similar results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E03-1076.txt | Citing Article:  P13-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004).</S> | Reference Offset:  ['1','93'] | Reference Text:  <S sid = 1 ssid = >Compounded words are a challenge for NLP applications such as machine translation (MT).</S><S sid = 93 ssid = >We do not want to break up a compound into parts that are prepositions or determiners, but only content words: nouns, adverbs, adjectives, and verbs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E03-1076.txt | Citing Article:  P13-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus.</S> | Reference Offset:  ['2','95'] | Reference Text:  <S sid = 2 ssid = >We introduce methods to learn splitting rules from monolingual and parallel corpora.</S><S sid = 95 ssid = >We then obtain statistics on the parts-ofspeech of words in the corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E03-1076.txt | Citing Article:  W10-1715.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003).</S> | Reference Offset:  ['51','124'] | Reference Text:  <S sid = 51 ssid = >Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): Since this metric is purely defined in terms of German word frequencies, there is not necessarily a relationship between the selected option and correspondence to English words.</S><S sid = 124 ssid = >7.3 Translation Quality with Phrase Based Machine Translation Compound words violate the bias for one-to-one word correspondences of word based SMT systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E03-1076.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003).</S> | Reference Offset:  ['60','112'] | Reference Text:  <S sid = 60 ssid = >One source of information about word correspondence is a parallel corpus: text in a foreign language, accompanied by translations into English.</S><S sid = 112 ssid = >Its main remaining source of error is the lack of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E03-1076.txt | Citing Article:  W10-1704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding.</S> | Reference Offset:  ['78','133'] | Reference Text:  <S sid = 78 ssid = >Such a translation only occurs when Grund is used as the first part of a compound.</S><S sid = 133 ssid = >We introduced various methods to split compound words into parts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E03-1076.txt | Citing Article:  W09-0426.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment.</S> | Reference Offset:  ['50','86'] | Reference Text:  <S sid = 50 ssid = >This insight leads us to define a splitting metric based on word frequency.</S><S sid = 86 ssid = >If the word has not been seen before, we use the frequency method as a back-off.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus.</S> | Reference Offset:  ['0','35'] | Reference Text:  <S sid = 0 ssid = >Empirical Methods For Compound Splitting</S><S sid = 35 ssid = >Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task.</S> | Reference Offset:  ['30','116'] | Reference Text:  <S sid = 30 ssid = >Larson et al. [2000] propose a data-driven method that combines compound splitting and word recombination for speech recognition.</S><S sid = 116 ssid = >First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3.</S> | Reference Offset:  ['66','111'] | Reference Text:  <S sid = 66 ssid = >This approach requires a translation lexicon.</S><S sid = 111 ssid = >If multiple biggest splits are possible, the one with the highest frequency score is taken. frequency based: split into most frequent words, as described in Section 4 using parallel: split guided by splitting knowledge from a parallel corpus, as described in Section 5 using parallel and POS: as previous, with an additional restriction on the POS of split parts, as described in Section 6 Since we developed our methods to improve on this metric, it comes as no surprise that the most sophisticated method that employs splitting knowledge from a parallel corpus and information about POS tags proves to be superior with 99.1% accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds.</S> | Reference Offset:  ['23','110'] | Reference Text:  <S sid = 23 ssid = >While the linguistic properties of compounds are widely studied [Langer, 1998], there has been only limited work on empirical methods to split up compounds for specific applications.</S><S sid = 110 ssid = >The columns in this table mean: correct split: words that should be split and were split correctly correct non: words that should not be split and were not wrong not: words that should be split but were not wrong faulty split: words that should be split, were split, but wrongly (either too much or too little) wrong split: words that should not be split, but were precision: (correct split) / (correct split + wrong faulty split + wrong superfluous split) recall: (correct split) / (correct split + wrong faulty split + wrong not split) accuracy: (correct) / (correct + wrong) To briefly review the methods: raw: unprocessed data with no splits eager: biggest split, i.e., the split into as many parts as possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts.</S> | Reference Offset:  ['51','110'] | Reference Text:  <S sid = 51 ssid = >Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): Since this metric is purely defined in terms of German word frequencies, there is not necessarily a relationship between the selected option and correspondence to English words.</S><S sid = 110 ssid = >The columns in this table mean: correct split: words that should be split and were split correctly correct non: words that should not be split and were not wrong not: words that should be split but were not wrong faulty split: words that should be split, were split, but wrongly (either too much or too little) wrong split: words that should not be split, but were precision: (correct split) / (correct split + wrong faulty split + wrong superfluous split) recall: (correct split) / (correct split + wrong faulty split + wrong not split) accuracy: (correct) / (correct + wrong) To briefly review the methods: raw: unprocessed data with no splits eager: biggest split, i.e., the split into as many parts as possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator.</S> | Reference Offset:  ['3','108'] | Reference Text:  <S sid = 3 ssid = >We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.</S><S sid = 108 ssid = >Given this gold standard, we can evaluate the splits proposed by the methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','138'] | Reference Text:  <S sid = 43 ssid = >An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.</S><S sid = 138 ssid = >Future machine translation models that are sensitive to such linguistic clues might benefit even more.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E03-1076.txt | Citing Article:  W10-1734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','138'] | Reference Text:  <S sid = 43 ssid = >An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.</S><S sid = 138 ssid = >Future machine translation models that are sensitive to such linguistic clues might benefit even more.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E03-1076.txt | Citing Article:  W11-2145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words.</S> | Reference Offset:  ['99','106'] | Reference Text:  <S sid = 99 ssid = >For each German NP/PP, we have a English translation.</S><S sid = 106 ssid = >Recall that our first objective is to break up German words into parts that have a one-to-one translation correspondence to English words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E03-1076.txt | Citing Article:  I08-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','138'] | Reference Text:  <S sid = 43 ssid = >An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.</S><S sid = 138 ssid = >Future machine translation models that are sensitive to such linguistic clues might benefit even more.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E03-1076.txt | Citing Article:  I08-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds.</S> | Reference Offset:  ['40','42'] | Reference Text:  <S sid = 40 ssid = >As fillers we allow s and es when splitting German words, which covers almost all cases.</S><S sid = 42 ssid = >To summarize: We try to cover the entire length of the compound with known words and fillers between words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E03-1076.txt | Citing Article:  W09-0429.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a).</S> | Reference Offset:  ['122','133'] | Reference Text:  <S sid = 122 ssid = >One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently.</S><S sid = 133 ssid = >We introduced various methods to split compound words into parts.</S> | Discourse Facet:  NA | Annotator: Automatic


