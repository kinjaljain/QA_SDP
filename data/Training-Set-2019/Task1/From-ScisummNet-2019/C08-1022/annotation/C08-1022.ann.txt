Citance Number: 1 | Reference Article:  C08-1022.txt | Citing Article:  W09-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors.</S> | Reference Offset:  ['2','9'] | Reference Text:  <S sid = 2 ssid = >We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.</S><S sid = 9 ssid = >We c ? 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C08-1022.txt | Citing Article:  W12-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we.</S> | Reference Offset:  ['9','186'] | Reference Text:  <S sid = 9 ssid = >We c ? 2008.</S><S sid = 186 ssid = >In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C08-1022.txt | Citing Article:  N10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier.</S> | Reference Offset:  ['24','26'] | Reference Text:  <S sid = 24 ssid = >Izumi et al (2004) train a maximum entropy classifier to recognise various er rors using contextual features.</S><S sid = 26 ssid = >Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C08-1022.txt | Citing Article:  N10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best results of 92.15% are reported by De Felice and Pulman (2008).</S> | Reference Offset:  ['9','107'] | Reference Text:  <S sid = 9 ssid = >We c ? 2008.</S><S sid = 107 ssid = >The best reported results to date on determiner selection are those in Turner and Charniak (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C08-1022.txt | Citing Article:  W12-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings.</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English</S><S sid = 9 ssid = >We c ? 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C08-1022.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['62','199'] | Reference Text:  <S sid = 62 ssid = >We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.</S><S sid = 199 ssid = >Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C08-1022.txt | Citing Article:  P10-2065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model.</S> | Reference Offset:  ['9','70'] | Reference Text:  <S sid = 9 ssid = >We c ? 2008.</S><S sid = 70 ssid = >Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C08-1022.txt | Citing Article:  W11-1412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006).</S> | Reference Offset:  ['26','56'] | Reference Text:  <S sid = 26 ssid = >Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.</S><S sid = 56 ssid = >170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C08-1022.txt | Citing Article:  P11-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al.</S> | Reference Offset:  ['2','9'] | Reference Text:  <S sid = 2 ssid = >We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.</S><S sid = 9 ssid = >We c ? 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C08-1022.txt | Citing Article:  D10-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors.</S> | Reference Offset:  ['9','27'] | Reference Text:  <S sid = 9 ssid = >We c ? 2008.</S><S sid = 27 ssid = >Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C08-1022.txt | Citing Article:  W12-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['62','199'] | Reference Text:  <S sid = 62 ssid = >We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.</S><S sid = 199 ssid = >Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.</S> | Discourse Facet:  NA | Annotator: Automatic


