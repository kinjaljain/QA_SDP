Citance Number: 1 | Reference Article:  N04-1021.txt | Citing Article:  P04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score (Och et al., 2004).</S> | Reference Offset:  ['0','195'] | Reference Text:  <S sid = 0 ssid = >A Smorgasbord Of Features For Statistical Machine Translation</S><S sid = 195 ssid = >Our single best feature, and in fact the only single feature to produce a truly significant improvement, was the IBM Model 1 score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1021.txt | Citing Article:  W06-2606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Och et al, 2004), the effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated.</S> | Reference Offset:  ['0','5'] | Reference Text:  <S sid = 0 ssid = >A Smorgasbord Of Features For Statistical Machine Translation</S><S sid = 5 ssid = >Despite the enormous progress in machine translation (MT) due to the use of statistical techniques in recent years, state-of-the-art statistical systems often produce translations with obvious errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1021.txt | Citing Article:  W06-2606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although the improvement on the IWSLT 04 set is only moderate, the results are nevertheless comparable or better to the ones from (Och et al, 2004).</S> | Reference Offset:  ['117','143'] | Reference Text:  <S sid = 117 ssid = >Unfortunately, this feature function did not help to obtain better results (it actually seems to significantly hurt performance).</S><S sid = 143 ssid = >Thus, we have the folAs the model is computationally expensive, we sorted the n-best list by the sentence length, and processed them from the shorter ones to the longer ones.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1021.txt | Citing Article:  P11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a workaround, parsers can rerank the translated output of translation systems (Och et al, 2004).</S> | Reference Offset:  ['23','103'] | Reference Text:  <S sid = 23 ssid = >Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).</S><S sid = 103 ssid = >We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1021.txt | Citing Article:  P12-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Och et al (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system.</S> | Reference Offset:  ['0','23'] | Reference Text:  <S sid = 0 ssid = >A Smorgasbord Of Features For Statistical Machine Translation</S><S sid = 23 ssid = >Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1021.txt | Citing Article:  P12-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Och et al (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective.</S> | Reference Offset:  ['116','122'] | Reference Text:  <S sid = 116 ssid = >The most straightforward way to integrate a statistical parser in the system would be the use of the (log of the) parser probability as a feature function.</S><S sid = 122 ssid = >We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1021.txt | Citing Article:  P12-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow Och et al (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >A Smorgasbord Of Features For Statistical Machine Translation</S><S sid = 15 ssid = >The goal is the translation of a text given in some source language into a target language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1021.txt | Citing Article:  N07-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A typical reranking approach to SMT (Och et al, 2004) uses a 1000 best list.</S> | Reference Offset:  ['65','68'] | Reference Text:  <S sid = 65 ssid = >Using this method with a 1000-best list, we obtain oracle translations that outperform the BLEU score of the human translations.</S><S sid = 68 ssid = >Here, using an 1000-best list, we obtain oracle translations with a relative human BLEU score of 88.5%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1021.txt | Citing Article:  P13-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Och et al, 2004) describe the use of syntactic features in the rescoring step.</S> | Reference Offset:  ['14','193'] | Reference Text:  <S sid = 14 ssid = >We then present a selection of new features, progressing from word-level features to those based to part-of-speech tags and syntactic chunks, and then to features based on Treebank-based syntactic parses of the source and target sentences.</S><S sid = 193 ssid = >In addition to experiments with single features we also integrated multiple features using a greedy approach where we integrated at each step the feature that most improves the BLEU score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1021.txt | Citing Article:  C10-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many solutions to the reordering problem have been proposed ,e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al,2006).</S> | Reference Offset:  ['113','123'] | Reference Text:  <S sid = 113 ssid = >This feature function can be thought of as a trade-off between purely word-based models, and full generative models based upon shallow syntax.</S><S sid = 123 ssid = >A tree-to-string model is one of several syntaxbased translation models used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1021.txt | Citing Article:  W07-0401.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains.</S> | Reference Offset:  ['4','14'] | Reference Text:  <S sid = 4 ssid = >We present results for a small selection of features at each level of syntactic representation.</S><S sid = 14 ssid = >We then present a selection of new features, progressing from word-level features to those based to part-of-speech tags and syntactic chunks, and then to features based on Treebank-based syntactic parses of the source and target sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1021.txt | Citing Article:  C10-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Oracle BLEU scores computed over k-best lists (Och et al, 2004) show that many high quality hypotheses are produced by first-pass SMT decoding.</S> | Reference Offset:  ['60','162'] | Reference Text:  <S sid = 60 ssid = >We computed the oracle translations, that is, the set of translations from our n-best list that yields the best BLEU score.1 We use the following two methods to compute the BLEU score of an oracle translation: 1Note that due to the corpus-level holistic nature of the BLEU score it is not trivial to compute the optimal set of oracle translations.</S><S sid = 162 ssid = >Of these 480, the model preferred the produced over the oracle 52% of the time, indicating that it does not in fact seem likely to significantly improve BLEU scores when used for reranking.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1021.txt | Citing Article:  W09-1804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual IR (Schonhofen et al, 2008) or re-score candidate translation outputs (Och et al, 2004).</S> | Reference Offset:  ['84','87'] | Reference Text:  <S sid = 84 ssid = >We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.</S><S sid = 87 ssid = >As defined by Brown et al. (1993), Model 1 gives a probability of any given translation pair, which is We used GIZA++ to train the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1021.txt | Citing Article:  D11-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Despite the notational similarities, our approach should not be confused with projected POS models, which use source side POS tags to model reordering (Och et al, 2004).</S> | Reference Offset:  ['107','112'] | Reference Text:  <S sid = 107 ssid = >Chinese POS sequences are projected to English using the word alignment.</S><S sid = 112 ssid = >The Projected POS feature function was one of the strongest performing shallow syntactic feature functions, with a %BLEU score of 31.8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1021.txt | Citing Article:  D09-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compared these results against an inverse IBM model 1 but the results were inconclusive which is consistent with the results presented in (Och et al, 2004) where no improvements were achieved using p (e|f).</S> | Reference Offset:  ['4','84'] | Reference Text:  <S sid = 4 ssid = >We present results for a small selection of features at each level of syntactic representation.</S><S sid = 84 ssid = >We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1021.txt | Citing Article:  N07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al, 2004).</S> | Reference Offset:  ['9','84'] | Reference Text:  <S sid = 9 ssid = >A high-quality statistical translation system is our baseline, and we add new features to the existing set, which are then combined in a log-linear model.</S><S sid = 84 ssid = >We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1021.txt | Citing Article:  N07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are ten feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004).</S> | Reference Offset:  ['32','84'] | Reference Text:  <S sid = 32 ssid = >Word Selection This feature is based on the lexical translation probabilities p(e|f), estimated using relative frequencies according to the highest-probability wordlevel alignment for each training sentence.</S><S sid = 84 ssid = >We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1021.txt | Citing Article:  N07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such an approach has been taken by Och et al (2004) for integrating sophisticated syntax-informed models in a phrase based SMT system.</S> | Reference Offset:  ['34','113'] | Reference Text:  <S sid = 34 ssid = >Phrase Alignment This feature favors monotonic alignment at the phrase level.</S><S sid = 113 ssid = >This feature function can be thought of as a trade-off between purely word-based models, and full generative models based upon shallow syntax.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1021.txt | Citing Article:  N07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This method is a straightforward application of the n-best re-ranking approach described in Och et al (2004).</S> | Reference Offset:  ['23','54'] | Reference Text:  <S sid = 23 ssid = >Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).</S><S sid = 54 ssid = >These n-best candidate translations are the basis for discriminative training of the model parameters and for re-ranking.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1021.txt | Citing Article:  P06-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many different feature functions were explored in (Och et al, 2004).</S> | Reference Offset:  ['23','84'] | Reference Text:  <S sid = 23 ssid = >Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).</S><S sid = 84 ssid = >We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.</S> | Discourse Facet:  NA | Annotator: Automatic


