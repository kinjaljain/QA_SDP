Citance Number: 1 | Reference Article:  D09-1005.txt | Citing Article:  N10-1141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).</S> | Reference Offset:  ['117','181'] | Reference Text:  <S sid = 117 ssid = >Under the first-order expectation semiring ER,R-, the inside algorithm of Figure 2 will return (Z, r) where r is a vector of n feature expectations.</S><S sid = 181 ssid = >We now show how to compute a few other quantities by choosing re appropriately.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D09-1005.txt | Citing Article:  N12-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009).</S> | Reference Offset:  ['64','149'] | Reference Text:  <S sid = 64 ssid = >First, at lines 2–3, the inside and outside algorithms are run using only the ke weights, obtaining only k (without x) but also obtaining all inside and outside weights ,Q, α ∈ K as a side effect.</S><S sid = 149 ssid = >Second, we can directly apply the inside-outside algorithm there, as we now see.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D09-1005.txt | Citing Article:  N12-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009).</S> | Reference Offset:  ['62','241'] | Reference Text:  <S sid = 62 ssid = >We are now interested in computing the following quantities on the hypergraph HG: Note that r/Z, s/Z, and t/Z are expectations under p of r(d), s(d), and r(d)s(d), respectively.</S><S sid = 241 ssid = >During test time, a similar procedure is followed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D09-1005.txt | Citing Article:  P12-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009).</S> | Reference Offset:  ['126','149'] | Reference Text:  <S sid = 126 ssid = >So what makes the inside-outside algorithm more efficient?</S><S sid = 149 ssid = >Second, we can directly apply the inside-outside algorithm there, as we now see.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D09-1005.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009).</S> | Reference Offset:  ['69','179'] | Reference Text:  <S sid = 69 ssid = >Then r/Z is the expected hypothesis length.</S><S sid = 179 ssid = >In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D09-1005.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A generic first-order expectation semiring is also provided (Li and Eisner, 2009).</S> | Reference Offset:  ['85','142'] | Reference Text:  <S sid = 85 ssid = >Note that, to compute t, one cannot simply construct a first-order expectation semiring by defining t(d) def = r(d)s(d) because t(d), unlike r(d) and s(d), is not additively decomposable over the hyperedges in d.5 Also, when r(d) and s(d) are identical, the second-order expectation semiring allows us to compute variance as t/Z − (r/Z)2, which is why we may call our second-order expectation semiring the variance semiring.</S><S sid = 142 ssid = >In fact, that We now observe that the second-order expectation semiring EP,R,S,T can be obtained indirectly by nesting one first-order expectation semiring inside another!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D09-1005.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009).</S> | Reference Offset:  ['71','220'] | Reference Text:  <S sid = 71 ssid = >Then r/Z is the risk (expected loss), which is useful in minimum-risk training.</S><S sid = 220 ssid = >All MR or MR+DA uses an approximated BLEU (Tromble et al., 2008) (for training only), while MERT uses the exact corpus BLEU in training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D09-1005.txt | Citing Article:  W10-2503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion.</S> | Reference Offset:  ['116','246'] | Reference Text:  <S sid = 116 ssid = >But the H and E operators here now denote appropriate operations within P, R, and S respectively (rather than the usual operations within R).</S><S sid = 246 ssid = >We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D09-1005.txt | Citing Article:  W11-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009).</S> | Reference Offset:  ['2','179'] | Reference Text:  <S sid = 2 ssid = >Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).</S><S sid = 179 ssid = >In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D09-1005.txt | Citing Article:  W11-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For inside-outside algorithm, see (Li and Eisner, 2009).</S> | Reference Offset:  ['126','149'] | Reference Text:  <S sid = 126 ssid = >So what makes the inside-outside algorithm more efficient?</S><S sid = 149 ssid = >Second, we can directly apply the inside-outside algorithm there, as we now see.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D09-1005.txt | Citing Article:  C10-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009).</S> | Reference Offset:  ['150','198'] | Reference Text:  <S sid = 150 ssid = >Given a hypergraph weighted by a second-order expectation semiring EP,R,S,T.</S><S sid = 198 ssid = >Our objective function could be computed with a first-order expectation semiring, but computing it along with its gradient requires a second-order one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D09-1005.txt | Citing Article:  C10-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set.</S> | Reference Offset:  ['0','71'] | Reference Text:  <S sid = 0 ssid = >First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests</S><S sid = 71 ssid = >Then r/Z is the risk (expected loss), which is useful in minimum-risk training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D09-1005.txt | Citing Article:  D11-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009).</S> | Reference Offset:  ['13','79'] | Reference Text:  <S sid = 13 ssid = >The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent.</S><S sid = 79 ssid = >Next suppose K is the expectation semiring (Eisner, 2002), shown in Table 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D09-1005.txt | Citing Article:  D11-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the detailed description, see Li and Eisner (2009) and its references.</S> | Reference Offset:  ['229','237'] | Reference Text:  <S sid = 229 ssid = >To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008).</S><S sid = 237 ssid = >Li and Khudanpur (2008)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D09-1005.txt | Citing Article:  W10-1718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009).</S> | Reference Offset:  ['149','246'] | Reference Text:  <S sid = 149 ssid = >Second, we can directly apply the inside-outside algorithm there, as we now see.</S><S sid = 246 ssid = >We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D09-1005.txt | Citing Article:  W10-1718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations.</S> | Reference Offset:  ['71','184'] | Reference Text:  <S sid = 71 ssid = >Then r/Z is the risk (expected loss), which is useful in minimum-risk training.</S><S sid = 184 ssid = >Expected Loss (Risk) Given a reference sentence y*, the expected loss (i.e., Bayes risk) of the hypotheses in the hypergraph is defined as, where Y(d) is the target yield of d and L(y, y*) is the loss of the hypothesis y with respect to the reference y*.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D09-1005.txt | Citing Article:  W11-2130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list.</S> | Reference Offset:  ['203','209'] | Reference Text:  <S sid = 203 ssid = >Smith and Eisner (2006) instead propose a differentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7).</S><S sid = 209 ssid = >Solving (14) for a given T requires computing the entropy H(p) and risk R(p) and their gradients with respect to θ and γ. Smith and Eisner (2006) followed MERT in constraining their decoder to only an n-best list, so for them, computing these quantities did not involve dynamic programming.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D09-1005.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009).</S> | Reference Offset:  ['100','149'] | Reference Text:  <S sid = 100 ssid = >However, r, s, and t can be positive or negative, and we cannot directly take the log of a negative number.</S><S sid = 149 ssid = >Second, we can directly apply the inside-outside algorithm there, as we now see.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D09-1005.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007).</S> | Reference Offset:  ['0','22'] | Reference Text:  <S sid = 0 ssid = >First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests</S><S sid = 22 ssid = >We implement the expectation and variance semirings in Joshua (Li et al., 2009a), and demonstrate their practical benefit by using minimumrisk training to improve Hiero (Chiang, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D09-1005.txt | Citing Article:  D11-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way.</S> | Reference Offset:  ['0','71'] | Reference Text:  <S sid = 0 ssid = >First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests</S><S sid = 71 ssid = >Then r/Z is the risk (expected loss), which is useful in minimum-risk training.</S> | Discourse Facet:  NA | Annotator: Automatic


