Citance Number: 1 | Reference Article:  N13-1039.txt | Citing Article:  P13-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >POS tagger in this work, we note that taggers optimized specifically for social media are now available and would likely have resulted in higher tagging accuracy (e.g. Owoputi et al (2013)).</S> | Reference Offset:  ['16','136'] | Reference Text:  <S sid = 16 ssid = >2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.</S><S sid = 136 ssid = >Our tagger achieves substantially higher accuracy than Gimpel et al. (2011).17 Feature ablation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N13-1039.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To test this, we train a CRF model (Lafferty et al, 2001) with simple orthographic features and word clusters (Owoputi et al, 2013) on the annotated Twitter data described in Gimpel et al (2011).</S> | Reference Offset:  ['16','28'] | Reference Text:  <S sid = 16 ssid = >2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.</S><S sid = 28 ssid = >2Although when compared to CRFs, MEMMs theoretically suffer from the “label bias” problem (Lafferty et al., 2001), our system substantially outperforms the CRF-based taggers of previous work; and when comparing to Gimpel et al. system with similar feature sets, we observed little difference in accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N13-1039.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 with 2,4,8,16 bit string prefixes estimated from a large Twitter corpus (Owoputi et al, 2013).</S> | Reference Offset:  ['69','144'] | Reference Text:  <S sid = 69 ssid = >Since Brown clusters are hierarchical in a binary tree, each word is associated with a tree path represented as a bitstring with length < 16; we use prefixes of the bitstring as features (for all prefix lengths E 12, 4, 6,... ,16}).</S><S sid = 144 ssid = >Compared to the tagger in Gimpel et al., most of our feature changes are in the new lexical features described in §3.5.20 We do not reuse the other lexical features from the previous work, including a phonetic normalizer (Metaphone), a name list consisting of words that are frequently capitalized, and distributional features trained on a much smaller unlabeled corpus; they are all worse than our new lexical features described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N13-1039.txt | Citing Article:  P14-2068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the CMU Twitter Part-of-Speech Tagger (Owoputi et al, 2013) to select only instances in the verb sense.</S> | Reference Offset:  ['18','161'] | Reference Text:  <S sid = 18 ssid = >To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features.</S><S sid = 161 ssid = >Ritter et al.’s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3% (Table 4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N13-1039.txt | Citing Article:  P14-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Part-of-speech tags are assigned based on Owoputi et al's tweet POS system (Owoputi et al, 2013).</S> | Reference Offset:  ['16','149'] | Reference Text:  <S sid = 16 ssid = >2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.</S><S sid = 149 ssid = >20Details on the exact feature set are available in a technical report (Owoputi et al., 2012), also available on the website.</S> | Discourse Facet:  NA | Annotator: Automatic


