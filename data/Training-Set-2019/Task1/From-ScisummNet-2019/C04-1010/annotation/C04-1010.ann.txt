Citance Number: 1 | Reference Article:  C04-1010.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005).</S> | Reference Offset:  ['68','101'] | Reference Text:  <S sid = 68 ssid = >Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S><S sid = 101 ssid = >portion of words that are assigned the correct head (or no head if the word is a root) (Eisner, 1996; Collins et al, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1010.txt | Citing Article:  P11-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004).</S> | Reference Offset:  ['13','15'] | Reference Text:  <S sid = 13 ssid = >The parser described in this paper is similar to that of Yamada and Matsumoto (2003) in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.</S><S sid = 15 ssid = >First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1010.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English).</S> | Reference Offset:  ['8','36'] | Reference Text:  <S sid = 8 ssid = >Deterministicmethods for dependency parsing have now been ap plied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).</S><S sid = 36 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1010.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005).</S> | Reference Offset:  ['63','64'] | Reference Text:  <S sid = 63 ssid = >3 Memory-Based Learning.</S><S sid = 64 ssid = >Memory-based learning and problem solving is based on two fundamental principles: learning is thesimple storage of experiences in memory, and solv ing a new problem is achieved by reusing solutionsfrom similar previously solved problems (Daele mans, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1010.txt | Citing Article:  C10-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004).</S> | Reference Offset:  ['98','148'] | Reference Text:  <S sid = 98 ssid = >Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP.</S><S sid = 148 ssid = >The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1010.txt | Citing Article:  W07-2217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear.</S> | Reference Offset:  ['16','110'] | Reference Text:  <S sid = 16 ssid = >This also means that the time complexity of the algorithm used here is linearin the size of the input, while the algorithm of Ya mada and Matsumoto is quadratic in the worst case.</S><S sid = 110 ssid = >words that are analyzed as such (Yamada and Matsumoto, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1010.txt | Citing Article:  P07-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004).</S> | Reference Offset:  ['63','68'] | Reference Text:  <S sid = 63 ssid = >3 Memory-Based Learning.</S><S sid = 68 ssid = >Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1010.txt | Citing Article:  P07-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser.</S> | Reference Offset:  ['36','68'] | Reference Text:  <S sid = 36 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).</S><S sid = 68 ssid = >Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1010.txt | Citing Article:  P07-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >TR.POS . . .</S><S sid = 148 ssid = >The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1010.txt | Citing Article:  P07-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy.</S> | Reference Offset:  ['36','68'] | Reference Text:  <S sid = 36 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).</S><S sid = 68 ssid = >Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C04-1010.txt | Citing Article:  D09-1127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack.</S> | Reference Offset:  ['36','50'] | Reference Text:  <S sid = 36 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).</S><S sid = 50 ssid = >n onto the stack, giving the configuration ?n|S,I,A?.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C04-1010.txt | Citing Article:  W06-2920.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006).</S> | Reference Offset:  ['36','83'] | Reference Text:  <S sid = 36 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).</S><S sid = 83 ssid = >Thesesettings are the result of extensive experiments partially reported in Nivre et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C04-1010.txt | Citing Article:  W06-2920.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees.</S> | Reference Offset:  ['42','75'] | Reference Text:  <S sid = 42 ssid = >Given an arbitrary configuration of the parser, there are four possible transitions to the next configuration (where t is the token on top of the stack, n is the next input token, w is any word, and r, r?</S><S sid = 75 ssid = >The two central elements in any configuration are the token on top of the stack (T) and the next input token(N), the tokens which may be connected by a de pendency arc in the next configuration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C04-1010.txt | Citing Article:  P11-3011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures.</S> | Reference Offset:  ['0','68'] | Reference Text:  <S sid = 0 ssid = >Deterministic Dependency Parsing Of English Text</S><S sid = 68 ssid = >Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C04-1010.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion.</S> | Reference Offset:  ['11','15'] | Reference Text:  <S sid = 11 ssid = >On the other hand, the best available parsers trained on thePenn Treebank, those of Collins (1997) and Charniak (2000), use statistical models for disambigua tion that make crucial use of dependency relations.</S><S sid = 15 ssid = >First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C04-1010.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously.</S> | Reference Offset:  ['36','98'] | Reference Text:  <S sid = 36 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).</S><S sid = 98 ssid = >Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C04-1010.txt | Citing Article:  C08-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004).</S> | Reference Offset:  ['8','9'] | Reference Text:  <S sid = 8 ssid = >Deterministicmethods for dependency parsing have now been ap plied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).</S><S sid = 9 ssid = >For English, the interest in dependency parsing has been weaker than for other languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C04-1010.txt | Citing Article:  P14-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel.</S> | Reference Offset:  ['72','78'] | Reference Text:  <S sid = 72 ssid = >whose domain is a finite space of parser states, which are abstractions over configurations.</S><S sid = 78 ssid = >Finally, we use a lookahead of three tokens, considering only their parts-of-speech.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C04-1010.txt | Citing Article:  P10-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words.</S> | Reference Offset:  ['73','74'] | Reference Text:  <S sid = 73 ssid = >For this purpose we define a number of features that can be used to define different models of parser state.</S><S sid = 74 ssid = >Figure 2 illustrates the features that are used to define parser states in the present study.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C04-1010.txt | Citing Article:  C10-2168.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007).</S> | Reference Offset:  ['1','15'] | Reference Text:  <S sid = 1 ssid = >This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.</S><S sid = 15 ssid = >First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.</S> | Discourse Facet:  NA | Annotator: Automatic


