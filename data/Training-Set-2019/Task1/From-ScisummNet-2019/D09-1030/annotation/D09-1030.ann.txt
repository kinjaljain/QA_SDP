Citance Number: 1 | Reference Article:  D09-1030.txt | Citing Article:  N10-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is possible that the length of stay of an annotator in the pool is not independent of her diligence; for example, Callison-Burch (2009) found in his AMT experiments with tasks related to machine translation that lazy annotators tended to stay longer and do more annotations.</S> | Reference Offset:  ['117','125'] | Reference Text:  <S sid = 117 ssid = >We provided these instructions: Edit Machine Translation Your task is to edit the machine translation making as few changes as possible so that it matches the meaning of the human translation and is good English.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D09-1030.txt | Citing Article:  P10-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon's Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations.</S> | Reference Offset:  ['115','125'] | Reference Text:  <S sid = 115 ssid = >We evaluated the feasibility of using Mechanical Turk to perform HTER.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D09-1030.txt | Citing Article:  W10-0705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Callison-Burch (2009) used MTurk to evaluate machine translations.</S> | Reference Offset:  ['61','125'] | Reference Text:  <S sid = 61 ssid = >Following Callison-Burch et al. (2008), we assigned a score to each of the 11 MT systems based on how often its translations were judged to be better than or equal to any other system.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D09-1030.txt | Citing Article:  W10-0731.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Callison-Burch, 2009) uses MTurk workers for manual evaluation of automatic translation quality and experiments with weighed voting to combine multiple annotations.</S> | Reference Offset:  ['17','125'] | Reference Text:  <S sid = 17 ssid = >Although it is not common for manual evaluation results to be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D09-1030.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The use of crowd sourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and Callison-Burch (2009).</S> | Reference Offset:  ['125','171'] | Reference Text:  <S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S><S sid = 171 ssid = >The cost of using Mechanical Turk is low enough that we might consider attempting quixotic things like human-in-the-loop minimum error rate training (Zaidan and Callison-Burch, 2009), or doubling the amount of training data available for Urdu.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D09-1030.txt | Citing Article:  W10-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators.</S> | Reference Offset:  ['52','113'] | Reference Text:  <S sid = 52 ssid = >To avoid letting careless annotators drag down results, we experimented with weighted voting.</S><S sid = 113 ssid = >The contribution of each component system is weighted by the expectation that it will produce good output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D09-1030.txt | Citing Article:  C10-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has also been used in MT evaluation (Callison-Burch, 2009), though that evaluation used reference translations.</S> | Reference Offset:  ['46','123'] | Reference Text:  <S sid = 46 ssid = >The Turkers were shown a source sentence, a reference translation, and translations from five MT systems.</S><S sid = 123 ssid = >If none of the five edits was deemed to be acceptable, then we used the edit distance between the MT and the reference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D09-1030.txt | Citing Article:  W10-0734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an example, among the collected material several translations in languages other than English revealed a massive and defective use of on-line translation tools by untrusted workers, as also observed by (Callison-Burch, 2009).</S> | Reference Offset:  ['87','125'] | Reference Text:  <S sid = 87 ssid = >We had bilingual graduate students translate the first 50 English sentences of that corpus into French, German and Spanish, so that we could re-use the multiple English reference translations.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D09-1030.txt | Citing Article:  W10-0710.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009).</S> | Reference Offset:  ['30','125'] | Reference Text:  <S sid = 30 ssid = >Turkers are free to select whichever HITs interest them.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D09-1030.txt | Citing Article:  W10-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, previous research shows the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, & O'Connor, 2008).</S> | Reference Offset:  ['13','138'] | Reference Text:  <S sid = 13 ssid = >Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.</S><S sid = 138 ssid = >The advantage of this type of evaluation is that the results have a natural interpretation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D09-1030.txt | Citing Article:  D10-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The value of this upper bound is quite consistent with the bound computed similarly by Callison-Burch (2009).</S> | Reference Offset:  ['64','81'] | Reference Text:  <S sid = 64 ssid = >An upper bound is indicated by the expert-expert bar.</S><S sid = 81 ssid = >This gives an upper bound on the expected quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D09-1030.txt | Citing Article:  N10-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['58','186'] | Reference Text:  <S sid = 58 ssid = >Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.</S><S sid = 186 ssid = >; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D09-1030.txt | Citing Article:  W10-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['58','186'] | Reference Text:  <S sid = 58 ssid = >Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.</S><S sid = 186 ssid = >; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D09-1030.txt | Citing Article:  N10-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests.</S> | Reference Offset:  ['74','169'] | Reference Text:  <S sid = 74 ssid = >We report on experiments evaluating translation quality with HTER and with reading comprehension tests.</S><S sid = 169 ssid = >We showed how a reading comprehension test could be created, administered, and graded, with only very minimal intervention.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D09-1030.txt | Citing Article:  W10-0703.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['58','186'] | Reference Text:  <S sid = 58 ssid = >Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.</S><S sid = 186 ssid = >; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D09-1030.txt | Citing Article:  W10-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009).</S> | Reference Offset:  ['14','125'] | Reference Text:  <S sid = 14 ssid = >These tasks included word sense disambiguation, word similarity, textual entailment, and temporal ordering of events, but not machine translation.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D09-1030.txt | Citing Article:  P11-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk.</S> | Reference Offset:  ['9','125'] | Reference Text:  <S sid = 9 ssid = >Therefore, having people evaluate translation output would be preferable, if it were more practical.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D09-1030.txt | Citing Article:  W11-0409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Over the last several of years, Mechanical Turk, introduced by Amazon as "artificial artificial intelligence", has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b).</S> | Reference Offset:  ['23','125'] | Reference Text:  <S sid = 23 ssid = >Amazon describes its Mechanical Turk web service1 as artificial artificial intelligence.</S><S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D09-1030.txt | Citing Article:  W10-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There have been several research papers on using MTurk to help natural language processing tasks, Callison-Burch (2009) used MTurk to evaluate machine translation results.</S> | Reference Offset:  ['13','138'] | Reference Text:  <S sid = 13 ssid = >Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.</S><S sid = 138 ssid = >The advantage of this type of evaluation is that the results have a natural interpretation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D09-1030.txt | Citing Article:  W10-1701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On another way, an application can combine active learning (Arora et al, 2009) and crowd sourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009).</S> | Reference Offset:  ['125','164'] | Reference Text:  <S sid = 125 ssid = >These systems were selected from WMT09 (Callison-Burch et al., 2009).</S><S sid = 164 ssid = >The low cost of the non-expert labor found on Mechanical Turk is cheap enough to collect redundant annotations, which can be utilized to ensure translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


