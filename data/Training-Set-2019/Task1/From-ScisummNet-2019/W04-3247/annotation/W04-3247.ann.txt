Citance Number: 1 | Reference Article:  W04-3247.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.</S> | Reference Offset:  ['0','14'] | Reference Text:  <S sid = 0 ssid = >LexPageRank: Prestige In Multi-Document Text Summarization</S><S sid = 14 ssid = >Test data for our experiments is taken from Document Understanding Conferences (DUC) 2004 summarization evaluation to compare our system also with other state-of-the-art summarization systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-3247.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['36','145'] | Reference Text:  <S sid = 36 ssid = >In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.</S><S sid = 145 ssid = >Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-3247.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['36','145'] | Reference Text:  <S sid = 36 ssid = >In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.</S><S sid = 145 ssid = >Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-3247.txt | Citing Article:  C10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004).</S> | Reference Offset:  ['3','4'] | Reference Text:  <S sid = 3 ssid = >We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.</S><S sid = 4 ssid = >In this model, a sentence connectivity matrix is constructed based on cosine similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-3247.txt | Citing Article:  D08-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality.</S> | Reference Offset:  ['3','141'] | Reference Text:  <S sid = 3 ssid = >We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.</S><S sid = 141 ssid = >We have presented a novel approach to define sentence centrality based on graph-based prestige scoring of sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-3247.txt | Citing Article:  W09-1608.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['36','145'] | Reference Text:  <S sid = 36 ssid = >In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.</S><S sid = 145 ssid = >Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-3247.txt | Citing Article:  C08-1124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first.</S> | Reference Offset:  ['29','92'] | Reference Text:  <S sid = 29 ssid = >First is how to define similarity between two sentences.</S><S sid = 92 ssid = >There are 8 different human judges for DUC 2004 Task 2, and 4 for DUC 2004 Task 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-3247.txt | Citing Article:  C08-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b).</S> | Reference Offset:  ['4','76'] | Reference Text:  <S sid = 4 ssid = >In this model, a sentence connectivity matrix is constructed based on cosine similarity.</S><S sid = 76 ssid = >The graph-based centrality approach we have introduced has several advantages over Centroid.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-3247.txt | Citing Article:  C08-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function.</S> | Reference Offset:  ['59','79'] | Reference Text:  <S sid = 59 ssid = >This method can be directly applied to the cosine similarity graph to find the most prestigious sentences in a document.</S><S sid = 79 ssid = >The degree of a node in the cosine similarity graph is an indication of how much common information the sentence has with other sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-3247.txt | Citing Article:  I08-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005).</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >LexPageRank: Prestige In Multi-Document Text Summarization</S><S sid = 8 ssid = >Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-3247.txt | Citing Article:  I08-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lex PageRank (Erkan and Radev, 2004) is one of such methods.</S> | Reference Offset:  ['71','115'] | Reference Text:  <S sid = 71 ssid = >We call this new measure of sentence similarity lexical PageRank, or LexPageRank.</S><S sid = 115 ssid = >We implemented the Degree and LexPageRank methods, and integrated into the MEAD system as new features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-3247.txt | Citing Article:  D11-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance.</S> | Reference Offset:  ['3','60'] | Reference Text:  <S sid = 3 ssid = >We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.</S><S sid = 60 ssid = >We use PageRank to weight each vote so that a vote that comes from a more prestigious sentence has a greater value in the centrality of a sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-3247.txt | Citing Article:  D07-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable.</S> | Reference Offset:  ['99','111'] | Reference Text:  <S sid = 99 ssid = >During the first step, the feature extractor, each sentence in the input document (or cluster of documents) is converted into a feature vector using the user-defined features.</S><S sid = 111 ssid = >This example indicates the three default MEAD features (Centroid, Position, LengthCutoff), and our new LexPageRank feature used in our experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-3247.txt | Citing Article:  D07-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap.</S> | Reference Offset:  ['6','53'] | Reference Text:  <S sid = 6 ssid = >We provide an evaluation of our method on DUC 2004 data.</S><S sid = 53 ssid = >PageRank is a method proposed for assigning a prestige score to each page in the Web independent of a specific query.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-3247.txt | Citing Article:  N06-2046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality.</S> | Reference Offset:  ['3','12'] | Reference Text:  <S sid = 3 ssid = >We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.</S><S sid = 12 ssid = >Then we introduce two new measures for centrality, Degree and LexPageRank, inspired from the “prestige” concept in social networks and based on our new approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-3247.txt | Citing Article:  C10-2058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004).</S> | Reference Offset:  ['23','52'] | Reference Text:  <S sid = 23 ssid = >Centroid-based summarization has given promising results in the past (Radev et al., 2001).</S><S sid = 52 ssid = >One of the most successful applications of prestige is PageRank (Page et al., 1998), the underlying technology behind the Google search engine.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-3247.txt | Citing Article:  P09-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001).</S> | Reference Offset:  ['20','23'] | Reference Text:  <S sid = 20 ssid = >In centroid-based summarization (Radev et al., 2000), the sentences that contain more words from the centroid of the cluster are considered as central.</S><S sid = 23 ssid = >Centroid-based summarization has given promising results in the past (Radev et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-3247.txt | Citing Article:  P09-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004).</S> | Reference Offset:  ['3','4'] | Reference Text:  <S sid = 3 ssid = >We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.</S><S sid = 4 ssid = >In this model, a sentence connectivity matrix is constructed based on cosine similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-3247.txt | Citing Article:  P09-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['36','145'] | Reference Text:  <S sid = 36 ssid = >In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.</S><S sid = 145 ssid = >Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-3247.txt | Citing Article:  C10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['36','145'] | Reference Text:  <S sid = 36 ssid = >In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.</S><S sid = 145 ssid = >Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


