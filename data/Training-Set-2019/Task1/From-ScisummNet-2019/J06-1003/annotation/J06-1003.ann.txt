Citance Number: 1 | Reference Article:  J06-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Its applications include word sense disambiguation, text summarization and information retrieval (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['7','14'] | Reference Text:  <S sid = 7 ssid = >Measures of relatedness or distance are used in such applica- tions as word sense disambiguation, determining the structure of texts, text sum- marization and annotation, information extraction and retrieval, automatic indexing, lexical selection, and the automatic correction of word errors in text.</S><S sid = 14 ssid = >Our purpose in this paper is to compare the performance of a number of measures of semantic relatedness that have been proposed for use in applications in natural language processing and information retrieval.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J06-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, those methods that are based on hierarchical, taxonomically structured resources are generally better suited for measuring semantic similarity than relatedness (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['5','28'] | Reference Text:  <S sid = 5 ssid = >In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.</S><S sid = 28 ssid = >Unfortunately, because of the sheer number of methods measuring similarity, as well as those measuring distance as the ?opposite?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J06-1003.txt | Citing Article:  W11-0149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, WordNet based measures are well known to be better suited to measure similarity than relatedness due to its hierarchical, taxonomic structure (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?</S><S sid = 13 ssid = >And what is it that makes some measures better than others?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J06-1003.txt | Citing Article:  N09-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['0','100'] | Reference Text:  <S sid = 0 ssid = >Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?</S><S sid = 100 ssid = >17 Computational Linguistics Volume 32, Number 1 Hirst and St-Onge (1998; St-Onge 1995) adapted Morris and Hirst?s (1991) seman- tic distance algorithm from Roget?s Thesaurus to WordNet.3 They distinguished two strengths of semantic relations in WordNet.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J06-1003.txt | Citing Article:  W09-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Semantic relatedness can denote every possible relation between two concepts, unlike semantic similarity, which typically denotes only certain hierarchical relations (like hypernymy and synonymy) and is often computed using hierarchical networks like WordNet (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['83','98'] | Reference Text:  <S sid = 83 ssid = >16 Budanitsky and Hirst Lexical Semantic Relatedness and its inverse, hypernymy; six meronymic (PART-OF) relations ?</S><S sid = 98 ssid = >They computed semantic similarity between two words as the length of the shortest path between them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J06-1003.txt | Citing Article:  E09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Budanitsky and Hirst (2006) provide an extensive survey of these measures.</S> | Reference Offset:  ['0','362'] | Reference Text:  <S sid = 0 ssid = >Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?</S><S sid = 362 ssid = >for each of the measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J06-1003.txt | Citing Article:  E09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Budanitsky and Hirst (2006) also point out an important distinction, between relatedness and similarity.</S> | Reference Offset:  ['18','298'] | Reference Text:  <S sid = 18 ssid = >Resnik (1995) attempts to demonstrate the distinction between the first two by way of example.</S><S sid = 298 ssid = >IS-A location (?a point or extent in space?)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J06-1003.txt | Citing Article:  E09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Budanitsky and Hirst (2006), we consider two frames as similar if they are linked via is-a like relations (e.g. GETTING and COMMERCE BUY), while as related if any relation stands between them (e.g. causation between KILLING and DEATH).</S> | Reference Offset:  ['83','101'] | Reference Text:  <S sid = 83 ssid = >16 Budanitsky and Hirst Lexical Semantic Relatedness and its inverse, hypernymy; six meronymic (PART-OF) relations ?</S><S sid = 101 ssid = >Two words are strongly related if one of the following holds: 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J06-1003.txt | Citing Article:  E09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also experiment with the Jiang and Conrath's (Jiang and Conrath, 1997) measure which relies only on the is-a hierarchy, but proved to be the best WordNet-based measure in the task of ranking words (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['280','282'] | Reference Text:  <S sid = 280 ssid = >From left to right and top to bottom: The word pairs rated by (a) Miller and Charles?s subjects; (b) by the Hirst?St-Onge similarity measure; (c) by the Jiang?Conrath distance measure; (d) by the Leacock?Chodorow similarity measure; (e) by the Lin similarity measure; and (f) by the Resnik similarity measure.</S><S sid = 282 ssid = >Measure M&C R&G ?????????????????????</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J06-1003.txt | Citing Article:  E09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A direct comparison to the word ranking task, suggests that ranking frames is harder than words, not only for humans (as reported in Section 3.2), but also for machines: Budanitsky and Hirst (2006) show that measures for ranking words get much closer to the human upper-bound than our measures do, confirming that frame relatedness is a fairly complex notion to model.</S> | Reference Offset:  ['225','362'] | Reference Text:  <S sid = 225 ssid = >)9 4.4.1 Comparison to Upper Bound.</S><S sid = 362 ssid = >for each of the measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J06-1003.txt | Citing Article:  W11-2214.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Budanitsky and Hirst (2006), we estimate the WordNet sense similarity using the method proposed by Jiang and Conrath (1997).</S> | Reference Offset:  ['4','166'] | Reference Text:  <S sid = 4 ssid = >An information-content?based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik.</S><S sid = 166 ssid = >Reacting to the disadvantages of Resnik?s method, Jiang and Conrath?s (1997) idea was to synthesize edge- and node- based techniques by restoring network edges to their dominant role in similarity com- putations, and using corpus statistics as a secondary, corrective factor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J06-1003.txt | Citing Article:  N09-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora.</S> | Reference Offset:  ['216','232'] | Reference Text:  <S sid = 216 ssid = >6 In their original experiments, Lin and Jiang and Conrath used SemCor, a sense-tagged subset of the Brown Corpus, as their empirical data; but we decided to follow Resnik in using the full and untagged corpus.</S><S sid = 232 ssid = >These discrepancies can be explained by possible minor differences in implementation (e.g., the compound-word recognition mechanism used in collecting the frequency data), differences between the versions of WordNet used in the experiments (Resnik), and differences in the corpora used to obtain the frequency data (Jiang and Conrath, Lin).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J06-1003.txt | Citing Article:  E12-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results.</S> | Reference Offset:  ['278','280'] | Reference Text:  <S sid = 278 ssid = >From left to right and top to bottom: The word pairs rated by (a) Rubenstein and Goodenough?s subjects; (b) by the Hirst?St-Onge similarity measure; (c) by the Jiang?Conrath distance measure; (d) by the Leacock?Chodorow similarity measure; (e) by the Lin similarity measure; and (f) by the Resnik similarity measure.</S><S sid = 280 ssid = >From left to right and top to bottom: The word pairs rated by (a) Miller and Charles?s subjects; (b) by the Hirst?St-Onge similarity measure; (c) by the Jiang?Conrath distance measure; (d) by the Leacock?Chodorow similarity measure; (e) by the Lin similarity measure; and (f) by the Resnik similarity measure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J06-1003.txt | Citing Article:  E12-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs.</S> | Reference Offset:  ['220','277'] | Reference Text:  <S sid = 220 ssid = >24 Budanitsky and Hirst Lexical Semantic Relatedness Table 1 Human and computer ratings of the Rubenstein?Goodenough set of word pairs (part 1 of 2).</S><S sid = 277 ssid = >28 Budanitsky and Hirst Lexical Semantic Relatedness Figure 2 Human and computer ratings of the Rubenstein?Goodenough set of word pairs, with sparse bands marked (see text).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J06-1003.txt | Citing Article:  E12-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['123','394'] | Reference Text:  <S sid = 123 ssid = >max{depth(c1), depth(c2)} (4) where r is the relation that holds between c1 and c2 and r?</S><S sid = 394 ssid = >Hirst?St-Onge 1 .056 .298 .091 3 .067 .159 .089 5 .069 .114 .079 MAX .051 .059 .049 Jiang?Conrath 1 .064 .536 .112 3 .086 .383 .135 5 .097 .326 .141 MAX .111 .233 .137 Leacock?Chodorow 1 .042 .702 .079 3 .052 .535 .094 5 .058 .463 .101 MAX .073 .356 .115 Lin 1 .047 .579 .086 3 .062 .421 .105 5 .067 .3</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J06-1003.txt | Citing Article:  E09-3009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SCM and SPE capture the two most important parameters of measuring semantic relatedness between terms (Budanitsky and Hirst, 2006), namely path length and senses depth in the used thesaurus.</S> | Reference Offset:  ['87','132'] | Reference Text:  <S sid = 87 ssid = >2.4 Computing Taxonomic Path Length A simple way to compute semantic relatedness in a taxonomy such as WordNet is to view it as a graph and identify relatedness with path length between the concepts: ?The shorter the path from one node to another, the more similar they are?</S><S sid = 132 ssid = >depth(lso(c1, c2)) (6) 2.5.3 Leacock and Chodorow?s Normalized Path Length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J06-1003.txt | Citing Article:  E09-3009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The reader can consult Budanitsky and Hirst (2006) to confirm that all the other measures of semantic relatedness we compare to, do not follow the same pattern as the human ratings, as closely as our measure of relatedness does (low y values for small x values and high y values for high x).</S> | Reference Offset:  ['25','281'] | Reference Text:  <S sid = 25 ssid = >to one another if their similarity or their relatedness is high, and otherwise they are ?distant?.</S><S sid = 281 ssid = >30 Budanitsky and Hirst Lexical Semantic Relatedness Table 3 The absolute values of the coefficients of correlation between human ratings of similarity (by Miller and Charles and by Rubenstein and Goodenough) and the five computational measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J06-1003.txt | Citing Article:  E09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview).</S> | Reference Offset:  ['0','87'] | Reference Text:  <S sid = 0 ssid = >Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?</S><S sid = 87 ssid = >2.4 Computing Taxonomic Path Length A simple way to compute semantic relatedness in a taxonomy such as WordNet is to view it as a graph and identify relatedness with path length between the concepts: ?The shorter the path from one node to another, the more similar they are?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J06-1003.txt | Citing Article:  N09-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is a large body of work on using WordNet to compute measures of lexical similarity (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['0','362'] | Reference Text:  <S sid = 0 ssid = >Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?</S><S sid = 362 ssid = >for each of the measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J06-1003.txt | Citing Article:  C10-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The ability to determine semantic relatedness between terms is useful for a variety of nlp applications, including word sense disambiguation, information extraction and retrieval, and text summarisation (Budanitsky and Hirst, 2006).</S> | Reference Offset:  ['2','7'] | Reference Text:  <S sid = 2 ssid = >University of Toronto The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed.</S><S sid = 7 ssid = >Measures of relatedness or distance are used in such applica- tions as word sense disambiguation, determining the structure of texts, text sum- marization and annotation, information extraction and retrieval, automatic indexing, lexical selection, and the automatic correction of word errors in text.</S> | Discourse Facet:  NA | Annotator: Automatic


