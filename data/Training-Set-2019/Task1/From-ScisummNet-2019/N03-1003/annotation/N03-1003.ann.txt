Citance Number: 1 | Reference Article:  N03-1003.txt | Citing Article:  N03-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)).</S> | Reference Offset:  ['25','41'] | Reference Text:  <S sid = 25 ssid = >In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.</S><S sid = 41 ssid = >Use of comparable corpora and minimal use of knowledge resources.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N03-1003.txt | Citing Article:  P13-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998).</S> | Reference Offset:  ['75','145'] | Reference Text:  <S sid = 75 ssid = >Pairwise MSA can be extended efficiently to multiple sequences via the iterative pairwise alignment, a polynomial-time method commonly used in computational biology (Durbin et al., 1998).3 The results can be represented in an intuitive form via a word lattice (see Figure 3), which compactly represents (n-gram) structural similarities between the cluster’s sentences.</S><S sid = 145 ssid = >Lattices have proven advantageous in a number of NLP contexts (Mangu et al., 2000; Bangalore et al., 2002; Barzilay and Lee, 2002; Pang et al., 2003), but were usually produced from (multi-)parallel data, which may not be readily available for many applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N03-1003.txt | Citing Article:  P13-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3).</S> | Reference Offset:  ['68','140'] | Reference Text:  <S sid = 68 ssid = >This is accomplished by applying hierarchical complete-link clustering to the sentences using a similarity metric based on word n-gram overlap ( ).</S><S sid = 140 ssid = >We presented an approach for generating sentence level paraphrases, a task not addressed previously.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N03-1003.txt | Citing Article:  P13-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the scoring function for MSA from Barzilay and Lee (2003).</S> | Reference Offset:  ['25','74'] | Reference Text:  <S sid = 25 ssid = >In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.</S><S sid = 74 ssid = >Pairwise MSA takes two sentences and a scoring function giving the similarity between words; it determines the highest-scoring way to perform insertions, deletions, and changes to transform one of the sentences into the other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N03-1003.txt | Citing Article:  P13-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle.</S> | Reference Offset:  ['81','135'] | Reference Text:  <S sid = 81 ssid = >The choice of 50% is not arbitrary — it can be proved using the pigeonhole principle that our strictmajority criterion imposes a unique linear ordering of the backbone nodes that respects the word ordering within the sentences, thus guaranteeing at least a degree of wellformedness and avoiding the problem of how to order backbone nodes occurring on parallel “branches” of the lattice.</S><S sid = 135 ssid = >Agreement according to the Kappa statistic was 0.6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N03-1003.txt | Citing Article:  W06-1603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR.</S> | Reference Offset:  ['0','24'] | Reference Text:  <S sid = 0 ssid = >Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment</S><S sid = 24 ssid = >Our work presents a novel knowledge-lean algorithm that uses multiple-sequence alignment (MSA) to learn to generate sentence-level paraphrases essentially from unannotated corpus data alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N03-1003.txt | Citing Article:  N12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods.</S> | Reference Offset:  ['75','145'] | Reference Text:  <S sid = 75 ssid = >Pairwise MSA can be extended efficiently to multiple sequences via the iterative pairwise alignment, a polynomial-time method commonly used in computational biology (Durbin et al., 1998).3 The results can be represented in an intuitive form via a word lattice (see Figure 3), which compactly represents (n-gram) structural similarities between the cluster’s sentences.</S><S sid = 145 ssid = >Lattices have proven advantageous in a number of NLP contexts (Mangu et al., 2000; Bangalore et al., 2002; Barzilay and Lee, 2002; Pang et al., 2003), but were usually produced from (multi-)parallel data, which may not be readily available for many applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N03-1003.txt | Citing Article:  P04-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003).</S> | Reference Offset:  ['25','56'] | Reference Text:  <S sid = 25 ssid = >In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.</S><S sid = 56 ssid = >However, extraction methods are not easily extended to generation methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N03-1003.txt | Citing Article:  P10-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003).</S> | Reference Offset:  ['0','24'] | Reference Text:  <S sid = 0 ssid = >Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment</S><S sid = 24 ssid = >Our work presents a novel knowledge-lean algorithm that uses multiple-sequence alignment (MSA) to learn to generate sentence-level paraphrases essentially from unannotated corpus data alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N03-1003.txt | Citing Article:  E09-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm.</S> | Reference Offset:  ['0','24'] | Reference Text:  <S sid = 0 ssid = >Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment</S><S sid = 24 ssid = >Our work presents a novel knowledge-lean algorithm that uses multiple-sequence alignment (MSA) to learn to generate sentence-level paraphrases essentially from unannotated corpus data alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N03-1003.txt | Citing Article:  W03-1608.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event.</S> | Reference Offset:  ['25','29'] | Reference Text:  <S sid = 25 ssid = >In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.</S><S sid = 29 ssid = >Rather, we use two comparable corpora, in our case, collections of articles produced by two different newswire agencies about the same events.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N03-1003.txt | Citing Article:  E12-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations.</S> | Reference Offset:  ['33','67'] | Reference Text:  <S sid = 33 ssid = >First, working on each of the comparable corpora separately, we compute lattices — compact graph-based representations — to find commonalities within (automatically derived) groups of structurally similar sentences.</S><S sid = 67 ssid = >Our first step is to cluster sentences into groups from which to learn useful patterns; for the multiple-sequence techniques we will use, this means that the sentences within clusters should describe similar events and have similar structure, as in the sentences of Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N03-1003.txt | Citing Article:  D10-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments.</S> | Reference Offset:  ['27','96'] | Reference Text:  <S sid = 27 ssid = >See Bangalore et al. (2002) and Barzilay and Lee (2002) for other uses of such data.</S><S sid = 96 ssid = >All evaluations involved judgments by native speakers of English who were not familiar with the paraphrasing systems under consideration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N03-1003.txt | Citing Article:  W12-4006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus.</S> | Reference Offset:  ['122','141'] | Reference Text:  <S sid = 122 ssid = >We are not aware of another system generating sentence-level paraphrases.</S><S sid = 141 ssid = >Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N03-1003.txt | Citing Article:  W08-0906.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003).</S> | Reference Offset:  ['1','16'] | Reference Text:  <S sid = 1 ssid = >We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.</S><S sid = 16 ssid = >One might initially suppose that sentence-level paraphrasing is simply the result of word-for-word or phraseby-phrase substitution applied in a domain- and contextindependent fashion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N03-1003.txt | Citing Article:  W08-0906.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al, 2004), word n-gram overlap (Barzilay and Lee, 2003) etc.</S> | Reference Offset:  ['68','92'] | Reference Text:  <S sid = 68 ssid = >This is accomplished by applying hierarchical complete-link clustering to the sentences using a similarity metric based on word n-gram overlap ( ).</S><S sid = 92 ssid = >To compare the set of argument values of two lattices, we simply count their word overlap, giving double weight to proper names and numbers and discarding auxiliaries (we purposely ignore order because paraphrases can consist of word re-orderings).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N03-1003.txt | Citing Article:  W11-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction.</S> | Reference Offset:  ['30','56'] | Reference Text:  <S sid = 30 ssid = >The use of related corpora is key: we can capture paraphrases that on the surface bear little resemblance but that, by the nature of the data, must be descriptions of the same information.</S><S sid = 56 ssid = >However, extraction methods are not easily extended to generation methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N03-1003.txt | Citing Article:  P06-2096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases.</S> | Reference Offset:  ['25','82'] | Reference Text:  <S sid = 25 ssid = >In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.</S><S sid = 82 ssid = >Once we have identified the backbone nodes as points of strong commonality, the next step is to identify the regions of variability (or, in lattice terms, many parallel disjoint paths) between them as (probably) corresponding to the arguments of the propositions that the sentences represent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N03-1003.txt | Citing Article:  P06-2096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003).</S> | Reference Offset:  ['25','147'] | Reference Text:  <S sid = 25 ssid = >In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.</S><S sid = 147 ssid = >We are grateful to many people for helping us in this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N03-1003.txt | Citing Article:  P07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004).</S> | Reference Offset:  ['119','135'] | Reference Text:  <S sid = 119 ssid = >In terms of agreement, the Kappa value (measuring pairwise agreement discounting chance occurrences9) on the common set was 0.54, which corresponds to moderate agreement (Landis and Koch, 1977).</S><S sid = 135 ssid = >Agreement according to the Kappa statistic was 0.6.</S> | Discourse Facet:  NA | Annotator: Automatic


