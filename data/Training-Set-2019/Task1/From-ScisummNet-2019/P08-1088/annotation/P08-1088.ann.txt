Citance Number: 1 | Reference Article:  P08-1088.txt | Citing Article:  W09-1702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall.</S> | Reference Offset:  ['11','79'] | Reference Text:  <S sid = 11 ssid = >Precision and recall are then measured over these bilingual lexicons.</S><S sid = 79 ssid = >Running EDITDIST (see section 4.3) on ENES-W yielded 61.1 p0.33, but precision quickly degrades for higher recall levels (see EDITDIST in table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1088.txt | Citing Article:  W09-1702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension.</S> | Reference Offset:  ['3','24'] | Reference Text:  <S sid = 3 ssid = >Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.</S><S sid = 24 ssid = >We propose the following generative model over matchings m and word types (s, t), which we call matching canonical correlation analysis (MCCA).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1088.txt | Citing Article:  W09-1702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon.</S> | Reference Offset:  ['62','97'] | Reference Text:  <S sid = 62 ssid = >Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs.</S><S sid = 97 ssid = >All of our experiments so far have exploited a small seed lexicon which has been derived from the evaluation lexicon (see section 4.3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1088.txt | Citing Article:  W09-1702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008).</S> | Reference Offset:  ['5','120'] | Reference Text:  <S sid = 5 ssid = >Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).</S><S sid = 120 ssid = >We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1088.txt | Citing Article:  W09-1702.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations.</S> | Reference Offset:  ['118','122'] | Reference Text:  <S sid = 118 ssid = >We manually examined the top 100 errors in the English-Spanish lexicon produced by our system on EN-ES-W. Of the top 100 errors: 21 were correct translations not contained in the Wiktionary lexicon (e.g. pintura to painting), 4 were purely morphological errors (e.g. airport to aeropuertos), 30 were semantically related (e.g. basketball to bÂ´eisbol), 15 were words with strong orthographic similarities (e.g. coast to costas), and 30 were difficult to categorize and fell into none of these categories.</S><S sid = 122 ssid = >Of the true errors, the most common arose from semantically related words which had strong context feature correlations (see table 4(b)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1088.txt | Citing Article:  P14-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with.</S> | Reference Offset:  ['1','73'] | Reference Text:  <S sid = 1 ssid = >We present a method for learning bilingual translation lexicons from monolingual corpora.</S><S sid = 73 ssid = >In this section, we explore feature representations of word types in our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1088.txt | Citing Article:  N10-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008).</S> | Reference Offset:  ['0','5'] | Reference Text:  <S sid = 0 ssid = >Learning Bilingual Lexicons from Monolingual Corpora</S><S sid = 5 ssid = >Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1088.txt | Citing Article:  C10-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features.</S> | Reference Offset:  ['81','90'] | Reference Text:  <S sid = 81 ssid = >We can represent orthographic features of a word type w by assigning a feature to each substring of length G 3.</S><S sid = 90 ssid = >For the remainder of this work, we will use MCCA to refer to our model using both orthographic and context features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1088.txt | Citing Article:  C10-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','131'] | Reference Text:  <S sid = 48 ssid = >We found that it was helpful to explicitly control the number of edges.</S><S sid = 131 ssid = >It remains to be seen how such lexicons can be best utilized, but they invite new approaches to the statistical translation of resource-poor languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1088.txt | Citing Article:  C10-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)).</S> | Reference Offset:  ['10','63'] | Reference Text:  <S sid = 10 ssid = >We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations.</S><S sid = 63 ssid = >We used two methods to derive a seed lexicon.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1088.txt | Citing Article:  C10-2070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns.</S> | Reference Offset:  ['85','115'] | Reference Text:  <S sid = 85 ssid = >One non-orthographic clue that word types s and t form a translation pair is that there is a strong correlation between the source words used with s and the target words used with t. To capture this information, we define context features for each word type w, consisting of counts of nouns which occur within a window of size 4 around w. Consider the translation pair (time, tiempo) illustrated in figure 2.</S><S sid = 115 ssid = >In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1088.txt | Citing Article:  P11-2071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms.</S> | Reference Offset:  ['3','10'] | Reference Text:  <S sid = 3 ssid = >Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.</S><S sid = 10 ssid = >We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1088.txt | Citing Article:  P11-2071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008).</S> | Reference Offset:  ['3','15'] | Reference Text:  <S sid = 3 ssid = >Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.</S><S sid = 15 ssid = >Our model is based on canonical correlation analysis (CCA)1 and explains matched word pairs via vectors in a common latent space.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1088.txt | Citing Article:  P11-2071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008).</S> | Reference Offset:  ['5','72'] | Reference Text:  <S sid = 5 ssid = >Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).</S><S sid = 72 ssid = >We will use MCCA (for matching CCA) to denote our model using the optimal feature set (see section 5.3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1088.txt | Citing Article:  D09-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008).</S> | Reference Offset:  ['1','5'] | Reference Text:  <S sid = 1 ssid = >We present a method for learning bilingual translation lexicons from monolingual corpora.</S><S sid = 5 ssid = >Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1088.txt | Citing Article:  D09-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008).</S> | Reference Offset:  ['5','54'] | Reference Text:  <S sid = 5 ssid = >Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).</S><S sid = 54 ssid = >The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1088.txt | Citing Article:  E12-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)).</S> | Reference Offset:  ['70','112'] | Reference Text:  <S sid = 70 ssid = >In all experiments, unless noted otherwise, we used a seed of size 100 obtained from Le and considered lexicons between the top n = 2,000 most frequent source and target noun word types which were not in the seed lexicon; each system proposed an already-ranked one-to-one translation lexicon amongst these n words.</S><S sid = 112 ssid = >There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1088.txt | Citing Article:  E12-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Learning Bilingual Lexicons from Monolingual Corpora</S><S sid = 1 ssid = >We present a method for learning bilingual translation lexicons from monolingual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1088.txt | Citing Article:  W09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Learning Bilingual Lexicons from Monolingual Corpora</S><S sid = 1 ssid = >We present a method for learning bilingual translation lexicons from monolingual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1088.txt | Citing Article:  N09-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.</S> | Reference Offset:  ['3','24'] | Reference Text:  <S sid = 3 ssid = >Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.</S><S sid = 24 ssid = >We propose the following generative model over matchings m and word types (s, t), which we call matching canonical correlation analysis (MCCA).</S> | Discourse Facet:  NA | Annotator: Automatic


