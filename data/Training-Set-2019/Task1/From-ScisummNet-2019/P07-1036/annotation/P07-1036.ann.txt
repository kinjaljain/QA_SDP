Citance Number: 1 | Reference Article:  P07-1036.txt | Citing Article:  P08-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We achieve competitive performance in comparison to alternate model families, in particular generative models such as MRFs trained with EM (Haghighi and Klein, 2006) and HMMs trained with soft constraints (Chang et al, 2007).</S> | Reference Offset:  ['171','177'] | Reference Text:  <S sid = 171 ssid = >(Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains.</S><S sid = 177 ssid = >(Haghighi and Klein, 2006) also worked on one of our data sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1036.txt | Citing Article:  P08-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another recent method that has been proposed for training sequence models with constraints is Chang et al (2007).</S> | Reference Offset:  ['21','168'] | Reference Text:  <S sid = 21 ssid = >It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework.</S><S sid = 168 ssid = >However, as shown, our proposed algorithm H&W&C for training with constraints is critical when the amount labeled data is small.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1036.txt | Citing Article:  P08-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['206','207'] | Reference Text:  <S sid = 206 ssid = >Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing.</S><S sid = 207 ssid = >Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1036.txt | Citing Article:  P13-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most constraints that prove useful for SRL (Chang et al., 2007) also require customization when applied to a new language, and some rely on language specific resources, such as a valency lexicon.</S> | Reference Offset:  ['4','130'] | Reference Text:  <S sid = 4 ssid = >Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance.</S><S sid = 130 ssid = >For example, consider the output 11101000 with the constraint that it should belong to the language 1*0*.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1036.txt | Citing Article:  P11-5005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Constraint driven learning (CoDL) was first introduced in Chang et al [2007], and has been used also in Chang et al [2008].</S> | Reference Offset:  ['34','92'] | Reference Text:  <S sid = 34 ssid = >(Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005).</S><S sid = 92 ssid = >In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1036.txt | Citing Article:  C10-2137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['206','207'] | Reference Text:  <S sid = 206 ssid = >Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing.</S><S sid = 207 ssid = >Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1036.txt | Citing Article:  N12-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Likewise, Chang et al (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.</S> | Reference Offset:  ['37','184'] | Reference Text:  <S sid = 37 ssid = >We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraintsâ€™ violation.</S><S sid = 184 ssid = >We use two constraints to illustrate the ideas.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1036.txt | Citing Article:  N12-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chang et al (2007) use a set of domain specific rules as automatic implicit feedback for training information extraction system.</S> | Reference Offset:  ['3','22'] | Reference Text:  <S sid = 3 ssid = >Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.</S><S sid = 22 ssid = >In the semi-supervised domain there are two main approaches for injecting domain specific knowledge.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1036.txt | Citing Article:  D09-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare our CRF model integrated with VE with two state-of-the-art models, i.e., constraint driven learning (Chang et al, 2007) and generalized expectation criteria (Mann and McCallum, 2008).</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Guiding Semi-Supervision with Constraint-Driven Learning</S><S sid = 25 ssid = >This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1036.txt | Citing Article:  D09-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Constraint-driven learning (Chang et al, 2007) expresses several kinds of constraints in a unified form.</S> | Reference Offset:  ['0','92'] | Reference Text:  <S sid = 0 ssid = >Guiding Semi-Supervision with Constraint-Driven Learning</S><S sid = 92 ssid = >In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1036.txt | Citing Article:  D09-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['206','207'] | Reference Text:  <S sid = 206 ssid = >Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing.</S><S sid = 207 ssid = >Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1036.txt | Citing Article:  D12-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Chang et al 2007) incorporates domain specific constraints in semi-supervised learning.</S> | Reference Offset:  ['22','97'] | Reference Text:  <S sid = 22 ssid = >In the semi-supervised domain there are two main approaches for injecting domain specific knowledge.</S><S sid = 97 ssid = >The semi-supervised learning with constraints is done with an EM-like procedure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1036.txt | Citing Article:  D11-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The learning algorithm in Figure 2 is an instance of augmented-loss training (Hall et al, 2011) which is closely related to the constraint driven learning algorithms of Chang et al (2007).</S> | Reference Offset:  ['92','105'] | Reference Text:  <S sid = 92 ssid = >In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S><S sid = 105 ssid = >Our training algorithm is summarized in Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1036.txt | Citing Article:  P12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that the objective function in Equation 5, if written in the additive form, leads to a cost function reminiscent of the one used in constraint-driven learning algorithm (CoDL) (Chang et al, 2007) (and similarly, posterior regularization (Ganchev et al, 2010), which we will discuss later at Section 6).</S> | Reference Offset:  ['92','135'] | Reference Text:  <S sid = 92 ssid = >In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S><S sid = 135 ssid = >The function d(y, 1C(x)) used is an approximation of a Hamming distance function, discussed in Section 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1036.txt | Citing Article:  P12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Constraint-driven learning (CoDL) (Chang et al, 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models.</S> | Reference Offset:  ['0','92'] | Reference Text:  <S sid = 0 ssid = >Guiding Semi-Supervision with Constraint-Driven Learning</S><S sid = 92 ssid = >In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1036.txt | Citing Article:  N10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al, 2007).</S> | Reference Offset:  ['2','92'] | Reference Text:  <S sid = 2 ssid = >In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms.</S><S sid = 92 ssid = >In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1036.txt | Citing Article:  N10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >corresponds to constraint satisfaction weights used in (Chang et al, 2007).</S> | Reference Offset:  ['34','110'] | Reference Text:  <S sid = 34 ssid = >(Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005).</S><S sid = 110 ssid = >We used maximum likelihood estimation of A but, in general, perceptron or quasiNewton can also be used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1036.txt | Citing Article:  N10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chang et al propose constraint-driven learning (CODL, Chang et al, 2007) which can be interpreted as a variation of self-training: Instances are selected for supervision based not only on the model's prediction, but also on their consistency with a set of user-defined constraints.</S> | Reference Offset:  ['0','116'] | Reference Text:  <S sid = 0 ssid = >Guiding Semi-Supervision with Constraint-Driven Learning</S><S sid = 116 ssid = >Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1036.txt | Citing Article:  N10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the same token label constraints as Chang et al (2007).</S> | Reference Offset:  ['17','49'] | Reference Text:  <S sid = 17 ssid = >Our algorithm pushes this intuition further, in that the use of constraints allows us to better exploit domain information as a way to label, along with the current learned model, unlabeled examples.</S><S sid = 49 ssid = >While the predicted label assignment (b) is generally coherent, some constraints are violated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1036.txt | Citing Article:  N10-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also report supervised results from (Chang et al, 2007) and SampleRank.</S> | Reference Offset:  ['8','171'] | Reference Text:  <S sid = 8 ssid = >However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000).</S><S sid = 171 ssid = >(Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains.</S> | Discourse Facet:  NA | Annotator: Automatic


