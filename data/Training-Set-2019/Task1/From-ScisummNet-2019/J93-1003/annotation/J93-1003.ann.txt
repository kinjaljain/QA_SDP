Citance Number: 1 | Reference Article:  J93-1003.txt | Citing Article:  W94-0103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The problem of low counts (i.e. linguistic patterns that were never, or rarely found) has not been analyzed appropriately inmost papers, as convincingly demonstrated in [Dunning, 1993].</S> | Reference Offset:  ['123','138'] | Reference Text:  <S sid = 123 ssid = >Tile convergence of the log of the likelihood ratio to the asymptotic distribution is demonstrated dramatically in Figure 4.</S><S sid = 138 ssid = >These counts were analyzed using the test for binomials described earlier, and the 50 most significant are tabulated in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J93-1003.txt | Citing Article:  P14-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993).</S> | Reference Offset:  ['114','156'] | Reference Text:  <S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S><S sid = 156 ssid = >In addition, there are a wide variety of distribution free methods that may avoid even the assumption that text can be modeled by multinomial distributions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J93-1003.txt | Citing Article:  P14-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The likelihood ratio tests (Dunning, 1993) is used for this purpose.</S> | Reference Offset:  ['95','130'] | Reference Text:  <S sid = 95 ssid = >Likelihood ratio tests are based on the idea that statistical hypotheses can be said to specify subspaces of the space described by the unknown parameters of the statistical model being used.</S><S sid = 130 ssid = >As will be seen, the ranking based on likelihood ratio tests does exactly this.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J93-1003.txt | Citing Article:  P03-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993).</S> | Reference Offset:  ['108','121'] | Reference Text:  <S sid = 108 ssid = >The likelihood ratio for a hypothesis is the ratio of the maximum value of the likelihood function over the subspace represented by the hypothesis to the maximum value of the likelihood function over the entire parameter space.</S><S sid = 121 ssid = >The pronounced disparity occurs when ki is larger than the value expected based on the observed value of k2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J93-1003.txt | Citing Article:  W08-1914.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many previous studies have shown that the log-likelihood ratio is well suited for this purpose (Dunning, 1993).</S> | Reference Offset:  ['114','160'] | Reference Text:  <S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S><S sid = 160 ssid = >For the binomial case, the log likelihood statistic is given by —21og A = 2 [log L(pi, ki, ni) + log L(p2, k2, n2) — log L(p, ki, ni) — log L(p, k2, n2)] where For the multinomial case, this statistic becomes —2 log = 2 [log L(Pi , + log L(P2, K2) — log L(Q, ) — log L(Q, K2)] where kji Ei kii Ei kii Eii kii ki log pi</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J93-1003.txt | Citing Article:  W08-1914.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998).</S> | Reference Offset:  ['126','130'] | Reference Text:  <S sid = 126 ssid = >The close agreement shows that the likelihood ratio measure produces accurate results over six decades of significance even in the range where the normal X2 measure diverges radically from the ideal.</S><S sid = 130 ssid = >As will be seen, the ranking based on likelihood ratio tests does exactly this.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J93-1003.txt | Citing Article:  W06-3307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is known that PMI gives undue importance to low frequency events (Dunning, 1993), therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus.</S> | Reference Offset:  ['47','143'] | Reference Text:  <S sid = 47 ssid = >For example, simple word counts made on a moderate-sized corpus show that words that have a frequency of less than one in 50,000 words make up about 20-30% of typical English language news-wire reports.</S><S sid = 143 ssid = >The overestimate of the significance of items that occur only a few times is dramatic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J93-1003.txt | Citing Article:  P01-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The measures2 - Mutual Information (Church and Hanks, 1989), the log-likelihood ratio test (Dunning, 1993), two statistical tests: t-test and chi square-test, and co-occurrence frequency - are applied to two sets of data: adjective-noun (AdjN) pairs and preposition-noun-verb (PNV) triples, where the AMs are applied to (PN,V) pairs.</S> | Reference Offset:  ['19','97'] | Reference Text:  <S sid = 19 ssid = >This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.</S><S sid = 97 ssid = >Such a test is called parametric.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J93-1003.txt | Citing Article:  P01-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, there is a widely held belief that and are inferior to other measures because they overestimate the collocativity of low-frequency candidates (cf. the remarks on the chi square measure in (Dunning, 1993)).</S> | Reference Offset:  ['38','143'] | Reference Text:  <S sid = 38 ssid = >This comparison is possible because the measure described in this paper has better asymptotic behavior than more traditional measures.</S><S sid = 143 ssid = >The overestimate of the significance of items that occur only a few times is dramatic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J93-1003.txt | Citing Article:  W11-1825.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tried two feature reduction methods: a simple count cutoff, and selection of the top n features in terms of log likelihood ratio (Dunning, 1993) with the target values.</S> | Reference Offset:  ['7','114'] | Reference Text:  <S sid = 7 ssid = >These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms.</S><S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J93-1003.txt | Citing Article:  C10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The LLR measurement measures stochastic dependency between two such random variables (Dunning, 1993), and is known to be equal to Mutual Information that is linearly scaled by the size of the corpus (Moore, 2004).</S> | Reference Offset:  ['42','56'] | Reference Text:  <S sid = 42 ssid = >The assumption that simple functions of the random variables being sampled are distributed normally or approximately normally underlies many common statistical tests.</S><S sid = 56 ssid = >In text analysis, the statistically based measures that have been used have usually been based on test statistics that are useful because, given certain assumptions, they have a known distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J93-1003.txt | Citing Article:  C10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an alternative for determining the probability of a positive association using P (PMI& gt; 0), we calculate LLR and assume that approximately LLR with one degree of freedom (Dunning, 1993).</S> | Reference Offset:  ['115','124'] | Reference Text:  <S sid = 115 ssid = >When j is 2 (the binomial), —2 log A will be X2 distributed with one degree of freedom.</S><S sid = 124 ssid = >In this figure, the straighter line was computed using a symbolic algebra package and represents the idealized one degree of freedom cumulative X2 distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J93-1003.txt | Citing Article:  W04-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many statistical metrics have been proposed, including point wise mutual information (MI) (Church et al 1990), mean and variance, hypothesis testing (t-test, chi square test, etc.), log-likelihood ratio (LR) (Dunning, 1993),statistic language model (Tomokiyo, et al 2003), and so on.</S> | Reference Offset:  ['28','107'] | Reference Text:  <S sid = 28 ssid = >The second approach is typified by much of the work of Gale and Church (Gale and Church this issue, and in press; Church et al. 1989).</S><S sid = 107 ssid = >More information about likelihood ratio tests can be found in texts on theoretical statistics (Mood et al. 1974).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J93-1003.txt | Citing Article:  W09-1705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given a contextual word cw that occurs in the paragraphs of bc, a log-likelihood ratio (G2) test is employed (Dunning, 1993), which checks if the distribution of cw in bc is similar to the distribution of cw in rc.</S> | Reference Offset:  ['57','158'] | Reference Text:  <S sid = 57 ssid = >This distribution is most commonly either the normal or X2 distribution.</S><S sid = 158 ssid = >Also, using the Poisson distribution instead of the multinomial as the limiting distribution for the distribution of counts may provide some benefits.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J93-1003.txt | Citing Article:  W04-3243.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since it was first introduced to the NLPcommunity by Dunning (1993), the G2 log-likelihood-ratio statistic has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations.</S> | Reference Offset:  ['114','160'] | Reference Text:  <S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S><S sid = 160 ssid = >For the binomial case, the log likelihood statistic is given by —21og A = 2 [log L(pi, ki, ni) + log L(p2, k2, n2) — log L(p, ki, ni) — log L(p, k2, n2)] where For the multinomial case, this statistic becomes —2 log = 2 [log L(Pi , + log L(P2, K2) — log L(Q, ) — log L(Q, K2)] where kji Ei kii Ei kii Eii kii ki log pi</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J93-1003.txt | Citing Article:  W04-3243.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dunning (1993) gives the formula for the statistic we are calling G2 in a form that is very compact, but not necessarily the most illuminating.</S> | Reference Offset:  ['65','117'] | Reference Text:  <S sid = 65 ssid = >The task of counting words can be cast into the form of a repeated sequence of such binary trials comparing each word in a text with the word being counted.</S><S sid = 117 ssid = >This form is where —21og A (kji — niqi)2 as in the multinomial case above and Interestingly, this expression is exactly the test statistic for Pearson's X2 test, although the form shown is not quite the customary one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J93-1003.txt | Citing Article:  C02-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As the strength of relevance between a target compound noun t and its co-occurring word r, the feature value of r, w (t, r) is defined by the log likelihood ratio (Dunning, 1993) as follows.</S> | Reference Offset:  ['108','114'] | Reference Text:  <S sid = 108 ssid = >The likelihood ratio for a hypothesis is the ratio of the maximum value of the likelihood function over the subspace represented by the hypothesis to the maximum value of the likelihood function over the entire parameter space.</S><S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J93-1003.txt | Citing Article:  N09-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We then ranked the collected query pairs using log likelihood ratio (LLR) (Dunning, 1993), which measures the dependence between q1 and q2 within the context of web queries (Jones et al, 2006b).</S> | Reference Offset:  ['107','114'] | Reference Text:  <S sid = 107 ssid = >More information about likelihood ratio tests can be found in texts on theoretical statistics (Mood et al. 1974).</S><S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J93-1003.txt | Citing Article:  C02-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The starting point is the log likelihood ratio (Dunning 1993).</S> | Reference Offset:  ['114','160'] | Reference Text:  <S sid = 114 ssid = >The likelihood ratio for this test is where Taking the logarithm of the likelihood ratio gives —21og A -= 2 [log L (pi , , ni) + log Up2 , k2 , n2) — log L(p, , ni) — log L(p,k2, n2)] • For the multinomial case, it is convenient to use the double subscripts and the abbreviations This expression implicitly involves n because E119 = n. Maximizing and taking the logarithm, —21og A = 2 [log L (Pi , ) + log L (P2, K2) — log L(Q, ) — log L (Q, K2)1 where If the null hypothesis holds, then the log-likelihood ratio is asymptotically X2 distributed with k/2 — 1 degrees of freedom.</S><S sid = 160 ssid = >For the binomial case, the log likelihood statistic is given by —21og A = 2 [log L(pi, ki, ni) + log L(p2, k2, n2) — log L(p, ki, ni) — log L(p, k2, n2)] where For the multinomial case, this statistic becomes —2 log = 2 [log L(Pi , + log L(P2, K2) — log L(Q, ) — log L(Q, K2)] where kji Ei kii Ei kii Eii kii ki log pi</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J93-1003.txt | Citing Article:  C02-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although log identifies collocations much better than competing approaches (Dunning 1993) in terms of its recall, it suffers from its relatively poor precision rates.</S> | Reference Offset:  ['7','129'] | Reference Text:  <S sid = 7 ssid = >These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms.</S><S sid = 129 ssid = >The results of such a bigram analysis should highlight collocations common in English as well as collocations peculiar to the financial nature of the analyzed text.</S> | Discourse Facet:  NA | Annotator: Automatic


