Citance Number: 1 | Reference Article:  N01-1023.txt | Citing Article:  P09-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers.</S> | Reference Offset:  ['5','149'] | Reference Text:  <S sid = 5 ssid = >The current crop of statistical parsers share a similar training methodology.</S><S sid = 149 ssid = >These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N01-1023.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used.</S> | Reference Offset:  ['114','139'] | Reference Text:  <S sid = 114 ssid = >The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTagger (Srinivas, 1997) which is a statistical model for tagging sentences with elementary lexicalized structures.</S><S sid = 139 ssid = >Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al., 1999)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N01-1023.txt | Citing Article:  N03-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other.</S> | Reference Offset:  ['140','142'] | Reference Text:  <S sid = 140 ssid = >• In our experiments, unlike (Blum and Mitchell, 1998) we do not balance the label priors when picking new labeled examples for addition to the training data.</S><S sid = 142 ssid = >In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N01-1023.txt | Citing Article:  P08-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al.</S> | Reference Offset:  ['4','104'] | Reference Text:  <S sid = 4 ssid = >Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.</S><S sid = 104 ssid = >We used EVALB (written by Satoshi Sekine and Michael Collins) which scores based on PARSEVAL (Black et al., 1991); with the standard parameter file (as per standard practice, part of speech brackets were not part of the evaluation).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N01-1023.txt | Citing Article:  H05-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','153'] | Reference Text:  <S sid = 48 ssid = >Since both of these steps ultimately have to agree with each other, we can utilize an iterative method called CoTraining that attempts to increase agreement between a pair of statistical models by exploiting mutual constraints between their output.</S><S sid = 153 ssid = >We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N01-1023.txt | Citing Article:  W11-0319.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Co-traininghas been successfully applied to various applications, such as statistical parsing (Sarkar, 2001).</S> | Reference Offset:  ['0','122'] | Reference Text:  <S sid = 0 ssid = >Applying Co-Training Methods To Statistical Parsing</S><S sid = 122 ssid = >(Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N01-1023.txt | Citing Article:  P09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001).</S> | Reference Offset:  ['13','139'] | Reference Text:  <S sid = 13 ssid = >In this paper, we use one such machine learning technique: Co-Training, which has been used successfully in several classification tasks like web page classification, word sense disambiguation and named-entity recognition.</S><S sid = 139 ssid = >Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al., 1999)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N01-1023.txt | Citing Article:  P02-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Applying Co-Training Methods To Statistical Parsing</S><S sid = 1 ssid = >We propose a novel Co-Training method for statistical parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N01-1023.txt | Citing Article:  N06-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sarkar (2001) and Steedman et al (2003 ) investigated using co-training for parsing.</S> | Reference Offset:  ['55','149'] | Reference Text:  <S sid = 55 ssid = >In the representation we use, parsing using a lexicalized grammar is done in two steps: Each of these two steps involves ambiguity which can be resolved using a statistical model.</S><S sid = 149 ssid = >These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N01-1023.txt | Citing Article:  P10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sarkar (2001) applied co-training to LTAG parsing, in which the super tagger and parser provide the two views.</S> | Reference Offset:  ['128','149'] | Reference Text:  <S sid = 128 ssid = >Their definition of Co-Training includes the notion (exploited in this paper) that different models can constrain each other by exploiting different ‘views’ of the data.</S><S sid = 149 ssid = >These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N01-1023.txt | Citing Article:  P02-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001).</S> | Reference Offset:  ['10','49'] | Reference Text:  <S sid = 10 ssid = >We use a CoTraining method (Yarowsky, 1995; Blum and Mitchell, 1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation (Yarowsky, 1995), document classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999) and apply this method to the more complex domain of statistical parsing.</S><S sid = 49 ssid = >Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and namedentity identification (Collins and Singer, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N01-1023.txt | Citing Article:  D12-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text.</S> | Reference Offset:  ['0','114'] | Reference Text:  <S sid = 0 ssid = >Applying Co-Training Methods To Statistical Parsing</S><S sid = 114 ssid = >The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTagger (Srinivas, 1997) which is a statistical model for tagging sentences with elementary lexicalized structures.</S> | Discourse Facet:  NA | Annotator: Automatic


