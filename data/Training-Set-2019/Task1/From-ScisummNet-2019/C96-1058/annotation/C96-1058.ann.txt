Citance Number: 1 | Reference Article:  C96-1058.txt | Citing Article:  P14-2107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Structures and rules for parsing with the (Eisner, 1996) algorithm.</S> | Reference Offset:  ['24','50'] | Reference Text:  <S sid = 24 ssid = >.are not lexicM, and (as far ~ as we know) lacl( a parsing algorithm of efficiency compara.ble to link grammars."</S><S sid = 50 ssid = >We SiAl)- ulate that the model discards flom the popula+tion tiny illegal structures that it generates; they do not appear in either training or test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C96-1058.txt | Citing Article:  P14-2107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In that work, as here, inference is simply the Eisner first-order parsing model (Eisner, 1996) shown in Figure 2. In order to score higher-order features, each chart item maintains a list of signatures, which represent subtrees consistent with the chart item.</S> | Reference Offset:  ['64','156'] | Reference Text:  <S sid = 64 ssid = >(This change (;an be rellected in the conceptual model, by stating that tire l,ij decisions are Hla(le ill increasing order of link length l i - - J l  and are no longer indepen(lent.)</S><S sid = 156 ssid = >When spans a and b are combined and one more link is added, it is easy to compute the resulting spans score: score(a), score(b)./?r(covering l ink)) When a span constitutes a parse of the whole input sentence, its score as just computed proves to be the parse probability, conditional on the tree root EOS, under model C. The highest-probability parse can therefore be built by dynamic program- ming, where we build and retain the highest- scoring span of each signature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C96-1058.txt | Citing Article:  P01-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >context-free rules Charniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4.</S> | Reference Offset:  ['7','92'] | Reference Text:  <S sid = 7 ssid = >Other researchers, not wishing to abandon context-flee grammar (CI"G) but disillusioned with its lexica] blind spot, have tried to re-parameterize stochas- tic CI"G in context-sensitive ways (Black et al., 1992) or have augmented the formalism with lex- ical headwords (Magerman, 1995; Collins, 11996).</S><S sid = 92 ssid = >This is a sort of lexicalized context-free model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C96-1058.txt | Citing Article:  P09-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Viterbi decoding is done using Eisner's algorithm (Eisner, 1996).</S> | Reference Offset:  ['1','230'] | Reference Text:  <S sid = 1 ssid = >200 S. 33rd  St. ,  Ph i lade lph ia ,  PA  19104-6"{89, USA j eisner@linc, cis.</S><S sid = 230 ssid = >St~ttisti(:~d decision-tree models for p~using, in Proceedings of the 33rd An- nual Meeting of the A CL, l~oston, MA.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C96-1058.txt | Citing Article:  D07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >1 of these edges, using an O (n3) dynamic programming algorithm (Eisner, 1996) for projective trees.</S> | Reference Offset:  ['207','230'] | Reference Text:  <S sid = 207 ssid = >Towards history- ba,sed gramnl~u:s: using richer mod(,.ls [br probabilis- tic i,~trsing.</S><S sid = 230 ssid = >St~ttisti(:~d decision-tree models for p~using, in Proceedings of the 33rd An- nual Meeting of the A CL, l~oston, MA.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C96-1058.txt | Citing Article:  D07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996).</S> | Reference Offset:  ['23','115'] | Reference Text:  <S sid = 23 ssid = >In principle, one couht model the distribution of dependency l)arses l()ur novel parsing algorithm a/so rescues depen dency from certain criticisins: "l)ependency granl- mars .</S><S sid = 115 ssid = >Let us consider dependency parsing in t;his f ramework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C96-1058.txt | Citing Article:  D09-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In a slightly more general formulation, it was first published by Eisner (1996).</S> | Reference Offset:  ['60','161'] | Reference Text:  <S sid = 60 ssid = >File sohttion is to nlodi/y (t) slightly, further conditioning l,lj on the number and/or  type of children of i that already sit between i and j.</S><S sid = 161 ssid = >Also, mat- ters are complicated slightly by the probabilities asso- ciated with the generation of STOP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C96-1058.txt | Citing Article:  P09-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For MST Parser, we use 1st order features and a projective decoder (Eisner, 1996).</S> | Reference Offset:  ['111','151'] | Reference Text:  <S sid = 111 ssid = >wit,h other analyses; if so, the parser disca,rds all but, the higlmsl,-scoring one.</S><S sid = 151 ssid = >4 Bottom-Up Probabilities Is this one parser really compatible with all three probability models?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C96-1058.txt | Citing Article:  W08-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels.</S> | Reference Offset:  ['9','181'] | Reference Text:  <S sid = 9 ssid = >The choice o t a simple syntactic structure is deliberate: we would like to ask some basic questions about where hx- ical relationships al)pear and how best, to exploit *This materia.l is based upon work supported un- der a National Science I%undation Graduate Fellow- ship, and has benefited greatly from discussions with Mike Collins, Dan M(:lame(l, Mitch Marcus and Ad- wait Ratnaparkhi.</S><S sid = 181 ssid = >= 2 for the prelim- inary experiment, rather than n = 3), but did not assign any links.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C96-1058.txt | Citing Article:  N10-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996).</S> | Reference Offset:  ['90','96'] | Reference Text:  <S sid = 90 ssid = >Each time a word i is added, it generates a Markov sequence of (tag,word) pairs to serve, as its left children, and an separate sequence of (tag,word) pairs as its right children.</S><S sid = 96 ssid = >(st) has one pa,rcnt, lcss cndwor(I; its sul)sl)+tn (b) lists two.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C96-1058.txt | Citing Article:  D11-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisner (1996) algorithm with non-projective rewriting and second order features.</S> | Reference Offset:  ['38','167'] | Reference Text:  <S sid = 38 ssid = >Second, a word is choseu conditional on each tag.</S><S sid = 167 ssid = >As usual, scores can be constructed from the bottom up (though tword(j) in the second factor of (7) is not available to the algorithm, j being outside the span, so we back off to word(j)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C96-1058.txt | Citing Article:  D11-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels.</S> | Reference Offset:  ['22','198'] | Reference Text:  <S sid = 22 ssid = >2 Probabilistic Dependencies It cannot be emphasized too strongly that a gram- marital rcprcsentalion (de4)endency parses, tag se- quen(-es, phrase-structure trees) does not entail any particular probability model.</S><S sid = 198 ssid = >6 Conc lus ions I~arc-bories dependency grammar which requires 1lO Ihik labels> no ~ral f l i i ia i  ,  and ItO fll~S tO lirlderstand iS a clean tcstbcd for studying the lexical a[liniLies of words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C96-1058.txt | Citing Article:  D07-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the Chu Liu-Edmonds (CLE) algorithm (McDonald et al, 2005b).</S> | Reference Offset:  ['167','230'] | Reference Text:  <S sid = 167 ssid = >As usual, scores can be constructed from the bottom up (though tword(j) in the second factor of (7) is not available to the algorithm, j being outside the span, so we back off to word(j)).</S><S sid = 230 ssid = >St~ttisti(:~d decision-tree models for p~using, in Proceedings of the 33rd An- nual Meeting of the A CL, l~oston, MA.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O (n3) time.</S> | Reference Offset:  ['24','169'] | Reference Text:  <S sid = 24 ssid = >.are not lexicM, and (as far ~ as we know) lacl( a parsing algorithm of efficiency compara.ble to link grammars."</S><S sid = 169 ssid = >Full results on a moderate corpus of 25,000+ tagged, dependency-annotated Wall Street Journal sentences, discussed in (Eis- ner, 1996), were not complete hi; press time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005).</S> | Reference Offset:  ['30','206'] | Reference Text:  <S sid = 30 ssid = >This is tire philosophy behind stochastic CF(I  (aelinek et a1.1992), "history-based" phrase-structure parsing (I-~lack et al., 1992), +m(I others.</S><S sid = 206 ssid = >(:k, Fred ,lelinck, et a.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech.</S> | Reference Offset:  ['24','115'] | Reference Text:  <S sid = 24 ssid = >.are not lexicM, and (as far ~ as we know) lacl( a parsing algorithm of efficiency compara.ble to link grammars."</S><S sid = 115 ssid = >Let us consider dependency parsing in t;his f ramework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996).</S> | Reference Offset:  ['23','115'] | Reference Text:  <S sid = 23 ssid = >In principle, one couht model the distribution of dependency l)arses l()ur novel parsing algorithm a/so rescues depen dency from certain criticisins: "l)ependency granl- mars .</S><S sid = 115 ssid = >Let us consider dependency parsing in t;his f ramework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C96-1058.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The (Eisner, 1996) algorithm is typically used for projective parsing.</S> | Reference Offset:  ['19','24'] | Reference Text:  <S sid = 19 ssid = >We il]ustrate how each hypothesis is (:xl)ressed in a depemteney framework, and how each can be used to guide our parser toward its favored so- lution.</S><S sid = 24 ssid = >.are not lexicM, and (as far ~ as we know) lacl( a parsing algorithm of efficiency compara.ble to link grammars."</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C96-1058.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference.</S> | Reference Offset:  ['177','180'] | Reference Text:  <S sid = 177 ssid = >In addition to models A, B, and C, described above, the pilot experiment evaluated two other models for comparison.</S><S sid = 180 ssid = >Model X did the same n-gram tagging as models A and B (~.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C96-1058.txt | Citing Article:  W09-1212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing.</S> | Reference Offset:  ['29','115'] | Reference Text:  <S sid = 29 ssid = >as on training data.</S><S sid = 115 ssid = >Let us consider dependency parsing in t;his f ramework.</S> | Discourse Facet:  NA | Annotator: Automatic


