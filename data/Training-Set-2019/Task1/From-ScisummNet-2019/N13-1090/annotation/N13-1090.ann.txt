Citance Number: 1 | Reference Article:  N13-1090.txt | Citing Article:  P13-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These vector representations capture interesting linear relationships (up to some accuracy), such as king-man+woman=queen (Mikolov et al, 2013).</S> | Reference Offset:  ['5','22'] | Reference Text:  <S sid = 5 ssid = >For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.</S><S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N13-1090.txt | Citing Article:  P14-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships.</S> | Reference Offset:  ['8','22'] | Reference Text:  <S sid = 8 ssid = >A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.</S><S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N13-1090.txt | Citing Article:  P14-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013).</S> | Reference Offset:  ['9','22'] | Reference Text:  <S sid = 9 ssid = >In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.</S><S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N13-1090.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al (2013a) specifically to test predict models.</S> | Reference Offset:  ['22','70'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 70 ssid = >To compare to previous systems, we report the average over all 69 relations in the test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N13-1090.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large).</S> | Reference Offset:  ['22','24'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N13-1090.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model.</S> | Reference Offset:  ['24','66'] | Reference Text:  <S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S><S sid = 66 ssid = >We conducted similar experiments with the semantic test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N13-1090.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words.</S> | Reference Offset:  ['9','58'] | Reference Text:  <S sid = 9 ssid = >In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.</S><S sid = 58 ssid = >The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N13-1090.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['78','79'] | Reference Text:  <S sid = 78 ssid = >We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.</S><S sid = 79 ssid = >Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N13-1090.txt | Citing Article:  P14-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b).</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Linguistic Regularities in Continuous Space Word Representations</S><S sid = 15 ssid = >Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N13-1090.txt | Citing Article:  P14-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings.</S> | Reference Offset:  ['10','22'] | Reference Text:  <S sid = 10 ssid = >As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.</S><S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N13-1090.txt | Citing Article:  P14-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >Linguistic Regularities in Continuous Space Word Representations</S><S sid = 3 ssid = >We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N13-1090.txt | Citing Article:  P14-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10).</S> | Reference Offset:  ['22','24'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N13-1090.txt | Citing Article:  P14-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Mikolov et al (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king-man+woman=queen on English data.</S> | Reference Offset:  ['5','60'] | Reference Text:  <S sid = 5 ssid = >For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.</S><S sid = 60 ssid = >LSA was trained on the same data as the RNN.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N13-1090.txt | Citing Article:  P14-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech.</S> | Reference Offset:  ['9','24'] | Reference Text:  <S sid = 9 ssid = >In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.</S><S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N13-1090.txt | Citing Article:  P14-2050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software.</S> | Reference Offset:  ['62','78'] | Reference Text:  <S sid = 62 ssid = >2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.</S><S sid = 78 ssid = >We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N13-1090.txt | Citing Article:  P14-2050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al, 2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b).</S> | Reference Offset:  ['22','24'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N13-1090.txt | Citing Article:  P14-2050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown).</S> | Reference Offset:  ['22','24'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N13-1090.txt | Citing Article:  P14-2050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014).</S> | Reference Offset:  ['22','48'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 48 ssid = >As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N13-1090.txt | Citing Article:  P14-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, their text-based vectors encode very rich information, such as king-man+woman=queen (Mikolov et al, 2013c).</S> | Reference Offset:  ['5','22'] | Reference Text:  <S sid = 5 ssid = >For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.</S><S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N13-1090.txt | Citing Article:  P14-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically.</S> | Reference Offset:  ['22','24'] | Reference Text:  <S sid = 22 ssid = >These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S><S sid = 24 ssid = >This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S> | Discourse Facet:  NA | Annotator: Automatic


