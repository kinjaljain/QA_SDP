Citance Number: 1 | Reference Article:  P06-1103.txt | Citing Article:  D07-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora.</S> | Reference Offset:  ['97','174'] | Reference Text:  <S sid = 97 ssid = >We use the perceptron (Rosenblatt, 1958) algorithm to train the model.</S><S sid = 174 ssid = >The algorithm can be naturally extended to comparable corpora of more than two languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1103.txt | Citing Article:  D08-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','179'] | Reference Text:  <S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S><S sid = 179 ssid = >This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1103.txt | Citing Article:  D08-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier.</S> | Reference Offset:  ['63','66'] | Reference Text:  <S sid = 63 ssid = >For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and EnglishKorean generative transliteration models, respectively.</S><S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1103.txt | Citing Article:  D08-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','179'] | Reference Text:  <S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S><S sid = 179 ssid = >This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1103.txt | Citing Article:  D08-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages.</S> | Reference Offset:  ['69','168'] | Reference Text:  <S sid = 69 ssid = >We take advantage of dynamically growing feature space to reduce the number of supervised training examples.</S><S sid = 168 ssid = >We developed a linear discriminative transliteration model, and presented a method to automatically generate features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1103.txt | Citing Article:  D10-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC.</S> | Reference Offset:  ['68','109'] | Reference Text:  <S sid = 68 ssid = >We extend our preliminary work in (Klementiev and Roth, 2006) to discover multi-word Named Entities and to take advantage of a dictionary (if one exists) to handle NEs which are partially or entirely translated.</S><S sid = 109 ssid = >The English side was tagged with a publicly available NER system based on the SNoW learning architecture (Roth, 1998), that is available on the same site.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1103.txt | Citing Article:  D10-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','179'] | Reference Text:  <S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S><S sid = 179 ssid = >This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1103.txt | Citing Article:  N09-3007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The iterative training algorithm described above is adopted from Klementiev and Roth (2006).</S> | Reference Offset:  ['100','102'] | Reference Text:  <S sid = 100 ssid = >As the iterative algorithm observes more data, it discovers and makes use of more features.</S><S sid = 102 ssid = >Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1103.txt | Citing Article:  P08-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method.</S> | Reference Offset:  ['39','66'] | Reference Text:  <S sid = 39 ssid = >We expect temporal sequence alignment to resolve many of such ambiguities.</S><S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1103.txt | Citing Article:  P08-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages.</S> | Reference Offset:  ['69','94'] | Reference Text:  <S sid = 69 ssid = >We take advantage of dynamically growing feature space to reduce the number of supervised training examples.</S><S sid = 94 ssid = >We build a feature vector from this example in the following manner: First, we split both words into all possible substrings of up to size two: We build a feature vector by coupling substrings from the two sets: We use the observation that transliteration tends to preserve phonetic sequence to limit the number of couplings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1103.txt | Citing Article:  P08-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006).</S> | Reference Offset:  ['68','142'] | Reference Text:  <S sid = 68 ssid = >We extend our preliminary work in (Klementiev and Roth, 2006) to discover multi-word Named Entities and to take advantage of a dictionary (if one exists) to handle NEs which are partially or entirely translated.</S><S sid = 142 ssid = >Once the transliteration model was trained, we ran the algorithm to discover multi-word NEs, augmenting candidate sets of dictionary words with their translations as described in Section 3.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information.</S> | Reference Offset:  ['8','68'] | Reference Text:  <S sid = 8 ssid = >Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs.</S><S sid = 68 ssid = >We extend our preliminary work in (Klementiev and Roth, 2006) to discover multi-word Named Entities and to take advantage of a dictionary (if one exists) to handle NEs which are partially or entirely translated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information.</S> | Reference Offset:  ['66','98'] | Reference Text:  <S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S><S sid = 98 ssid = >The model activation provides the score we use to select best transliterations on line 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles.</S> | Reference Offset:  ['33','104'] | Reference Text:  <S sid = 33 ssid = >Time sequence scoring is then used to rerank the list and choose the candidate best temporally aligned with the NE.</S><S sid = 104 ssid = >We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['66','179'] | Reference Text:  <S sid = 66 ssid = >The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).</S><S sid = 179 ssid = >This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information.</S> | Reference Offset:  ['0','70'] | Reference Text:  <S sid = 0 ssid = >Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora</S><S sid = 70 ssid = >In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a transliteration model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates.</S> | Reference Offset:  ['30','70'] | Reference Text:  <S sid = 30 ssid = >We score NEs similarity with a linear transliteration model.</S><S sid = 70 ssid = >In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a transliteration model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1103.txt | Citing Article:  N09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This configuration is equivalent to the model used in (Klementiev and Roth, 2006b).</S> | Reference Offset:  ['101','119'] | Reference Text:  <S sid = 101 ssid = >This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version of SNoW (Roth, 1998).</S><S sid = 119 ssid = >In each iteration, we used the current transliteration model to find a list of 30 best transliteration equivalence classes for each NE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1103.txt | Citing Article:  P08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006).</S> | Reference Offset:  ['56','60'] | Reference Text:  <S sid = 56 ssid = >Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora.</S><S sid = 60 ssid = >(Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1103.txt | Citing Article:  E09-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity.</S> | Reference Offset:  ['59','79'] | Reference Text:  <S sid = 59 ssid = >(Hetland, 2004) surveys recent methods for scoring time sequences for similarity.</S><S sid = 79 ssid = >We use a method called the F-index (Hetland, 2004) to implement the similarity function on line 8 of the algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


