Citance Number: 1 | Reference Article:  W10-2805.txt | Citing Article:  D11-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings.</S> | Reference Offset:  ['0','17'] | Reference Text:  <S sid = 0 ssid = >A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S><S sid = 17 ssid = >In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W10-2805.txt | Citing Article:  W11-0136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose.</S> | Reference Offset:  ['96','110'] | Reference Text:  <S sid = 96 ssid = >This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.</S><S sid = 110 ssid = >Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W10-2805.txt | Citing Article:  D12-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors.</S> | Reference Offset:  ['2','43'] | Reference Text:  <S sid = 2 ssid = >In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.</S><S sid = 43 ssid = >By simply combining the vector representations of the independent Adjectives and Nouns in our data-set (v1 and v2) we built an additive prediction model (v1 + v2) and a simplified pointwise multiplicative prediction model (v1 x v2) for each candidate pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W10-2805.txt | Citing Article:  D12-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives.</S> | Reference Offset:  ['39','50'] | Reference Text:  <S sid = 39 ssid = >Thus, the resulting space consists in a matrix with 40, 000 x 500 dimensions.</S><S sid = 50 ssid = >We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W10-2805.txt | Citing Article:  D12-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300.</S> | Reference Offset:  ['28','42'] | Reference Text:  <S sid = 28 ssid = >Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.</S><S sid = 42 ssid = >In particular, we produced our regression analysis with the pls package (Mevik & Wehrens, 2007), which implements PLSR and a number of very useful functions for cross-validation, prediction, error analysis, etc.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W10-2805.txt | Citing Article:  W11-2506.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators.</S> | Reference Offset:  ['0','107'] | Reference Text:  <S sid = 0 ssid = >A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S><S sid = 107 ssid = >Modelling compositionality as a machine learning task implies that a great number of different “types” of composition (functions combining vectors) may be learned from natural language samples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W10-2805.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010).</S> | Reference Offset:  ['3','22'] | Reference Text:  <S sid = 3 ssid = >We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model.</S><S sid = 22 ssid = >Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W10-2805.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model.</S> | Reference Offset:  ['0','66'] | Reference Text:  <S sid = 0 ssid = >A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S><S sid = 66 ssid = >Since the main use of DSMs is to extract similar vectors from a multidimensional space (representing related documents, distributional synonyms, etc.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W10-2805.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors.</S> | Reference Offset:  ['0','51'] | Reference Text:  <S sid = 0 ssid = >A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S><S sid = 51 ssid = >We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W10-2805.txt | Citing Article:  P13-2083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed.</S> | Reference Offset:  ['18','96'] | Reference Text:  <S sid = 18 ssid = >We extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and, finally, a multinomial multiple regression model by Partial Least Squares Regression (PLSR).</S><S sid = 96 ssid = >This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W10-2805.txt | Citing Article:  P14-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model.</S> | Reference Offset:  ['22','100'] | Reference Text:  <S sid = 22 ssid = >Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).</S><S sid = 100 ssid = >The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W10-2805.txt | Citing Article:  W11-0115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies.</S> | Reference Offset:  ['17','102'] | Reference Text:  <S sid = 17 ssid = >In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.</S><S sid = 102 ssid = >This is work in progress, but the results look very promising.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W10-2805.txt | Citing Article:  W11-0115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector.</S> | Reference Offset:  ['25','106'] | Reference Text:  <S sid = 25 ssid = >We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.</S><S sid = 106 ssid = >A last word on the view of semantic compositionality suggested by our approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W10-2805.txt | Citing Article:  W11-0115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression.</S> | Reference Offset:  ['46','100'] | Reference Text:  <S sid = 46 ssid = >The model’s parameters were estimated by performing 10-fold cross-validation during the training phase.</S><S sid = 100 ssid = >The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W10-2805.txt | Citing Article:  W11-0115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Let us start by setting the syntactic relation that we want to focus on for the purposes of this study: following Guevara (2010) and Baroni and Zamparelli (2010), I model the semantic composition of adjacent Adjective-Noun pairs expressing attributive modification of a nominal head.</S> | Reference Offset:  ['15','108'] | Reference Text:  <S sid = 15 ssid = >On the contrary, since the the very nature of compositionality depends on the semantic relation being instantiated in a syntactic structure, we propose that the composition of vector representations must be modelled as a relation-specific phenomenon.</S><S sid = 108 ssid = >In principle, any semantic relation instantiated by any syntactic structure could be learned if sufficient data is provided.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W10-2805.txt | Citing Article:  P13-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Guevara (2010) and Mitchell and Lapata (2010).</S> | Reference Offset:  ['13','22'] | Reference Text:  <S sid = 13 ssid = >Widdows (2008), Mitchell & Lapata (2008), Giesbrecht (2009), Baroni & Lenci (2009), who propose various DSM approaches to represent argument structure, subject-verb and verb-object co-selection.</S><S sid = 22 ssid = >Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W10-2805.txt | Citing Article:  P13-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)).</S> | Reference Offset:  ['89','100'] | Reference Text:  <S sid = 89 ssid = >The additive model has the most varied set of neighbours, but the majority of them are additive-neighbours.</S><S sid = 100 ssid = >The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W10-2805.txt | Citing Article:  P13-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices: c= Au+Bv Since the Mitchell and Lapata and full add models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be com posed into the phrase of interest.</S> | Reference Offset:  ['20','50'] | Reference Text:  <S sid = 20 ssid = >In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: In the DSM literature, the additive model has become a de facto standard approach to approximate the composed meaning of a group of words (or a document) as the sum of their vectors (which results in the centroid of the starting vectors).</S><S sid = 50 ssid = >We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W10-2805.txt | Citing Article:  E12-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.</S> | Reference Offset:  ['51','110'] | Reference Text:  <S sid = 51 ssid = >We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.</S><S sid = 110 ssid = >Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W10-2805.txt | Citing Article:  P14-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: f comp R (~u, ~v)= W 1 ~u+W 2 ~v. We will further use the following equivalent formulation: f comp R (~u, ~v)= W R [~u; ~v] where W R? R d? 2dand [~u; ~v] is the vertical concatenation of the two vectors (using Matlab notation).</S> | Reference Offset:  ['14','96'] | Reference Text:  <S sid = 14 ssid = >Current approaches to compositionality in DSMs are based on the application of a simple geometric operation on the basis of individual vectors (vector addition, pointwisemultiplication of corresponding dimensions, tensor product) which should in principle approximate the composition of any two given vectors.</S><S sid = 96 ssid = >This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


