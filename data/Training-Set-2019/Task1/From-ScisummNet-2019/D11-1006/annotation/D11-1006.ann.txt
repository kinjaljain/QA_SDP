Citance Number: 1 | Reference Article:  D11-1006.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011).</S> | Reference Offset:  ['17','18'] | Reference Text:  <S sid = 17 ssid = >Most notably, it increases our understanding of how computers (and possibly humans) learn in the absence of any explicit feedback.</S><S sid = 18 ssid = >However, the gold POS tag assumption weakens any conclusions that can be drawn, as part-of-speech are also a form of syntactic analysis, only shallower.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D11-1006.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors.</S> | Reference Offset:  ['79','92'] | Reference Text:  <S sid = 79 ssid = >The key observation is that part-of-speech tags contain a significant amount of information for unlabeled dependency parsing.</S><S sid = 92 ssid = >First, part-of-speech tags contain a significant amount of information for parsing unlabeled dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D11-1006.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor.</S> | Reference Offset:  ['16','57'] | Reference Text:  <S sid = 16 ssid = >The study of unsupervised grammar induction has many merits.</S><S sid = 57 ssid = >Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D11-1006.txt | Citing Article:  P14-2095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011).</S> | Reference Offset:  ['28','165'] | Reference Text:  <S sid = 28 ssid = >The model is trained to update towards parses that are in high agreement with a source side English parse based on constraints drawn from alignments in the parallel data.</S><S sid = 165 ssid = >For these experiments we still only use English-target parallel data because that is the format of the readily available data in the Europarl corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D11-1006.txt | Citing Article:  P14-2095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer.</S> | Reference Offset:  ['98','164'] | Reference Text:  <S sid = 98 ssid = >In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al.</S><S sid = 164 ssid = >For the multi-source projected parser, the procedure is identical to that in Section 3.2 except that we use the multi-source direct transfer model to seed the algorithm instead of the English-only direct transfer model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D11-1006.txt | Citing Article:  P14-2095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011).</S> | Reference Offset:  ['169','176'] | Reference Text:  <S sid = 169 ssid = >The third (multi-dir.) is the multi-source direct transfer system.</S><S sid = 176 ssid = >For the multi-source projected system the results are mixed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D11-1006.txt | Citing Article:  D12-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do.</S> | Reference Offset:  ['9','223'] | Reference Text:  <S sid = 9 ssid = >Subsequently, researchers have begun to look at both porting these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007).</S><S sid = 223 ssid = >As previously mentioned, the latter has been explored in both Søgaard (2011) and Cohen et al. (2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011).</S> | Reference Offset:  ['84','223'] | Reference Text:  <S sid = 84 ssid = >In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language.</S><S sid = 223 ssid = >As previously mentioned, the latter has been explored in both Søgaard (2011) and Cohen et al. (2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011).</S> | Reference Offset:  ['24','78'] | Reference Text:  <S sid = 24 ssid = >We empirically show that directly transferring delexicalized models (i.e. parsing a foreign language POS sequence with an English parser) already outperforms state-of-the-art unsupervised parsers by a significant margin.</S><S sid = 78 ssid = >For example, when training and testing a parser on our English data, a parser with all features obtains an UAS of 89.3%3 whereas a delexicalized parser – a parser that only has nonlexical features – obtains an UAS of 82.5%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011).</S> | Reference Offset:  ['78','103'] | Reference Text:  <S sid = 78 ssid = >For example, when training and testing a parser on our English data, a parser with all features obtains an UAS of 89.3%3 whereas a delexicalized parser – a parser that only has nonlexical features – obtains an UAS of 82.5%.</S><S sid = 103 ssid = >It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011).</S> | Reference Offset:  ['155','185'] | Reference Text:  <S sid = 155 ssid = >Table 2 shows the matrix of source-target language UAS for all nine languages we consider (the original eight target languages plus English).</S><S sid = 185 ssid = >Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3.</S> | Reference Offset:  ['140','202'] | Reference Text:  <S sid = 140 ssid = >An empirical comparison to Ganchev et al. (2009) is given in Section 5.</S><S sid = 202 ssid = >Ganchev et al. also report results for Bulgarian.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['85','229'] | Reference Text:  <S sid = 85 ssid = >UAS for all sentence lengths without punctuation are given in Table 1.</S><S sid = 229 ssid = >We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank.</S> | Reference Offset:  ['7','88'] | Reference Text:  <S sid = 7 ssid = >This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).</S><S sid = 88 ssid = >We trained on sentences of length 10 or less and evaluated on all sentences from the test set.4 For DMV, we reversed the direction of all dependencies if this led to higher performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D11-1006.txt | Citing Article:  D12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters).</S> | Reference Offset:  ['164','169'] | Reference Text:  <S sid = 164 ssid = >For the multi-source projected parser, the procedure is identical to that in Section 3.2 except that we use the multi-source direct transfer model to seed the algorithm instead of the English-only direct transfer model.</S><S sid = 169 ssid = >The third (multi-dir.) is the multi-source direct transfer system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D11-1006.txt | Citing Article:  D12-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['85','229'] | Reference Text:  <S sid = 85 ssid = >UAS for all sentence lengths without punctuation are given in Table 1.</S><S sid = 229 ssid = >We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D11-1006.txt | Citing Article:  D12-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011).</S> | Reference Offset:  ['60','70'] | Reference Text:  <S sid = 60 ssid = >For this we used the Europarl corpus version 5 (Koehn, 2005).</S><S sid = 70 ssid = >Preliminary experiments using a different dependency parser – MSTParser (McDonald et al., 2005) – resulted in similar empirical observations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D11-1006.txt | Citing Article:  D12-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011).</S> | Reference Offset:  ['29','223'] | Reference Text:  <S sid = 29 ssid = >We use the augmented-loss learning procedure (Hall et al., 2011) which is closely related to constraint driven learning (Chang et al., 2007; Chang et al., 2010).</S><S sid = 223 ssid = >As previously mentioned, the latter has been explored in both Søgaard (2011) and Cohen et al. (2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D11-1006.txt | Citing Article:  D12-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle.</S> | Reference Offset:  ['7','167'] | Reference Text:  <S sid = 7 ssid = >This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).</S><S sid = 167 ssid = >The first (best-source) is the direct transfer results for the oracle single-best source language per target language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D11-1006.txt | Citing Article:  D12-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011).</S> | Reference Offset:  ['98','154'] | Reference Text:  <S sid = 98 ssid = >In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al.</S><S sid = 154 ssid = >In this section we examine whether this is also true for parser transfer.</S> | Discourse Facet:  NA | Annotator: Automatic


