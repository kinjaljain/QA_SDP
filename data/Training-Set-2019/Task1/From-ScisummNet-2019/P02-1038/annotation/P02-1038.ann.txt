Citance Number: 1 | Reference Article:  P02-1038.txt | Citing Article:  P03-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002).</S> | Reference Offset:  ['31','111'] | Reference Text:  <S sid = 31 ssid = >Optimizing the corresponding parameters A1 and A2 of the model in Eq.</S><S sid = 111 ssid = >In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1038.txt | Citing Article:  P14-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model.</S> | Reference Offset:  ['37','109'] | Reference Text:  <S sid = 37 ssid = >We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model.</S><S sid = 109 ssid = >In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1038.txt | Citing Article:  H05-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An alternate way to optimize weights over translation features is described in Och and Ney (2002).</S> | Reference Offset:  ['60','75'] | Reference Text:  <S sid = 60 ssid = >For example, using a function k(·) that counts how many verb groups exist in the source or the target sentence, we can define the following feature, which is 1 if each of the two sentences contains the same number of verb groups: In the same way, we can introduce semantic features or pragmatic features such as the dialogue act classification.</S><S sid = 75 ssid = >Yet, the criterion as it is described in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1038.txt | Citing Article:  D11-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002).</S> | Reference Offset:  ['19','43'] | Reference Text:  <S sid = 19 ssid = >5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach.</S><S sid = 43 ssid = >As specific MT method, we use the alignment template approach (Och et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1038.txt | Citing Article:  D11-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002).</S> | Reference Offset:  ['64','111'] | Reference Text:  <S sid = 64 ssid = >This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999).</S><S sid = 111 ssid = >In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1038.txt | Citing Article:  P11-2074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002).</S> | Reference Offset:  ['75','118'] | Reference Text:  <S sid = 75 ssid = >Yet, the criterion as it is described in Eq.</S><S sid = 118 ssid = >2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1038.txt | Citing Article:  H05-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations.</S> | Reference Offset:  ['37','118'] | Reference Text:  <S sid = 37 ssid = >We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model.</S><S sid = 118 ssid = >2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1038.txt | Citing Article:  P07-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002).</S> | Reference Offset:  ['25','43'] | Reference Text:  <S sid = 25 ssid = >In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions: This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task.</S><S sid = 43 ssid = >As specific MT method, we use the alignment template approach (Och et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1038.txt | Citing Article:  D09-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002).</S> | Reference Offset:  ['19','107'] | Reference Text:  <S sid = 19 ssid = >5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach.</S><S sid = 107 ssid = >The use of direct maximum entropy translation models for statistical machine translation has been suggested by (Papineni et al., 1997; Papineni et al., 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1038.txt | Citing Article:  P06-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['45','123'] | Reference Text:  <S sid = 45 ssid = >The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered.</S><S sid = 123 ssid = >In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1038.txt | Citing Article:  P06-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).</S> | Reference Offset:  ['19','54'] | Reference Text:  <S sid = 19 ssid = >5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach.</S><S sid = 54 ssid = >To use these three component models in a direct maximum entropy approach, we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p(fJ1 |eI1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1038.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set.</S> | Reference Offset:  ['90','101'] | Reference Text:  <S sid = 90 ssid = >• IER (information item error rate): The test sentences are segmented into information items.</S><S sid = 101 ssid = >Figure 3 show how the sentence error rate (SER) on the test corpus improves during the iterations of the GIS algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1038.txt | Citing Article:  P06-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training And Maximum Entropy Models For Statistical Machine Translation</S><S sid = 1 ssid = >We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1038.txt | Citing Article:  W05-0834.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists.</S> | Reference Offset:  ['75','106'] | Reference Text:  <S sid = 75 ssid = >Yet, the criterion as it is described in Eq.</S><S sid = 106 ssid = >We see that adding new features also has an effect on the other model scaling factors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1038.txt | Citing Article:  E06-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002).</S> | Reference Offset:  ['0','110'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training And Maximum Entropy Models For Statistical Machine Translation</S><S sid = 110 ssid = >Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1038.txt | Citing Article:  W07-0725.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002).</S> | Reference Offset:  ['11','25'] | Reference Text:  <S sid = 11 ssid = >Typically, Eq.</S><S sid = 25 ssid = >In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions: This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1038.txt | Citing Article:  W11-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002).</S> | Reference Offset:  ['99','100'] | Reference Text:  <S sid = 99 ssid = >The following three rows show the results if we add the word penalty, an additional class-based five-gram GIS algorithm for maximum entropy training of alignment templates. language model and the conventional dictionary features.</S><S sid = 100 ssid = >We observe improved error rates for using the word penalty and the class-based language model as additional features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1038.txt | Citing Article:  D07-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['45','123'] | Reference Text:  <S sid = 45 ssid = >The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered.</S><S sid = 123 ssid = >In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1038.txt | Citing Article:  C10-1124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002).</S> | Reference Offset:  ['19','110'] | Reference Text:  <S sid = 19 ssid = >5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach.</S><S sid = 110 ssid = >Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1038.txt | Citing Article:  W06-2601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002).</S> | Reference Offset:  ['0','110'] | Reference Text:  <S sid = 0 ssid = >Discriminative Training And Maximum Entropy Models For Statistical Machine Translation</S><S sid = 110 ssid = >Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


