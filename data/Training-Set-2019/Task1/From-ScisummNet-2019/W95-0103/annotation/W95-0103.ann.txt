Citance Number: 1 | Reference Article:  W95-0103.txt | Citing Article:  W97-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Brill and Resnik (1994) applied Error-Driven Transformation Based Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model.</S> | Reference Offset:  ['7','56'] | Reference Text:  <S sid = 7 ssid = >(In this case the VP attachment is correct): NP-attach: (joined ((the board) (as a nonexecutive director))) VP-attach: ((joined (the board)) (as a nonexecutive director)) Work by Ratnaparkhi, Reynar and Roukos [RRR94] and Brill and Resnik [BR94] has considered corpus-based approaches to this problem, using a set of examples to train a model which is then used to make attachment decisions on test data.</S><S sid = 56 ssid = >They use a maximum entropy model which also considers subsets of the quadruple.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W95-0103.txt | Citing Article:  W97-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As we have argued in Zavrel and Daelemans (1997), this corresponds exactly to the behavior of the Back-Off algorithm of Collins and Brooks (1995), so that it comes as no surprise that the accuracy of both methods is the same.</S> | Reference Offset:  ['90','110'] | Reference Text:  <S sid = 90 ssid = >For this reason the two methods deserve closer comparison.</S><S sid = 110 ssid = >The accuracy figure is then the percentage accuracy on the test cases where the (v, nl, n2) counts were used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W95-0103.txt | Citing Article:  W97-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995).</S> | Reference Offset:  ['54','87'] | Reference Text:  <S sid = 54 ssid = >'Personal communication from Brill.</S><S sid = 87 ssid = >All results are for the IBM data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W95-0103.txt | Citing Article:  W97-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Brooks (1995) used a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results).</S> | Reference Offset:  ['61','105'] | Reference Text:  <S sid = 61 ssid = >Crucially they ignore low-count events in training data by imposing a frequency cut-off somewhere between 3 and 5.</S><S sid = 105 ssid = >The results were as follows: We have excluded tuples which do not contain a preposition from the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W95-0103.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The perhaps underwhelming human performance is partially due to misclassifications by the Treebank assemblers who made these determinations by hand, and also unclear cases, which we discuss in the next section. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model.</S> | Reference Offset:  ['0','24'] | Reference Text:  <S sid = 0 ssid = >Prepositional Phrase Attachment Through A Backed-Off Model</S><S sid = 24 ssid = >All results in this section are on the IBM training and test data, with the exception of the two 'average human' results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W95-0103.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Abney, Schapire, and Singer (1999) used the dataset from Collins and Brooks (1995) with a boosting algorithm and achieved 85.4% accuracy. Their algorithm also was able to order the specific data points by how much weight they were assigned by the learning algorithm.</S> | Reference Offset:  ['19','41'] | Reference Text:  <S sid = 19 ssid = >The accuracy of the algorithm is then the percentage of attachments it gets 'correct' on test data, using the A values taken from the treebank as the reference set.</S><S sid = 41 ssid = >The attachment decisions for these triples were unknown, so an unsupervised training method was used (section 5.2 describes the algorithm in more detail).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W95-0103.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995).</S> | Reference Offset:  ['46','84'] | Reference Text:  <S sid = 46 ssid = >[BR94] use 12,000 training and 500 test examples.</S><S sid = 84 ssid = >In an effort to reduce sparse data problems the following processing was run over both test and training data: These modifications are similar to those performed on the corpus used by [BR94].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W95-0103.txt | Citing Article:  P14-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet.</S> | Reference Offset:  ['84','105'] | Reference Text:  <S sid = 84 ssid = >In an effort to reduce sparse data problems the following processing was run over both test and training data: These modifications are similar to those performed on the corpus used by [BR94].</S><S sid = 105 ssid = >The results were as follows: We have excluded tuples which do not contain a preposition from the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W95-0103.txt | Citing Article:  H05-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Brooks (1995) used a supervised back-off model to achieve 84.5% precision on the Ratnaparkhi test set.</S> | Reference Offset:  ['17','111'] | Reference Text:  <S sid = 17 ssid = >This set was used during development of the attachment algorithm, ensuring that there was no implicit training of the method on the test set itself.</S><S sid = 111 ssid = >The development set with no morphological processing was used for these tests.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W95-0103.txt | Citing Article:  P06-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995).</S> | Reference Offset:  ['26','28'] | Reference Text:  <S sid = 26 ssid = >'Most likely for each preposition' means use the attachment seen most often in training data for the preposition seen in the test quadruple.</S><S sid = 28 ssid = >A reasonable lower bound seems to be 72.2% as scored by the 'Most likely for each preposition' method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W95-0103.txt | Citing Article:  P06-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins and Brooks, 1995) also present a model with multiple back offs.</S> | Reference Offset:  ['0','105'] | Reference Text:  <S sid = 0 ssid = >Prepositional Phrase Attachment Through A Backed-Off Model</S><S sid = 105 ssid = >The results were as follows: We have excluded tuples which do not contain a preposition from the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W95-0103.txt | Citing Article:  P00-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Later, Collins and Brooks (1995) achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >Prepositional Phrase Attachment Through A Backed-Off Model</S><S sid = 3 ssid = >Results on Wall Street Journal data of 84.5% accuracy are obtained using this method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W95-0103.txt | Citing Article:  P00-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our experiments, we only considered features that contained P since the preposition is the most important lexical item (Collins and Brooks, 1995).</S> | Reference Offset:  ['73','75'] | Reference Text:  <S sid = 73 ssid = >A key observation in choosing between these tuples is that the preposition is particularly important to the attachment decision.</S><S sid = 75 ssid = >Section 6.2 describes experiments which show that tuples containing the preposition are much better indicators of attachment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W95-0103.txt | Citing Article:  P00-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We describe the different classifiers below: cl base: the baseline described in Section 7.2clR1: uses a maximum entropy model (Ratnaparkhi et al, 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) cl CB: uses a backed-off model (Collins and Brooks, 1995 )clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 )clHR6: uses lexical preference (Hindle and Rooth, 1993 )clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) cl Pl: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm.</S> | Reference Offset:  ['93','100'] | Reference Text:  <S sid = 93 ssid = >The decision was made as follows': If we ignore n2 then the IBM data is equivalent to Hindle and Rooth's (v,n1, p) triples. with the advantage of the attachment decision being known, allowing a supervised algorithm.</S><S sid = 100 ssid = >A possible criticism of the backed-off estimate is that it uses low count events without any smoothing, which has been shown to be a mistake in similar problems such as n-gram language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W95-0103.txt | Citing Article:  P00-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The accuracy is reported in (Collins and Brooks, 1995).</S> | Reference Offset:  ['60','110'] | Reference Text:  <S sid = 60 ssid = >Results of 77.7% (words only) and 81.6% (words and classes) are reported.</S><S sid = 110 ssid = >The accuracy figure is then the percentage accuracy on the test cases where the (v, nl, n2) counts were used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W95-0103.txt | Citing Article:  W04-1505.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >p (Rjright; a; b) =# (R; right; a; b)# (right; a; b) (6) e.g. for the Verb-PP attachment relation pobj (following (Collins and Brooks, 1995) including the description noun 7) p (pobjjright; verb; prep ;desc: noun) =# (pobj; right; verb; prep ;desc: noun)# (right; verb; prep ;desc: noun) The distance (measured in chunks) between a head and a dependent is a limiting factor for the probability of a dependency between them.</S> | Reference Offset:  ['12','20'] | Reference Text:  <S sid = 12 ssid = >For each such VP the head verb, first head noun, preposition and second head noun were extracted, along with the attachment decision (1 for noun attachment, 0 for verb).</S><S sid = 20 ssid = >The probability of the attachment variable A being 1 or 0 (signifying noun or verb attachment respectively) is a probability, p, which is conditional on the values of the words in the quadruple.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W95-0103.txt | Citing Article:  W99-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995).</S> | Reference Offset:  ['0','18'] | Reference Text:  <S sid = 0 ssid = >Prepositional Phrase Attachment Through A Backed-Off Model</S><S sid = 18 ssid = >A PP-attachment algorithm must take each quadruple (V = v, Ni = nl, P = p, N2 = n2) in test data and decide whether the attachment variable A =.-- 0 or 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W95-0103.txt | Citing Article:  W99-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the same training and test data as Collins and Brooks (1995).</S> | Reference Offset:  ['10','33'] | Reference Text:  <S sid = 10 ssid = >The training and test data were supplied by IBM, being identical to that used in [RRR94].</S><S sid = 33 ssid = >A quadruple may appear in test data which has never been seen in training data. ie. f(v, nl,p, n2) = 0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W95-0103.txt | Citing Article:  W04-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach can be seen as an extension of (Collins and Brooks, 1995) from PP-attachment to most dependency relations.</S> | Reference Offset:  ['26','101'] | Reference Text:  <S sid = 26 ssid = >'Most likely for each preposition' means use the attachment seen most often in training data for the preposition seen in the test quadruple.</S><S sid = 101 ssid = >In particular, quadruples and triples seen in test data will frequently be seen only once or twice in training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W95-0103.txt | Citing Article:  C02-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994).</S> | Reference Offset:  ['7','54'] | Reference Text:  <S sid = 7 ssid = >(In this case the VP attachment is correct): NP-attach: (joined ((the board) (as a nonexecutive director))) VP-attach: ((joined (the board)) (as a nonexecutive director)) Work by Ratnaparkhi, Reynar and Roukos [RRR94] and Brill and Resnik [BR94] has considered corpus-based approaches to this problem, using a set of examples to train a model which is then used to make attachment decisions on test data.</S><S sid = 54 ssid = >'Personal communication from Brill.</S> | Discourse Facet:  NA | Annotator: Automatic


