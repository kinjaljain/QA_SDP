Citance Number: 1 | Reference Article:  P06-1124.txt | Citing Article:  N12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events.</S> | Reference Offset:  ['20','22'] | Reference Text:  <S sid = 20 ssid = >We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods (Chen and Goodman, 1998).</S><S sid = 22 ssid = >Our interpretation is more useful than past interpretations involving marginal constraints (Kneser and Ney, 1995; Chen and Goodman, 1998) or maximum-entropy models (Goodman, 2004) as it can recover the exact formulation of interpolated Kneser-Ney, and actually produces superior results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1124.txt | Citing Article:  N12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form.</S> | Reference Offset:  ['0','160'] | Reference Text:  <S sid = 0 ssid = >A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes</S><S sid = 160 ssid = >The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1124.txt | Citing Article:  P11-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >For d = 0, we have a Dirichlet distribution and the number of unique words grows more slowly at O(0 log T).</S><S sid = 168 ssid = >Both panels are for the full training set and n = 3. helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1124.txt | Citing Article:  P11-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a).</S> | Reference Offset:  ['109','116'] | Reference Text:  <S sid = 109 ssid = >Gibbs sampling keeps track of the current state of each variable of interest in the model, and iteratively resamples the state of each variable given the current states of all other variables.</S><S sid = 116 ssid = >The parameters Θ are sampled using an auxiliary variable sampler as detailed in (Teh, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1124.txt | Citing Article:  P11-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Growing discounts of this sort were previously suggested by the model of Teh (2006).</S> | Reference Offset:  ['99','127'] | Reference Text:  <S sid = 99 ssid = >Further details can be obtained at (Teh, 2006).</S><S sid = 127 ssid = >Since the discounts in a hierarchical Pitman-Yor language model are limited to between 0 and 1, we see that modified Kneser-Ney is not an approximation of the hierarchical PitmanYor language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1124.txt | Citing Article:  D12-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006).</S> | Reference Offset:  ['36','152'] | Reference Text:  <S sid = 36 ssid = >There is in general no known analytic form for the density of PY(d, θ, G0) when the vocabulary is finite.</S><S sid = 152 ssid = >We have described using a hierarchical PitmanYor process as a language model and shown that it gives performance superior to state-of-the-art methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1124.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >For d = 0, we have a Dirichlet distribution and the number of unique words grows more slowly at O(0 log T).</S><S sid = 168 ssid = >Both panels are for the full training set and n = 3. helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1124.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >For d = 0, we have a Dirichlet distribution and the number of unique words grows more slowly at O(0 log T).</S><S sid = 168 ssid = >Both panels are for the full training set and n = 3. helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1124.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >For d = 0, we have a Dirichlet distribution and the number of unique words grows more slowly at O(0 log T).</S><S sid = 168 ssid = >Both panels are for the full training set and n = 3. helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1124.txt | Citing Article:  D10-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events.</S> | Reference Offset:  ['20','22'] | Reference Text:  <S sid = 20 ssid = >We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods (Chen and Goodman, 1998).</S><S sid = 22 ssid = >Our interpretation is more useful than past interpretations involving marginal constraints (Kneser and Ney, 1995; Chen and Goodman, 1998) or maximum-entropy models (Goodman, 2004) as it can recover the exact formulation of interpolated Kneser-Ney, and actually produces superior results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1124.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006).</S> | Reference Offset:  ['152','153'] | Reference Text:  <S sid = 152 ssid = >We have described using a hierarchical PitmanYor process as a language model and shown that it gives performance superior to state-of-the-art methods.</S><S sid = 153 ssid = >In addition, we have shown that the state-of-the-art method of interpolated KneserNey can be interpreted as approximate inference in the hierarchical Pitman-Yor language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1124.txt | Citing Article:  W09-0210.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results.</S> | Reference Offset:  ['99','157'] | Reference Text:  <S sid = 99 ssid = >Further details can be obtained at (Teh, 2006).</S><S sid = 157 ssid = >Though (MacKay and Peto, 1994) had the right intuition to look at smoothing techniques as the outcome of hierarchical Bayesian models, the use of the Dirichlet distribution as a prior was shown to lead to non-competitive cross-entropy results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1124.txt | Citing Article:  N12-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006).</S> | Reference Offset:  ['159','160'] | Reference Text:  <S sid = 159 ssid = >We have shown that with a suitable choice of priors (namely the Pitman-Yor process), Bayesian methods can be competitive with the best smoothing techniques.</S><S sid = 160 ssid = >The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1124.txt | Citing Article:  P11-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006).</S> | Reference Offset:  ['160','162'] | Reference Text:  <S sid = 160 ssid = >The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).</S><S sid = 162 ssid = >Both the hierarchical Dirichlet process and the hierarchical Pitman-Yor process are examples of Bayesian nonparametric processes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1124.txt | Citing Article:  P11-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem.</S> | Reference Offset:  ['79','160'] | Reference Text:  <S sid = 79 ssid = >We refer to this as the hierarchical Chinese restaurant process.</S><S sid = 160 ssid = >The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1124.txt | Citing Article:  E12-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006).</S> | Reference Offset:  ['24','122'] | Reference Text:  <S sid = 24 ssid = >Thus the contributions of this paper are threefold: in proposing a langauge model with excellent performance and the accompanying advantages of Bayesian probabilistic models, in proposing a novel and efficient inference scheme for the model, and in establishing the direct correspondence between interpolated Kneser-Ney and the Bayesian approach.</S><S sid = 122 ssid = >The correspondence to interpolated Kneser-Ney is now straightforward.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1124.txt | Citing Article:  P13-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution.</S> | Reference Offset:  ['61','74'] | Reference Text:  <S sid = 61 ssid = >We describe an n-gram language model based on a hierarchical extension of the Pitman-Yor process.</S><S sid = 74 ssid = >Again we may treat each Gu as a distribution over the current word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1124.txt | Citing Article:  P13-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006).</S> | Reference Offset:  ['79','99'] | Reference Text:  <S sid = 79 ssid = >We refer to this as the hierarchical Chinese restaurant process.</S><S sid = 99 ssid = >Further details can be obtained at (Teh, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1124.txt | Citing Article:  W10-2809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['52','168'] | Reference Text:  <S sid = 52 ssid = >For d = 0, we have a Dirichlet distribution and the number of unique words grows more slowly at O(0 log T).</S><S sid = 168 ssid = >Both panels are for the full training set and n = 3. helpful comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1124.txt | Citing Article:  W12-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings.</S> | Reference Offset:  ['9','20'] | Reference Text:  <S sid = 9 ssid = >A large number of smoothing methods have been proposed in the literature (see (Chen and Goodman, 1998; Goodman, 2001; Rosenfeld, 2000) for good overviews).</S><S sid = 20 ssid = >We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods (Chen and Goodman, 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


