Citance Number: 1 | Reference Article:  H05-1010.txt | Citing Article:  W06-1204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For parameter optimization for the word alignment task, Taskar, Simon and Klein (Taskar et al, 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link.</S> | Reference Offset:  ['1','40'] | Reference Text:  <S sid = 1 ssid = >We present a discriminative, large margin approach to feature-based matching for word alignment.</S><S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  H05-1010.txt | Citing Article:  W06-1204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It should be noted that previous word-alignment experiments such as Taskar, Simon and Klein (Taskar et al, 2005) have been done with very large datasets and there is little word-order variation in the languages involved.</S> | Reference Offset:  ['40','162'] | Reference Text:  <S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S><S sid = 162 ssid = >3.2 Scaling Experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  H05-1010.txt | Citing Article:  P14-1138.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative mod els, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006).</S> | Reference Offset:  ['5','144'] | Reference Text:  <S sid = 5 ssid = >The standard approach to word alignment from sentence-aligned bitexts has been to constructmodels which generate sentences of one language from the other, then fitting those genera tive models with EM (Brown et al, 1990; Och and Ney, 2003).</S><S sid = 144 ssid = >One source of constraint which our model stilldoes not explicitly capture is the first-order de pendency between alignment positions, as in theHMM model (Vogel et al, 1996) and IBM models 4+.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  H05-1010.txt | Citing Article:  P11-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, Taskar et al (2005) cast word alignment as a maximum weighted matching problem and propose a framework for learning word pair scores as a function of arbitrary features of that pair.</S> | Reference Offset:  ['14','16'] | Reference Text:  <S sid = 14 ssid = >Word alignment is cast as a maximum weighted matching problem (Cormen et al, 1990) in which each pair of words (e j , f k ) in a sentence pair (e, f) is associated with a score s jk (e, f) reflecting the desirability of the alignment of that pair.</S><S sid = 16 ssid = >This view of alignment as graph matching isnot, in itself, new: Melamed (2000) uses com petitive linking to greedily construct matchingswhere the pair score is a measure of word to-word association, and Matusov et al (2004) find exact maximum matchings where the pair scores come from the alignment posteriors of generative models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  H05-1010.txt | Citing Article:  P11-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We followed the work in (Taskar et al, 2005) and split the original test set into 347 test examples, and 100 training examples for parameters tuning.</S> | Reference Offset:  ['100','154'] | Reference Text:  <S sid = 100 ssid = >For the training data, we split the original test set into 100 trainingexamples and 347 test examples.</S><S sid = 154 ssid = >5The number of such features which can be learned depends on the number of training examples, and since some of our experiments used only a few dozen training examples we did not make heavy use of this feature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al, 2005) use a one-to-one constraint, where words in either sentence can participate in at most one link.</S> | Reference Offset:  ['16','120'] | Reference Text:  <S sid = 16 ssid = >This view of alignment as graph matching isnot, in itself, new: Melamed (2000) uses com petitive linking to greedily construct matchingswhere the pair score is a measure of word to-word association, and Matusov et al (2004) find exact maximum matchings where the pair scores come from the alignment posteriors of generative models.</S><S sid = 120 ssid = >We can get something much like the combination of Dice and competitive linking by running with just one feature on each pair: the Dice value of that pair?s words.2 With just a Dice feature ? meaning no learning is needed yet ? we achieve an AER of 29.8, between the Dice with competitive linking result of 34.0 and Model 1 of 25.9 given in Och and Ney (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Though Tsochantaridis et al (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al, 2005).</S> | Reference Offset:  ['40','49'] | Reference Text:  <S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S><S sid = 49 ssid = >We use an SVM-like hinge upper bound on the loss</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To create a matching alignment solution, we reproduce the approach of (Taskar et al, 2005) within the framework described in Section 4.1: 1.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Matching Approach To Word Alignment</S><S sid = 1 ssid = >We present a discriminative, large margin approach to feature-based matching for word alignment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the same feature representation as (Taskar et al, 2005), with some small exceptions.</S> | Reference Offset:  ['40','87'] | Reference Text:  <S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S><S sid = 87 ssid = >(Korpelevich, 1976; He and Liao, 2002; Taskar et al, 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first baseline, matching is the matching SVM described in Section 4.2.1, which is a re-implementation of the state-of-the art work in (Taskar et al, 2005).</S> | Reference Offset:  ['0','40'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Matching Approach To Word Alignment</S><S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first thing to note is that our Matching baseline is achieving scores in line with (Taskar et al, 2005), which reports an AER of 0.107 using similar features and the same training and test sets.</S> | Reference Offset:  ['100','153'] | Reference Text:  <S sid = 100 ssid = >For the training data, we split the original test set into 100 trainingexamples and 347 test examples.</S><S sid = 153 ssid = >However, our model can4It is important to note that while our matching algo rithm has no first-order effects, the features can encode such effects in this way, or in better ways ? e.g. using as features posteriors from the HMM model in the style of Matusov et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  H05-1010.txt | Citing Article:  P06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work borrows heavily from (Taskar et al, 2005), which uses a max-margin approach with a weighted maximum matching aligner.</S> | Reference Offset:  ['40','82'] | Reference Text:  <S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S><S sid = 82 ssid = >(w) = ?w/max(?, ||w||).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  H05-1010.txt | Citing Article:  W07-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Taskar et al, 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</S> | Reference Offset:  ['14','23'] | Reference Text:  <S sid = 14 ssid = >Word alignment is cast as a maximum weighted matching problem (Cormen et al, 1990) in which each pair of words (e j , f k ) in a sentence pair (e, f) is associated with a score s jk (e, f) reflecting the desirability of the alignment of that pair.</S><S sid = 23 ssid = >We model the alignment prediction task as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  H05-1010.txt | Citing Article:  W07-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dice Coefficient of the source word and the target word (Taskar et al, 2005).</S> | Reference Offset:  ['115','118'] | Reference Text:  <S sid = 115 ssid = >value is the Dice coefficient (Dice, 1945): Dice(e, f) = 2CEF (e, f)C E (e)C F (f) Here, C E and C F are counts of word occurrences in each language, while C EF is the number of co-occurrences of the two words.</S><S sid = 118 ssid = >As observed in Melamed (2000), this use ofDice misses the crucial constraint of competition: a candidate source word with high asso ciation to a target word may be unavailable for alignment because some other target has an even better affinity for that source word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  H05-1010.txt | Citing Article:  W07-1428.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al, 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h.</S> | Reference Offset:  ['14','87'] | Reference Text:  <S sid = 14 ssid = >Word alignment is cast as a maximum weighted matching problem (Cormen et al, 1990) in which each pair of words (e j , f k ) in a sentence pair (e, f) is associated with a score s jk (e, f) reflecting the desirability of the alignment of that pair.</S><S sid = 87 ssid = >(Korpelevich, 1976; He and Liao, 2002; Taskar et al, 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  H05-1010.txt | Citing Article:  W07-1428.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Commitment Extraction, we used an word alignment technique first introduced in (Taskar et al, 2005b) in order to select the commitment extracted from t (henceforth, ct) which represents the best alignment for each of the commitments extracted from h (henceforth, ch).</S> | Reference Offset:  ['0','87'] | Reference Text:  <S sid = 0 ssid = >A Discriminative Matching Approach To Word Alignment</S><S sid = 87 ssid = >(Korpelevich, 1976; He and Liao, 2002; Taskar et al, 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  H05-1010.txt | Citing Article:  W07-1428.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As with (Taskar et al, 2005b), we use the large-margin structured prediction model.</S> | Reference Offset:  ['39','40'] | Reference Text:  <S sid = 39 ssid = >2.1 Large-margin estimation.</S><S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  H05-1010.txt | Citing Article:  W10-2913.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Less general approaches based on matching have been proposed in (Matusov et al, 2004) and (Taskar et al., 2005).</S> | Reference Offset:  ['40','153'] | Reference Text:  <S sid = 40 ssid = >We follow the large-margin formulation of Taskar et al (2005a).</S><S sid = 153 ssid = >However, our model can4It is important to note that while our matching algo rithm has no first-order effects, the features can encode such effects in this way, or in better ways ? e.g. using as features posteriors from the HMM model in the style of Matusov et al (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  H05-1010.txt | Citing Article:  E06-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Permutation space methods include weighted maximum matching (Taskar et al, 2005), and approximations to maximum matching like competitive linking (Melamed, 2000).</S> | Reference Offset:  ['14','16'] | Reference Text:  <S sid = 14 ssid = >Word alignment is cast as a maximum weighted matching problem (Cormen et al, 1990) in which each pair of words (e j , f k ) in a sentence pair (e, f) is associated with a score s jk (e, f) reflecting the desirability of the alignment of that pair.</S><S sid = 16 ssid = >This view of alignment as graph matching isnot, in itself, new: Melamed (2000) uses com petitive linking to greedily construct matchingswhere the pair score is a measure of word to-word association, and Matusov et al (2004) find exact maximum matchings where the pair scores come from the alignment posteriors of generative models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  H05-1010.txt | Citing Article:  W09-1804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Taskar et al, 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set.</S> | Reference Offset:  ['41','95'] | Reference Text:  <S sid = 41 ssid = >Our input is a set of training instances {(x i ,y i )}m i=1, where each in stance consists of a sentence pair x i and a target 74 alignment y i . We would like to find parameters.</S><S sid = 95 ssid = >This corpus consists of 1.1M automatically aligned sentences, and comes with a validation set of 39 sentence pairs and a test set of 447 sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


