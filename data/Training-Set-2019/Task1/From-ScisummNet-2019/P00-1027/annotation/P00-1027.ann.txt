Citance Number: 1 | Reference Article:  P00-1027.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech.</S> | Reference Offset:  ['18','37'] | Reference Text:  <S sid = 18 ssid = >On learning the past tense of English verbs.</S><S sid = 37 ssid = >In further clarification of the task description, the morphological induction described in this paper assumes, and is based on, only the following limited set of (often optional) available resources: (a) A table (such as Table 2) of the inflectional parts of speech of the given language, along with a list of the canonical suffixes for each part of speech.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P00-1027.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >MED has been applied to the morphology induction problem by other researchers (such as Yarowsky and Wicentowski, 2000).</S> | Reference Offset:  ['28','64'] | Reference Text:  <S sid = 28 ssid = >This paper presents an original and successful algorithm for the nearly unsupervised induction of inflectional morphological analyzers, with a focus on highly irregular forms not typically handled by other morphology induction algorithms.</S><S sid = 64 ssid = >How can we overcome this problem?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P00-1027.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that of Yarowsky and Wicentowski (2000), who assume some information about regular morphology in order to induce irregular morphology.</S> | Reference Offset:  ['47','60'] | Reference Text:  <S sid = 47 ssid = >However, handling of irregular words was largely excluded from this work, as Goldsmith assumed a strictly concatenative morphology without models for stem changes.</S><S sid = 60 ssid = >The algorithm described below directly addresses this gap, while successfully inducing more regular analyses without supervision as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P00-1027.txt | Citing Article:  W02-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['49','134'] | Reference Text:  <S sid = 49 ssid = >Voutilainen (1995) has approached this problem in a finitestate framework, and Hakkani-Thr et al. (2000) have done so using a trigram tagger, with the assumption of a concatenative affixation model.</S><S sid = 134 ssid = >Test True (Convg) CS+FS+LS+MS (Itr 1) CS+FS+LS CS+FS LS only Word Root Score (Itr 1) (Itr 1) (Itr 1) got get go 1.30 go go go gut knew know know 1.35 know know know know took take take 1.50 take take take toot blew blow blow 1.80 blow blow blow blow became become become 2.35 become become become become made make make 2.40 make make make mate clung cling cling 2.55 cling cling cling cling drew draw draw 2.65 draw draw draw draw swore swear swear 2.80 swear swear swear store wore wear wear 3.10 wear wear wear wire came come come 3.55 come come come come thought think think 3.60 think think think thump flung fling fling 4.60 fling fling fling fling brought bring bring 5.35 bring bring bring brighten strove strive strive 5.85 strive strive straddle strive stuck stick stick 6.00 stick stick stabilize stock swept sweep sweep 6.20 sweep sweep sweep swap shone shine shine 6.55 shine shine shine shine woke wake wake 6.95 wake wake wind wake clove cleave cleave 7.35 cleave cleave cleave close bore bear bear 7.75 bear bar bear bare meant mean mean 8.20 mean mean manage mount lent lend lend 9.25 lend lend lend lend slew slay slit 10.06 slit slight slight slow struck strike strike 11.60 strike strike strike strut bought buy buy 12.20 buy buy buy bound bit bite bite 13.60 bite bite betray bet dove dive dive 17.25 dive dive dash dive burnt burn burp 17.30 burp burp burp burn went go want 18.29 want want want want caught catch catch 18.35 catch cut catch cough dealt deal deal 21.45 deal deal disagree deal</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P00-1027.txt | Citing Article:  W02-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yarowsky and Wicentowski (2000) propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations).</S> | Reference Offset:  ['28','129'] | Reference Text:  <S sid = 28 ssid = >This paper presents an original and successful algorithm for the nearly unsupervised induction of inflectional morphological analyzers, with a focus on highly irregular forms not typically handled by other morphology induction algorithms.</S><S sid = 129 ssid = >This paper has presented an original algorithm capable of inducing the accurate morphological analysis of even highly irregular verbs, starting with no paired <inflection,root> examples for training and no prior seeding of legal morphological transformations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P00-1027.txt | Citing Article:  W02-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yarowsky and Wicentowski (2000) report an accuracy of over 99% for their best model and a test set of 3888 pairs.</S> | Reference Offset:  ['116','121'] | Reference Text:  <S sid = 116 ssid = >However, the hypothesis that these measures model independent and complementary evidence sources is supported by the roughly additive combined accuracy of 71.6%.8 The final performance of the full converged CS+FS+LS+MS model at 99.2% accuracy on the full test set, and 99.7% accuracy on inflections requiring analysis beyond simple concatenative suffixation, is quite remarkable given that the algorithm had absolutely no <inflection,root> examples as training data, and had no prior inventory of stem changes available, with only a slight statistical bias in favor of shorter stem changes with smaller Levenshtein distance, and with the minimal search-simplifying assumption in all the models that candidate alignments must begin with a the same VC * prefix.9 Given a starting point where all single character X-+17 changes at the point of suffixation are equally likely, the processes of elison (e-+e), gemination (e.g.</S><S sid = 121 ssid = >Although they don't breakdown this performance by word type, their included FOILDL program trained from 250 pairs and applied to our evaluation set achieved 100% accuracy on the pairs with simple +ed concatenation, 84% accuracy on stem changing (non-concat) pairs and 5% accuracy on the highly irregular pairs, with 89% overall accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P00-1027.txt | Citing Article:  W02-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >From the point of view of the evaluation of the algorithm, we should design an assessment scheme that would make our experimental results more directly comparable to those of Yarowsky and Wicentowski (2000), Schone and Jurafsky (2000) and others.</S> | Reference Offset:  ['122','124'] | Reference Text:  <S sid = 122 ssid = >Other available supervised learning results (e.g.</S><S sid = 124 ssid = >While not directly comparable with our text-based data, their performance is significantly worse than Mooney and Calif's FOILDL on common phonological paired data, suggesting that FOILDL is a generally competitive reference point for our results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P00-1027.txt | Citing Article:  W04-0109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The development of the WordFrame model was motivated by work originally presented in Yarowsky and Wicentowski (2000).</S> | Reference Offset:  ['43','110'] | Reference Text:  <S sid = 43 ssid = >Brent (1993, 1999), de Marcken (1995), Kazakov (1997) and Goldsmith (2000) have each focused on the problem of unsupervised learning of morphological systems as essentially a segmentation task, yielding a morphologically plausible and statistically motivated partition of stems and affixes.</S><S sid = 110 ssid = >Current empirical evaluation of this work focuses on its accuracy in analyzing the often highly irregular past tense of English verbs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P00-1027.txt | Citing Article:  W04-0109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes.</S> | Reference Offset:  ['0','93'] | Reference Text:  <S sid = 0 ssid = >Minimally Supervised Morphological Analysis By Multimodal Alignment</S><S sid = 93 ssid = >The final stem-change probabilities then are an interpolation with the trained model Pi and the initial baseline (P0) model described in Section 6.1: P( c —x 13 I root, suffix, POS) = Ai P0( a —x /3 I suffix) + (1 — Ai) Pi( a —x /3 I root, suffix, POS) The Levenshtein distance models are reestimated as observed in Section 5, while the context similarity model can be improved through better self-learned lemmatization of the modelled context words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P00-1027.txt | Citing Article:  W04-0109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Yarowsky and Wicentowski (2000), the end-of-string model is trained from inflection-root pairs acquired through unsupervised methods.</S> | Reference Offset:  ['83','88'] | Reference Text:  <S sid = 83 ssid = >First note that because the triple of <root> + <stemchange> + <suffix> uniquely determines a resulting inflection, one can effectively compute P(inflection I root, suffix, POS) by P(stemchange I root, suffix, POS), i.e. for any root=-ya, suffix=+o- and inflection=-0a, Using statistics such as shown in Table 7, it is thus possible to compute the generation (or alignment) probability for an inflection given root and suffix using the simple interpolated backoff model in (1) where Ai is a function of the relative sample size of the conditioning event, and lastk(root) indicates the final k characters of the root.</S><S sid = 88 ssid = >6.1 Baseline Model for Morphological Transformation Probabilities On the first iteration, no inflection/root pairs are available for estimating the above models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P00-1027.txt | Citing Article:  W04-0109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (Yarowsky and Wicentowski, 2000), (Baroni et al, 2002) and (Wicentowski, 2002).</S> | Reference Offset:  ['49','58'] | Reference Text:  <S sid = 49 ssid = >Voutilainen (1995) has approached this problem in a finitestate framework, and Hakkani-Thr et al. (2000) have done so using a trigram tagger, with the assumption of a concatenative affixation model.</S><S sid = 58 ssid = >Recently applied to Polish, the model also assumes concatenative morphology and treats non-concatenative irregular forms through table lookup.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P00-1027.txt | Citing Article:  W02-0602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yarowsky and Wicentowski (2000) have developed a system that learns such rules given a preliminary morphological hypothesis and part of speech tags.</S> | Reference Offset:  ['38','102'] | Reference Text:  <S sid = 38 ssid = >These suffixes not only serve as mnemonic tags for the POS labels, but they can also be used to obtain a noisy set of candidate examples for each part of speech.' the given language is useful to the extraction of context similarity features.</S><S sid = 102 ssid = >This principle suggests that for a given part of speech, a root should not have more than one inflection nor should multiple inflections in the same part of speech share the same root.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P00-1027.txt | Citing Article:  W06-2005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly as Yarowsky and Wicentowski (2000), we assume that, in any language, vowels are more mutable in inflection than consonants, thus for example replacing a for i is cheaper that replacing s by r.</S> | Reference Offset:  ['16','74'] | Reference Text:  <S sid = 16 ssid = >Practical bootof morphological analyzers. of the Conference on Natural Language Learning.</S><S sid = 74 ssid = >In morphological systems worldwide, vowels and vowel clusters are relatively mutable through morphological processes, while consonants generally tend to have a lower probability of change during inflection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P00-1027.txt | Citing Article:  D10-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yarowsky and Wicentowski (2000) use similar statistics to identify words related by inflection, but they gather their counts from a much smaller corpus.</S> | Reference Offset:  ['33','69'] | Reference Text:  <S sid = 33 ssid = >While the probabilistic analyzer trained in Step 2 remains useful for previously unseen words, such words are typically quite regular and most of the difficult substance of the lemmatization problem can often be captured by a large root+Posinflection mapping table and a simple transducer to handle residual forms.</S><S sid = 69 ssid = >To do so, we use simple non-parametric statistics to calculate the probability of a particularvBD ratio by examining how frequently other such ratios in a similar range have been seen in the corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P00-1027.txt | Citing Article:  N07-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yarowsky and Wicentowski (2000) propose an interesting algorithm that employs four similarity measures to successfully identify the most probable root of a highly irregular word.</S> | Reference Offset:  ['60','114'] | Reference Text:  <S sid = 60 ssid = >The algorithm described below directly addresses this gap, while successfully inducing more regular analyses without supervision as well.</S><S sid = 114 ssid = >Table 9 shows the performance of several of the investigated similarity measures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P00-1027.txt | Citing Article:  W02-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000) for all the other languages.</S> | Reference Offset:  ['41','122'] | Reference Text:  <S sid = 41 ssid = >There is a rich tradition of supervised and unsupervised learning in the domain of morphology.</S><S sid = 122 ssid = >Other available supervised learning results (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P00-1027.txt | Citing Article:  W04-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yarowsky and Wicentowski (2000) present a corpus-based approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.</S> | Reference Offset:  ['65','130'] | Reference Text:  <S sid = 65 ssid = >Relative corpus frequency is one useful evidence source.</S><S sid = 130 ssid = >It does so by treating morphological analysis predominantly as an alignment task in a large corpus, performing the effective collaboration of four original similarity measures based on expected frequency distributions, context, morphologically-weighted Levenshtein similarity and an iteratively bootstrapped model of affixation and stem-change probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


