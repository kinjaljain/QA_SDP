Citance Number: 1 | Reference Article:  P02-1001.txt | Citing Article:  D09-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The calculation of expected counts can be formulated using the expectation semiring frame work of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations.</S> | Reference Offset:  ['136','173'] | Reference Text:  <S sid = 136 ssid = >2 has infinitely many) but expected counts derived from the paths.</S><S sid = 173 ssid = >For example, if every arc had value 1, then expected value would be expected path length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1001.txt | Citing Article:  D09-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests.</S> | Reference Offset:  ['136','173'] | Reference Text:  <S sid = 136 ssid = >2 has infinitely many) but expected counts derived from the paths.</S><S sid = 173 ssid = >For example, if every arc had value 1, then expected value would be expected path length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1001.txt | Citing Article:  W10-1718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009).</S> | Reference Offset:  ['29','229'] | Reference Text:  <S sid = 29 ssid = >It is common to define further useful operations (as macros), which modify existing relations not by editing their source code but simply by operating on them “from outside.” ∗A brief version of this work, with some additional material, first appeared as (Eisner, 2001a).</S><S sid = 229 ssid = >The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1001.txt | Citing Article:  H05-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or epsilon-cycles in FSMs (Eisner, 2002).</S> | Reference Offset:  ['220','221'] | Reference Text:  <S sid = 220 ssid = >Knight and Graehl, 20If xi and yi are acyclic (e.g., fully observed strings), and f (or rather its FST) has no a : a cycles, then composition will “unroll” f into an acyclic machine.</S><S sid = 221 ssid = >If only xi is acyclic, then the composition is still acyclic if domain(f) has no a cycles.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1001.txt | Citing Article:  D08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use standard algorithms (Eisner, 2002) to compute the path sums as well as their gradients with respect to theta for optimization (section 4.1).</S> | Reference Offset:  ['141','196'] | Reference Text:  <S sid = 141 ssid = >Alternatively, discard EM and use gradient-based optimization.</S><S sid = 196 ssid = >The same semiring may be used to compute gradients.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1001.txt | Citing Article:  D08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the OpenFST library (Allauzen et al, 2007) to implement all finite-state computations, using the expectation semiring (Eisner, 2002) for training.</S> | Reference Offset:  ['100','222'] | Reference Text:  <S sid = 100 ssid = >Such approaches have been tried recently in restricted cases (McCallum et al., 2000; Eisner, 2001b; Lafferty et al., 2001).</S><S sid = 222 ssid = >1998), although such data could also be used; (4) training of branching noisy channels (footnote 7); (5) discriminative training with incomplete data; (6) training of conditional MEMMs (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) on unbounded sequences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hyper graphs).</S> | Reference Offset:  ['163','173'] | Reference Text:  <S sid = 163 ssid = >For other parameterizations, the path must instead yield a vector of arc traversal counts or feature counts.</S><S sid = 173 ssid = >For example, if every arc had value 1, then expected value would be expected path length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we apply the expectation semiring (Eisner, 2002) to a hyper graph (or packed forest) rather than just a lattice.</S> | Reference Offset:  ['149','191'] | Reference Text:  <S sid = 149 ssid = >(Of course, the method of this paper can train such compositions.)</S><S sid = 191 ssid = >The other “magical” property of the expectation semiring is that it automatically keeps track of the tangled parameter counts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisner (2002) uses closed semirings that are also equipped with a Kleene closure operator.</S> | Reference Offset:  ['28','135'] | Reference Text:  <S sid = 28 ssid = >Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed.</S><S sid = 135 ssid = >The M step uses not individual path probabilities (Fig.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['85','230'] | Reference Text:  <S sid = 85 ssid = >Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 −)arcs).</S><S sid = 230 ssid = >For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, Eisner (2002, section 5) observes that this is inefficient when n is large.</S> | Reference Offset:  ['43','145'] | Reference Text:  <S sid = 43 ssid = >In short, current finite-state toolkits include no training algorithms, because none exist for the large space of statistical models that the toolkits can in principle describe and run.</S><S sid = 145 ssid = >Per-state joint normalization (Eisner, 2001b, §8.2) is similar but drops the dependence on a.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This follows Eisner (2002), who similarly generalized the forward-backward algorithm.</S> | Reference Offset:  ['42','217'] | Reference Text:  <S sid = 42 ssid = >For example, the forward-backward algorithm (Baum, 1972) trains only Hidden Markov Models, while (Ristad and Yianilos, 1996) trains only stochastic edit distance.</S><S sid = 217 ssid = >Here, the forward and backward probabilities can be computed in time only O(m + n log n) (Fredman and Tar an, 1987). k-best variants are also possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.</S> | Reference Offset:  ['180','218'] | Reference Text:  <S sid = 180 ssid = >Multiplication and addition are replaced by binary operations ® and ® on K. Thus ® is used to combine arc weights into a path weight and ® is used to combine the weights of alternative paths.</S><S sid = 218 ssid = >We have exhibited a training algorithm for parameterized finite-state machines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.</S> | Reference Offset:  ['208','229'] | Reference Text:  <S sid = 208 ssid = >• In many cases of interest, Ti is an acyclic graph.20 Then Tar an’s method computes w0j for each j in topologically sorted order, thereby finding ti in a linear number of ⊕ and ⊗ operations.</S><S sid = 229 ssid = >The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1001.txt | Citing Article:  P04-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This logic can be used with the expectation semiring (Eisner, 2002) to find the maximum likelihood estimates of the parameters of a word-to-word translation model.</S> | Reference Offset:  ['118','196'] | Reference Text:  <S sid = 118 ssid = >Maximum-likelihood estimation guesses 0ˆ to be the 0 maximizing Hi fθ(xi, yi).</S><S sid = 196 ssid = >The same semiring may be used to compute gradients.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1001.txt | Citing Article:  P04-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisner (2002) has claimed that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs.</S> | Reference Offset:  ['86','229'] | Reference Text:  <S sid = 86 ssid = >A more subtle example is weighted FSAs that approximate PCFGs (Nederhof, 2000; Mohri and Nederhof, 2001), or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation.</S><S sid = 229 ssid = >The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1001.txt | Citing Article:  P05-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths (Eisner, 2002) using a simple dynamic programming algorithm akin to the forward algorithm.</S> | Reference Offset:  ['218','221'] | Reference Text:  <S sid = 218 ssid = >We have exhibited a training algorithm for parameterized finite-state machines.</S><S sid = 221 ssid = >If only xi is acyclic, then the composition is still acyclic if domain(f) has no a cycles.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1001.txt | Citing Article:  P05-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['85','230'] | Reference Text:  <S sid = 85 ssid = >Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 −)arcs).</S><S sid = 230 ssid = >For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1001.txt | Citing Article:  P09-2069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers.</S> | Reference Offset:  ['0','228'] | Reference Text:  <S sid = 0 ssid = >Parameter Estimation For Probabilistic Finite-State Transducers</S><S sid = 228 ssid = >Such techniques build on our parameter estimation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1001.txt | Citing Article:  E09-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Eisner (2002) describes the expectation semiring for parameter learning.</S> | Reference Offset:  ['0','191'] | Reference Text:  <S sid = 0 ssid = >Parameter Estimation For Probabilistic Finite-State Transducers</S><S sid = 191 ssid = >The other “magical” property of the expectation semiring is that it automatically keeps track of the tangled parameter counts.</S> | Discourse Facet:  NA | Annotator: Automatic


