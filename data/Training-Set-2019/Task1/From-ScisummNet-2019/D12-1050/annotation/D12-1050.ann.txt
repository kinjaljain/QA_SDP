Citance Number: 1 | Reference Article:  D12-1050.txt | Citing Article:  P13-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012).</S> | Reference Offset:  ['14','22'] | Reference Text:  <S sid = 14 ssid = >Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001).</S><S sid = 22 ssid = >Socher et al. (2011a) and Socher et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D12-1050.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blacoe and Lapata (2012) compare count and predict representations as input to composition functions.</S> | Reference Offset:  ['29','184'] | Reference Text:  <S sid = 29 ssid = >Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate.</S><S sid = 184 ssid = >These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D12-1050.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).</S> | Reference Offset:  ['21','22'] | Reference Text:  <S sid = 21 ssid = >Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008).</S><S sid = 22 ssid = >Socher et al. (2011a) and Socher et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D12-1050.txt | Citing Article:  P13-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5.</S> | Reference Offset:  ['0','138'] | Reference Text:  <S sid = 0 ssid = >A Comparison of Vector-based Representations for Semantic Composition</S><S sid = 138 ssid = >Table 3 summarizes the performance of the various models on the phrase similarity dataset.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D12-1050.txt | Citing Article:  P13-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['63','201'] | Reference Text:  <S sid = 63 ssid = >Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·): where W ∈ Rd×|D |is a matrix of parameters to be learned.</S><S sid = 201 ssid = >We acknowledge the support of EPSRC through project grant EP/I032916/1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D12-1050.txt | Citing Article:  P14-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.</S> | Reference Offset:  ['154','171'] | Reference Text:  <S sid = 154 ssid = >For each of our three vector sources and three different compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (“con”) or subtraction (“sub”); (b) a vector encoding which words appear therein (“enc”); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Seni1, the length of Seni2, and the unigram overlap among the two sentences.</S><S sid = 171 ssid = >Also note that the best performing models, namely DM with addition and SDS with multiplication, use a basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D12-1050.txt | Citing Article:  P14-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used.</S> | Reference Offset:  ['22','178'] | Reference Text:  <S sid = 22 ssid = >Socher et al. (2011a) and Socher et al.</S><S sid = 178 ssid = >This model is more sophisticated than the one we used in our experiments (see Table 4 and 5).</S> | Discourse Facet:  NA | Annotator: Automatic


