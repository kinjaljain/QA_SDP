Citance Number: 1 | Reference Article:  D07-1072.txt | Citing Article:  P13-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus.</S> | Reference Offset:  ['232','241'] | Reference Text:  <S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S><S sid = 241 ssid = >From the grammar in Figure 6, we generated 2000 trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1072.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large.</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1072.txt | Citing Article:  P10-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al, 2007), and use probabilistic grammar models other than PCFGs.</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1072.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','286'] | Reference Text:  <S sid = 97 ssid = >Assume that z1 is always fixed to a special START state, so we do not need to generate it.</S><S sid = 286 ssid = >696</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1072.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While Liang et al (2007) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data.</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1072.txt | Citing Article:  N09-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(General grammars with infinite numbers of nonterminals were studied by (Liang et al, 2007b)).</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1072.txt | Citing Article:  N09-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a).</S> | Reference Offset:  ['10','277'] | Reference Text:  <S sid = 10 ssid = >DP mixture models have since been extended to hierarchical Dirichlet processes (HDPs) and HDP-HMMs (Teh et al, 2006; Beal et al., 2002) and applied to many different types of clustering/induction problems in NLP (Johnson et al., 2006; Goldwater et al, 2006).In this paper, we present the hierarchical Dirich let process PCFG (HDP-PCFG).</S><S sid = 277 ssid = >Examples of such penalized likelihood procedures in clude Stolcke and Omohundro (1994), which used an asymptotic Bayesian model selection criterion and Petrov et al (2006), which used a split-merge algorithm which procedurally determines when to switch between grammars of various complexities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1072.txt | Citing Article:  D10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007).</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1072.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','286'] | Reference Text:  <S sid = 97 ssid = >Assume that z1 is always fixed to a special START state, so we do not need to generate it.</S><S sid = 286 ssid = >696</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D07-1072.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We'd like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive state splitting found in the iHMM and iPCFG (Liang et al., 2007).</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D07-1072.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al (2007).</S> | Reference Offset:  ['195','232'] | Reference Text:  <S sid = 195 ssid = >We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D07-1072.txt | Citing Article:  W08-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007).</S> | Reference Offset:  ['102','232'] | Reference Text:  <S sid = 102 ssid = >In general, we do not know the appropriate number of grammar symbols, so our strategy is to let the number of grammar symbols be infinite and place a DP prior over grammar symbols.</S><S sid = 232 ssid = >B????T )]See Liang et al (2007) for the derivation of the gradient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D07-1072.txt | Citing Article:  D12-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of alpha; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics.</S> | Reference Offset:  ['21','131'] | Reference Text:  <S sid = 21 ssid = >Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al, 2005; Petrov et al, 2006).</S><S sid = 131 ssid = >In many natural language applications, there is a hard distinction between pre-terminal symbols(those that only emit a word) and non-terminal sym bols (those that only rewrite as two non-terminal or pre-terminal symbols).</S> | Discourse Facet:  NA | Annotator: Automatic


