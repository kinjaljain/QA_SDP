Citance Number: 1 | Reference Article:  P03-1051.txt | Citing Article:  N04-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The segmentation model is similar to the one presented by Lee et al (2003), and obtains an accuracy of about 98%.</S> | Reference Offset:  ['0','103'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 103 ssid = >We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-1051.txt | Citing Article:  P13-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMMsegmenter (Lee et al, 2003).</S> | Reference Offset:  ['74','80'] | Reference Text:  <S sid = 74 ssid = >We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size.</S><S sid = 80 ssid = >The baseline performances are obtained by assigning each token the most frequently occurring segmentation in the manually segmented training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-1051.txt | Citing Article:  N12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al, 2003).</S> | Reference Offset:  ['23','29'] | Reference Text:  <S sid = 23 ssid = >Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions.</S><S sid = 29 ssid = >The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-1051.txt | Citing Article:  W05-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lee et al (2003) demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation.</S> | Reference Offset:  ['26','106'] | Reference Text:  <S sid = 26 ssid = >(Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations.</S><S sid = 106 ssid = >Our future work includes (i) application of the current technique to other highly inflected languages, (ii) application of the unsupervised stem acquisition technique on about 1 billion word unsegmented Arabic corpus, and (iii) adoption of a novel morphological analysis technique to handle irregular morphology, as realized in Arabic broken plurals YL+S (ktAb) 'book' vs. ��„�< (ktb) 'books'.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-1051.txt | Citing Article:  W05-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','110'] | Reference Text:  <S sid = 40 ssid = >This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 & 6.</S><S sid = 110 ssid = >We would like to thank Martin Franz for discussions on language model building, and his help with the use of ViaVoice language model toolkit.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-1051.txt | Citing Article:  W05-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As in (Lee et al, 2003), we used unsupervised training data which is automatically segmented to discover previously unseen stems.</S> | Reference Offset:  ['32','65'] | Reference Text:  <S sid = 32 ssid = >We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus.</S><S sid = 65 ssid = >Unsupervised acquisition of new stems from an automatically segmented new corpus is a three-step process: (i) select new stem candidates on the basis of a frequency threshold, (ii) filter out new stem candidates containing a sub-string with a high likelihood of being a prefix, suffix, or prefix-suffix.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-1051.txt | Citing Article:  W07-0804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >context sensitive Arabic stemmer (Lee et al 2003) to overcome the morphological complexity of Arabic.</S> | Reference Offset:  ['0','29'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 29 ssid = >The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-1051.txt | Citing Article:  P06-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al, 2003).</S> | Reference Offset:  ['20','103'] | Reference Text:  <S sid = 20 ssid = >The input to the morpheme segmenter is a sequence of Arabic tokens – we use a tokenizer that looks only at white space and other punctuation, e.g. quotation marks, parentheses, period, comma, etc.</S><S sid = 103 ssid = >We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-1051.txt | Citing Article:  P06-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We propose in the following an extension to the aforementioned FST model, where we jointly determines not only diacritics but segmentation into affixes as described in (Lee et al, 2003).</S> | Reference Offset:  ['0','13'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 13 ssid = >Their algorithm does not handle multiple affixes per word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-1051.txt | Citing Article:  H05-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An Arabicsegmenter similar to (Lee et al, 2003) provides the segmentation features.</S> | Reference Offset:  ['29','33'] | Reference Text:  <S sid = 29 ssid = >The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).</S><S sid = 33 ssid = >However, we first describe the segmentation algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-1051.txt | Citing Article:  N07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This produces a segmentation view of the arabic source words (Lee et al., 2003).</S> | Reference Offset:  ['0','26'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 26 ssid = >(Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-1051.txt | Citing Article:  W06-3103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Lee et al, 2003) a statistical approach for Arabic word segmentation was presented.</S> | Reference Offset:  ['0','103'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 103 ssid = >We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-1051.txt | Citing Article:  W06-3103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','110'] | Reference Text:  <S sid = 40 ssid = >This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 & 6.</S><S sid = 110 ssid = >We would like to thank Martin Franz for discussions on language model building, and his help with the use of ViaVoice language model toolkit.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-1051.txt | Citing Article:  W10-3601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The algorithm is inspired with the work on the segmentation of Arabic words (Lee et al, 2003).</S> | Reference Offset:  ['2','33'] | Reference Text:  <S sid = 2 ssid = >However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate's similarity to stems occurring in the training corpus.</S><S sid = 33 ssid = >However, we first describe the segmentation algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P03-1051.txt | Citing Article:  P05-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lee et al (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus.</S> | Reference Offset:  ['74','82'] | Reference Text:  <S sid = 74 ssid = >We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size.</S><S sid = 82 ssid = >Regardless of the manually segmented training corpus size, use of trigram language model probabilities reduces the word error rate of the corresponding baseline by approximately 50%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P03-1051.txt | Citing Article:  P05-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lee et al (2003) show that the unsupervised use of the large corpus for stem identification increases accuracy.</S> | Reference Offset:  ['32','93'] | Reference Text:  <S sid = 32 ssid = >We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus.</S><S sid = 93 ssid = >Interestingly, the segmenter developed from a 110K manually segmented corpus has the lowest percentage of “unknown stem” errors at 39.6% indicating that our unsupervised acquisition of new stems is working well, as well as suggesting to use a larger unsegmented corpus for unsupervised stem acquisition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P03-1051.txt | Citing Article:  W09-0805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lee et al (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach.</S> | Reference Offset:  ['0','104'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 104 ssid = >Our Arabic word segmentation system implementing the algorithm achieves around 97% segmentation accuracy on a development test corpus containing 28,449 word tokens.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P03-1051.txt | Citing Article:  W09-0805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As estimated by (Lee et al, 2003), we set the probability of ?u/k? to be 1E? 9.</S> | Reference Offset:  ['39','89'] | Reference Text:  <S sid = 39 ssid = >In that case, we use an “UNKNOWN” class in the trigram language model with the model probability given by p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where UNK_Fraction is 1e-9 determined on empirical grounds.</S><S sid = 89 ssid = >Prefix, suffix, prefix-suffix likelihood score to further filter out illegitimate stem candidates was set at 0.5 for the segmenters developed from 10K, 20K, and 40K manually segmented corpora, whereas it was set at 0.85 for the segmenters developed from a 110K manually segmented corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P03-1051.txt | Citing Article:  W09-0805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We found that the value proposed by (Lee et al, 2003) for Arabic gives good results also for Hebrew.</S> | Reference Offset:  ['29','79'] | Reference Text:  <S sid = 29 ssid = >The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).</S><S sid = 79 ssid = >The experimental results are shown in Table 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P03-1051.txt | Citing Article:  W05-0706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moving on to Arabic, Lee et al (2003) describe a word segmentation system for Arabic that uses an n gram language model over morphemes.</S> | Reference Offset:  ['0','19'] | Reference Text:  <S sid = 0 ssid = >Language Model Based Arabic Word Segmentation</S><S sid = 19 ssid = >Given an Arabic sentence, we use a trigram language model on morphemes to segment it into a sequence of morphemes {m1, m2, ...,mn}.</S> | Discourse Facet:  NA | Annotator: Automatic


