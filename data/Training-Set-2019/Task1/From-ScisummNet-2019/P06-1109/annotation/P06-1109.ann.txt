Citance Number: 1 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006).</S> | Reference Offset:  ['0','12'] | Reference Text:  <S sid = 0 ssid = >An All-Subtrees Approach To Unsupervised Parsing</S><S sid = 12 ssid = >Bod (2006) shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approach known as Data-Oriented Parsing (DOP).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004).</S> | Reference Offset:  ['10','111'] | Reference Text:  <S sid = 10 ssid = >On Penn Wall Street Journal po-s-strings â‰¤ 10 (WSJ10), Klein and Manning (2002) report 71.1% unlabeled f-score with CCM.</S><S sid = 111 ssid = >After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006).</S> | Reference Offset:  ['40','111'] | Reference Text:  <S sid = 40 ssid = >U-DOP extends DOP1 to unsupervised parsing (Bod 2006).</S><S sid = 111 ssid = >After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator.</S> | Reference Offset:  ['3','83'] | Reference Text:  <S sid = 3 ssid = >We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.</S><S sid = 83 ssid = >(Zollmann and Sima'an 2005 propose a different consistent estimator for DOP, which we cannot go into here).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work.</S> | Reference Offset:  ['52','111'] | Reference Text:  <S sid = 52 ssid = >U-DOP therefore randomly samples a large subset from the total number of parse trees from the chart (see Bod 2006) and next converts the subtrees from these parse trees into a PCFG-reduction (Goodman 2003).</S><S sid = 111 ssid = >After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006).</S> | Reference Offset:  ['80','117'] | Reference Text:  <S sid = 80 ssid = >Unfortunately, no compact PCFG-reduction of MLDOP is known.</S><S sid = 117 ssid = >Note that UML-DOP achieves these improved results with fewer subtrees than U-DOP, due to UML-DOP's more drastic pruning of subtrees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn's WSJ10 which contains 7422 sentences ? 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ? 10 words after removing punctuation.</S> | Reference Offset:  ['103','113'] | Reference Text:  <S sid = 103 ssid = >To compare UML-DOP to U-DOP, we started out with the WSJ10 corpus, which contains 7422 sentences <_ 10 words after removing empty elements and punctuation.</S><S sid = 113 ssid = >2002) both containing 2200+ sentences <_ 10 words after removing punctuation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All trees in the test set were binarized beforehand, in the same way as in Bod (2006).</S> | Reference Offset:  ['123','124'] | Reference Text:  <S sid = 123 ssid = >To this end, we used a random 90%/10% division of WSJ40 into a training set and a test set.</S><S sid = 124 ssid = >The ML-PCFG had thus access to the Penn WSJ trees in the training set, while UML-DOP had to bootstrap all structure from the flat strings from the training set to next parse the 10% test set -- clearly a much more challenging task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM.</S> | Reference Offset:  ['114','115'] | Reference Text:  <S sid = 114 ssid = >Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.</S><S sid = 115 ssid = >Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score.</S> | Reference Offset:  ['130','135'] | Reference Text:  <S sid = 130 ssid = >For these splits, UML-DOP achieved an average f-score of 66.9%, while ML-PCFG obtained an average f-score of 64.7%.</S><S sid = 135 ssid = >To be sure, the unbinarized version of the treebank PCFG obtains 89.0% average f-score on WSJ10 and 72.3% average f-score on WSJ40.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1109.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction.</S> | Reference Offset:  ['40','111'] | Reference Text:  <S sid = 40 ssid = >U-DOP extends DOP1 to unsupervised parsing (Bod 2006).</S><S sid = 111 ssid = >After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1109.txt | Citing Article:  W07-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b).</S> | Reference Offset:  ['43','58'] | Reference Text:  <S sid = 43 ssid = >Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.</S><S sid = 58 ssid = >Of course, if we only had the sentence Investors suffered heavy losses in our corpus, there would be no difference in probability between the five parse trees in figure 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1109.txt | Citing Article:  W07-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper.</S> | Reference Offset:  ['21','87'] | Reference Text:  <S sid = 21 ssid = >In this paper we will show that an unsupervised version of ML-DOP can be constructed along the lines of U-DOP.</S><S sid = 87 ssid = >As initial probabilities we use the subtrees' relative frequencies as described in section 2 (smoothed by Good-Turing -- see Bod 1998), though it would also be interesting to see how the model works with other initial parameters, in particular with the usage frequencies proposed by Zuidema (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1109.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here.</S> | Reference Offset:  ['43','114'] | Reference Text:  <S sid = 43 ssid = >Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.</S><S sid = 114 ssid = >Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1109.txt | Citing Article:  P07-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models.</S> | Reference Offset:  ['43','115'] | Reference Text:  <S sid = 43 ssid = >Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.</S><S sid = 115 ssid = >Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1109.txt | Citing Article:  C08-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work.</S> | Reference Offset:  ['12','40'] | Reference Text:  <S sid = 12 ssid = >Bod (2006) shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approach known as Data-Oriented Parsing (DOP).</S><S sid = 40 ssid = >U-DOP extends DOP1 to unsupervised parsing (Bod 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1109.txt | Citing Article:  W09-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work.</S> | Reference Offset:  ['43','114'] | Reference Text:  <S sid = 43 ssid = >Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.</S><S sid = 114 ssid = >Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1109.txt | Citing Article:  D10-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10).</S> | Reference Offset:  ['43','114'] | Reference Text:  <S sid = 43 ssid = >Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.</S><S sid = 114 ssid = >Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1109.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007).</S> | Reference Offset:  ['14','47'] | Reference Text:  <S sid = 14 ssid = >Bod (2006) reports that U-DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ.</S><S sid = 47 ssid = >The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1109.txt | Citing Article:  P09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models.</S> | Reference Offset:  ['0','40'] | Reference Text:  <S sid = 0 ssid = >An All-Subtrees Approach To Unsupervised Parsing</S><S sid = 40 ssid = >U-DOP extends DOP1 to unsupervised parsing (Bod 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


