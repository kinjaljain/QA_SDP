Citance Number: 1 | Reference Article:  W04-3237.txt | Citing Article:  W06-1615.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chelba and Acero (2004) first traina classifier on the source data.</S> | Reference Offset:  ['8','107'] | Reference Text:  <S sid = 8 ssid = >We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text.</S><S sid = 107 ssid = >The “There’s no data like more data” rule-of-thumb could be amended by “..., especially if it’s the right data!”.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-3237.txt | Citing Article:  P08-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation.</S> | Reference Offset:  ['8','108'] | Reference Text:  <S sid = 8 ssid = >We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text.</S><S sid = 108 ssid = >As future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, in-domain data for this and other problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-3237.txt | Citing Article:  P07-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004).</S> | Reference Offset:  ['60','61'] | Reference Text:  <S sid = 60 ssid = >The regularized log-likelihood of the adaptation training data becomes: The adaptation is performed in stages: Fadapt \ Fbackground4 introduced in the model receive 0 weight.</S><S sid = 61 ssid = >The resulting model is thus equivalent with the background model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-3237.txt | Citing Article:  P07-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general.</S> | Reference Offset:  ['1','44'] | Reference Text:  <S sid = 1 ssid = >A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.</S><S sid = 44 ssid = >A simple approach to sequence labeling is the maximum entropy Markov model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-3237.txt | Citing Article:  P07-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data.</S> | Reference Offset:  ['56','57'] | Reference Text:  <S sid = 56 ssid = >A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.</S><S sid = 57 ssid = >A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-3237.txt | Citing Article:  P08-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior.</S> | Reference Offset:  ['1','41'] | Reference Text:  <S sid = 1 ssid = >A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.</S><S sid = 41 ssid = >We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-3237.txt | Citing Article:  P07-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation.</S> | Reference Offset:  ['15','21'] | Reference Text:  <S sid = 15 ssid = >The adaptation procedure proves to be quite effective in further reducing the capitalization error of the WSJ MEMM capitalizer on BN test data.</S><S sid = 21 ssid = >Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-3237.txt | Citing Article:  P07-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation.</S> | Reference Offset:  ['55','71'] | Reference Text:  <S sid = 55 ssid = >In the adaptation scenario we already have a MaxEnt model trained on the background data and we wish to make best use of the adaptation data by balancing the two.</S><S sid = 71 ssid = >We did not experiment with various tying schemes although this is a promising research direction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-3237.txt | Citing Article:  N09-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior.</S> | Reference Offset:  ['41','64'] | Reference Text:  <S sid = 41 ssid = >We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).</S><S sid = 64 ssid = >As shown in Appendix A, the update equations are very similar to the 0-mean case: The effect of the prior is to keep the model parameters λi close to the background ones.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-3237.txt | Citing Article:  N09-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data.</S> | Reference Offset:  ['1','56'] | Reference Text:  <S sid = 1 ssid = >A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.</S><S sid = 56 ssid = >A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-3237.txt | Citing Article:  N06-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases.</S> | Reference Offset:  ['1','44'] | Reference Text:  <S sid = 1 ssid = >A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.</S><S sid = 44 ssid = >A simple approach to sequence labeling is the maximum entropy Markov model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-3237.txt | Citing Article:  N06-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','110'] | Reference Text:  <S sid = 40 ssid = >The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).</S><S sid = 110 ssid = >Special thanks to Adwait Ratnaparkhi for making available the code for his MEMM tagger and MaxEnt trainer.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-3237.txt | Citing Article:  D08-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004).</S> | Reference Offset:  ['90','94'] | Reference Text:  <S sid = 90 ssid = >There are a number of parameters to be tuned on development data.</S><S sid = 94 ssid = >The adaptation procedure was found cut-off threshold used for feature selection on CNNtrn adaptation data; the entry corresponding to the cut-off threshold of 106 represents the number of features in the background model to be insensitive to the number of reestimation iterations, and, more surprisingly, to the number of features added to the background model from the adaptation data, as shown in 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-3237.txt | Citing Article:  W10-2601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text.</S> | Reference Offset:  ['21','41'] | Reference Text:  <S sid = 21 ssid = >Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text.</S><S sid = 41 ssid = >We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


