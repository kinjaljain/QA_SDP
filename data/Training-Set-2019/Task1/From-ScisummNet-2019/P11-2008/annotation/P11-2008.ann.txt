Citance Number: 1 | Reference Article:  P11-2008.txt | Citing Article:  P14-2114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finally, we use a POS tagger trained on tweets (Gimpel et al, 2011) to perform POS tagging on the tweets data and apart from the previously recognised named entities, only words tagged with nouns, verbs or adjectives are kept.</S> | Reference Offset:  ['12','19'] | Reference Text:  <S sid = 12 ssid = >Our contributions are as follows: • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we provide our annotated corpus and trained POS tagger to the research community.</S><S sid = 19 ssid = >Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P11-2008.txt | Citing Article:  P13-2090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, POS taggers for Twitter are only available for a limited number of languages such as English (Gimpel et al, 2011).</S> | Reference Offset:  ['8','11'] | Reference Text:  <S sid = 8 ssid = >Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993).</S><S sid = 11 ssid = >In this paper, we produce an English POS tagger that is designed especially for Twitter data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P11-2008.txt | Citing Article:  P14-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Table 3: Explicit requests for sharing (where only occurrences POS-tagged as verbs count, according to the Gimpel et al (2011) tagger).</S> | Reference Offset:  ['19','83'] | Reference Text:  <S sid = 19 ssid = >Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.</S><S sid = 83 ssid = >The results are shown in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P11-2008.txt | Citing Article:  P14-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text.</S> | Reference Offset:  ['5','19'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 19 ssid = >Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P11-2008.txt | Citing Article:  P14-2071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al, 2011).</S> | Reference Offset:  ['5','36'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 36 ssid = >Another tag, —, is used for tokens marking specific Twitter discourse functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P11-2008.txt | Citing Article:  P13-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This poses considerable problems for traditional NLP tools, which we redeveloped with other domains in mind, which of ten make strong assumptions about orthographic uniformity (i.e., there is just one way to spell you). One approach to cope with this problem is to annotate in-domain data (Gimpel et al, 2011).</S> | Reference Offset:  ['1','9'] | Reference Text:  <S sid = 1 ssid = >We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter.</S><S sid = 9 ssid = >Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P11-2008.txt | Citing Article:  P14-2083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More specifically, we had lay annotators on the crowd sourcing platform Crowdflower re-annotate the training section of Gimpel et al (2011).</S> | Reference Offset:  ['5','96'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 96 ssid = >Finally, we note that, even though 1,000 training examples may seem small, the test set accuracy when training on only 500 tweets drops to 87.66%, a decrease of only 1.7% absolute.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P11-2008.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We crowd source the training section of the data from Gimpel et al (2011) 2 with POS tags.</S> | Reference Offset:  ['5','42'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 42 ssid = >Most of our tags are refinements of those categories, which in turn are groupings of PTB WSJ tags (see column 2 of Table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P11-2008.txt | Citing Article:  W12-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recognizing the limitations of existing systems, Gimpel et al (2011) develop a POS tagger specifically for Twitter, by creating a training corpus as well as devising a tag set that includes parts of speech that are uniquely found in on line language, such as emoticons (smilies).</S> | Reference Offset:  ['12','78'] | Reference Text:  <S sid = 12 ssid = >Our contributions are as follows: • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we provide our annotated corpus and trained POS tagger to the research community.</S><S sid = 78 ssid = >Our evaluation was designed to test the efficacy of this feature set for part-of-speech tagging given limited training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P11-2008.txt | Citing Article:  W12-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this work, we focus on Twitter because the labeled corpus by Gimpel et al (2011) allows us to quantitatively evaluate our approach.</S> | Reference Offset:  ['5','12'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 12 ssid = >Our contributions are as follows: • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we provide our annotated corpus and trained POS tagger to the research community.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P11-2008.txt | Citing Article:  W12-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We then present POS tagging results on the Twitter POS dataset (Gimpel et al, 2011).</S> | Reference Offset:  ['5','12'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 12 ssid = >Our contributions are as follows: • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we provide our annotated corpus and trained POS tagger to the research community.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P11-2008.txt | Citing Article:  W12-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our data is the recent Twitter POS dataset released at ACL 2011 by Gimpel et al (2011) consisting of approximately 26,000 words across 1,827 tweets.</S> | Reference Offset:  ['5','22'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 22 ssid = >A total of 2,217 tweets were distributed to the annotators in this stage; 390 were identified as non-English and removed, leaving 1,827 annotated tweets (26,436 tokens).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P11-2008.txt | Citing Article:  W12-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Twitter dataset uses a domain-dependent tag set of 25 tags that are described in (Gimpel et al, 2011).</S> | Reference Offset:  ['5','84'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 84 ssid = >Our tagger with the full feature set achieves a relative error reduction of 25% compared to the Stanford tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P11-2008.txt | Citing Article:  W12-0601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this experiment, we again make use of the Twitter POS dataset (Gimpel et al, 2011).</S> | Reference Offset:  ['5','19'] | Reference Text:  <S sid = 5 ssid = >The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).</S><S sid = 19 ssid = >Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P11-2008.txt | Citing Article:  P14-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tokenize each tweet with Twitter NLP (Gimpel et al, 2011), remove the @ user and URLs of each tweet, and filter the tweets that are too short (< 7 words).</S> | Reference Offset:  ['30','31'] | Reference Text:  <S sid = 30 ssid = >Thus, we sought to design a coarse tagset that would capture standard parts of speech3 (noun, verb, etc.) as well as categories for token varieties seen mainly in social media: URLs and email addresses; emoticons; Twitter hashtags, of the form #tagname, which the author may supply to categorize a tweet; and Twitter at-mentions, of the form @user, which link to other Twitter users from within a tweet.</S><S sid = 31 ssid = >Hashtags and at-mentions can also serve as words or phrases within a tweet; e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P11-2008.txt | Citing Article:  P14-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A tweet-specific tokenizer (Gimpel et al, 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003).</S> | Reference Offset:  ['19','83'] | Reference Text:  <S sid = 19 ssid = >Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.</S><S sid = 83 ssid = >The results are shown in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


