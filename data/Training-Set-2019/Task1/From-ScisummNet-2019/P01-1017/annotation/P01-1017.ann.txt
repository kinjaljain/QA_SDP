Citance Number: 1 | Reference Article:  P01-1017.txt | Citing Article:  W02-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','152'] | Reference Text:  <S sid = 47 ssid = >Thus only p(h j t,l, m, u, i), the distribution for the head of c, looks at two lexical items (i and h itself), and none of the distributions look at three lexical items as do the trigram distribution of Equation 4 and the previously discussed parsing language models [4, 15].</S><S sid = 152 ssid = >However, this too is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P01-1017.txt | Citing Article:  W02-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','152'] | Reference Text:  <S sid = 47 ssid = >Thus only p(h j t,l, m, u, i), the distribution for the head of c, looks at two lexical items (i and h itself), and none of the distributions look at three lexical items as do the trigram distribution of Equation 4 and the previously discussed parsing language models [4, 15].</S><S sid = 152 ssid = >However, this too is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P01-1017.txt | Citing Article:  W02-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','152'] | Reference Text:  <S sid = 47 ssid = >Thus only p(h j t,l, m, u, i), the distribution for the head of c, looks at two lexical items (i and h itself), and none of the distributions look at three lexical items as do the trigram distribution of Equation 4 and the previously discussed parsing language models [4, 15].</S><S sid = 152 ssid = >However, this too is a topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P01-1017.txt | Citing Article:  P14-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >That is, we parse all sentences using the Charniak's parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence.</S> | Reference Offset:  ['80','121'] | Reference Text:  <S sid = 80 ssid = >Rather we interpolate the probabilities of the entire sentences.</S><S sid = 121 ssid = >The grammar model is, in some sense, always better than the trigram model, but if the parser bungles the parse, then the grammar model is impacted very badly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P01-1017.txt | Citing Article:  P04-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model presented by Charniak (Charniak, 2001) identifies both syntactic structural and lexical dependencies that aid in language modeling.</S> | Reference Offset:  ['47','134'] | Reference Text:  <S sid = 47 ssid = >Thus only p(h j t,l, m, u, i), the distribution for the head of c, looks at two lexical items (i and h itself), and none of the distributions look at three lexical items as do the trigram distribution of Equation 4 and the previously discussed parsing language models [4, 15].</S><S sid = 134 ssid = >We have presented two grammar-based language models, both of which significantly improve upon both the trigram model baseline for the task (by 24% for the better of the two) and the best previous grammar-based language model (by 14%).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P01-1017.txt | Citing Article:  P04-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model (Charniak, 2001).</S> | Reference Offset:  ['68','79'] | Reference Text:  <S sid = 68 ssid = >We also empirically checked that our individual distributions (p(t 1 l, m, u, i), and p(h 1 t,l, m, u, i) from Equation 5 and p(L 1 l, t, h, m, u), p(M 1 l, t, h, m, u), and p(R 1 l, t, h, m, u) from Equation 5) sum to one for a large, random, selection of conditioning events2 As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established threshold) have their probabilities recomputed according to the complete probability model of Equation 5.</S><S sid = 79 ssid = >Note, however, that because our parser does not define probabilities for each word based upon previous words (as with trigram) it is not possible to do the integration at the word level.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P01-1017.txt | Citing Article:  P04-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These parses are then rescored using a lexicalized syntactic model (Charniak, 2001).</S> | Reference Offset:  ['68','69'] | Reference Text:  <S sid = 68 ssid = >We also empirically checked that our individual distributions (p(t 1 l, m, u, i), and p(h 1 t,l, m, u, i) from Equation 5 and p(L 1 l, t, h, m, u), p(M 1 l, t, h, m, u), and p(R 1 l, t, h, m, u) from Equation 5) sum to one for a large, random, selection of conditioning events2 As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established threshold) have their probabilities recomputed according to the complete probability model of Equation 5.</S><S sid = 69 ssid = >Both searches are conducted using dynamic programming.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P01-1017.txt | Citing Article:  P04-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent (Charniak, 2001).</S> | Reference Offset:  ['91','99'] | Reference Text:  <S sid = 91 ssid = >The most obvious extension would be to condition upon not just one’s parent’s head, but one’s grandparent’s as well.</S><S sid = 99 ssid = >The reader may remember that h is the head of the current constituent, while i is the head of its parent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P01-1017.txt | Citing Article:  P04-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001).</S> | Reference Offset:  ['41','70'] | Reference Text:  <S sid = 41 ssid = >We have taken the immediate-head parser described in [3] as our starting point.</S><S sid = 70 ssid = >The parser as described in the previous section was trained and tested on the data used in the previously described grammar-based language modeling research [4,15].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P01-1017.txt | Citing Article:  W04-0307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Charniak (Charniak, 2000) developed a state-of-the-art statistical CFG parser and then built an effective language model based on it (Charniak, 2001).</S> | Reference Offset:  ['1','15'] | Reference Text:  <S sid = 1 ssid = >We present two language models based upon an “immediate-head” parser — our name for a parser that conditions events below a constituent head of While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology.</S><S sid = 15 ssid = >In both cases the grammar based language model computes the probability of the next word based upon the previous words of the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P01-1017.txt | Citing Article:  W06-3122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our system embeds Phramer2 (used for minimum error rate training, decoding, decoding tools), Pharaoh (Koehn, 2004) (decoding), Carmel 3 (helper for Pharaoh in n-best generation), Charniak's parser (Charniak, 2001) (language model) and SRILM4 (n-gram LM construction).</S> | Reference Offset:  ['2','112'] | Reference Text:  <S sid = 2 ssid = >The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammarbased language model.</S><S sid = 112 ssid = >When we run the trigram and new grammar model in tandem we get a perplexity of 126.07, a reduction of 8% over the best previous tandem model and 24% over the best trigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P01-1017.txt | Citing Article:  W06-3122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used Charniak's parser as an additional LM (Charniak, 2001) in reranking.</S> | Reference Offset:  ['50','70'] | Reference Text:  <S sid = 50 ssid = >See [3] for all the details on the equations as well as the smoothing used. of one or more such symbols.</S><S sid = 70 ssid = >The parser as described in the previous section was trained and tested on the data used in the previously described grammar-based language modeling research [4,15].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P01-1017.txt | Citing Article:  P04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing.</S> | Reference Offset:  ['49','121'] | Reference Text:  <S sid = 49 ssid = >We break up a traditional probabilistic context-free grammar (PCFG) rule into a left-hand side with a label l(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence 1We simplify slightly in this section.</S><S sid = 121 ssid = >The grammar model is, in some sense, always better than the trigram model, but if the parser bungles the parse, then the grammar model is impacted very badly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P01-1017.txt | Citing Article:  P04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parses were automatically produced by the parser of Charniak (2001).</S> | Reference Offset:  ['18','124'] | Reference Text:  <S sid = 18 ssid = >Neither of these models uses an immediatehead parser.</S><S sid = 124 ssid = >Based upon a few observations on sentences from the development corpus for which the trigram model gave higher probabilities we hypothesized that reason (3), bungled parses, is primary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P01-1017.txt | Citing Article:  P04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al (2002)).</S> | Reference Offset:  ['111','112'] | Reference Text:  <S sid = 111 ssid = >We see that the grammar perplexity is reduced to 130.20, a reduction of 10% over our first model, 14% over the previous best grammar model (152.26%), and 22% over the best of the above trigram models for the task (167.02).</S><S sid = 112 ssid = >When we run the trigram and new grammar model in tandem we get a perplexity of 126.07, a reduction of 8% over the best previous tandem model and 24% over the best trigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P01-1017.txt | Citing Article:  P04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hall (2003) is a lattice-parser related to Charniak (2001).</S> | Reference Offset:  ['13','18'] | Reference Text:  <S sid = 13 ssid = >The research presented in this paper is most closely related to two previous efforts, that by Chelba and Jelinek [4] (C&J) and that by Roark [15], and this review concentrates on these two papers.</S><S sid = 18 ssid = >Neither of these models uses an immediatehead parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P01-1017.txt | Citing Article:  P04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system.</S> | Reference Offset:  ['79','81'] | Reference Text:  <S sid = 79 ssid = >Note, however, that because our parser does not define probabilities for each word based upon previous words (as with trigram) it is not possible to do the integration at the word level.</S><S sid = 81 ssid = >This is a much less powerful technique than the word-level interpolation used by both C&J and Roark, but we still observe a significant gain in performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P01-1017.txt | Citing Article:  P04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another contributing factor to the accuracy of Charniak (2001) is the size of the training set: 20M words larger than that used in this work.</S> | Reference Offset:  ['73','119'] | Reference Text:  <S sid = 73 ssid = >As in previous work, files F0 to F20 are used for training, F21-F22 for development, and F23-F24 for testing.</S><S sid = 119 ssid = >The grammar model and the trigram model capture different facts about the distribution of words in the language, and for some set of sentences one distribution will perform better than the other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P01-1017.txt | Citing Article:  P05-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniaket al, 2001).</S> | Reference Offset:  ['10','15'] | Reference Text:  <S sid = 10 ssid = >Virtually all current speech recognition systems use the so-called trigram language model in which the probability of a string is broken down into conditional probabilities on each word given On the other hand, in the last few years there has been interest in designing language models based upon parsing and Equation 2.</S><S sid = 15 ssid = >In both cases the grammar based language model computes the probability of the next word based upon the previous words of the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P01-1017.txt | Citing Article:  W04-2609.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Noun noun (adjective noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak, 2001) (for XWN we used the gold parse trees).</S> | Reference Offset:  ['22','104'] | Reference Text:  <S sid = 22 ssid = >Sometimes the immediate head of a constituent occurs after it (e.g, in noun-phrases, where the head is typically the rightmost noun) and thus is not available for conditioning by a strict left-to-right parser.</S><S sid = 104 ssid = >This is not true, but Lauer [11] shows that about two-thirds of all branching in base-noun-phrases is leftward.</S> | Discourse Facet:  NA | Annotator: Automatic


