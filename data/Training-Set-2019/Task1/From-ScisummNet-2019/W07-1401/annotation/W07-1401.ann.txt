Citance Number: 1 | Reference Article:  W07-1401.txt | Citing Article:  C08-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entailment framework (Giampiccolo et al, 2007).</S> | Reference Offset:  ['8','9'] | Reference Text:  <S sid = 8 ssid = >For example, in addition to textual entailment, QA systems need to handle issues such as answer retrieval and question type recognition.</S><S sid = 9 ssid = >By separating out the general problem of textual entailment from these task-specific problems, progress on semantic inference for many application areas can be promoted.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W07-1401.txt | Citing Article:  P12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007).</S> | Reference Offset:  ['0','54'] | Reference Text:  <S sid = 0 ssid = >The Third PASCAL Recognizing Textual Entailment Challenge</S><S sid = 54 ssid = >§ The hypothesis must be fully entailed by the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W07-1401.txt | Citing Article:  P12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recognizing textual entailment is to determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al, 2007).</S> | Reference Offset:  ['0','77'] | Reference Text:  <S sid = 0 ssid = >The Third PASCAL Recognizing Textual Entailment Challenge</S><S sid = 77 ssid = >Then they picked sentence pairs with high lexical overlap, preferably where at least one of the sentences was taken from the summary (this sentence usually played the role of t).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W07-1401.txt | Citing Article:  E09-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The average accuracy of the systems in the RTE-3 challenge is around 61% (Giampiccolo et al, 2007).</S> | Reference Offset:  ['128','131'] | Reference Text:  <S sid = 128 ssid = >In fact, the average accuracy achieved in the QA setting (0.71) was 20 points higher than that achieved in the IE setting (0.52); the average accuracy in the IR and Sum settings was 0.66 and 0.58 respectively.</S><S sid = 131 ssid = >As for the LONG pairs, which represented a new element of this year’s challenge, no substantial difference was noted in the systems’ performances: the average accuracy over the long pairs was 58.72%, compared to 61.93% over the short ones.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W07-1401.txt | Citing Article:  P10-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of course, if examples were also annotated with explanations in a consistent format, this could form the basis of a new evaluation of the kind essayed in the pilot study in (Giampiccolo et al, 2007).</S> | Reference Offset:  ['119','120'] | Reference Text:  <S sid = 119 ssid = >Burchardt et al. ), Verbnet (Bobrow et al.) and Propbank (e.g.</S><S sid = 120 ssid = >Adams et al.).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W07-1401.txt | Citing Article:  N10-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al, 2006), RTE3 (Giampiccolo et al, 2007), and RTE5, along with the standard split between training and test sets.</S> | Reference Offset:  ['0','124'] | Reference Text:  <S sid = 0 ssid = >The Third PASCAL Recognizing Textual Entailment Challenge</S><S sid = 124 ssid = >Delmonte, Bar-Haim et al., Iftene and Balahur-Dobrescu).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W07-1401.txt | Citing Article:  N10-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the best systems in RTE2 and RTE3 (Giampiccolo et al, 2007) have an accuracy 10% higher than the others but they generally use resources that are not publicly available.</S> | Reference Offset:  ['122','125'] | Reference Text:  <S sid = 122 ssid = >However, as in RTE-2, the use of large semantic resources is still a crucial factor affecting the performance of systems (see, for instance, the use of a large corpus of entailment examples in Hickl and Bensley).</S><S sid = 125 ssid = >The accuracy achieved by the participating systems ranges from 49% to 80% (considering the best run of each group), while most of the systems obtained a score in between 59% and 66%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W07-1401.txt | Citing Article:  D10-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These features include whether the two strings are the same, two terms have the same stem, the similarity between the two terms either based on WordNet or distributional statistics (Lin, 1998). To learn the alignment model for nouns, we annotated the noun alignments for the development data used in PASCAL RTE-3 Challenge (Giampiccolo et al., 2007) and trained a logistic regression model based on the features in Table 1.</S> | Reference Offset:  ['112','139'] | Reference Text:  <S sid = 112 ssid = >On the other hand, RTE-3 confirmed that both machine learning using lexical-syntactic features and transformation-based approaches on dependency representations are well consolidated techniques to address textual entailment.</S><S sid = 139 ssid = >In particular, visible progress in defining several new principled scenarios for RTE was represented, such as Hickl’s commitment-based approach, Bar Haim’s proof system, Harmeling’s probabilistic model, and Standford’s use of Natural Logic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W07-1401.txt | Citing Article:  P10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A first step towards a more comprehensive notion of entailment was taken with RTE-3 (Giampiccolo et al,2007), when paragraph-length texts were first included and constituted 17% of the texts in the test set.</S> | Reference Offset:  ['23','44'] | Reference Text:  <S sid = 23 ssid = >In particular, a limited number of longer texts, i.e. up to a paragraph in length, were incorporated in order to move toward more comprehensive scenarios, which incorporate the need for discourse analysis.</S><S sid = 44 ssid = >In the test set they were about 17% of the total.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W07-1401.txt | Citing Article:  E09-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Typical examples of such relations are given in (Giampiccolo et al, 2007) or those holding between question and answer.</S> | Reference Offset:  ['68','69'] | Reference Text:  <S sid = 68 ssid = >Then they transformed the question-answer pairs into t-h pairs as follows: § An answer term of the expected answer type was picked from the answer passage either a correct or an incorrect one.</S><S sid = 69 ssid = >§ The question was turned into an affirmative sentence plugging in the answer term.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W07-1401.txt | Citing Article:  P09-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a second application-oriented evaluation we measured the contributions of our (filtered) Wikipedia resource and WordNet to RTE inference (Giampiccolo et al, 2007).</S> | Reference Offset:  ['88','117'] | Reference Text:  <S sid = 88 ssid = >As in previous challenges, pairs on which the annotators disagreed were filtered-out.</S><S sid = 117 ssid = >Extended WordNet is also a common resource (for instance in Iftene and BalahurDobrescu) and the Extended Wordnet Knowledge Base has been successfully used in (Tatu and Moldovan).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W07-1401.txt | Citing Article:  C10-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In RTE-3 (Giampiccolo et al, 2007), where some paragraph-long texts were included, inter sentential relations became relevant for correct inference.</S> | Reference Offset:  ['119','120'] | Reference Text:  <S sid = 119 ssid = >Burchardt et al. ), Verbnet (Bobrow et al.) and Propbank (e.g.</S><S sid = 120 ssid = >Adams et al.).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W07-1401.txt | Citing Article:  C10-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For semantically oriented tools such as SRL systems, it is important to also assess their results w.r.t. the task which they are meant support namely reasoning: Do the semantic representations built by SRL help in making the correct inferences? Can they be used, for instance, to determine whether a given sentence answers a given question? or whether the content of one sentence follow from that another? As explained in (Giampiccolo et al, 2007), entailment recognition is a first, major step towards answering these questions.</S> | Reference Offset:  ['67','81'] | Reference Text:  <S sid = 67 ssid = >For the QA (Question Answering) task, annotators used questions taken from the datasets of official QA competitions, such as TREC QA and QA@CLEF datasets, and the corresponding answers extracted from the Web by actual QA systems.</S><S sid = 81 ssid = >Each SCU is identified by a label, a sentence in natural language which expresses the content.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W07-1401.txt | Citing Article:  C10-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['55','150'] | Reference Text:  <S sid = 55 ssid = >Judgment must be NO if the hypothesis includes parts that cannot be inferred from the text.</S><S sid = 150 ssid = >We also thank David Askey, who helped manage the RTE 3 website.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W07-1401.txt | Citing Article:  W09-2502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >see the reports on RTE-1 (Dagan et al, 2005), RTE-2 (Bar-Haim et al, 2006), RTE-3 (Giampiccolo et al, 2007), the RTE-3 PILOT (Voorhees, 2008), RTE-4 (Giampicolo et al, 2008), and RTE-5 (TAC, 2009) The problem of bias is quite general and widely known.</S> | Reference Offset:  ['119','120'] | Reference Text:  <S sid = 119 ssid = >Burchardt et al. ), Verbnet (Bobrow et al.) and Propbank (e.g.</S><S sid = 120 ssid = >Adams et al.).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W07-1401.txt | Citing Article:  C10-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Given text T and hypothesis H, the task consists on determining whether or not H can be inferred by T (Giampiccolo et al, 2007). CSR axioms Several examples of the RTE3challenge can be solved by applying CSR (Table 5). The rest of this section depicts the axioms involved in detecting entailment for each pair.</S> | Reference Offset:  ['53','55'] | Reference Text:  <S sid = 53 ssid = >In choosing the pairs, the following judgment criteria and guidelines were considered: § As entailment is a directional relation, the hypothesis must be entailed by the given text, but the text need not be entailed by the hypothesis.</S><S sid = 55 ssid = >Judgment must be NO if the hypothesis includes parts that cannot be inferred from the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W07-1401.txt | Citing Article:  P08-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The RTE main task addressed this issue by including a candidate entailment pair in the test set only if multiple annotators agreed on its disposition (Giampiccolo et al, 2007).</S> | Reference Offset:  ['44','87'] | Reference Text:  <S sid = 44 ssid = >In the test set they were about 17% of the total.</S><S sid = 87 ssid = >Each pair of the dataset was judged by three annotators.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W07-1401.txt | Citing Article:  P08-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >RTE organizers reported an agreement rate of about 88% among their annotators for the two-way task (Giampiccolo et al, 2007).</S> | Reference Offset:  ['89','119'] | Reference Text:  <S sid = 89 ssid = >On the test set, the average agreement between each pair of annotators who shared at least 100 examples was 87.8%, with an average Kappa level of 0.75, regarded as substantial agreement according to Landis and Koch (1997).</S><S sid = 119 ssid = >Burchardt et al. ), Verbnet (Bobrow et al.) and Propbank (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W07-1401.txt | Citing Article:  P09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, the ranking may be combined with target language information in order to choose the best translation, thus improving translation quality. We position the problem of generating alternative texts for translation within the Textual Entailment (TE) framework (Giampiccolo et al, 2007).</S> | Reference Offset:  ['6','119'] | Reference Text:  <S sid = 6 ssid = >In the recent years, this task has raised great interest since applied semantic inference concerns many practical Natural Language Processing (NLP) applications, such as Question Answering (QA), Information Extraction (IE), Summarization, Machine Translation and Paraphrasing, and certain types of queries in Information Retrieval (IR).</S><S sid = 119 ssid = >Burchardt et al. ), Verbnet (Bobrow et al.) and Propbank (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W07-1401.txt | Citing Article:  P09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Textual Entailment (TE) has recently become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications (Giampiccolo et al., 2007).</S> | Reference Offset:  ['9','41'] | Reference Text:  <S sid = 9 ssid = >By separating out the general problem of textual entailment from these task-specific problems, progress on semantic inference for many application areas can be promoted.</S><S sid = 41 ssid = >The textual entailment recognition task required the participating systems to decide, given two text snippets t and h, whether t entails h. Textual entailment is defined as a directional relation between two text fragments, called text (t, the entailing text), and hypothesis (h, the entailed text), so that a human being, with common understanding of language and common background knowledge, can infer that h is most likely true on the basis of the content of t. As in the previous challenges, the RTE-3 dataset consisted of 1600 text-hypothesis pairs, equally divided into a development set and a test set.</S> | Discourse Facet:  NA | Annotator: Automatic


