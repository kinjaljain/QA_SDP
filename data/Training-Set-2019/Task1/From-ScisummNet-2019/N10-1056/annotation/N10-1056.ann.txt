Citance Number: 1 | Reference Article:  N10-1056.txt | Citing Article:  P11-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles.</S> | Reference Offset:  ['1','9'] | Reference Text:  <S sid = 1 ssid = >We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.</S><S sid = 9 ssid = >The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N10-1056.txt | Citing Article:  S12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia</S><S sid = 8 ssid = >One major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N10-1056.txt | Citing Article:  S12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010).</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia</S><S sid = 8 ssid = >One major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N10-1056.txt | Citing Article:  P11-4017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006).</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia</S><S sid = 21 ssid = >We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ “Cat”) and a sequence dk of article versions caused by successive edits.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N10-1056.txt | Citing Article:  D11-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories.</S> | Reference Offset:  ['9','11'] | Reference Text:  <S sid = 9 ssid = >The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.</S><S sid = 11 ssid = >Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N10-1056.txt | Citing Article:  D11-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data.</S> | Reference Offset:  ['2','11'] | Reference Text:  <S sid = 2 ssid = >We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.</S><S sid = 11 ssid = >Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N10-1056.txt | Citing Article:  D11-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account.</S> | Reference Offset:  ['9','11'] | Reference Text:  <S sid = 9 ssid = >The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.</S><S sid = 11 ssid = >Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N10-1056.txt | Citing Article:  W11-1601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system.</S> | Reference Offset:  ['11','65'] | Reference Text:  <S sid = 11 ssid = >Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.</S><S sid = 65 ssid = >He provides a list of 17,900 simple words — words that do not need further simplification — and a list of 2000 transformation pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N10-1056.txt | Citing Article:  S12-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs.</S> | Reference Offset:  ['1','8'] | Reference Text:  <S sid = 1 ssid = >We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.</S><S sid = 8 ssid = >One major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N10-1056.txt | Citing Article:  P14-2066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010).</S> | Reference Offset:  ['12','13'] | Reference Text:  <S sid = 12 ssid = >Simplification is strongly related to but distinct from paraphrasing and machine translation (MT).</S><S sid = 13 ssid = >While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N10-1056.txt | Citing Article:  W12-4006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications.</S> | Reference Offset:  ['1','27'] | Reference Text:  <S sid = 1 ssid = >We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.</S><S sid = 27 ssid = >(We defer detailed description of how we extract lexical edit instances from data to §3.1.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N10-1056.txt | Citing Article:  P13-1162.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010).</S> | Reference Offset:  ['13','55'] | Reference Text:  <S sid = 13 ssid = >While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing).</S><S sid = 55 ssid = >We obtained the revision histories of both SimpleEW (November 2009 snapshot) and ComplexEW (January 2008 snapshot).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N10-1056.txt | Citing Article:  P12-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010).</S> | Reference Offset:  ['1','4'] | Reference Text:  <S sid = 1 ssid = >We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.</S><S sid = 4 ssid = >Nothing is more simple than greatness; indeed, to be simple is to be great.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N10-1056.txt | Citing Article:  P11-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia.</S> | Reference Offset:  ['1','8'] | Reference Text:  <S sid = 1 ssid = >We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.</S><S sid = 8 ssid = >One major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N10-1056.txt | Citing Article:  E12-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification.</S> | Reference Offset:  ['10','42'] | Reference Text:  <S sid = 10 ssid = >Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications.</S><S sid = 42 ssid = >We therefore sought to use comments to identify “trusted” revisions wherein the extracted lexical edit instances (see §3.1) would be likely to be simplifications.</S> | Discourse Facet:  NA | Annotator: Automatic


