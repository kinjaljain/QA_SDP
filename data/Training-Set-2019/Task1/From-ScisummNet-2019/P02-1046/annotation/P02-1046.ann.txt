Citance Number: 1 | Reference Article:  P02-1046.txt | Citing Article:  W03-0201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A bootstrapping approach using machine learning is a possible alternative that will be explored in the future (Abney 2002).</S> | Reference Offset:  ['0','65'] | Reference Text:  <S sid = 0 ssid = >Bootstrapping</S><S sid = 65 ssid = >However, three other cases are possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1046.txt | Citing Article:  P12-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis.</S> | Reference Offset:  ['47','56'] | Reference Text:  <S sid = 47 ssid = >They show that, if view independence is satisfied, then the agreement rate between opposing-view rules F and G upper bounds the error of F (or G).</S><S sid = 56 ssid = >Theorem 2 states that disagreement upper bounds error.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1046.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption.</S> | Reference Offset:  ['18','155'] | Reference Text:  <S sid = 18 ssid = >I show that that independence assumption is remarkably powerful, and violated in the data; however, I show that a weaker assumption suffices.</S><S sid = 155 ssid = >The first assumption is precision independence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1046.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations.</S> | Reference Offset:  ['101','185'] | Reference Text:  <S sid = 101 ssid = >In this section, we introduce a weaker assumption, one that is satisfied by the data, and we show that theorem 2 holds under this weaker assumption.</S><S sid = 185 ssid = >We have also given a theoretical analysis of the Yarowsky algorithm for the first time, and shown that it can be justified by an independence assumption that is quite distinct from the independence assumption that co-training is based on.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1046.txt | Citing Article:  P03-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dasgupta et al (2001) and Abney (2002) conducted theoretical analyses on the performance (generalization error) of co-training.</S> | Reference Offset:  ['46','137'] | Reference Text:  <S sid = 46 ssid = >The second lack is emended in (Dasgupta et al., 2001).</S><S sid = 137 ssid = >The following theorem is based on (Dasgupta et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1046.txt | Citing Article:  P03-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Abney (2002) refined Dasgupta et als result by relaxing the view independence assumption with a new constraint.</S> | Reference Offset:  ['17','155'] | Reference Text:  <S sid = 17 ssid = >First, (Dasgupta et al., 2001) assume the same conditional independence assumption as proposed by Blum and Mitchell.</S><S sid = 155 ssid = >The first assumption is precision independence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1046.txt | Citing Article:  W03-0403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Rather than comparing the two learners on whether they categorically select the same preferred parse on a number of examples, we can view active learning as the inverse of agreement-based co-training (Abney, 2002).</S> | Reference Offset:  ['9','47'] | Reference Text:  <S sid = 9 ssid = >They also give an intuitive explanation of why co-training works, in terms of maximizing agreement on unlabeled data between classifiers based on different “views” of the data.</S><S sid = 47 ssid = >They show that, if view independence is satisfied, then the agreement rate between opposing-view rules F and G upper bounds the error of F (or G).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1046.txt | Citing Article:  N03-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Further avenues to explore include the development of selection methods to efficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al (2002) and Abney (2002).</S> | Reference Offset:  ['14','137'] | Reference Text:  <S sid = 14 ssid = >In recent work, (Dasgupta et al., 2001) prove that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on a different “view” of the data.</S><S sid = 137 ssid = >The following theorem is based on (Dasgupta et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1046.txt | Citing Article:  W03-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We have implemented the Greedy Agreement Algorithm (Abney, 2002) which, based on two independent views of the data, is able to learn two binary classifiers from a set of hand-typed seed rules.</S> | Reference Offset:  ['9','75'] | Reference Text:  <S sid = 9 ssid = >They also give an intuitive explanation of why co-training works, in terms of maximizing agreement on unlabeled data between classifiers based on different “views” of the data.</S><S sid = 75 ssid = >Let F and G be arbitrary rules based on independent views.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1046.txt | Citing Article:  W03-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See (Abney, 2002) for a formal proof that this algorithm tends to gradually reduce the classification error given the adequate seed rules.</S> | Reference Offset:  ['50','129'] | Reference Text:  <S sid = 50 ssid = >I give a geometric proof sketch here; the reader is referred to the original paper for a formal proof.</S><S sid = 129 ssid = >It begins with two seed rules, one for each view.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1046.txt | Citing Article:  W03-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In fact, results are reported to be competitive against more sophisticated methods (Co-DL, Co Boost, etc.) for this specific task in (Abney, 2002).</S> | Reference Offset:  ['1','120'] | Reference Text:  <S sid = 1 ssid = >disambiguation rivaling supervised methods.</S><S sid = 120 ssid = >In fact, we can show Hence, in particular, we may write dy = a − b.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1046.txt | Citing Article:  W03-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Third, how the algorithm, presented in (Abney, 2002) for binary classification, can be extended to a multi class problem.</S> | Reference Offset:  ['26','27'] | Reference Text:  <S sid = 26 ssid = >A binary rule F can be thought of as the characteristic function of the set of instances {x : F(x) = +}.</S><S sid = 27 ssid = >Multi-class rules also define useful sets when a particular target class t is understood.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1046.txt | Citing Article:  W03-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These results are comparable to the ones presented in (Abney, 2002), taking into account, apart from the language change, that we have introduced a fourth class to be treated the same as the other three.</S> | Reference Offset:  ['6','27'] | Reference Text:  <S sid = 6 ssid = >The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics.</S><S sid = 27 ssid = >Multi-class rules also define useful sets when a particular target class t is understood.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1046.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T.</S> | Reference Offset:  ['94','179'] | Reference Text:  <S sid = 94 ssid = >The precision of F is P(Y |F).</S><S sid = 179 ssid = >Intuitively, the Yarowsky algorithm increases recall while holding precision above a threshold that represents the desired precision of the final classifier.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1046.txt | Citing Article:  W04-2402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence.</S> | Reference Offset:  ['71','110'] | Reference Text:  <S sid = 71 ssid = >Rule independence is a very strong assumption; one remarkable consequence will show just how strong it is.</S><S sid = 110 ssid = >If p1 = 0.5, then weak rule dependence reduces to independence: if p1 = 0.5 and weak rule dependence is satisfied, then dy must be 0, which is to say, F and G must be conditionally independent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1046.txt | Citing Article:  W09-2202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although this result was previously observed in a different context by Abney in (Abney, 2002), he does not use it to derive a semi-supervised learning algorithm.</S> | Reference Offset:  ['1','12'] | Reference Text:  <S sid = 1 ssid = >disambiguation rivaling supervised methods.</S><S sid = 12 ssid = >The proof they give does not actually apply directly to the co-training algorithm, nor does it directly justify the intuitive account in terms of classifier agreement on unlabeled data, nor, for that matter, does the co-training algorithm directly seek to find classifiers that agree on unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1046.txt | Citing Article:  W06-1624.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Semantic Classifiers Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002).</S> | Reference Offset:  ['5','154'] | Reference Text:  <S sid = 5 ssid = >The term bootstrapping here refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.</S><S sid = 154 ssid = >We write G∗ for the set of instances labeled by the current classifier, that is, {x : G(x) =� +}.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1046.txt | Citing Article:  E06-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.</S> | Reference Offset:  ['5','154'] | Reference Text:  <S sid = 5 ssid = >The term bootstrapping here refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.</S><S sid = 154 ssid = >We write G∗ for the set of instances labeled by the current classifier, that is, {x : G(x) =� +}.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1046.txt | Citing Article:  N03-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Abney (2002) argues that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices.</S> | Reference Offset:  ['18','155'] | Reference Text:  <S sid = 18 ssid = >I show that that independence assumption is remarkably powerful, and violated in the data; however, I show that a weaker assumption suffices.</S><S sid = 155 ssid = >The first assumption is precision independence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1046.txt | Citing Article:  N03-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Co-training algorithms such as CoBoost (Collins and Singer, 1999) and Greedy Agreement (Abney, 2002) that explicitly trade classifier agreement on unlabeled data against error on labeled data may be more robust to the underlying assumptions of co-training and can conceivably perform better than the Blum and Mitchell algorithm for problems without a natural feature split.</S> | Reference Offset:  ['45','145'] | Reference Text:  <S sid = 45 ssid = >The algorithm that Blum and Mitchell describe does not explicitly search for rules with good agreement; nor does agreement rate play any direct role in the learnability proof given in the Blum and Mitchell paper.</S><S sid = 145 ssid = >All four algorithms essentially perform equally well; the advantage of the greedy agreement algorithm is that we have an explanation for why it performs well.</S> | Discourse Facet:  NA | Annotator: Automatic


