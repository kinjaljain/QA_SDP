Citance Number: 1 | Reference Article:  P99-1069.txt | Citing Article:  P00-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale.</S> | Reference Offset:  ['20','71'] | Reference Text:  <S sid = 20 ssid = >Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).</S><S sid = 71 ssid = >The problem of estimating parameters for log-linear models is not new.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P99-1069.txt | Citing Article:  P00-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model.</S> | Reference Offset:  ['110','114'] | Reference Text:  <S sid = 110 ssid = >Table 1 summarizes the basic properties of these corpora.</S><S sid = 114 ssid = >Table 2 describes the properties of the features used for each corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P99-1069.txt | Citing Article:  P00-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000).</S> | Reference Offset:  ['68','72'] | Reference Text:  <S sid = 68 ssid = >In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992).</S><S sid = 72 ssid = >It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P99-1069.txt | Citing Article:  D11-1148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000).</S> | Reference Offset:  ['105','113'] | Reference Text:  <S sid = 105 ssid = >We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).</S><S sid = 113 ssid = >Because slightly different grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P99-1069.txt | Citing Article:  P04-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)).</S> | Reference Offset:  ['96','103'] | Reference Text:  <S sid = 96 ssid = >This is an instance of over fitting, and it can be addressed, as is customary, by adding a regularization term that promotes small values of 0 to the objective function.</S><S sid = 103 ssid = >This suggests another possible objective function: choose 0 to maximize the number Co (c3) of times the maximum likelihood parse (under 0) is in fact the correct parse, in the training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P99-1069.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000).</S> | Reference Offset:  ['10','73'] | Reference Text:  <S sid = 10 ssid = >Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.</S><S sid = 73 ssid = >Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P99-1069.txt | Citing Article:  N06-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999).</S> | Reference Offset:  ['113','132'] | Reference Text:  <S sid = 113 ssid = >Because slightly different grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below.</S><S sid = 132 ssid = >This paper described a log-linear model for SUBGs and evaluated two estimators for such models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P99-1069.txt | Citing Article:  W03-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999).</S> | Reference Offset:  ['94','95'] | Reference Text:  <S sid = 94 ssid = >Such infinite parameter values indicate that the model treats pseudo-maximal features categorically; i.e., any parse with a non-maximal feature value is assigned a zero conditional probability.</S><S sid = 95 ssid = >Of course, a feature which is pseudo-maximal over the training corpus is not necessarily pseudo-maximal for all yields.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P99-1069.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large.</S> | Reference Offset:  ['33','102'] | Reference Text:  <S sid = 33 ssid = >For example, the adjunct attachment ambiguity in (1) results in alternative syntactic structures which use the same productions the same number of times in each derivation, so a model with only production features would necessarily assign them the same likelihood.</S><S sid = 102 ssid = >One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P99-1069.txt | Citing Article:  W07-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as.</S> | Reference Offset:  ['54','69'] | Reference Text:  <S sid = 54 ssid = >The derivation trees of the PCFG can be mapped onto a set containing all of the SUBG's syntactic analyses.</S><S sid = 69 ssid = >Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P99-1069.txt | Citing Article:  N04-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999).</S> | Reference Offset:  ['1','20'] | Reference Text:  <S sid = 1 ssid = >Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.</S><S sid = 20 ssid = >Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P99-1069.txt | Citing Article:  W04-3202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999).</S> | Reference Offset:  ['1','20'] | Reference Text:  <S sid = 1 ssid = >Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.</S><S sid = 20 ssid = >Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P99-1069.txt | Citing Article:  C02-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','140'] | Reference Text:  <S sid = 43 ssid = >There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).</S><S sid = 140 ssid = >In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P99-1069.txt | Citing Article:  C02-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use.</S> | Reference Offset:  ['68','102'] | Reference Text:  <S sid = 68 ssid = >In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992).</S><S sid = 102 ssid = >One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P99-1069.txt | Citing Article:  W06-1661.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i).</S> | Reference Offset:  ['62','67'] | Reference Text:  <S sid = 62 ssid = >This motivates an alternative strategy involving a data-based estimate of E9(f3): where y(w) is the yield belonging to the syntactic analysis w, and yi = y(wi) is the yield belonging to the i'th sample in the training corpus.</S><S sid = 67 ssid = >Of course (6) is no longer the gradient of the likelihood function, but fortunately it is (exactly) the gradient of (the log of) another criterion: Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P99-1069.txt | Citing Article:  C02-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)).</S> | Reference Offset:  ['20','71'] | Reference Text:  <S sid = 20 ssid = >Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).</S><S sid = 71 ssid = >The problem of estimating parameters for log-linear models is not new.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P99-1069.txt | Citing Article:  W07-1203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','140'] | Reference Text:  <S sid = 43 ssid = >There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).</S><S sid = 140 ssid = >In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P99-1069.txt | Citing Article:  W03-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999).</S> | Reference Offset:  ['96','105'] | Reference Text:  <S sid = 96 ssid = >This is an instance of over fitting, and it can be addressed, as is customary, by adding a regularization term that promotes small values of 0 to the objective function.</S><S sid = 105 ssid = >We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P99-1069.txt | Citing Article:  W02-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999).</S> | Reference Offset:  ['67','82'] | Reference Text:  <S sid = 67 ssid = >Of course (6) is no longer the gradient of the likelihood function, but fortunately it is (exactly) the gradient of (the log of) another criterion: Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields.</S><S sid = 82 ssid = >As for pseudo-likelihood: So that maximizing pseudo-likelihood (at large samples) amounts to minimizing the average (over yields) divergence between the true and estimated conditional distributions of analyses given yields.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P99-1069.txt | Citing Article:  W02-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism).</S> | Reference Offset:  ['43','97'] | Reference Text:  <S sid = 43 ssid = >There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).</S><S sid = 97 ssid = >A common choice is to add a quadratic to the log-likelihood, which corresponds to multiplying the likelihood itself by a normal distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


