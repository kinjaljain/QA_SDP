Citance Number: 1 | Reference Article:  J07-2003.txt | Citing Article:  P14-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See (Chiang, 2007) for more details.</S> | Reference Offset:  ['109','133'] | Reference Text:  <S sid = 109 ssid = >We now discuss the details of the decoder, focusing attention on efficiently calculating English language-model probabilities for possible translations, which is the primary technical challenge.</S><S sid = 133 ssid = >We describe here how to do this using the lazy algorithm of Huang and Chiang (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J07-2003.txt | Citing Article:  W11-2166.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this section, we express the hierarchical phrase based extraction technique of (Chiang, 2007) as an extraction program.</S> | Reference Offset:  ['0','50'] | Reference Text:  <S sid = 0 ssid = >Hierarchical Phrase-Based Translation</S><S sid = 50 ssid = >The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J07-2003.txt | Citing Article:  W11-2166.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, (Chiang, 2007) lists six criteria that he uses in practice to restrict the generation of Hiero rules.</S> | Reference Offset:  ['240','244'] | Reference Text:  <S sid = 240 ssid = >The other baseline, which we call Hiero Monotone, is the same as Hiero except with the limitation that extracted rules cannot have any nonterminal symbols on their righthand sides.</S><S sid = 244 ssid = >We performed minimum-error-rate training separately on Hiero and Hiero Monotone to maximize their BLEU scores on the development set; the feature weights for Hiero are shown in Table 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J07-2003.txt | Citing Article:  W11-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language.</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Hierarchical Phrase-Based Translation</S><S sid = 15 ssid = >Following convention, we call the source language “French” and the target language “English”; the translation of a French sentence f into an English sentence e is modeled as: The phrase-based translation model P( f  |e) “encodes” e into f by the following steps: Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J07-2003.txt | Citing Article:  P12-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope.</S> | Reference Offset:  ['0','6'] | Reference Text:  <S sid = 0 ssid = >Hierarchical Phrase-Based Translation</S><S sid = 6 ssid = >We present a statistical machine translation model that uses hierarchical phrases—phrases that contain subphrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J07-2003.txt | Citing Article:  P12-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate the distribution of these rules in the same way as Chiang (2007).</S> | Reference Offset:  ['4','9'] | Reference Text:  <S sid = 4 ssid = >We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.</S><S sid = 9 ssid = >We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J07-2003.txt | Citing Article:  P12-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent.</S> | Reference Offset:  ['60','229'] | Reference Text:  <S sid = 60 ssid = >At each step, two linked nonterminals are rewritten using the two components of a single rule.</S><S sid = 229 ssid = >We tested the rescoring method (k = 103 and 104), the intersection method, and the cube-pruning method (e = 0, 0.1, and 0.2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J07-2003.txt | Citing Article:  P12-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compared our loose decoder and tight decoder with our in-house hierarchical phrase-based decoder (Chiang, 2007) and the tree-to-string decoder (Liu et al., 2006).</S> | Reference Offset:  ['233','235'] | Reference Text:  <S sid = 233 ssid = >Finally, the cubepruning decoder runs almost as fast as the rescoring decoder and translates almost as well as the intersecting decoder.</S><S sid = 235 ssid = >We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J07-2003.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right.</S> | Reference Offset:  ['15','233'] | Reference Text:  <S sid = 15 ssid = >Following convention, we call the source language “French” and the target language “English”; the translation of a French sentence f into an English sentence e is modeled as: The phrase-based translation model P( f  |e) “encodes” e into f by the following steps: Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002).</S><S sid = 233 ssid = >Finally, the cubepruning decoder runs almost as fast as the rescoring decoder and translates almost as well as the intersecting decoder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J07-2003.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn 2 |V | g−1 ) where n is the sentence length and |V | is the English vocabulary size (Huang and Chiang, 2007).</S> | Reference Offset:  ['129','133'] | Reference Text:  <S sid = 129 ssid = >This gives the decoding algorithm an asymptotic time complexity of O(n).</S><S sid = 133 ssid = >We describe here how to do this using the lazy algorithm of Huang and Chiang (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J07-2003.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007).</S> | Reference Offset:  ['3','8'] | Reference Text:  <S sid = 3 ssid = >Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.</S><S sid = 8 ssid = >Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J07-2003.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following previous work (Chiang, 2007), we assume a constant number of English translations for each foreign word in the input sentence, so |V|= O (n).</S> | Reference Offset:  ['65','170'] | Reference Text:  <S sid = 65 ssid = >We then extract from each word-aligned sentence pair a set of rules that are consistent with the word alignments.</S><S sid = 170 ssid = >First, assume that the LM expects a whole sentence to be preceded by (m − 1) start-of-sentence symbols (s) and followed by a single end-of-sentence symbol (/s).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J07-2003.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The work of Watanabe et al (2006) is closest in spirit to ours: they also design an incremental decoding algorithm, but for the hierarchical phrase-based system (Chiang, 2007) instead.</S> | Reference Offset:  ['0','235'] | Reference Text:  <S sid = 0 ssid = >Hierarchical Phrase-Based Translation</S><S sid = 235 ssid = >We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J07-2003.txt | Citing Article:  W11-2160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A Hiero grammar (Chiang, 2007) is an SCFG with only one type of nonterminal symbol, traditionally labeled X.</S> | Reference Offset:  ['57','240'] | Reference Text:  <S sid = 57 ssid = >In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: where X is a nonterminal, γ and α are both strings of terminals and nonterminals, and — is a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α.</S><S sid = 240 ssid = >The other baseline, which we call Hiero Monotone, is the same as Hiero except with the limitation that extracted rules cannot have any nonterminal symbols on their righthand sides.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J07-2003.txt | Citing Article:  W11-2160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chiang (2007) gives reasonable heuristic choices for these parameters when extracting a Hiero grammar, and Lopez (2008) confirms some of them (maximum rule span of 10, maximum number of sourceside symbols at 5, and maximum number of nonterminals at 2 per rule).</S> | Reference Offset:  ['73','105'] | Reference Text:  <S sid = 73 ssid = >For example, given the initial phrases shown in Figure 2b, we could form the rule where k is an index not used in γ and α, is a rule of (f, e, —).</S><S sid = 105 ssid = >If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J07-2003.txt | Citing Article:  N09-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed.</S> | Reference Offset:  ['181','189'] | Reference Text:  <S sid = 181 ssid = >5.3.3 Pruning.</S><S sid = 189 ssid = >5.3.4 Cube Pruning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J07-2003.txt | Citing Article:  N09-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Venugopal et al (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007).</S> | Reference Offset:  ['189','235'] | Reference Text:  <S sid = 189 ssid = >5.3.4 Cube Pruning.</S><S sid = 235 ssid = >We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J07-2003.txt | Citing Article:  N09-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first decoder, Hiero Cube Pruning (HCP), is a k-best decoder using cube pruning implemented as described by Chiang (2007).</S> | Reference Offset:  ['181','189'] | Reference Text:  <S sid = 181 ssid = >5.3.3 Pruning.</S><S sid = 189 ssid = >5.3.4 Cube Pruning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J07-2003.txt | Citing Article:  N09-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the HCP system, MET is done following Chiang (2007).</S> | Reference Offset:  ['15','254'] | Reference Text:  <S sid = 15 ssid = >Following convention, we call the source language “French” and the target language “English”; the translation of a French sentence f into an English sentence e is modeled as: The phrase-based translation model P( f  |e) “encodes” e into f by the following steps: Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002).</S><S sid = 254 ssid = >We have shown how, through training with simple methods inspired by phrase-based models, and translating using a modified CKY with cube pruning, this challenge can be met.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J07-2003.txt | Citing Article:  N10-1128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A language model was incorporated using cube pruning (Huang and Chiang, 2007), using a 200 best limit at each node during LM integration.</S> | Reference Offset:  ['133','189'] | Reference Text:  <S sid = 133 ssid = >We describe here how to do this using the lazy algorithm of Huang and Chiang (2005).</S><S sid = 189 ssid = >5.3.4 Cube Pruning.</S> | Discourse Facet:  NA | Annotator: Automatic


