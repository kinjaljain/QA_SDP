Citance Number: 1 | Reference Article:  H05-1012.txt | Citing Article:  P13-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment.</S> | Reference Offset:  ['171','213'] | Reference Text:  <S sid = 171 ssid = >These significance tests indicate that the MaxEnt algorithm presented above is significantly better than either GIZA++ or HMM.</S><S sid = 213 ssid = >The prime reason for this is that features extracted from the alignments are aggregated over the training corpusand this process helps good alignments to have significantly better counts than errors in alignment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  H05-1012.txt | Citing Article:  P13-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al (2011).</S> | Reference Offset:  ['77','205'] | Reference Text:  <S sid = 77 ssid = >aligns to ?Al#?</S><S sid = 205 ssid = >The columns next to the words indicate whether the alignments are ?good?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  H05-1012.txt | Citing Article:  P13-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We experimented with two different supervised aligners: a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al, 2011).</S> | Reference Offset:  ['0','60'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Word Aligner For Arabic-English Machine Translation</S><S sid = 60 ssid = >We use the maximum entropy formulation (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  H05-1012.txt | Citing Article:  C10-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In each language, the rule extraction was performed using approximately 1.2M sentence pairs aligned using a maxent aligner (Ittycheriah and Roukos, 2005) trained using a variety of domains (Europarl, computer manuals) and a maximum entropy parser for English (Ratnaparkhi, 1999).</S> | Reference Offset:  ['0','11'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Word Aligner For Arabic-English Machine Translation</S><S sid = 11 ssid = >These phrases are extracted using word alignment algorithms that are trained on parallel corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  H05-1012.txt | Citing Article:  C10-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To quantify this effect, we learn reordering rules using three sets of alignments: HMM alignments, alignments from a supervised MaxEnt aligner (Ittycheriah and Roukos, 2005), and hand alignments.</S> | Reference Offset:  ['190','205'] | Reference Text:  <S sid = 190 ssid = >Combination of the phrasal features from theHMM and MaxEnt alignments results in the ?Combined?</S><S sid = 205 ssid = >The columns next to the words indicate whether the alignments are ?good?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  H05-1012.txt | Citing Article:  D08-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005).</S> | Reference Offset:  ['11','179'] | Reference Text:  <S sid = 11 ssid = >These phrases are extracted using word alignment algorithms that are trained on parallel corpora.</S><S sid = 179 ssid = >aligned to the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  H05-1012.txt | Citing Article:  D08-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set CE16K, which consists of 16K sentence pairs, to get relatively clean rules, free from alignment errors.</S> | Reference Offset:  ['76','179'] | Reference Text:  <S sid = 76 ssid = >A small dictionary is used (with 71 rules) to restrict the set of Ara bic segments that can align to English stopwords, for example that ?the?</S><S sid = 179 ssid = >aligned to the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  H05-1012.txt | Citing Article:  D07-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005).</S> | Reference Offset:  ['64','215'] | Reference Text:  <S sid = 64 ssid = >and ?i(h, f) are binary valued feature functions.</S><S sid = 215 ssid = >This paper presented a word aligner trained on anno tated data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  H05-1012.txt | Citing Article:  P06-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ittycheriah and Roukos (2005) trained a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperformed a GIZA++ baseline.</S> | Reference Offset:  ['156','179'] | Reference Text:  <S sid = 156 ssid = >These algorithms, as well as the Model 1 smoothing for the MaxEnt aligner, are all trained on a corpus of 500K sentence pairsfrom the UN parallel corpus and the LDC news cor pora released for 2005 (LDC, 2005).</S><S sid = 179 ssid = >aligned to the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  H05-1012.txt | Citing Article:  N07-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005).</S> | Reference Offset:  ['179','187'] | Reference Text:  <S sid = 179 ssid = >aligned to the next word.</S><S sid = 187 ssid = >As training data, we use the UN parallel corpus and the LDC news corpora released in 2005.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  H05-1012.txt | Citing Article:  N07-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 F-score (Ittycheriah and Roukos, 2005): Pr= |A? S||A| Rc= |A? S| |S| AER= 1? 2PrRc Pr+Rc where A links are proposed and S links are gold.</S> | Reference Offset:  ['28','126'] | Reference Text:  <S sid = 28 ssid = >The range of li is from 0 to K and there are M of these links.</S><S sid = 126 ssid = >In order to measure alignment performance, we use the standard AER measure (Och and Ney, 2000) but consider all links as sure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  H05-1012.txt | Citing Article:  N07-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ittycheriah and Roukos (2005) used only the top 50 sentences in IBMAC test data.</S> | Reference Offset:  ['122','143'] | Reference Text:  <S sid = 122 ssid = >As a test of alignment, we use the first 50 sentences of the MT03 Evaluationtest set which has 1313 Arabic words and 1528 En glish words 2.</S><S sid = 143 ssid = >We varied the training data size from 1K sentences to the complete set in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  H05-1012.txt | Citing Article:  N07-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set - the details of such adaptation were not provided and thus it is not clear how to replicate them to compare fairly.</S> | Reference Offset:  ['156','187'] | Reference Text:  <S sid = 156 ssid = >These algorithms, as well as the Model 1 smoothing for the MaxEnt aligner, are all trained on a corpus of 500K sentence pairsfrom the UN parallel corpus and the LDC news cor pora released for 2005 (LDC, 2005).</S><S sid = 187 ssid = >As training data, we use the UN parallel corpus and the LDC news corpora released in 2005.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  H05-1012.txt | Citing Article:  C10-2147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For word alignments we use the Maximum Entropy aligner described in (Ittycheriah and Roukos, 2005) that is trained using hand aligned training data.</S> | Reference Offset:  ['0','60'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Word Aligner For Arabic-English Machine Translation</S><S sid = 60 ssid = >We use the maximum entropy formulation (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  H05-1012.txt | Citing Article:  C10-2147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To find the most likely alignment we use the same algorithm as in (Ittycheriah and Roukos, 2005) since the structure of the model is unchanged.</S> | Reference Offset:  ['23','166'] | Reference Text:  <S sid = 23 ssid = >In order to describe the algorithm, we will need to first describe the direct link model.</S><S sid = 166 ssid = >7.3.2 GIZA Alignment In order to contrast our algorithm, we ranGIZA++ in the standard configuration which im plies 5 iterations of IBM Model 1, HMM, Model 3 and Model 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  H05-1012.txt | Citing Article:  D10-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The data in this corpus is automatically aligned using a technique presented in (Ittycheriah and Roukos, 2005).</S> | Reference Offset:  ['35','187'] | Reference Text:  <S sid = 35 ssid = >In the aligner presented here, ?is always set to 0.5.</S><S sid = 187 ssid = >As training data, we use the UN parallel corpus and the LDC news corpora released in 2005.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  H05-1012.txt | Citing Article:  D11-1125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data.</S> | Reference Offset:  ['0','60'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Word Aligner For Arabic-English Machine Translation</S><S sid = 60 ssid = >We use the maximum entropy formulation (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  H05-1012.txt | Citing Article:  P06-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of 37.8 and (Ittycheriah and Roukos, 2005) reports a BLEU score of 48.0.</S> | Reference Offset:  ['184','214'] | Reference Text:  <S sid = 184 ssid = >HMM 0.459 0.419 0.456 MaxEnt 0.468 0.433 0.451 Combined 0.479 0.437 0.465 Significance 0.017 0.020 ? Table 5: Machine Translation Performance using the NIST 2005 Bleu scorerscribed in (Tillmann and Ney, 2003).</S><S sid = 214 ssid = >Align ing rare words correctly should help performance but since their count is low it is not reflected in bleu scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  H05-1012.txt | Citing Article:  N07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We assume that each sentence pair in the training corpus is word-aligned (e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004)).</S> | Reference Offset:  ['156','179'] | Reference Text:  <S sid = 156 ssid = >These algorithms, as well as the Model 1 smoothing for the MaxEnt aligner, are all trained on a corpus of 500K sentence pairsfrom the UN parallel corpus and the LDC news cor pora released for 2005 (LDC, 2005).</S><S sid = 179 ssid = >aligned to the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  H05-1012.txt | Citing Article:  P09-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['111','224'] | Reference Text:  <S sid = 111 ssid = >95.2 ? 93.2 Table 1: F-measure for human performance on word alignment for Arabic-English.</S><S sid = 224 ssid = >This paper owes much to the collaboration of the Statistical MT group at IBM.</S> | Discourse Facet:  NA | Annotator: Automatic


