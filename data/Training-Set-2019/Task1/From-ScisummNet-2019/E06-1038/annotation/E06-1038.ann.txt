Citance Number: 1 | Reference Article:  E06-1038.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald (McDonald, 2006) independently proposed a new machine learning approach.</S> | Reference Offset:  ['53','198'] | Reference Text:  <S sid = 53 ssid = >In this section we described a discriminative online learning approach to sentence compression, the core of which is a decoding algorithm that searches the entire space of compressions.</S><S sid = 198 ssid = >In this paper we have described a new system for sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1038.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph.</S> | Reference Offset:  ['34','58'] | Reference Text:  <S sid = 34 ssid = >The algorithm then either shifts (considers new words and subtrees for x), reduces (combines subtrees from x into possibly new tree constructions) or drops (drops words and subtrees from x) on each step of the algorithm.</S><S sid = 58 ssid = >We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1038.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint.</S> | Reference Offset:  ['58','67'] | Reference Text:  <S sid = 58 ssid = >We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S><S sid = 67 ssid = >That is, the algorithm will return the highest scoring compression regardless of length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1038.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA.</S> | Reference Offset:  ['162','173'] | Reference Text:  <S sid = 162 ssid = >The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.</S><S sid = 173 ssid = >When looking at importance, we see that our system actually does the best – even better than humans.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1038.txt | Citing Article:  P14-2054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference.</S> | Reference Offset:  ['58','74'] | Reference Text:  <S sid = 58 ssid = >We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S><S sid = 74 ssid = >We simply augment the dynamic programming table and calculate C[i][r], which is the score of the best compression of length r that ends at word xi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1038.txt | Citing Article:  P08-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S><S sid = 228 ssid = >This work was supported by NSF ITR grants 0205448 and 0428193.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1038.txt | Citing Article:  C10-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006).</S> | Reference Offset:  ['104','135'] | Reference Text:  <S sid = 104 ssid = >To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).</S><S sid = 135 ssid = >However, we use these syntactic constraints as soft evidence in our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1038.txt | Citing Article:  P09-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S><S sid = 228 ssid = >This work was supported by NSF ITR grants 0205448 and 0428193.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1038.txt | Citing Article:  N07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model.</S> | Reference Offset:  ['56','131'] | Reference Text:  <S sid = 56 ssid = >Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).</S><S sid = 131 ssid = >This is similar in purpose to the source model from the noisy-channel system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1038.txt | Citing Article:  W11-1606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006).</S> | Reference Offset:  ['0','29'] | Reference Text:  <S sid = 0 ssid = >Discriminative Sentence Compression With Soft Syntactic Evidence</S><S sid = 29 ssid = >P(x|y) is the channel model, the probability that the long sentence is an expansion of the compressed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1038.txt | Citing Article:  W11-1606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S><S sid = 228 ssid = >This work was supported by NSF ITR grants 0205448 and 0428193.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1038.txt | Citing Article:  W09-1802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008).</S> | Reference Offset:  ['173','222'] | Reference Text:  <S sid = 173 ssid = >When looking at importance, we see that our system actually does the best – even better than humans.</S><S sid = 222 ssid = >Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1038.txt | Citing Article:  W06-2932.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005).</S> | Reference Offset:  ['8','109'] | Reference Text:  <S sid = 8 ssid = >We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005).</S><S sid = 109 ssid = >It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1038.txt | Citing Article:  W09-1801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.</S> | Reference Offset:  ['1','104'] | Reference Text:  <S sid = 1 ssid = >We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.</S><S sid = 104 ssid = >To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1038.txt | Citing Article:  W09-1801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S><S sid = 228 ssid = >This work was supported by NSF ITR grants 0205448 and 0428193.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E06-1038.txt | Citing Article:  C08-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora.</S> | Reference Offset:  ['163','202'] | Reference Text:  <S sid = 163 ssid = >We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly as well in their evaluation, and the compression rate of the decision tree model is nearer to our system (around 57-58%).</S><S sid = 202 ssid = >This system has many advantages over previous approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E06-1038.txt | Citing Article:  D11-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both these systems reported results outperforming previous systems such as McDonald (2006).</S> | Reference Offset:  ['165','179'] | Reference Text:  <S sid = 165 ssid = >Results are shown in Table 1.</S><S sid = 179 ssid = >This is quantified in the standard deviation of the two systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E06-1038.txt | Citing Article:  P13-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006).</S> | Reference Offset:  ['58','208'] | Reference Text:  <S sid = 58 ssid = >We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S><S sid = 208 ssid = >Another advantage is distance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E06-1038.txt | Citing Article:  P13-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression.</S> | Reference Offset:  ['28','54'] | Reference Text:  <S sid = 28 ssid = >The noisy-channel model defines the problem as finding the compressed sentence with maximum conditional probability P(y) is the source model, which is a PCFG plus bigram language model.</S><S sid = 54 ssid = >Let the score of a compression y for a sentence x as In particular, we are going to factor this score using a first-order Markov assumption on the words in the compressed sentence Finally, we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E06-1038.txt | Citing Article:  D07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm.</S> | Reference Offset:  ['23','199'] | Reference Text:  <S sid = 23 ssid = >In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions.</S><S sid = 199 ssid = >This system uses discriminative large-margin learning techniques coupled with a decoding algorithm that searches the space of all compressions.</S> | Discourse Facet:  NA | Annotator: Automatic


