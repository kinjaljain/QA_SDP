Citance Number: 1 | Reference Article:  P09-1116.txt | Citing Article:  P14-2131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009).</S> | Reference Offset:  ['58','69'] | Reference Text:  <S sid = 58 ssid = >The features are the words (tokens) in the window.</S><S sid = 69 ssid = >Second, it reduces the variance in the sizes of the feature vectors of the phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1116.txt | Citing Article:  C10-2137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features.</S> | Reference Offset:  ['17','216'] | Reference Text:  <S sid = 17 ssid = >However, the supervised learning algorithms can typically identify useful clusters and assign proper weights to them, effectively adapting the clusters to the domain.</S><S sid = 216 ssid = >In the two-stage cluster-based approaches such as ours, clustering is mostly decoupled from the supervised learning problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1116.txt | Citing Article:  P11-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task.</S> | Reference Offset:  ['1','231'] | Reference Text:  <S sid = 1 ssid = >We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.</S><S sid = 231 ssid = >We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1116.txt | Citing Article:  P11-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances.</S> | Reference Offset:  ['22','65'] | Reference Text:  <S sid = 22 ssid = >Phrases are much less so because the words in a phrase provide contexts for one another.</S><S sid = 65 ssid = >The idea is that if we have already seen 300K instances of a phrase, we should have already collected enough data for the phrase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1116.txt | Citing Article:  P11-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','235'] | Reference Text:  <S sid = 73 ssid = >The most straightforward MapReduce implementation of K-Means would be to have mappers perform Step ii and reducers perform Step iii.</S><S sid = 235 ssid = >The authors wish to thank the anonymous reviewers for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1116.txt | Citing Article:  C10-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009).</S> | Reference Offset:  ['45','183'] | Reference Text:  <S sid = 45 ssid = >To obtain a list of phrases to be clustered, we followed the approach in (Lin et al., 2008) by collecting 20 million unique queries from an anonymized query log that are found in a 700 billion token web corpus with a minimum frequency count of 100.</S><S sid = 183 ssid = >Moreover, we use soft clustering instead of hard clustering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1116.txt | Citing Article:  P12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word.</S> | Reference Offset:  ['15','99'] | Reference Text:  <S sid = 15 ssid = >Under this approach, even if a word is not found in the training data, it may still fire cluster-based features as long as it shares cluster assignments with some words in the labeled data.</S><S sid = 99 ssid = >For natural language words and phrases, the soft cluster assignments often reveal different senses of a word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1116.txt | Citing Article:  P11-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data.</S> | Reference Offset:  ['37','157'] | Reference Text:  <S sid = 37 ssid = >Our named entity recognition system achieves an F1-score of 90.90 on the CoNLL 2003 English data set, which is about 1 point higher than the previous best result.</S><S sid = 157 ssid = >The best F-score of 90.90, which is about 1 point higher than the previous best result, is obtained with a combination of clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1116.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.</S> | Reference Offset:  ['1','35'] | Reference Text:  <S sid = 1 ssid = >We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.</S><S sid = 35 ssid = >We present a distributed version of a much simpler K-Means clustering that allows us to cluster tens of millions of elements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1116.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa.</S> | Reference Offset:  ['160','171'] | Reference Text:  <S sid = 160 ssid = >The goal of query classification is to determine to which ones of a predefined set of classes a query belongs.</S><S sid = 171 ssid = >Since the query classes are not mutually exclusive, we treat the query classification task as 67 binary classification problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1116.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','235'] | Reference Text:  <S sid = 73 ssid = >The most straightforward MapReduce implementation of K-Means would be to have mappers perform Step ii and reducers perform Step iii.</S><S sid = 235 ssid = >The authors wish to thank the anonymous reviewers for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1116.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009).</S> | Reference Offset:  ['125','126'] | Reference Text:  <S sid = 125 ssid = >In comparison, there are 79 templates in (Suzuki and Isozaki, 2008).</S><S sid = 126 ssid = >Part-of-speech tags were used in the topranked systems in CoNLL 2003, as well as in many follow up studies that used the data set (Ando and Zhang 2005; Suzuki and Isozaki 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1116.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.</S> | Reference Offset:  ['1','35'] | Reference Text:  <S sid = 1 ssid = >We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.</S><S sid = 35 ssid = >We present a distributed version of a much simpler K-Means clustering that allows us to cluster tens of millions of elements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1116.txt | Citing Article:  N12-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >K-Means clustering algorithm described in Lin and Wu (2009).</S> | Reference Offset:  ['70','98'] | Reference Text:  <S sid = 70 ssid = >K-Means is an embarrassingly parallelizable algorithm.</S><S sid = 98 ssid = >Although K-Means is generally described as a hard clustering algorithm (each element belongs to at most one cluster), it can produce soft clustering simply by assigning an element to all clusters whose similarity to the element is greater than a threshold.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1116.txt | Citing Article:  N12-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word.</S> | Reference Offset:  ['40','54'] | Reference Text:  <S sid = 40 ssid = >Given a set of elements represented as feature vectors and a number, k, of desired clusters, the K-Means algorithm consists of the following steps: i.</S><S sid = 54 ssid = >Following previous approaches to distributional clustering of words, we represent the contexts of a phrase as a feature vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1116.txt | Citing Article:  N12-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores.</S> | Reference Offset:  ['98','145'] | Reference Text:  <S sid = 98 ssid = >Although K-Means is generally described as a hard clustering algorithm (each element belongs to at most one cluster), it can produce soft clustering simply by assigning an element to all clusters whose similarity to the element is greater than a threshold.</S><S sid = 145 ssid = >Instead, any word above a minimum frequency threshold is included.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1116.txt | Citing Article:  N12-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments).</S> | Reference Offset:  ['20','46'] | Reference Text:  <S sid = 20 ssid = >In addition to word-clusters, we also use phraseclusters as features.</S><S sid = 46 ssid = >Note that many of these queries are not phrases in the linguistic sense. language, english spanish language, learn foreign language, free english learning, language study english, spanish immersion course, how to speak french, spanish learning games, .....</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1116.txt | Citing Article:  N12-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs.</S> | Reference Offset:  ['32','96'] | Reference Text:  <S sid = 32 ssid = >Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm.</S><S sid = 96 ssid = >The Brown algorithm uses essentially the same information as our 1-word window clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1116.txt | Citing Article:  N12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER.</S> | Reference Offset:  ['0','183'] | Reference Text:  <S sid = 0 ssid = >Phrase Clustering for Discriminative Learning</S><S sid = 183 ssid = >Moreover, we use soft clustering instead of hard clustering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1116.txt | Citing Article:  W12-1914.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['73','235'] | Reference Text:  <S sid = 73 ssid = >The most straightforward MapReduce implementation of K-Means would be to have mappers perform Step ii and reducers perform Step iii.</S><S sid = 235 ssid = >The authors wish to thank the anonymous reviewers for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


