Citance Number: 1 | Reference Article:  E09-1013.txt | Citing Article:  C10-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents.</S> | Reference Offset:  ['22','30'] | Reference Text:  <S sid = 22 ssid = >Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S><S sid = 30 ssid = >This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E09-1013.txt | Citing Article:  C10-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns.</S> | Reference Offset:  ['0','132'] | Reference Text:  <S sid = 0 ssid = >Bayesian Word Sense Induction</S><S sid = 132 ssid = >This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E09-1013.txt | Citing Article:  P14-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009).</S> | Reference Offset:  ['53','63'] | Reference Text:  <S sid = 53 ssid = >Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.</S><S sid = 63 ssid = >This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E09-1013.txt | Citing Article:  D11-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance.</S> | Reference Offset:  ['3','40'] | Reference Text:  <S sid = 3 ssid = >Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.</S><S sid = 40 ssid = >In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E09-1013.txt | Citing Article:  P14-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009).</S> | Reference Offset:  ['96','148'] | Reference Text:  <S sid = 96 ssid = >1 word window).</S><S sid = 148 ssid = >Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E09-1013.txt | Citing Article:  C10-2094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009).</S> | Reference Offset:  ['18','61'] | Reference Text:  <S sid = 18 ssid = >In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.</S><S sid = 61 ssid = >Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E09-1013.txt | Citing Article:  D11-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007).</S> | Reference Offset:  ['47','64'] | Reference Text:  <S sid = 47 ssid = >Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.</S><S sid = 64 ssid = >Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E09-1013.txt | Citing Article:  W10-4173.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >LDA model has also been applied to WSI (Brody and Lapata, 2009).</S> | Reference Offset:  ['42','66'] | Reference Text:  <S sid = 42 ssid = >Graph-based methods have also been applied to the sense induction task.</S><S sid = 66 ssid = >Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E09-1013.txt | Citing Article:  P10-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009).</S> | Reference Offset:  ['53','64'] | Reference Text:  <S sid = 53 ssid = >Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.</S><S sid = 64 ssid = >Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E09-1013.txt | Citing Article:  P11-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010).</S> | Reference Offset:  ['131','209'] | Reference Text:  <S sid = 131 ssid = >The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.</S><S sid = 209 ssid = >Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E09-1013.txt | Citing Article:  C10-2069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006).</S> | Reference Offset:  ['22','50'] | Reference Text:  <S sid = 22 ssid = >Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S><S sid = 50 ssid = >For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E09-1013.txt | Citing Article:  W12-3305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system.</S> | Reference Offset:  ['0','70'] | Reference Text:  <S sid = 0 ssid = >Bayesian Word Sense Induction</S><S sid = 70 ssid = >We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E09-1013.txt | Citing Article:  W12-3305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009).</S> | Reference Offset:  ['22','162'] | Reference Text:  <S sid = 22 ssid = >Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S><S sid = 162 ssid = >This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E09-1013.txt | Citing Article:  N10-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008).</S> | Reference Offset:  ['42','66'] | Reference Text:  <S sid = 42 ssid = >Graph-based methods have also been applied to the sense induction task.</S><S sid = 66 ssid = >Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E09-1013.txt | Citing Article:  E12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity.</S> | Reference Offset:  ['22','50'] | Reference Text:  <S sid = 22 ssid = >Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S><S sid = 50 ssid = >For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E09-1013.txt | Citing Article:  E12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD.</S> | Reference Offset:  ['32','35'] | Reference Text:  <S sid = 32 ssid = >The remainder of this paper is structured as follows.</S><S sid = 35 ssid = >We discuss our results in Section 6, and conclude in Section 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E09-1013.txt | Citing Article:  D10-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009).</S> | Reference Offset:  ['4','70'] | Reference Text:  <S sid = 4 ssid = >Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.</S><S sid = 70 ssid = >We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E09-1013.txt | Citing Article:  D10-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['84','228'] | Reference Text:  <S sid = 84 ssid = >In other words, we could simply assume that the contexts we wish to model are the union of all our features.</S><S sid = 228 ssid = >We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E09-1013.txt | Citing Article:  W11-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task.</S> | Reference Offset:  ['22','176'] | Reference Text:  <S sid = 22 ssid = >Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S><S sid = 176 ssid = >The most probable words for each sense are also shown.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E09-1013.txt | Citing Article:  W11-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007).</S> | Reference Offset:  ['31','125'] | Reference Text:  <S sid = 31 ssid = >We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.</S><S sid = 125 ssid = >For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


