Citance Number: 1 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).</S> | Reference Offset:  ['5','9'] | Reference Text:  <S sid = 5 ssid = >Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic pars ing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003).</S><S sid = 9 ssid = >Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004).</S> | Reference Offset:  ['158','161'] | Reference Text:  <S sid = 158 ssid = >Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).</S><S sid = 161 ssid = >More over, the fact that our parser uses labeled dependenciesis probably also significant, since the possibility of using information from previously assigned (labeled) de pendencies during parsing seems to have a positive effect on accuracy (Nivre, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).</S> | Reference Offset:  ['14','33'] | Reference Text:  <S sid = 14 ssid = >2.1 Dependency Graphs.</S><S sid = 33 ssid = >For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S> | Reference Offset:  ['80','82'] | Reference Text:  <S sid = 80 ssid = >However, in order to maintain the efficiency of the parser, the classifier must also be implemented in such a way that each transition can still be performed in constant time.Previous work in this area includes the use of memory based learning to guide a standard shift-reduce parser(Veenstra and Daelemans, 2000) and the use of support vector machines to guide a deterministic depen dency parser (Yamada and Matsumoto, 2003).</S><S sid = 82 ssid = >2.4 Memory-Based Learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004).</S> | Reference Offset:  ['9','118'] | Reference Text:  <S sid = 9 ssid = >Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.</S><S sid = 118 ssid = >Instead, we have to simulate the parser on the tree bank in order to derive, for each sentence, the transition sequence corresponding to the correct dependency tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These settings are the result of extensive experiments partially reported in Nivre et al (2004).</S> | Reference Offset:  ['123','137'] | Reference Text:  <S sid = 123 ssid = >(The final test set has not been used at all in the experiments reported in this paper.)</S><S sid = 137 ssid = >The results in the first column were obtained with the default settings of the TiMBL package, in particular: ? The IB1 classification algorithm (Aha et al, 1991).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004).</S> | Reference Offset:  ['98','132'] | Reference Text:  <S sid = 98 ssid = >In addition to the word form itself (TOP), we consider its part-of-speech (as assigned by an automatic part-of-speech tagger in a preprocessing phase), the dependency type by which it is related to its head (which may or may not be available in a given configuration depending on whether the head is to the left or to the right of the token in question), and the dependency types by which it is related to its leftmost and rightmost dependent, respectively (where the currentrightmost dependent may or may not be the rightmost de pendent in the complete dependency tree).</S><S sid = 132 ssid = >The attachment score is computed as theproportion of tokens (excluding punctuation) that are as signed the correct head (or no head if the token is a root).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004).</S> | Reference Offset:  ['134','158'] | Reference Text:  <S sid = 134 ssid = >However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al, 1999), we will give this measure as well.In order to measure label accuracy, we also define a la beled attachment score, where both the head and the label must be correct, but which is otherwise computed in the same way as the ordinary (unlabeled) attachment score.</S><S sid = 158 ssid = >Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-2407.txt | Citing Article:  P14-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004).</S> | Reference Offset:  ['5','88'] | Reference Text:  <S sid = 5 ssid = >Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic pars ing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003).</S><S sid = 88 ssid = >Moreover, the memory-based approach can easily handle multi-class classification, unlike the support vector machines used by Yamada and Matsumoto (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English).</S> | Reference Offset:  ['107','161'] | Reference Text:  <S sid = 107 ssid = >This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S><S sid = 161 ssid = >More over, the fact that our parser uses labeled dependenciesis probably also significant, since the possibility of using information from previously assigned (labeled) de pendencies during parsing seems to have a positive effect on accuracy (Nivre, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >The transitions Left-Arc and Right-Arc are subject to conditions that ensure that the graph conditions Uniquelabel and Single head are satisfied.</S><S sid = 173 ssid = >We are grateful to three anonymous reviewers for constructive com ments on the preliminary version of the paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance.</S> | Reference Offset:  ['89','144'] | Reference Text:  <S sid = 89 ssid = >For the experiments reported in this paper, we have used the software package TiMBL (Tilburg MemoryBased Learner), which provides a variety of metrics, al gorithms, and extra functions on top of the classical knearest neighbor classification kernel, such as value distance metrics and distance weighted class voting (Daele mans et al, 2003).</S><S sid = 144 ssid = >Distance weighted class voting with inverse distance weighting (Dudani, 1976).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004).</S> | Reference Offset:  ['156','158'] | Reference Text:  <S sid = 156 ssid = >The unlabeled attachment score is naturally higher, and it is worth noting that the relative differ ence between the MBL lexical model and the other twomodels is much smaller.</S><S sid = 158 ssid = >Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-2407.txt | Citing Article:  P05-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004).</S> | Reference Offset:  ['9','14'] | Reference Text:  <S sid = 9 ssid = >Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.</S><S sid = 14 ssid = >2.1 Dependency Graphs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-2407.txt | Citing Article:  P05-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004).</S> | Reference Offset:  ['82','107'] | Reference Text:  <S sid = 82 ssid = >2.4 Memory-Based Learning.</S><S sid = 107 ssid = >This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-2407.txt | Citing Article:  P11-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','173'] | Reference Text:  <S sid = 54 ssid = >The transitions Left-Arc and Right-Arc are subject to conditions that ensure that the graph conditions Uniquelabel and Single head are satisfied.</S><S sid = 173 ssid = >We are grateful to three anonymous reviewers for constructive com ments on the preliminary version of the paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-2407.txt | Citing Article:  P11-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b).</S> | Reference Offset:  ['82','172'] | Reference Text:  <S sid = 82 ssid = >2.4 Memory-Based Learning.</S><S sid = 172 ssid = >The memory-based classifiers used in the experi ments were constructed using the Tilburg Memory-BasedLearner (TiMBL) (Daelemans et al, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-2407.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004).</S> | Reference Offset:  ['0','107'] | Reference Text:  <S sid = 0 ssid = >Memory-Based Dependency Parsing</S><S sid = 107 ssid = >This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-2407.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions.</S> | Reference Offset:  ['95','107'] | Reference Text:  <S sid = 95 ssid = >For this purpose we define a number of features that can be used to define different models of parser state.</S><S sid = 107 ssid = >This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-2407.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004).</S> | Reference Offset:  ['83','148'] | Reference Text:  <S sid = 83 ssid = >Memory-based learning and problem solving is based ontwo fundamental principles: learning is the simple stor age of experiences in memory, and solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans, 1999).</S><S sid = 148 ssid = >(1991).</S> | Discourse Facet:  NA | Annotator: Automatic


