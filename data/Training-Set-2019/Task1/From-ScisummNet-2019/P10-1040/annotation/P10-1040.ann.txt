Citance Number: 1 | Reference Article:  P10-1040.txt | Citing Article:  P14-2131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['90','244'] | Reference Text:  <S sid = 90 ssid = >We also create a corrupted or noise n-gram x˜ = (w1, ... , wn_q, ˜wn), where ˜wn # wn is chosen uniformly from the vocabulary.1 For convenience, we write e(x) to mean e(w1) ® ... ® e(wn).</S><S sid = 244 ssid = >Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P10-1040.txt | Citing Article:  P13-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008).</S> | Reference Offset:  ['30','168'] | Reference Text:  <S sid = 30 ssid = >We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1.</S><S sid = 168 ssid = >The Collobert and Weston (2008) (C&W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P10-1040.txt | Citing Article:  P13-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010).</S> | Reference Offset:  ['155','221'] | Reference Text:  <S sid = 155 ssid = >For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary—a curriculum training strategy (Elman, 1993; Bengio et al., 2009; Spitkovsky et al., 2010)—would provide better results than cleaning.</S><S sid = 221 ssid = >(In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P10-1040.txt | Citing Article:  P13-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01.</S> | Reference Offset:  ['94','175'] | Reference Text:  <S sid = 94 ssid = >We minimize this loss stochastically over the n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table.</S><S sid = 175 ssid = >We used a learning rate of 1e-3 for both model parameters and embedding parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P10-1040.txt | Citing Article:  W12-1908.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models.</S> | Reference Offset:  ['152','200'] | Reference Text:  <S sid = 152 ssid = >In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.</S><S sid = 200 ssid = >But, if only one word representation is to be used, Brown clusters have the highest accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P10-1040.txt | Citing Article:  W12-1908.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters.</S> | Reference Offset:  ['64','200'] | Reference Text:  <S sid = 64 ssid = >Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov & Roth, 2009), PCFG parsing (Candito & Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009).</S><S sid = 200 ssid = >But, if only one word representation is to be used, Brown clusters have the highest accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P10-1040.txt | Citing Article:  P11-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1.</S> | Reference Offset:  ['182','185'] | Reference Text:  <S sid = 182 ssid = >We can scale the embeddings by a hyperparameter, to control their standard deviation.</S><S sid = 185 ssid = >However, these curves demonstrate that a reasonable choice of scale factor is such that the embeddings have a standard deviation of 0.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P10-1040.txt | Citing Article:  S12-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Further details and evaluations of these embeddings are discussed in Turian et al (2010).</S> | Reference Offset:  ['37','201'] | Reference Text:  <S sid = 37 ssid = >In most of the other approaches discussed, the columns represent word contexts.</S><S sid = 201 ssid = >Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P10-1040.txt | Citing Article:  P14-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010).</S> | Reference Offset:  ['1','201'] | Reference Text:  <S sid = 1 ssid = >If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.</S><S sid = 201 ssid = >Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P10-1040.txt | Citing Article:  P14-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010).</S> | Reference Offset:  ['16','80'] | Reference Text:  <S sid = 16 ssid = >Each dimension’s value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature.</S><S sid = 80 ssid = >Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P10-1040.txt | Citing Article:  P14-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here.</S> | Reference Offset:  ['171','176'] | Reference Text:  <S sid = 171 ssid = >We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows.</S><S sid = 176 ssid = >We induced embeddings with 100 dimensions over 5-gram windows, and embeddings with 50 dimensions over 5-gram windows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P10-1040.txt | Citing Article:  P14-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well.</S> | Reference Offset:  ['157','160'] | Reference Text:  <S sid = 157 ssid = >The cleaned RCV1 corpus has 269K word types.</S><S sid = 160 ssid = >RCV1 is a superset of the CoNLL03 corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P10-1040.txt | Citing Article:  P14-2037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)).</S> | Reference Offset:  ['30','86'] | Reference Text:  <S sid = 30 ssid = >We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1.</S><S sid = 86 ssid = >This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P10-1040.txt | Citing Article:  P14-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising.</S> | Reference Offset:  ['59','152'] | Reference Text:  <S sid = 59 ssid = >The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).</S><S sid = 152 ssid = >In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P10-1040.txt | Citing Article:  W11-0315.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters.</S> | Reference Offset:  ['59','163'] | Reference Text:  <S sid = 59 ssid = >The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).</S><S sid = 163 ssid = >We also induced 100, 320, and 3200 Brown clusters, for comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P10-1040.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We downloaded these embeddings from Turian et al (2010).</S> | Reference Offset:  ['184','201'] | Reference Text:  <S sid = 184 ssid = >In Turian et al. (2009), we were not able to prescribe a default value for scaling the embeddings.</S><S sid = 201 ssid = >Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P10-1040.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a).</S> | Reference Offset:  ['64','83'] | Reference Text:  <S sid = 64 ssid = >Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov & Roth, 2009), PCFG parsing (Candito & Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009).</S><S sid = 83 ssid = >Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P10-1040.txt | Citing Article:  W11-2155.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus.</S> | Reference Offset:  ['59','165'] | Reference Text:  <S sid = 59 ssid = >The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).</S><S sid = 165 ssid = >Because Brown clusters are hierarchical, we can use cluster supersets as features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P10-1040.txt | Citing Article:  P11-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79.</S> | Reference Offset:  ['36','152'] | Reference Text:  <S sid = 36 ssid = >LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context.</S><S sid = 152 ssid = >In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P10-1040.txt | Citing Article:  P11-1097.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature.</S> | Reference Offset:  ['11','22'] | Reference Text:  <S sid = 11 ssid = >One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a significant increase in accuracy.</S><S sid = 22 ssid = >One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical.</S> | Discourse Facet:  NA | Annotator: Automatic


