Citance Number: 1 | Reference Article:  W06-1203.txt | Citing Article:  W09-0213.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They measure compositionality as a combination of two similarity values: firstly, similar to (Katz and Giesbrecht, 2006), the similarity (cosine similarity) between the context of a VNC and the contexts of its constituent words.</S> | Reference Offset:  ['35','95'] | Reference Text:  <S sid = 35 ssid = >Their hypothesis is that high LSA-based similarity between the MWE and each of its constituent parts is indicative of compositionality.</S><S sid = 95 ssid = >The resulting cosine similarity values range from 0.01 to 0.80.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-1203.txt | Citing Article:  P10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainning data.</S> | Reference Offset:  ['93','110'] | Reference Text:  <S sid = 93 ssid = >We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn’t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).</S><S sid = 110 ssid = >A second problem concerns the relationship between the literal and the non-literal meaning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-1203.txt | Citing Article:  D09-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal usages of a given expression in the training data.</S> | Reference Offset:  ['93','110'] | Reference Text:  <S sid = 93 ssid = >We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn’t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).</S><S sid = 110 ssid = >A second problem concerns the relationship between the literal and the non-literal meaning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-1203.txt | Citing Article:  E09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal us ages of a given expression in the training data.</S> | Reference Offset:  ['93','110'] | Reference Text:  <S sid = 93 ssid = >We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn’t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).</S><S sid = 110 ssid = >A second problem concerns the relationship between the literal and the non-literal meaning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-1203.txt | Citing Article:  E09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006).</S> | Reference Offset:  ['63','68'] | Reference Text:  <S sid = 63 ssid = >Our next task was to investigate whether this difference could be used in particular cases to determine what the intended use of an MWE in a particular context was.</S><S sid = 68 ssid = >To summarize Experiment I, which is a variant of a supervised phrase sense disambiguation task, demonstrates that we can use LSA to distinguish between literal and the idiomatic usage of an MWE by using local linguistic context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-1203.txt | Citing Article:  E09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers.</S> | Reference Offset:  ['4','118'] | Reference Text:  <S sid = 4 ssid = >Identifying non-compositional (or idiomatic) multi-word expressions (MWEs) is an important subtask for any computational system (Sag et al., 2002), and significant attention has been paid to practical methods for solving this problem in recent years (Lin, 1999; Baldwin et al., 2003; Villada Moir´on and Tiedemann, 2006).</S><S sid = 118 ssid = >There is some evidence (Baldwin et al., 2003) that part of speech tagging might improve results in this kind of task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-1203.txt | Citing Article:  D09-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors.</S> | Reference Offset:  ['0','112'] | Reference Text:  <S sid = 0 ssid = >Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis</S><S sid = 112 ssid = >If the meanings are similar, it is likely that local context will be inadequate to distinguish a compositional from a non-compositional use of the expression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-1203.txt | Citing Article:  N10-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)).</S> | Reference Offset:  ['33','43'] | Reference Text:  <S sid = 33 ssid = >Baldwin et al., (2003) focus more narrowly on distinguishing English noun-noun compounds and verb-particle constructions which are compositional from those which are not compositional.</S><S sid = 43 ssid = >In his comparison among a number of different heuristics for identifying non-compositional noun-noun compounds, Zhai did his evaluation by applying each heuristic to a corpus of items hand-classified as to their compositionality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-1203.txt | Citing Article:  D08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic.</S> | Reference Offset:  ['18','79'] | Reference Text:  <S sid = 18 ssid = >In the first experiment we seek to confirm that the local context of a known idiom can reliably distinguish idiomatic uses from non-idiomatic uses.</S><S sid = 79 ssid = >We calculated the estimated compositional meaning vector by taking it to be the sum of the meaning vector of the parts, i.e., the compositional meaning of an expression w1w2 consisting of two words is taken to be sum of the meaning vectors for the constituent words.6 In order to maximize the independent contribution of the constituent words, the meaning vectors for these words were always computed from contexts in which they appear alone (that is, not in the local context of the other constituent).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-1203.txt | Citing Article:  D08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006).</S> | Reference Offset:  ['27','47'] | Reference Text:  <S sid = 27 ssid = >His method was to compare the mutual information measure of the constituents parts of an MWE with the mutual information of similar expressions obtained by substituting one of the constituents with a related word obtained by thesaurus lookup.</S><S sid = 47 ssid = >This kind of dimensionality reduction has been shown to improve performance in a number of text-based domains (Berry et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-1203.txt | Citing Article:  C10-2144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose.</S> | Reference Offset:  ['0','20'] | Reference Text:  <S sid = 0 ssid = >Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis</S><S sid = 20 ssid = >In our experiments we make use of lexical semantic analysis (LSA) as a model of contextsimilarity (Deerwester et al., 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-1203.txt | Citing Article:  W11-0807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token.</S> | Reference Offset:  ['86','111'] | Reference Text:  <S sid = 86 ssid = >This can be illustrated by considering, for example, the MWE fire breathing.</S><S sid = 111 ssid = >Our technique relies on these meaning being highly distinct.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-1203.txt | Citing Article:  C10-2078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Among the earliest studies on token-based classification were the ones by Hashimoto et al (2006) on Japanese and Katz and Giesbrecht (2006) on German.</S> | Reference Offset:  ['4','47'] | Reference Text:  <S sid = 4 ssid = >Identifying non-compositional (or idiomatic) multi-word expressions (MWEs) is an important subtask for any computational system (Sag et al., 2002), and significant attention has been paid to practical methods for solving this problem in recent years (Lin, 1999; Baldwin et al., 2003; Villada Moir´on and Tiedemann, 2006).</S><S sid = 47 ssid = >This kind of dimensionality reduction has been shown to improve performance in a number of text-based domains (Berry et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-1203.txt | Citing Article:  C10-2078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples.</S> | Reference Offset:  ['64','93'] | Reference Text:  <S sid = 64 ssid = >To evaluate this, we did a 10-fold cross-validation study, calculating the literal and idiomatic vectors for ins Wasser fallen on the basis of the training data and doing a simple nearest neighbor classification of each memember of the test set on the basis of the meaning vectors computed from its local context (the 30 word window).</S><S sid = 93 ssid = >We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn’t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-1203.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such techniques either do not use any information regarding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non compositionality (Katz and Giesbrecht, 2006).</S> | Reference Offset:  ['24','25'] | Reference Text:  <S sid = 24 ssid = >Recent work which attempts to discriminate between compositional and non-compositional MWEs include Lin (1999), who used mutualinformation measures identify such phrases, Baldwin et al. (2003), who compare the distribution of the head of the MWE with the distribution of the entire MWE, and Vallada Moir´on & Tiedemann (2006), who use a word-alignment strategy to identify non-compositional MWEs making use of parallel texts.</S><S sid = 25 ssid = >Schone & Jurafsky (2001) applied LSA to MWE identification, althought they did not focus on distinguishing compositional from non-compositional MWEs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-1203.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data.</S> | Reference Offset:  ['64','97'] | Reference Text:  <S sid = 64 ssid = >To evaluate this, we did a 10-fold cross-validation study, calculating the literal and idiomatic vectors for ins Wasser fallen on the basis of the training data and doing a simple nearest neighbor classification of each memember of the test set on the basis of the meaning vectors computed from its local context (the 30 word window).</S><S sid = 97 ssid = >Indeed of the MWEs with a similarity values of under 0.1, just over half are MWEs which were hand-annotated to have non-literal uses.10 It used in their idiomatic sense (apparently for humorous effect) particularly frequently in contexts in which elements of the literal meaning were also present.11 is clear then that the technique described is, prima facie, capable of detecting idiomatic MWEs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-1203.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional.</S> | Reference Offset:  ['75','112'] | Reference Text:  <S sid = 75 ssid = >To accomplish this task we took the following approach.</S><S sid = 112 ssid = >If the meanings are similar, it is likely that local context will be inadequate to distinguish a compositional from a non-compositional use of the expression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-1203.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006).</S> | Reference Offset:  ['103','107'] | Reference Text:  <S sid = 103 ssid = >To compare our method with that proposed by Baldwin et al. (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to the MWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE.</S><S sid = 107 ssid = >There are a number of issues that complicate the workability of the unsupervised technique described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-1203.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our results using 1NN, 72:4%, are comparable to those of Katz and Giesbrecht (2006) using this method on their German data (72%).</S> | Reference Offset:  ['106','122'] | Reference Text:  <S sid = 106 ssid = >Even in the case of the nouns, however, the results are, for the most part, markedly inferior to the results we achieved using the composed vectors.</S><S sid = 122 ssid = >Rather promising results were attained using only local context, however.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-1203.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment.</S> | Reference Offset:  ['40','111'] | Reference Text:  <S sid = 40 ssid = >As noted by Sag et al. (2002) many MWEs are simply “institutionalized phrases” whose meanings are perfectly compositional, but whose frequency of use (or other non-linguistic factors) make them highly salient.</S><S sid = 111 ssid = >Our technique relies on these meaning being highly distinct.</S> | Discourse Facet:  NA | Annotator: Automatic


