Citance Number: 1 | Reference Article:  P96-1024.txt | Citing Article:  W01-0720.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Two measures are used to evaluate the parses: lexical accuracy, which is the percentage of correctly tagged words compared to the extracted gold standard corpus (Watkinson and Manandhar, 2001) and average crossing bracket rate (CBR) (Goodman, 1996).</S> | Reference Offset:  ['10','39'] | Reference Text:  <S sid = 10 ssid = >There are many different ways to evaluate these parses.</S><S sid = 39 ssid = >It is often called the Crossing Brackets Rate.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P96-1024.txt | Citing Article:  P14-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008).</S> | Reference Offset:  ['81','122'] | Reference Text:  <S sid = 81 ssid = >Let us define a new function, g(s,t, X).</S><S sid = 122 ssid = >For this experiment, a very simple grammar was induced by counting, using a portion of the Penn Tree Bank, version 0.5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P96-1024.txt | Citing Article:  P06-2101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).</S> | Reference Offset:  ['64','139'] | Reference Text:  <S sid = 64 ssid = >Consider writing a parser for a domain such as machine assisted translation.</S><S sid = 139 ssid = >Using this technique, along with other optimizations, we achieved a 500 times speedup.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P96-1024.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A probability model permits alternative decoding procedures (Goodman, 1996).</S> | Reference Offset:  ['87','137'] | Reference Text:  <S sid = 87 ssid = >The entry maxc [1, n] contains the expected number of correct constituents, given the model.</S><S sid = 137 ssid = >We have used the technique outlined in this paper in other work (Goodman, 1996) to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the possible derivations was a slow Monte Carlo algorithm (Bod, 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P96-1024.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005).</S> | Reference Offset:  ['80','134'] | Reference Text:  <S sid = 80 ssid = >The Inside probability is defined as e(s,t, X) = P(X Os) and the Outside probability is f(s,t, X) = P(S 3-1 X n W1 wt-1-1)â€¢ Note that while Baker and others have used these probabilites for inducing grammars, here they are used only for parsing.</S><S sid = 134 ssid = >Similarly, the Bracketed Recall Algorithm improves performance (versus Labelled Tree) on Consistent Brackets and Bracketed Recall criteria.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P96-1024.txt | Citing Article:  P06-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Their algorithm is therefore the labelled recall algorithm of Goodman (1996) but applied to rules.</S> | Reference Offset:  ['15','133'] | Reference Text:  <S sid = 15 ssid = >Then, in Section 3, we discuss the Labelled Recall Algorithm, a new algorithm that maximizes performance on the Labelled Recall Rate.</S><S sid = 133 ssid = >In particular, the Labelled Recall Algorithm can improve performance versus the Labelled Tree Algorithm on the Consistent Brackets, Labelled Recall, and Bracketed Recall criteria.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P96-1024.txt | Citing Article:  P06-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al (2005).</S> | Reference Offset:  ['23','137'] | Reference Text:  <S sid = 23 ssid = >In this paper we assume all guessed parse trees are binary branching.</S><S sid = 137 ssid = >We have used the technique outlined in this paper in other work (Goodman, 1996) to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the possible derivations was a slow Monte Carlo algorithm (Bod, 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P96-1024.txt | Citing Article:  P11-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (This is similar to the maximum recall objective for approximate inference (Goodman, 1996b)).</S> | Reference Offset:  ['70','98'] | Reference Text:  <S sid = 70 ssid = >Similar counting holds for the other three.</S><S sid = 98 ssid = >The algorithm for Bracketed Recall parsing is extremely similar to that for Labelled Recall parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P96-1024.txt | Citing Article:  P07-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Goodman (1996) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse.</S> | Reference Offset:  ['67','97'] | Reference Text:  <S sid = 67 ssid = >The Labelled Recall Algorithm finds that tree TG which has the highest expected value for the Labelled Recall Rate, LINc (where L is the number of correct labelled constituents, and Nc is the number of nodes in the correct parse).</S><S sid = 97 ssid = >(Remember that B is the number of brackets that are correct, and Nc is the number of constituents in the correct parse.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P96-1024.txt | Citing Article:  P05-3031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The performance of web-page structuring algorithms can be evaluated via the nested-list form of tree by bracketed recall and bracketed precision (Goodman, 1996).</S> | Reference Offset:  ['96','134'] | Reference Text:  <S sid = 96 ssid = >For the Bracketed Recall Algorithm, we find the parse that maximizes the expected Bracketed Recall Rate, BINc.</S><S sid = 134 ssid = >Similarly, the Bracketed Recall Algorithm improves performance (versus Labelled Tree) on Consistent Brackets and Bracketed Recall criteria.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P96-1024.txt | Citing Article:  N10-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996).</S> | Reference Offset:  ['88','141'] | Reference Text:  <S sid = 88 ssid = >The Labelled Recall Algorithm maximizes the expected number of correct labelled constituents.</S><S sid = 141 ssid = >Furthermore, we will show that the two algorithms presented, the Labelled Recall Algorithm and the Bracketed Recall Algorithm, are both special cases of a more general algorithm, the General Recall Algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P96-1024.txt | Citing Article:  P10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We then compute outside scores for bi spans under a max-sum (Goodman, 1996).</S> | Reference Offset:  ['52','99'] | Reference Text:  <S sid = 52 ssid = >Unfortunately, this criterion is relatively difficult to maximize, since it is time-consuming to compute the probability that a particular constituent crosses some constituent in the correct parse.</S><S sid = 99 ssid = >The only required change is that we sum over the symbols X to calculate max_g, rather than maximize over them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P96-1024.txt | Citing Article:  P05-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >Thus, the expected value of L for any of these trees is 1.75.</S><S sid = 144 ssid = >I would also like to thank Stanley Chen, Andrew Kehler, Lillian Lee, and Stuart Shieber for helpful discussions, and comments on earlier drafts, and the anonymous reviewers for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P96-1024.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >Thus, the expected value of L for any of these trees is 1.75.</S><S sid = 144 ssid = >I would also like to thank Stanley Chen, Andrew Kehler, Lillian Lee, and Stuart Shieber for helpful discussions, and comments on earlier drafts, and the anonymous reviewers for their comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P96-1024.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Goodman (1996b), Petrov and Klein (2007), and Matsuzaki et al (2005) describe the details of constituent, rule-sum and variational objectives respectively.</S> | Reference Offset:  ['52','104'] | Reference Text:  <S sid = 52 ssid = >Unfortunately, this criterion is relatively difficult to maximize, since it is time-consuming to compute the probability that a particular constituent crosses some constituent in the correct parse.</S><S sid = 104 ssid = >In both experiments the grammars could not parse some sentences, 0.5% and 9%, respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P96-1024.txt | Citing Article:  P03-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002).</S> | Reference Offset:  ['21','98'] | Reference Text:  <S sid = 21 ssid = >Let wa denote word a of the sentence under consideration.</S><S sid = 98 ssid = >The algorithm for Bracketed Recall parsing is extremely similar to that for Labelled Recall parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P96-1024.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996).</S> | Reference Offset:  ['20','98'] | Reference Text:  <S sid = 20 ssid = >Finally, we discuss the relationship of these metrics to parsing algorithms.</S><S sid = 98 ssid = >The algorithm for Bracketed Recall parsing is extremely similar to that for Labelled Recall parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P96-1024.txt | Citing Article:  P11-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008).</S> | Reference Offset:  ['19','137'] | Reference Text:  <S sid = 19 ssid = >Next, we define the different metrics used in evaluation.</S><S sid = 137 ssid = >We have used the technique outlined in this paper in other work (Goodman, 1996) to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the possible derivations was a slow Monte Carlo algorithm (Bod, 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P96-1024.txt | Citing Article:  D07-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A closely related method, applied by Goodman (1996) is called minimum-risk decoding.</S> | Reference Offset:  ['39','42'] | Reference Text:  <S sid = 39 ssid = >It is often called the Crossing Brackets Rate.</S><S sid = 42 ssid = >This metric is closely related to the Bracketed Tree Rate.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P96-1024.txt | Citing Article:  N07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While the most probable parse problem is NP-complete (Simaan, 1992), several approximate methods exist, including n-best re-ranking by parse likelihood, the labeled bracket alorithm of Goodman (1996), and a variational approximation introduced in Matsuzakiet al (2005).</S> | Reference Offset:  ['1','7'] | Reference Text:  <S sid = 1 ssid = >Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.</S><S sid = 7 ssid = >Various methods can be used for finding these parses.</S> | Discourse Facet:  NA | Annotator: Automatic


