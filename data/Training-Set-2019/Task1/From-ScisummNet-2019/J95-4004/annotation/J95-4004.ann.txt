Citance Number: 1 | Reference Article:  J95-4004.txt | Citing Article:  P96-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described.</S> | Reference Offset:  ['114','197'] | Reference Text:  <S sid = 114 ssid = >The direct correlation between rules and performance improvement in transformation-based learning can make the learned rules more readily interpretable than decision tree rules for increasing population purity.'</S><S sid = 197 ssid = >If the most likely tag for unknown words can be assigned with high accuracy, then the contextual rules can be used to improve accuracy, as described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J95-4004.txt | Citing Article:  P96-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus.</S> | Reference Offset:  ['36','258'] | Reference Text:  <S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S><S sid = 258 ssid = >This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation (Brill and Resnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J95-4004.txt | Citing Article:  P96-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The other tagger was the rule-based tagger of Brill (Brill, 1995).</S> | Reference Offset:  ['36','258'] | Reference Text:  <S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S><S sid = 258 ssid = >This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation (Brill and Resnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J95-4004.txt | Citing Article:  E12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning.</S> | Reference Offset:  ['115','194'] | Reference Text:  <S sid = 115 ssid = >In this section we describe the practical application of transformation-based learning to part-of-speech tagging.'</S><S sid = 194 ssid = >So far, we have not addressed the problem of unknown words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J95-4004.txt | Citing Article:  E12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Dojchinova and Mihov (2004) mapped their initial tag set of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules.</S> | Reference Offset:  ['217','248'] | Reference Text:  <S sid = 217 ssid = >Using the tagger without lexicalized rules, an overall accuracy of 96.3% and an unknown word accuracy of 82.0% is obtained.</S><S sid = 248 ssid = >Ideally, we would like to achieve as large an increase in accuracy with as few extra tags as possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J95-4004.txt | Citing Article:  C02-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used Brill's tagger (Brill, 1995) and Memory-Based Shallow Parser (Daelemansetal., 1999) to analyze English sentences.</S> | Reference Offset:  ['36','258'] | Reference Text:  <S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S><S sid = 258 ssid = >This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation (Brill and Resnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J95-4004.txt | Citing Article:  E06-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill's part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list.</S> | Reference Offset:  ['36','105'] | Reference Text:  <S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S><S sid = 105 ssid = >Even if decision trees are applied to a corpus in a left-to-right fashion, they are allowed only one pass in which to properly classify.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J95-4004.txt | Citing Article:  W04-2422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >According to (Brill, 1995), a Transformation-Based Error-Driven learning application is defined by 1. The initial annotation scheme 2. The space of allowable transformations 3. The iterative algorithm for choosing a transformation sequence.</S> | Reference Offset:  ['40','93'] | Reference Text:  <S sid = 40 ssid = >Figure 1 illustrates how transformation-based error-driven learning works.</S><S sid = 93 ssid = >There are a number of practical differences between transformation-based error-driven learning and learning decision trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J95-4004.txt | Citing Article:  W04-2422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, in a part-of-speech tagging task, the initial an notation may assign each token its most likely tag without any regard to context (Brill, 1995).</S> | Reference Offset:  ['33','157'] | Reference Text:  <S sid = 33 ssid = >Consider the part-of-speech tagging example above.</S><S sid = 157 ssid = >The tenth transformation is for the token 's, which is a separate token in the Penn Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J95-4004.txt | Citing Article:  W04-2422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The addition of a look-ahead searcher has been suggested (Brill, 1995), but we have not seen it implemented in a research context, likely due to the fact that a straightforward implementation of the concept would at minimum square the amount of time required for training.</S> | Reference Offset:  ['54','252'] | Reference Text:  <S sid = 54 ssid = >Other more sophisticated search techniques could be used, such as simulated annealing or learning with a look-ahead window, but we have not yet explored these alternatives.</S><S sid = 252 ssid = >Each known word in the test corpus was tagged with all tags seen with that word in the training corpus and the five most likely unknown-word tags were assigned to all words not seen in the training corpus.'</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J95-4004.txt | Citing Article:  N03-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detection (Reynar and Ratnaparkhi, 1997), and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications (Brill, 1995).</S> | Reference Offset:  ['36','245'] | Reference Text:  <S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S><S sid = 245 ssid = >The initial-state annotator is the tagging output of the previously described one-best transformation-based tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J95-4004.txt | Citing Article:  E09-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TBL is a machine learning approach that has been employed to solve a number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (Brill, 1995).</S> | Reference Offset:  ['0','36'] | Reference Text:  <S sid = 0 ssid = >Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging</S><S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J95-4004.txt | Citing Article:  P05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratna park hi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al, 2003).</S> | Reference Offset:  ['188','254'] | Reference Text:  <S sid = 188 ssid = >Accuracy of that tagger was 97.0%.</S><S sid = 254 ssid = >The transformation-based tagger obtained the same accuracy with 1.43 tags per word, one third the number of additional tags as the baseline tagger.'</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J95-4004.txt | Citing Article:  I08-4028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A class sequence example Transformation-based learning is a symbolic machine learning method, introduced by (Eric Brill, 1995).</S> | Reference Offset:  ['55','65'] | Reference Text:  <S sid = 55 ssid = >Figure 2 shows an example of learning transformations.</S><S sid = 65 ssid = >For example, take the sequence:</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J95-4004.txt | Citing Article:  W02-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There has been a modest amount of previous work on improving probabilistic decision lists, as well as a fair amount of work in related fields, especially in transformation-based learning (Brill, 1995).</S> | Reference Offset:  ['93','260'] | Reference Text:  <S sid = 93 ssid = >There are a number of practical differences between transformation-based error-driven learning and learning decision trees.</S><S sid = 260 ssid = >This work was funded in part by NSF grant IRI-9502312.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J95-4004.txt | Citing Article:  W02-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, in part-of-speech tagging (Brill, 1995), when the tag of one word is changed, it changes the answers to questions for nearby words.</S> | Reference Offset:  ['99','225'] | Reference Text:  <S sid = 99 ssid = >For instance, whether the previous word is tagged as to-infinitival or to-preposition may be a good cue for determining the part of speech of a word.'</S><S sid = 225 ssid = >For tagging unknown words, each word is initially assigned a part-of-speech tag based on word and word-distribution features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J95-4004.txt | Citing Article:  W02-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, since probabilistic decision lists are probabilistic analogs of TBLs, we compared to TBL (Brill,1995).</S> | Reference Offset:  ['36','45'] | Reference Text:  <S sid = 36 ssid = >This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994).</S><S sid = 45 ssid = >Once text has been passed through the initial-state annotator, it is then compared to the truth.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J95-4004.txt | Citing Article:  W06-0141.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Partly inheriting from (Brill, 1995), we applied error-driven learning to filter prefixes in Sp and suffixes in Ss.</S> | Reference Offset:  ['40','93'] | Reference Text:  <S sid = 40 ssid = >Figure 1 illustrates how transformation-based error-driven learning works.</S><S sid = 93 ssid = >There are a number of practical differences between transformation-based error-driven learning and learning decision trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J95-4004.txt | Citing Article:  W02-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank.</S> | Reference Offset:  ['142','216'] | Reference Text:  <S sid = 142 ssid = >In the nonlexicalized tagger, the transformation templates we use are: Change tag a to tag b when: where a, b, z and w are variables over the set of parts of speech.</S><S sid = 216 ssid = >To our knowledge, this is the highest overall tagging accuracy ever quoted on the Penn Treebank Corpus when making the open vocabulary assumption.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J95-4004.txt | Citing Article:  C04-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995).</S> | Reference Offset:  ['183','238'] | Reference Text:  <S sid = 183 ssid = >These results are summarized in table 1.</S><S sid = 238 ssid = >In table 2, we show tagging results obtained on a number of different corpora, in each case training on roughly 9.5 x 105 words total and testing on a separate test set of 1.5-2 x 108 words.</S> | Discourse Facet:  NA | Annotator: Automatic


