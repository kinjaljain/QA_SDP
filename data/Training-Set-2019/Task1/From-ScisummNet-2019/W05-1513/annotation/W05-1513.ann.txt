Citance Number: 1 | Reference Article:  W05-1513.txt | Citing Article:  W05-1515.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items.</S> | Reference Offset:  ['6','111'] | Reference Text:  <S sid = 6 ssid = >Two classifier-based deterministic dependency parsers for English have been proposed recently (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003).</S><S sid = 111 ssid = >Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W05-1513.txt | Citing Article:  W05-1515.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers.</S> | Reference Offset:  ['6','126'] | Reference Text:  <S sid = 6 ssid = >Two classifier-based deterministic dependency parsers for English have been proposed recently (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003).</S><S sid = 126 ssid = >We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W05-1513.txt | Citing Article:  W05-1515.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005).</S> | Reference Offset:  ['66','111'] | Reference Text:  <S sid = 66 ssid = >Training the parser is accomplished by training its classifier.</S><S sid = 111 ssid = >Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion.</S> | Reference Offset:  ['13','85'] | Reference Text:  <S sid = 13 ssid = >Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S><S sid = 85 ssid = >As mentioned before, our parser shares similarities with the dependency parsers of Yamada and Matsumoto (2003) and Nivre and Scholz (2004) in that it uses a classifier to guide the parsing process in deterministic fashion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005).</S> | Reference Offset:  ['13','27'] | Reference Text:  <S sid = 13 ssid = >Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S><S sid = 27 ssid = >Our parser employs a basic bottom-up shift-reduce parsing algorithm, requiring only a single pass over the input string.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors.</S> | Reference Offset:  ['129','136'] | Reference Text:  <S sid = 129 ssid = >While in many natural language processing tasks different classifiers perform at similar levels of accuracy, we have observed a dramatic difference between using support vector machines and a memory-based learner.</S><S sid = 136 ssid = >Using SVMs for classification, the parser has labeled constituent precision and recall higher than 87% when using the correct part-of-speech tags, and slightly higher than 86% when using automatically assigned partof-speech tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005).</S> | Reference Offset:  ['71','141'] | Reference Text:  <S sid = 71 ssid = >This set of features and corresponding actions is then used to train a classifier, resulting in a complete parser.</S><S sid = 141 ssid = >Future work includes the investigation of the effects of individual features, the use of additional classification features, and the use of different classifiers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005).</S> | Reference Offset:  ['109','111'] | Reference Text:  <S sid = 109 ssid = >The total number of training instances was about 1.5 million.</S><S sid = 111 ssid = >Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes.</S> | Reference Offset:  ['66','112'] | Reference Text:  <S sid = 66 ssid = >Training the parser is accomplished by training its classifier.</S><S sid = 112 ssid = >This greatly reduced the time required to train the SVMs, but even with the splitting of the training set, total training time was about 62 hours.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W05-1513.txt | Citing Article:  P06-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005).</S> | Reference Offset:  ['95','110'] | Reference Text:  <S sid = 95 ssid = >Using decision trees and fewer features, Kalt’s parser has significantly faster training and parsing times, but its accuracy is much lower than that of our parser.</S><S sid = 110 ssid = >The classifier in the SVM-based parser (denoted by SVMpar) uses the polynomial kernel with degree 2, following the work of Yamada and Matsumoto (2003) on SVM-based deterministic dependency parsing, and a one-against-all scheme for multi-class classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W05-1513.txt | Citing Article:  W06-3603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples.</S> | Reference Offset:  ['13','72'] | Reference Text:  <S sid = 13 ssid = >Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S><S sid = 72 ssid = >When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W05-1513.txt | Citing Article:  W07-2211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005).</S> | Reference Offset:  ['90','133'] | Reference Text:  <S sid = 90 ssid = >Our parser only pursues one path per sentence, but it is very fast and of comparable accuracy (see section 4).</S><S sid = 133 ssid = >Additionally, it is considerably faster than lexicalized PCFG-based parsers, and offers a good alternative for when fast parsing is needed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W05-1513.txt | Citing Article:  P06-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['71','144'] | Reference Text:  <S sid = 71 ssid = >This set of features and corresponding actions is then used to train a classifier, resulting in a complete parser.</S><S sid = 144 ssid = >Additionally, we plan to investigate the use of the beam strategy of Ratnaparkhi (1997) to pursue multiple parses while keeping the run-time linear.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W05-1513.txt | Citing Article:  P06-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005).</S> | Reference Offset:  ['11','36'] | Reference Text:  <S sid = 11 ssid = >For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002).</S><S sid = 36 ssid = >An example of transformation/detransformation is shown in figure 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W05-1513.txt | Citing Article:  P06-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples.</S> | Reference Offset:  ['13','72'] | Reference Text:  <S sid = 13 ssid = >Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S><S sid = 72 ssid = >When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W05-1513.txt | Citing Article:  I08-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used.</S> | Reference Offset:  ['13','33'] | Reference Text:  <S sid = 13 ssid = >Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S><S sid = 33 ssid = >The transformed (or “binarized”) trees may then be used for training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W05-1513.txt | Citing Article:  I08-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures.</S> | Reference Offset:  ['72','126'] | Reference Text:  <S sid = 72 ssid = >When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).</S><S sid = 126 ssid = >We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W05-1513.txt | Citing Article:  P06-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks.</S> | Reference Offset:  ['13','94'] | Reference Text:  <S sid = 13 ssid = >Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S><S sid = 94 ssid = >The parser in Kalt (2004) uses a similar algorithm to the one described here, but the classification task is framed differently.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W05-1513.txt | Citing Article:  P06-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees.</S> | Reference Offset:  ['29','35'] | Reference Text:  <S sid = 29 ssid = >In order to use trees with arbitrary branching for training, or generating them with the parser, we employ an instance of the transformation/detransformation process described in (Johnson, 1998).</S><S sid = 35 ssid = >This involves the removal of non-terminals introduced in the transformation process, producing trees with arbitrary branching.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W05-1513.txt | Citing Article:  P06-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time.</S> | Reference Offset:  ['0','84'] | Reference Text:  <S sid = 0 ssid = >A Classifier-Based Parser With Linear Run-Time Complexity</S><S sid = 84 ssid = >Thus, the parser runs in linear time, assuming that classifying a parser action is done in constant time.</S> | Discourse Facet:  NA | Annotator: Automatic


