Citance Number: 1 | Reference Article:  N07-1051.txt | Citing Article:  D07-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar methods were applied by Matsuzaki et al (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations.</S> | Reference Offset:  ['7','172'] | Reference Text:  <S sid = 7 ssid = >Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006).</S><S sid = 172 ssid = >In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N07-1051.txt | Citing Article:  P14-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al, 2005).</S> | Reference Offset:  ['142','172'] | Reference Text:  <S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S><S sid = 172 ssid = >In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N07-1051.txt | Citing Article:  C10-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al, 2005), which is a bracketed corpus in (almost) PTB style.</S> | Reference Offset:  ['142','188'] | Reference Text:  <S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S><S sid = 188 ssid = >4 shows the performance on the Brown corpus during hierarchical training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N07-1051.txt | Citing Article:  P13-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On standard evaluations using both the Penn Tree bank and the Penn Chinese Treebank, our parser gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser.</S> | Reference Offset:  ['36','142'] | Reference Text:  <S sid = 36 ssid = >Empirically, the gains on the English Penn treebank level off after 6 rounds.</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N07-1051.txt | Citing Article:  D12-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use Berkeley PCFG parser (Petrov and Klein, 2007) for all experiments.</S> | Reference Offset:  ['136','142'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N07-1051.txt | Citing Article:  D12-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that Algorithm 1 & 2 rely on the use of Berkeley parser (Petrov and Klein, 2007).</S> | Reference Offset:  ['136','142'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N07-1051.txt | Citing Article:  P11-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Berkeley and BUBS parsers both parse with the Berkeley latent-variable grammar (Petrov and Klein, 2007b), while the Charniak parser uses a lexicalized grammar, and the exhaustive CKY algorithm is run with a simple Markov order-2 grammar.</S> | Reference Offset:  ['28','45'] | Reference Text:  <S sid = 28 ssid = >We consider PCFG grammars which are derived from a raw treebank as in Petrov et al. (2006): A simple X-bar grammar is created by binarizing the treebank trees.</S><S sid = 45 ssid = >Most relevant to our work is Charniak and Johnson (2005) which uses a pre-parse phase to rapidly parse with a very coarse, unlexicalized treebank grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N07-1051.txt | Citing Article:  P14-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech.</S> | Reference Offset:  ['7','142'] | Reference Text:  <S sid = 7 ssid = >Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N07-1051.txt | Citing Article:  P14-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the experiments in this paper, we will use the Berkeley parser (Petrov and Klein, 2007) and the related Maryland parser (Huang and Harper,2011).</S> | Reference Offset:  ['136','142'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N07-1051.txt | Citing Article:  P12-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures.</S> | Reference Offset:  ['136','142'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N07-1051.txt | Citing Article:  P12-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to parse HFE.</S> | Reference Offset:  ['66','136'] | Reference Text:  <S sid = 66 ssid = >First, the treebank used to train G may not be available.</S><S sid = 136 ssid = >(2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N07-1051.txt | Citing Article:  W10-1401.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The split-merge smooth implementation of (Petrov et al, 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al, 2009) and for many other languages (Petrov and Klein, 2007).</S> | Reference Offset:  ['142','172'] | Reference Text:  <S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S><S sid = 172 ssid = >In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N07-1051.txt | Citing Article:  W12-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the Berkeley Parser (Petrov and Klein, 2007) with the standard model they provide for building syntactic parse trees and defined the patterns for extracting various syntactic features from the trees using the Tregex pattern matcher (Levy and Andrew, 2006).</S> | Reference Offset:  ['142','181'] | Reference Text:  <S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S><S sid = 181 ssid = >On English, the parser is outperformed only by the reranking parser of Charniak and Johnson (2005), which has access to a variety of features which cannot be captured by a generative model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N07-1051.txt | Citing Article:  P09-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For this work, we have re-implemented and enhanced the Berkeley parser (Petrov and Klein 2007) in several ways: (1) developed a new method to handle rare words in English and Chinese; (2) developed a new model of unknown Chinese words based on characters in the word; (3) increased robustness by adding adaptive modification of pruning thresholds and smoothing of word emission probabilities.</S> | Reference Offset:  ['51','169'] | Reference Text:  <S sid = 51 ssid = >The contributions of our method are that we derive sequences of refinements in a new way (Sec.</S><S sid = 169 ssid = >Most research on parsing has focused on English and parsing performance on other languages is generally significantly lower.3 Recently, there have been some attempts to adapt parsers developed for English to other languages (Levy and Manning, 2003; Cowan and Collins, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N07-1051.txt | Citing Article:  W11-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To obtain parse trees over both sides of each parallel corpus, we used the English and Chinese grammars of the Berkeley parser (Petrov and Klein, 2007).</S> | Reference Offset:  ['136','142'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N07-1051.txt | Citing Article:  P08-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['48','193'] | Reference Text:  <S sid = 48 ssid = >In their work, the extra pruning was with grammars even coarser than the raw treebank grammar, such as a grammar in which all nonterminals are collapsed.</S><S sid = 193 ssid = >Acknowledgments We would like to thank Eugene Charniak, Mark Johnson and Noah Smith for helpful discussions and comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N07-1051.txt | Citing Article:  W12-4501.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007).</S> | Reference Offset:  ['136','142'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 142 ssid = >2 using the parser of Petrov et al. (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N07-1051.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training.</S> | Reference Offset:  ['37','136'] | Reference Text:  <S sid = 37 ssid = >In Petrov et al. (2006), some simple smoothing is also shown to be effective.</S><S sid = 136 ssid = >(2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N07-1051.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This process not only guarantees that the clusters are hierarchical, it also avoids the state drift discussed by Petrov and Klein (2007).</S> | Reference Offset:  ['39','136'] | Reference Text:  <S sid = 39 ssid = >Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case.</S><S sid = 136 ssid = >(2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N07-1051.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A similar observation was reported in the parsing literature, where coarse-to-fine inference with multiple passes of roughly equal complexity produces tremendous speed-ups (Petrov and Klein, 2007).</S> | Reference Offset:  ['136','190'] | Reference Text:  <S sid = 136 ssid = >(2007).</S><S sid = 190 ssid = >The coarse-to-fine scheme presented here, in conjunction with the risk-appropriate parse selection methodology, allows fast, accurate parsing, in multiple languages and domains.</S> | Discourse Facet:  NA | Annotator: Automatic


