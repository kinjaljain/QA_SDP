Citance Number: 1 | Reference Article:  P99-1065.txt | Citing Article:  W00-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','120'] | Reference Text:  <S sid = 59 ssid = >The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.</S><S sid = 120 ssid = >It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P99-1065.txt | Citing Article:  W00-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','120'] | Reference Text:  <S sid = 59 ssid = >The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.</S><S sid = 120 ssid = >It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P99-1065.txt | Citing Article:  P13-2105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins et al (1999) used transformed constituency tree bank from Prague Dependency Treebank for constituent parsing on Czech.</S> | Reference Offset:  ['11','34'] | Reference Text:  <S sid = 11 ssid = >The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception: following the Praguian linguistic tradition, the syntactic annotation is based on dependencies rather than phrase structures.</S><S sid = 34 ssid = >The Czech PDT contains dependency annotations, but no tree structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P99-1065.txt | Citing Article:  P05-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is consistent with the findings of Collins et al (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%.</S> | Reference Offset:  ['33','101'] | Reference Text:  <S sid = 33 ssid = >Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (Jelinek et al. 94; Magerman 95; Ratnaparkhi 97; Charniak 97; Collins 96; Collins 97)); several (e.g., (Eisner 96; Collins 96; Collins 97; Charniak 97)) emphasize the use of parameters associated with dependencies between pairs of words.</S><S sid = 101 ssid = >We ran three versions of the parser over the final test set: the baseline version, the full model with all additions, and the full model with everything but the bigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P99-1065.txt | Citing Article:  P05-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','120'] | Reference Text:  <S sid = 59 ssid = >The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.</S><S sid = 120 ssid = >It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P99-1065.txt | Citing Article:  P05-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','120'] | Reference Text:  <S sid = 59 ssid = >The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.</S><S sid = 120 ssid = >It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P99-1065.txt | Citing Article:  P05-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999).</S> | Reference Offset:  ['11','29'] | Reference Text:  <S sid = 11 ssid = >The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception: following the Praguian linguistic tradition, the syntactic annotation is based on dependencies rather than phrase structures.</S><S sid = 29 ssid = >Other rules in the tree contribute similar sets of probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P99-1065.txt | Citing Article:  P05-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, we used the method of Collins et al (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features.</S> | Reference Offset:  ['70','84'] | Reference Text:  <S sid = 70 ssid = >Statistical parsers of English typically make use of the roughly 50 POS tags used in the Penn Treebank corpus, but the Czech PDT corpus provides a much richer set of POS tags, with over 3000 possible tags defined by the tagging system and over 1000 tags actually found in the corpus.</S><S sid = 84 ssid = >(The detailed POS was used for the main POS values D, J, V, and X; the case field was used for the other possible main POS values.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P99-1065.txt | Citing Article:  P05-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Czech parser of Collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using English.</S> | Reference Offset:  ['0','63'] | Reference Text:  <S sid = 0 ssid = >A Statistical Parser For Czech</S><S sid = 63 ssid = >The parsers of (Collins 96,97) encoded this as a hard constraint.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P99-1065.txt | Citing Article:  C10-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lattice parsing (Chappelier et al, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.</S> | Reference Offset:  ['43','78'] | Reference Text:  <S sid = 43 ssid = >Figure 4 illustrates how the baseline transformation method can lead to parsing errors in relative clause cases.</S><S sid = 78 ssid = >Table 2 shows a phrase from the corpus, with the alternative possible tags and machine-selected tag for each word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P99-1065.txt | Citing Article:  E06-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajiˇc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al (1999).</S> | Reference Offset:  ['11','88'] | Reference Text:  <S sid = 11 ssid = >The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception: following the Praguian linguistic tradition, the syntactic annotation is based on dependencies rather than phrase structures.</S><S sid = 88 ssid = >We explored two different methods, bottom-up and topdown, for automatically deriving POS tag sets based on counts of governing and dependent tags extracted from the parse trees that the parser constructs from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P99-1065.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','120'] | Reference Text:  <S sid = 59 ssid = >The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.</S><S sid = 120 ssid = >It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P99-1065.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','120'] | Reference Text:  <S sid = 59 ssid = >The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.</S><S sid = 120 ssid = >It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P99-1065.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999).</S> | Reference Offset:  ['24','110'] | Reference Text:  <S sid = 24 ssid = >Each rule has the fonnl : 'With the exception of the top rule in the tree, which has the form TOP H (h) .</S><S sid = 110 ssid = >This is a rule-based system which is based on a manually designed set of rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P99-1065.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This F-measure is based on the recall and precision figures reported in Figure 7.15 in Collins (1999).</S> | Reference Offset:  ['51','110'] | Reference Text:  <S sid = 51 ssid = >See figure 5.</S><S sid = 110 ssid = >This is a rule-based system which is based on a manually designed set of rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P99-1065.txt | Citing Article:  D09-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al, 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002).</S> | Reference Offset:  ['4','7'] | Reference Text:  <S sid = 4 ssid = >Much of the recent research on statistical parsing has focused on English; languages other than English are likely to pose new problems for statistical methods.</S><S sid = 7 ssid = >Thus the techniques and results found for Czech should be relevant to parsing several other languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P99-1065.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The mechanism we employ for incorporating morphology into the PCFG model (the Model 1 parser in (Collins, 1999)) is the modification of its part-of speech (POS) tag set; in this paper, we explain how this mechanism allows the parser to better capture morphological constraints.</S> | Reference Offset:  ['20','101'] | Reference Text:  <S sid = 20 ssid = >The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model.</S><S sid = 101 ssid = >We ran three versions of the parser over the final test set: the baseline version, the full model with all additions, and the full model with everything but the bigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P99-1065.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The authors in (Collins et al, 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech.</S> | Reference Offset:  ['47','113'] | Reference Text:  <S sid = 47 ssid = >In these cases the baseline approach gives tree structures such as that in figure 5(a).</S><S sid = 113 ssid = >(Collins 99) describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of (Collins 97).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P99-1065.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ourbaseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tag set containing almost no morphological information.</S> | Reference Offset:  ['20','101'] | Reference Text:  <S sid = 20 ssid = >The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model.</S><S sid = 101 ssid = >We ran three versions of the parser over the final test set: the baseline version, the full model with all additions, and the full model with everything but the bigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P99-1065.txt | Citing Article:  P05-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is also true of the adaptation of the Collins parser for Czech (Collins et al, 1999) and the finite-state dependency parser for Turkish by Oflazer (2003).</S> | Reference Offset:  ['0','61'] | Reference Text:  <S sid = 0 ssid = >A Statistical Parser For Czech</S><S sid = 61 ssid = >The parser of (Collins 96) used punctuation as an indication of phrasal boundaries.</S> | Discourse Facet:  NA | Annotator: Automatic


