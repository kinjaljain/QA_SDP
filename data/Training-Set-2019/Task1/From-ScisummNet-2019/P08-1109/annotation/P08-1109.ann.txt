Citance Number: 1 | Reference Article:  P08-1109.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','151'] | Reference Text:  <S sid = 47 ssid = >Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG.</S><S sid = 151 ssid = >This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1109.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008).</S> | Reference Offset:  ['17','140'] | Reference Text:  <S sid = 17 ssid = >For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%.</S><S sid = 140 ssid = >The most recent similar research is (Petrov et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1109.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS.</S> | Reference Offset:  ['29','74'] | Reference Text:  <S sid = 29 ssid = >We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.</S><S sid = 74 ssid = >In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1109.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008).</S> | Reference Offset:  ['77','78'] | Reference Text:  <S sid = 77 ssid = >This function, Lˆ is designed to approximate the true function L based off a small subset of the training data represented by Db.</S><S sid = 78 ssid = >Here b, the batch size, means that Db is created by drawing b training examples, with replacement, from the training set D. With this notation we can express the stochastic evaluation of the function as Lˆ (Db;e).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1109.txt | Citing Article:  P12-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','151'] | Reference Text:  <S sid = 47 ssid = >Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG.</S><S sid = 151 ssid = >This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1109.txt | Citing Article:  P11-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS.</S> | Reference Offset:  ['140','146'] | Reference Text:  <S sid = 140 ssid = >The most recent similar research is (Petrov et al., 2007).</S><S sid = 146 ssid = >We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1109.txt | Citing Article:  W10-2925.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008).</S> | Reference Offset:  ['67','131'] | Reference Text:  <S sid = 67 ssid = >Lastly, we iterate through the data multiple times, so if we can compute this information just once, we will save time on all subsequent iterations on that sentence.</S><S sid = 131 ssid = >Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1109.txt | Citing Article:  W10-2925.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms.</S> | Reference Offset:  ['12','82'] | Reference Text:  <S sid = 12 ssid = >This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse.</S><S sid = 82 ssid = >We found that an initial gain of η0 = 0.1 and batch size between 15 and 30 was optimal for this application.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1109.txt | Citing Article:  W10-2925.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing.</S> | Reference Offset:  ['19','131'] | Reference Text:  <S sid = 19 ssid = >This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods.</S><S sid = 131 ssid = >Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1109.txt | Citing Article:  N10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008).</S> | Reference Offset:  ['58','80'] | Reference Text:  <S sid = 58 ssid = >Unlike (Taskar et al., 2004), our algorithm has the advantage of being easily parallelized (see footnote 7 in their paper).</S><S sid = 80 ssid = >The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1109.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser.</S> | Reference Offset:  ['20','105'] | Reference Text:  <S sid = 20 ssid = >In this work, we provide just such a framework for training a feature-rich discriminative parser.</S><S sid = 105 ssid = >For both WSJ15 and WSJ40, we trained a generative model; a discriminative model, which used lexicon features, but no grammar features other than the rules themselves; and a feature-based model which had access to all features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1109.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).</S> | Reference Offset:  ['74','131'] | Reference Text:  <S sid = 74 ssid = >In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time.</S><S sid = 131 ssid = >Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1109.txt | Citing Article:  E09-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['47','151'] | Reference Text:  <S sid = 47 ssid = >Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG.</S><S sid = 151 ssid = >This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1109.txt | Citing Article:  P09-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005).</S> | Reference Offset:  ['32','144'] | Reference Text:  <S sid = 32 ssid = >Our parsing model is based on a conditional random field model, however, unlike previous TreeCRF work, e.g., (Cohn and Blunsom, 2005; Jousse et al., 2006), we do not assume a particular tree structure, and instead find the most likely structure and labeling.</S><S sid = 144 ssid = >Because they were focusing on grammar splitting they, like (Johnson, 2001), did not employ any features, and, like (Taskar et al., 2004), they saw only small gains from switching from generative to discriminative training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1109.txt | Citing Article:  P11-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)).</S> | Reference Offset:  ['136','140'] | Reference Text:  <S sid = 136 ssid = >More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004).</S><S sid = 140 ssid = >The most recent similar research is (Petrov et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1109.txt | Citing Article:  D12-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008).</S> | Reference Offset:  ['72','76'] | Reference Text:  <S sid = 72 ssid = >Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent.</S><S sid = 76 ssid = >Utilization of stochastic optimization routines requires the implementation of a stochastic objective function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1109.txt | Citing Article:  W09-1214.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing.</S> | Reference Offset:  ['38','40'] | Reference Text:  <S sid = 38 ssid = >We then define a conditional probability distribution over entire trees, using the standard CRF distribution, shown in (1).</S><S sid = 40 ssid = >The partition function Zs, which makes the probability of all possible parses sum to unity, is defined over all structures as well as all labelings of those structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1109.txt | Citing Article:  D12-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008).</S> | Reference Offset:  ['17','94'] | Reference Text:  <S sid = 17 ssid = >For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%.</S><S sid = 94 ssid = >For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1109.txt | Citing Article:  N09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008).</S> | Reference Offset:  ['80','136'] | Reference Text:  <S sid = 80 ssid = >The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.</S><S sid = 136 ssid = >More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1109.txt | Citing Article:  D09-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008).</S> | Reference Offset:  ['120','145'] | Reference Text:  <S sid = 120 ssid = >In Figure 3 we show for an example from section 22 the parse trees produced by our generative model and our feature-based discriminative model, and the correct parse.</S><S sid = 145 ssid = >We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length < 15 and the first results for such a parser trained and tested on sentences of length < 40.</S> | Discourse Facet:  NA | Annotator: Automatic


