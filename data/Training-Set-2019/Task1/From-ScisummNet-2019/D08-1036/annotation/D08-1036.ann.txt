Citance Number: 1 | Reference Article:  D08-1036.txt | Citing Article:  P13-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.</S> | Reference Offset:  ['30','89'] | Reference Text:  <S sid = 30 ssid = >On the other hand, Goldwater and Griffiths (2007) reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task.</S><S sid = 89 ssid = >In this section we compare their performance for English part-ofspeech tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1036.txt | Citing Article:  D10-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration.</S> | Reference Offset:  ['33','71'] | Reference Text:  <S sid = 33 ssid = >It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.</S><S sid = 71 ssid = >An explicit sampler represents and samples the HMM parameters 0 and 0 in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1036.txt | Citing Article:  N10-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction).</S> | Reference Offset:  ['27','28'] | Reference Text:  <S sid = 27 ssid = >Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007).</S><S sid = 28 ssid = >These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t  |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1036.txt | Citing Article:  N10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices.</S> | Reference Offset:  ['66','127'] | Reference Text:  <S sid = 66 ssid = >First, the sampler can either be pointwise or blocked.</S><S sid = 127 ssid = >Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1036.txt | Citing Article:  N10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a > 0.99, so we skip this step.</S> | Reference Offset:  ['85','86'] | Reference Text:  <S sid = 85 ssid = >This is done by first computing the parameters 0* and 0* of a proposal HMM using (7). scribed above to produce a proposal state sequence t* for the words in the sentence.</S><S sid = 86 ssid = >Finally, we use a Metropolis-Hastings accept-reject step to decide whether to update the current state sequence for the sentence with the proposal t*, or whether to keep the current state sequence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1036.txt | Citing Article:  D10-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score.</S> | Reference Offset:  ['97','99'] | Reference Text:  <S sid = 97 ssid = >We can partially address this by cross-validation.</S><S sid = 99 ssid = >We call the accuracy of the resulting tagging the crossvalidation accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1036.txt | Citing Article:  P10-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['126','127'] | Reference Text:  <S sid = 126 ssid = >Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.</S><S sid = 127 ssid = >Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1036.txt | Citing Article:  P10-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI.</S> | Reference Offset:  ['91','92'] | Reference Text:  <S sid = 91 ssid = >Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation ofInformation (VI) described by Meilˇa (2003) as an evaluation of an unsupervised tagging.</S><S sid = 92 ssid = >However as Goldwater (p.c.) points out, this may not be an ideal evaluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1036.txt | Citing Article:  P10-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details.</S> | Reference Offset:  ['91','94'] | Reference Text:  <S sid = 91 ssid = >Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation ofInformation (VI) described by Meilˇa (2003) as an evaluation of an unsupervised tagging.</S><S sid = 94 ssid = >Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1036.txt | Citing Article:  P10-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type.</S> | Reference Offset:  ['99','120'] | Reference Text:  <S sid = 99 ssid = >We call the accuracy of the resulting tagging the crossvalidation accuracy.</S><S sid = 120 ssid = >Variational Bayes converges faster than all of the other estimators we examined here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1036.txt | Citing Article:  P10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['126','127'] | Reference Text:  <S sid = 126 ssid = >Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.</S><S sid = 127 ssid = >Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1036.txt | Citing Article:  P10-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance.</S> | Reference Offset:  ['21','103'] | Reference Text:  <S sid = 21 ssid = >One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.</S><S sid = 103 ssid = >Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1036.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008).</S> | Reference Offset:  ['6','119'] | Reference Text:  <S sid = 6 ssid = >We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.</S><S sid = 119 ssid = >Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1036.txt | Citing Article:  W10-2911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models).</S> | Reference Offset:  ['77','118'] | Reference Text:  <S sid = 77 ssid = >As Johnson et al. (2007) explains, samples of the HMM parameters 0 and 0 can be obtained using (5) if required.</S><S sid = 118 ssid = >But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1036.txt | Citing Article:  W11-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy.</S> | Reference Offset:  ['97','119'] | Reference Text:  <S sid = 97 ssid = >We can partially address this by cross-validation.</S><S sid = 119 ssid = >Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1036.txt | Citing Article:  D10-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['126','127'] | Reference Text:  <S sid = 126 ssid = >Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.</S><S sid = 127 ssid = >Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1036.txt | Citing Article:  D10-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['126','127'] | Reference Text:  <S sid = 126 ssid = >Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.</S><S sid = 127 ssid = >Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1036.txt | Citing Article:  D10-1078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping.</S> | Reference Offset:  ['96','98'] | Reference Text:  <S sid = 96 ssid = >If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state.</S><S sid = 98 ssid = >We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1036.txt | Citing Article:  C10-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text.</S> | Reference Offset:  ['103','106'] | Reference Text:  <S sid = 103 ssid = >Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.</S><S sid = 106 ssid = >The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1036.txt | Citing Article:  D12-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy.</S> | Reference Offset:  ['21','99'] | Reference Text:  <S sid = 21 ssid = >One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.</S><S sid = 99 ssid = >We call the accuracy of the resulting tagging the crossvalidation accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


