Citance Number: 1 | Reference Article:  P03-1021.txt | Citing Article:  C04-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003).</S> | Reference Offset:  ['12','30'] | Reference Text:  <S sid = 12 ssid = >In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as measured by automatic evaluation criteria such as word error rate and BLEU.</S><S sid = 30 ssid = >4 yields parameters that are optimal with respect to translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-1021.txt | Citing Article:  P14-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003).</S> | Reference Offset:  ['114','117'] | Reference Text:  <S sid = 114 ssid = >Table 1 provides some statistics on the training, development and test corpus used.</S><S sid = 117 ssid = >The development corpus was used to optimize the parameters of the log-linear model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-1021.txt | Citing Article:  N04-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','161'] | Reference Text:  <S sid = 40 ssid = >In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given reference translations. multi-reference position independent error rate (mPER): This criterion ignores the word order by treating a sentence as a bag-of-words and computing the minimum number of substitutions, insertions, deletions needed to transform the hypothesis into the closest of the given reference translations.</S><S sid = 161 ssid = >This work was supported by DARPA-ITO grant 66001-00-1-9814.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-1021.txt | Citing Article:  P09-2058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set.</S> | Reference Offset:  ['38','134'] | Reference Text:  <S sid = 38 ssid = >Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002).</S><S sid = 134 ssid = >The use of log-linear models for statistical machine translation was suggested by Papineni et al. (1997) and Och and Ney (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-1021.txt | Citing Article:  W12-0117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','161'] | Reference Text:  <S sid = 40 ssid = >In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given reference translations. multi-reference position independent error rate (mPER): This criterion ignores the word order by treating a sentence as a bag-of-words and computing the minimum number of substitutions, insertions, deletions needed to transform the hypothesis into the closest of the given reference translations.</S><S sid = 161 ssid = >This work was supported by DARPA-ITO grant 66001-00-1-9814.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-1021.txt | Citing Article:  W06-3115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).</S> | Reference Offset:  ['1','89'] | Reference Text:  <S sid = 1 ssid = >Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.</S><S sid = 89 ssid = >The basic feature functions of our model are identical to the alignment template approach (Och and Ney, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-1021.txt | Citing Article:  W10-1705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components.</S> | Reference Offset:  ['0','152'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 152 ssid = >Which error rate should be optimized during training?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-1021.txt | Citing Article:  P14-1128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data.</S> | Reference Offset:  ['0','152'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 152 ssid = >Which error rate should be optimized during training?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-1021.txt | Citing Article:  W11-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus.</S> | Reference Offset:  ['116','126'] | Reference Text:  <S sid = 116 ssid = >The basic feature functions were trained using the training corpus.</S><S sid = 126 ssid = >Between BLEU and NIST, the differences are more moderate, but by optimizing on NIST, we still obtain a large improvement when measured with NIST compared to optimizing on BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-1021.txt | Citing Article:  P09-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding.</S> | Reference Offset:  ['0','152'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 152 ssid = >Which error rate should be optimized during training?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-1021.txt | Citing Article:  D09-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function.</S> | Reference Offset:  ['23','59'] | Reference Text:  <S sid = 23 ssid = >In this framework, we have a set of feature functions .</S><S sid = 59 ssid = >Note, that the resulting objective function might still have local optima, which makes the optimization hard compared to using the objective function of Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-1021.txt | Citing Article:  N09-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The component features are weighted to minimize a translation error criterion on a development set (Och, 2003).</S> | Reference Offset:  ['41','144'] | Reference Text:  <S sid = 41 ssid = >BLEU score: This criterion computes the geometric mean of the precision of-grams of various lengths between a hypothesis and a set of reference translations multiplied by a factor BP that penalizes short sentences: NIST score: This criterion computes a weighted precision of-grams between a hypothesis and a set of reference translations multiplied by a factor BP’ that penalizes short sentences: Heredenotes the weighted precision ofgrams in the translation.</S><S sid = 144 ssid = >We presented alternative training criteria for loglinear statistical machine translation models which are directly related to translation quality: an unsmoothed error count and a smoothed error count on a development corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-1021.txt | Citing Article:  N09-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU.</S> | Reference Offset:  ['0','152'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 152 ssid = >Which error rate should be optimized during training?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-1021.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['40','161'] | Reference Text:  <S sid = 40 ssid = >In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given reference translations. multi-reference position independent error rate (mPER): This criterion ignores the word order by treating a sentence as a bag-of-words and computing the minimum number of substitutions, insertions, deletions needed to transform the hypothesis into the closest of the given reference translations.</S><S sid = 161 ssid = >This work was supported by DARPA-ITO grant 66001-00-1-9814.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P03-1021.txt | Citing Article:  W11-2102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 1 ssid = >Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P03-1021.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008).</S> | Reference Offset:  ['0','38'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 38 ssid = >Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P03-1021.txt | Citing Article:  W11-1215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003).</S> | Reference Offset:  ['98','152'] | Reference Text:  <S sid = 98 ssid = >This can happen because with the modified model scaling factors the -best list can change significantly and can include sentences not in the existing-best list.</S><S sid = 152 ssid = >Which error rate should be optimized during training?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P03-1021.txt | Citing Article:  P13-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003).</S> | Reference Offset:  ['42','91'] | Reference Text:  <S sid = 42 ssid = >We use .</S><S sid = 91 ssid = >In addition to the feature functions described in (Och and Ney, 2002), our system includes a phrase penalty (the number of alignment templates used) and special alignment features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P03-1021.txt | Citing Article:  P14-2091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Minimum Error Rate Training In Statistical Machine Translation</S><S sid = 1 ssid = >Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P03-1021.txt | Citing Article:  W12-4207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6.</S> | Reference Offset:  ['23','75'] | Reference Text:  <S sid = 23 ssid = >In this framework, we have a set of feature functions .</S><S sid = 75 ssid = >5) using a log-linear model (Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


