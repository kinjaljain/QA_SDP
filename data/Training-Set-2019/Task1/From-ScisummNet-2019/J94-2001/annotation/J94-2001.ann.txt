Citance Number: 1 | Reference Article:  J94-2001.txt | Citing Article:  A94-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.</S> | Reference Offset:  ['29','115'] | Reference Text:  <S sid = 29 ssid = >The reasons for this preference are presumably that: However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.</S><S sid = 115 ssid = >We tried two different constraints: The tw-constrained ML training is similar to the standard ML training, except that the probabilities p(t/w) are not changed at the end of an iteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J94-2001.txt | Citing Article:  A94-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.</S> | Reference Offset:  ['5','126'] | Reference Text:  <S sid = 5 ssid = >Experiments show that the best training is obtained by using as much tagged text as possible.</S><S sid = 126 ssid = >Conclusion The results presented in this paper show that estimating the parameters of the model by counting relative frequencies over a very large amount of hand-tagged text lead to the best tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J94-2001.txt | Citing Article:  P10-2039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the problem formulation of Merialdo (1994), in which we are given a dictionary of possible tags for each word type.</S> | Reference Offset:  ['37','73'] | Reference Text:  <S sid = 37 ssid = >However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary.</S><S sid = 73 ssid = >In some sense, this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags that it actually had within the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J94-2001.txt | Citing Article:  D10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years Merialdo (1994).</S> | Reference Offset:  ['1','14'] | Reference Text:  <S sid = 1 ssid = >In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.</S><S sid = 14 ssid = >A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J94-2001.txt | Citing Article:  P13-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also did not consider morphological analyzers as a form of type supervision, as suggested by Merialdo (1994).</S> | Reference Offset:  ['20','40'] | Reference Text:  <S sid = 20 ssid = >Consider a sentence W = w1w2 wn, and a sequence of tags T =-- tit2 tn, of the same length.</S><S sid = 40 ssid = >But such sequences may occur if we consider other texts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J94-2001.txt | Citing Article:  W02-0102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Advanced students might also want to read about a modern supervised trigram tagger (Brants, 2000), or the mixed results when one actually trains trigram taggers by EM (Merialdo, 1994).</S> | Reference Offset:  ['73','131'] | Reference Text:  <S sid = 73 ssid = >In some sense, this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags that it actually had within the text.</S><S sid = 131 ssid = >I also want to thank one of the referees for his judicious comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J94-2001.txt | Citing Article:  E09-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM.</S> | Reference Offset:  ['87','127'] | Reference Text:  <S sid = 87 ssid = >This is possible since the FB algorithm is able to train the model using the word sequence only.</S><S sid = 127 ssid = >Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J94-2001.txt | Citing Article:  P10-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data.</S> | Reference Offset:  ['29','127'] | Reference Text:  <S sid = 29 ssid = >The reasons for this preference are presumably that: However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.</S><S sid = 127 ssid = >Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J94-2001.txt | Citing Article:  D08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004).</S> | Reference Offset:  ['0','81'] | Reference Text:  <S sid = 0 ssid = >Tagging English Text With A Probabilistic Model</S><S sid = 81 ssid = >In this case, all alignments for a sentence are equally probable, so that the choice of the correct tag is just a choice at random.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J94-2001.txt | Citing Article:  C10-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >EM was first used in POS tagging in (Merialdo, 1994) which showed that except in conditions where there are no labeled training data at all, EM performs very poorly.</S> | Reference Offset:  ['6','12'] | Reference Text:  <S sid = 6 ssid = >They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.</S><S sid = 12 ssid = >They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J94-2001.txt | Citing Article:  P09-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type.</S> | Reference Offset:  ['37','72'] | Reference Text:  <S sid = 37 ssid = >However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary.</S><S sid = 72 ssid = >We built a dictionary that indicates the list of possible tags for each word, by taking all the words that occur in this text and, for each word, all the tags that are assigned to it somewhere in the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J94-2001.txt | Citing Article:  P09-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full 45-tag set.</S> | Reference Offset:  ['89','122'] | Reference Text:  <S sid = 89 ssid = >The only constraints in this model came from the values k(w It) that were set to zero when the tag t was not possible for the word w (as found in the dictionary).</S><S sid = 122 ssid = >As in the previous experiment, the results in Table 5 show the number of tagging errors when the model is trained with the standard or t-constrained ML training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J94-2001.txt | Citing Article:  P06-2028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994).</S> | Reference Offset:  ['12','61'] | Reference Text:  <S sid = 12 ssid = >They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.</S><S sid = 61 ssid = >We also take advantage of the environment that we have set up to perform other experiments, described in Section 7.3, that have some theoretical interest, but did not bring any improvement in practice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J94-2001.txt | Citing Article:  W10-2925.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We can surmise from the log-likelihood plot that the drop in accuracy is not due to the optimization being led astray, but probably rather due to the complex relationship between likelihood and task specific evaluation metrics in unsupervised learning (Merialdo, 1994).</S> | Reference Offset:  ['57','127'] | Reference Text:  <S sid = 57 ssid = >The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large number of alignments.</S><S sid = 127 ssid = >Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J94-2001.txt | Citing Article:  D12-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Work on type-supervision goes back to (Merialdo, 1994), who introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization.</S> | Reference Offset:  ['53','116'] | Reference Text:  <S sid = 53 ssid = >This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden).</S><S sid = 116 ssid = >The results in Table 4 show the number of tagging errors when the model is trained with the standard or tw-constrained ML training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J94-2001.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Such work has for instance been based on hidden Markov models (Merialdo, 1994).</S> | Reference Offset:  ['6','53'] | Reference Text:  <S sid = 6 ssid = >They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.</S><S sid = 53 ssid = >This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J94-2001.txt | Citing Article:  W04-3235.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging.</S> | Reference Offset:  ['28','127'] | Reference Text:  <S sid = 28 ssid = >It is achieved using a dynamic programming scheme. where 0(W), is the tag assigned to word w, by the tagging procedure in the context of the sentence W. We call this procedure Maximum It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level.</S><S sid = 127 ssid = >Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J94-2001.txt | Citing Article:  P05-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)).</S> | Reference Offset:  ['25','128'] | Reference Text:  <S sid = 25 ssid = >0:W--+T=0(W) There are (at least) two measures for the quality of a tagging procedure: In practice, performance at sentence level is generally lower than performance at word level, since all the words have to be tagged correctly for the sentence to be tagged correctly.</S><S sid = 128 ssid = >In our experiments, ML training degrades the performance unless the initial model is already very bad.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J94-2001.txt | Citing Article:  P06-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Merialdo (1994) reports an accuracy of 86.6% for an unsupervised word-based HMM,.</S> | Reference Offset:  ['13','37'] | Reference Text:  <S sid = 13 ssid = >In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.</S><S sid = 37 ssid = >However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J94-2001.txt | Citing Article:  N10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the common problem formulation for this task described by Merialdo (1994).</S> | Reference Offset:  ['17','27'] | Reference Text:  <S sid = 17 ssid = >Through these different approaches, some common points have emerged: These kinds of considerations fit nicely inside a probabilistic formulation of the problem (Beale 1985; Garside and Leech 1985), which offers the following advantages: In this paper we present a particular probabilistic model, the triclass model, and results from experiments involving different ways to estimate its parameters, with the intention of maximizing the ability of the model to tag text accurately.</S><S sid = 27 ssid = >In the probabilistic formulation of the tagging problem we assume that the alignments are generated by a probabilistic model according to a probability distribution: p(W, T) In this case, depending on the criterion that we choose for evaluation, the optimal tagging procedure is as follows: We call this procedure Viterbi tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


