Citance Number: 1 | Reference Article:  E06-1032.txt | Citing Article:  W06-1610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006).</S> | Reference Offset:  ['0','41'] | Reference Text:  <S sid = 0 ssid = >Re-Evaluation The Role Of Bleu In Machine Translation Research</S><S sid = 41 ssid = >Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1032.txt | Citing Article:  W06-1610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall.</S> | Reference Offset:  ['33','73'] | Reference Text:  <S sid = 33 ssid = >Because Bleu is precision based, and because recall is difficult to formulate over multiple reference translations, a brevity penalty is introduced to compensate for the possibility of proposing highprecision hypothesis translations which are too short.</S><S sid = 73 ssid = >Therefore omitting content-bearing lexical items does not carry a greater penalty than omitting function words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1032.txt | Citing Article:  W06-1610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006).</S> | Reference Offset:  ['12','52'] | Reference Text:  <S sid = 12 ssid = >We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality.</S><S sid = 52 ssid = >It is also therefore possible to have a higher Bleu score without any genuine improvement in translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1032.txt | Citing Article:  W12-6216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006).</S> | Reference Offset:  ['40','66'] | Reference Text:  <S sid = 40 ssid = >The primary reason that Bleu is viewed as a useful stand-in for manual evaluation is that it has been shown to correlate with human judgments of translation quality.</S><S sid = 66 ssid = >However, at today’s levels the amount of variation that Bleu admits is unacceptably high.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1032.txt | Citing Article:  W11-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality.</S> | Reference Offset:  ['118','130'] | Reference Text:  <S sid = 118 ssid = >This serves as another significant counter-example to Bleu’s correlation with human judgments of translation quality, and further increases the concern that Bleu may not be appropriate for comparing systems which employ different translation strategies.</S><S sid = 130 ssid = >In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1032.txt | Citing Article:  W08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although Callison-Burch et al (2006) have recently called into question the utility of BLEU.</S> | Reference Offset:  ['9','86'] | Reference Text:  <S sid = 9 ssid = >However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements.</S><S sid = 86 ssid = >This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1032.txt | Citing Article:  P07-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006).</S> | Reference Offset:  ['67','137'] | Reference Text:  <S sid = 67 ssid = >Figure 1 gives a scatterplot of each of the hypothesis translations produced by the second best Bleu system from the 2005 NIST MT Evaluation.</S><S sid = 137 ssid = >Appropriate uses for Bleu include tracking broad, incremental changes to a single system, comparing systems which employ similar translation strategies (such as comparing phrase-based statistical machine translation systems with other phrase-based statistical machine translation systems), and using Bleu as an objective function to optimize the values of parameters such as feature weights in log linear translation models, until a better metric has been proposed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1032.txt | Citing Article:  P07-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006).</S> | Reference Offset:  ['20','123'] | Reference Text:  <S sid = 20 ssid = >The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations.</S><S sid = 123 ssid = >Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1032.txt | Citing Article:  D07-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006).</S> | Reference Offset:  ['110','138'] | Reference Text:  <S sid = 110 ssid = >We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems.</S><S sid = 138 ssid = >Inappropriate uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1032.txt | Citing Article:  W07-0411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread.</S> | Reference Offset:  ['74','142'] | Reference Text:  <S sid = 74 ssid = >The problem is further exacerbated by Bleu not having any facilities for matching synonyms or lexical variants.</S><S sid = 142 ssid = >For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1032.txt | Citing Article:  W07-0411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation.</S> | Reference Offset:  ['20','110'] | Reference Text:  <S sid = 20 ssid = >The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations.</S><S sid = 110 ssid = >We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1032.txt | Citing Article:  W10-1735.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006).</S> | Reference Offset:  ['45','57'] | Reference Text:  <S sid = 45 ssid = >In order to allow some amount of variant order in phrases, Bleu places no explicit constraints on the order that matching n-grams occur in.</S><S sid = 57 ssid = >Here we denote bigram mismatches for the hypothesis translation given in Table 1 with vertical bars: Appeared calm  |when  |he was  |taken | to the American plane  |,  |which will | to Miami, Florida.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1032.txt | Citing Article:  N07-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['142','143'] | Reference Text:  <S sid = 142 ssid = >For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.</S><S sid = 143 ssid = >The authors are grateful to Amittai Axelrod, Frank Keller, Beata Kouchnir, Jean Senellart, and Matthew Stone for their feedback on drafts of this paper, and to Systran for providing translations of the Europarl test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1032.txt | Citing Article:  N07-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['142','143'] | Reference Text:  <S sid = 142 ssid = >For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.</S><S sid = 143 ssid = >The authors are grateful to Amittai Axelrod, Frank Keller, Beata Kouchnir, Jean Senellart, and Matthew Stone for their feedback on drafts of this paper, and to Systran for providing translations of the Europarl test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1032.txt | Citing Article:  N07-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality.</S> | Reference Offset:  ['16','52'] | Reference Text:  <S sid = 16 ssid = >We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examples of Bleu vastly underestimating the translation quality of systems.</S><S sid = 52 ssid = >It is also therefore possible to have a higher Bleu score without any genuine improvement in translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E06-1032.txt | Citing Article:  P09-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements.</S> | Reference Offset:  ['87','118'] | Reference Text:  <S sid = 87 ssid = >In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.</S><S sid = 118 ssid = >This serves as another significant counter-example to Bleu’s correlation with human judgments of translation quality, and further increases the concern that Bleu may not be appropriate for comparing systems which employ different translation strategies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E06-1032.txt | Citing Article:  C08-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006).</S> | Reference Offset:  ['41','124'] | Reference Text:  <S sid = 41 ssid = >Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations.</S><S sid = 124 ssid = >Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E06-1032.txt | Citing Article:  W12-3149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g.</S> | Reference Offset:  ['18','140'] | Reference Text:  <S sid = 18 ssid = >The rationale behind the development of Bleu (Papineni et al., 2002) is that human evaluation of machine translation can be time consuming and expensive.</S><S sid = 140 ssid = >Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al., 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E06-1032.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator.</S> | Reference Offset:  ['8','80'] | Reference Text:  <S sid = 8 ssid = >All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003).</S><S sid = 80 ssid = >Bleu’s inability to distinguish between randomly generated variations in translation hints that it may not correlate with human judgments of translation quality in some cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E06-1032.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001).</S> | Reference Offset:  ['18','41'] | Reference Text:  <S sid = 18 ssid = >The rationale behind the development of Bleu (Papineni et al., 2002) is that human evaluation of machine translation can be time consuming and expensive.</S><S sid = 41 ssid = >Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations.</S> | Discourse Facet:  NA | Annotator: Automatic


