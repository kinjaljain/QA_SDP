Citance Number: 1 | Reference Article:  P97-1003.txt | Citing Article:  C00-2118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The last two counts (CAUS and ANIM) were performed on a 29-million word parsed corpus (gall Street Journal 1988, provided by Michael Collins (Collins, 1997)).</S> | Reference Offset:  ['3','8'] | Reference Text:  <S sid = 3 ssid = >Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 8 ssid = >A PCFG can be lexicalised by associating a headword with each non-terminal in a parse tree; thus far, (Magerman 95; Jelinek et al. 94) and (Collins 96), which both make heavy use of lexical information, have reported the best statistical parsing performance on Wall Street Journal text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P97-1003.txt | Citing Article:  W04-0819.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These sentences were parsed with the Collins' parser (Collins, 1997).</S> | Reference Offset:  ['91','102'] | Reference Text:  <S sid = 91 ssid = >The parser was trained on sections 02 - 21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al. 93) (approximately 40,000 sentences), and tested on section 23 (2,416 sentences).</S><S sid = 102 ssid = >The model in (Collins 96) is deficient, that is for most sentences S, ET P(T I S) < 1, because probability mass is lost to dependency structures which violate the hard constraint that no links may cross.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P97-1003.txt | Citing Article:  C02-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Substantial improvements have been made to parse western language such as English, and many powerful models have been proposed (Brill 1993, Collins 1997).</S> | Reference Offset:  ['92','117'] | Reference Text:  <S sid = 92 ssid = >We use the PARSEVAL measures (Black et al. 91) to compare performance: number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse Crossing Brackets = number of constituents which violate constituent boundaries with a constituent in the treebank parse.</S><S sid = 117 ssid = >It is interesting to note that Models 1, 2 or 3 could be used as language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P97-1003.txt | Citing Article:  W02-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This model is very similar to the markovized rule models in Collins (1997).</S> | Reference Offset:  ['14','108'] | Reference Text:  <S sid = 14 ssid = >The work makes two advances over previous models: First, Model 1 performs significantly better than (Collins 96), and Models 2 and 3 give further improvements — our final results are 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 108 ssid = >(Eisner 96) proposes 3 dependency models, and gives results that show that a generative model similar to Model 1 performs best of the three.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P97-1003.txt | Citing Article:  W06-2920.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (1997)'s parser and its reimplementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al, 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikel's web page, Arabic.</S> | Reference Offset:  ['14','15'] | Reference Text:  <S sid = 14 ssid = >The work makes two advances over previous models: First, Model 1 performs significantly better than (Collins 96), and Models 2 and 3 give further improvements — our final results are 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 15 ssid = >Second, the parsers in (Collins 96) and (NIagerman 95; Jelinek et al. 94) produce trees without information about whmovement or subcategorisation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P97-1003.txt | Citing Article:  P01-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, empty nodes (represented as -NONE in the tree bank) were turned into rules that generated the empty string (), and there was no collapsing of categories (such as PRT and ADVP) as is of ten done in parsing work (Collins, 1997, etc.).</S> | Reference Offset:  ['20','65'] | Reference Text:  <S sid = 20 ssid = >In a PCFG, for a tree derived by n applications of context-free re-write rules LH Si RHS, 1 i n, The re-write rules are either internal to the tree, where LHS is a non-terminal and RHS is a string of one or more non-terminals; or lexical, where LHS is a part of speech tag and RHS is a word.</S><S sid = 65 ssid = >In particular, the subcategorisation probabilities are smeared by extraction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P97-1003.txt | Citing Article:  W04-2506.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997, can be used to capture the binary dependencies between the head of each phrase.</S> | Reference Offset:  ['23','74'] | Reference Text:  <S sid = 23 ssid = >Each rule now has the form3: H is the head-child of the phrase, which inherits the head-word h from its parent P. L1...L7, and are left and right modifiers of H. Either n or m may be zero, and n = m = 0 for unary rules.</S><S sid = 74 ssid = >The generative process is extended to choose between these cases after generating the head of the phrase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P97-1003.txt | Citing Article:  N09-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the state split grammars of Petrov et al (2006) are all too large to construct unpruned charts in memory.</S> | Reference Offset:  ['0','105'] | Reference Text:  <S sid = 0 ssid = >Three Generative Lexicalized Models For Statistical Parsing</S><S sid = 105 ssid = >(Charniak 95) also uses a lexicalised generative model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P97-1003.txt | Citing Article:  W04-1503.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not.</S> | Reference Offset:  ['1','116'] | Reference Text:  <S sid = 1 ssid = >In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.</S><S sid = 116 ssid = >There has recently been interest in using dependency-based parsing models in speech recognition, for example (Stolcke 96).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P97-1003.txt | Citing Article:  P06-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The supervised component is Collins' parser (Collins, 1997), trained on the Wall Street Journal.</S> | Reference Offset:  ['3','91'] | Reference Text:  <S sid = 3 ssid = >Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 91 ssid = >The parser was trained on sections 02 - 21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al. 93) (approximately 40,000 sentences), and tested on section 23 (2,416 sentences).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P97-1003.txt | Citing Article:  W08-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['45','124'] | Reference Text:  <S sid = 45 ssid = >Adjuncts in the Penn Treebank We add the &quot;-C&quot; suffix to all non-terminals in training data which satisfy the following conditions: In addition, the first child following the head of a prepositional phrase is marked as a complement.</S><S sid = 124 ssid = >This work has also benefited greatly from suggestions and advice from Scott Miller.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P97-1003.txt | Citing Article:  W08-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the tree bank data.</S> | Reference Offset:  ['25','85'] | Reference Text:  <S sid = 25 ssid = >The addition of lexical heads leads to an enormous number of potential rules, making direct estimation of P(RHS I LHS) infeasible because of sparse data problems.</S><S sid = 85 ssid = >All words occurring less than 5 times in training data, and words in test data which have never been seen in training, are replaced with the &quot;UNKNOWN&quot; token.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P97-1003.txt | Citing Article:  D08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Judge et al (2006) produced a corpus of 4,000 questions annotated with syntactic trees, and obtained an improvement in parsing accuracy for Bikel's reimplementation of the Collins parser (Collins, 1997) by training a new parser model with a combination of newspaper and question data.</S> | Reference Offset:  ['3','10'] | Reference Text:  <S sid = 3 ssid = >Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 10 ssid = >This paper proposes three new parsing models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P97-1003.txt | Citing Article:  W06-2902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability.</S> | Reference Offset:  ['11','13'] | Reference Text:  <S sid = 11 ssid = >Model 1 is essentially a generative version of the model described in (Collins 96).</S><S sid = 13 ssid = >In Model 3 we give a probabilistic treatment of wh-movement, which is derived from the analysis given in Generalized Phrase Structure Grammar (Gazdar et al. 95).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P97-1003.txt | Citing Article:  C02-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997).</S> | Reference Offset:  ['89','116'] | Reference Text:  <S sid = 89 ssid = >For unknown words, the output from the tagger described in (Ratnaparkhi 96) is used as the single possible tag for that word.</S><S sid = 116 ssid = >There has recently been interest in using dependency-based parsing models in speech recognition, for example (Stolcke 96).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P97-1003.txt | Citing Article:  W12-3201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins (1997)'s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic.</S> | Reference Offset:  ['14','15'] | Reference Text:  <S sid = 14 ssid = >The work makes two advances over previous models: First, Model 1 performs significantly better than (Collins 96), and Models 2 and 3 give further improvements — our final results are 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 15 ssid = >Second, the parsers in (Collins 96) and (NIagerman 95; Jelinek et al. 94) produce trees without information about whmovement or subcategorisation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P97-1003.txt | Citing Article:  W03-1706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our model is thus a simplification of more sophisticated models which integrate PCFGs with features, such as those in Magerman (1995), Collins (1997) and Goodman (1997).</S> | Reference Offset:  ['14','104'] | Reference Text:  <S sid = 14 ssid = >The work makes two advances over previous models: First, Model 1 performs significantly better than (Collins 96), and Models 2 and 3 give further improvements — our final results are 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</S><S sid = 104 ssid = >The generative model can condition on any structure that has been previously generated - we exploit this in models 2 and 3 - whereas (Collins 96) is restricted to conditioning on features of the surface string alone.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P97-1003.txt | Citing Article:  P03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997).</S> | Reference Offset:  ['39','59'] | Reference Text:  <S sid = 39 ssid = >A post-processing stage could add this detail to the parser output, but we give two reasons for making the distinction while parsing: First, identifying complements is complex enough to warrant a probabilistic treatment.</S><S sid = 59 ssid = >Another obstacle to extracting predicate-argument structure from parse trees is wh-movement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P97-1003.txt | Citing Article:  P03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >At last, the dependency parser presented in (Collins, 1997) is used to generate the full parse.</S> | Reference Offset:  ['24','90'] | Reference Text:  <S sid = 24 ssid = >Figure 1 shows a tree which will be used as an example throughout this paper.</S><S sid = 90 ssid = >A CKY style dynamic programming chart parser is used to find the maximum probability tree for each sentence (see figure 6).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P97-1003.txt | Citing Article:  P08-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For getting the syntax trees, the latest version of Collins' parser (Collins, 1997) was used.</S> | Reference Offset:  ['4','11'] | Reference Text:  <S sid = 4 ssid = >Generative models of syntax have been central in linguistics since they were introduced in (Chomsky 57).</S><S sid = 11 ssid = >Model 1 is essentially a generative version of the model described in (Collins 96).</S> | Discourse Facet:  NA | Annotator: Automatic


