Citance Number: 1 | Reference Article:  P99-1004.txt | Citing Article:  P14-2049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well.</S> | Reference Offset:  ['0','86'] | Reference Text:  <S sid = 0 ssid = >Measures Of Distributional Similarity</S><S sid = 86 ssid = >We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P99-1004.txt | Citing Article:  I08-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx.</S> | Reference Offset:  ['30','43'] | Reference Text:  <S sid = 30 ssid = >Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.</S><S sid = 43 ssid = >It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)â€¢ Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P99-1004.txt | Citing Article:  P06-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others.</S> | Reference Offset:  ['35','47'] | Reference Text:  <S sid = 35 ssid = >If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999), as is the case for relative frequencies, then we may write the confusion probability as follows: Note that it incorporates unigram probabilities as well as the two distributions q and r. Finally, Kendall's T which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993).</S><S sid = 47 ssid = >Finally, we did not use the KL divergence because it requires a smoothed base language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P99-1004.txt | Citing Article:  P06-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992).</S> | Reference Offset:  ['16','30'] | Reference Text:  <S sid = 16 ssid = >(van Rijsbergen, 1979, pg.</S><S sid = 30 ssid = >Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P99-1004.txt | Citing Article:  P06-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990).</S> | Reference Offset:  ['42','75'] | Reference Text:  <S sid = 42 ssid = >Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a).</S><S sid = 75 ssid = >4 The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P99-1004.txt | Citing Article:  P06-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship.</S> | Reference Offset:  ['0','81'] | Reference Text:  <S sid = 0 ssid = >Measures Of Distributional Similarity</S><S sid = 81 ssid = >40n a related note, an anonymous reviewer cited the 30 following example from the psychology literature: we can say Smith's lecture is like a sleeping pill, but &quot;not the other way round&quot;. average error rate Error rates (averages and ranges) definition of commonality is left to the user (several different definitions are proposed for different tasks).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P99-1004.txt | Citing Article:  C10-1143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions.</S> | Reference Offset:  ['0','28'] | Reference Text:  <S sid = 0 ssid = >Measures Of Distributional Similarity</S><S sid = 28 ssid = >The cosine metric and Jaccard's coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P99-1004.txt | Citing Article:  C04-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure.</S> | Reference Offset:  ['30','80'] | Reference Text:  <S sid = 30 ssid = >Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.</S><S sid = 80 ssid = >Note that at a = 1, the skew divergence is exactly the KL divergence, and s1/2 is twice one of the summands of JS (note that it is still asymmetric).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P99-1004.txt | Citing Article:  E09-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand.</S> | Reference Offset:  ['15','71'] | Reference Text:  <S sid = 15 ssid = >On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised.</S><S sid = 71 ssid = >The Jensen-Shannon divergence and the L1 norm can be computed simply by knowing the values of q and r on Vqr.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P99-1004.txt | Citing Article:  W12-4102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >Measures Of Distributional Similarity</S><S sid = 8 ssid = >There are many plausible measures of distributional similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P99-1004.txt | Citing Article:  W06-1406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','88'] | Reference Text:  <S sid = 43 ssid = >It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)â€¢ Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.</S><S sid = 88 ssid = >IRI9712068.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P99-1004.txt | Citing Article:  W09-0204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We are interested in some distributional similarities (Lee, 1999) given certain context.</S> | Reference Offset:  ['0','33'] | Reference Text:  <S sid = 0 ssid = >Measures Of Distributional Similarity</S><S sid = 33 ssid = >Whether we mean f or C â€” f should be clear from context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P99-1004.txt | Citing Article:  W06-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences.</S> | Reference Offset:  ['23','77'] | Reference Text:  <S sid = 23 ssid = >This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al., 1999).</S><S sid = 77 ssid = >But the substitutability of one word for another need not symmetric.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P99-1004.txt | Citing Article:  W06-1105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second approach is from Lee (1999).</S> | Reference Offset:  ['7','82'] | Reference Text:  <S sid = 7 ssid = >Both use P(v) to estimate P(v In) when (n, v) is unseen, essentially ignoring the identity of n. An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m. We focus on distributional rather than semantic similarity (e.g., Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions â€” although the words &quot;chance&quot; and &quot;probability&quot; are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in.</S><S sid = 82 ssid = >We view the empirical approach taken in this paper as complementary to Lin's.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P99-1004.txt | Citing Article:  C02-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >Measures Of Distributional Similarity</S><S sid = 8 ssid = >There are many plausible measures of distributional similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P99-1004.txt | Citing Article:  C02-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999).</S> | Reference Offset:  ['9','22'] | Reference Text:  <S sid = 9 ssid = >In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.</S><S sid = 22 ssid = >Using this insight, we developed an information-theoretic metric, the skew divergence, which incorporates the support-intersection data in an asymmetric fashion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P99-1004.txt | Citing Article:  W06-0101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable.</S> | Reference Offset:  ['30','86'] | Reference Text:  <S sid = 30 ssid = >Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.</S><S sid = 86 ssid = >We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P99-1004.txt | Citing Article:  W04-2411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions.</S> | Reference Offset:  ['49','80'] | Reference Text:  <S sid = 49 ssid = >That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools.</S><S sid = 80 ssid = >Note that at a = 1, the skew divergence is exactly the KL divergence, and s1/2 is twice one of the summands of JS (note that it is still asymmetric).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P99-1004.txt | Citing Article:  W04-2411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['43','88'] | Reference Text:  <S sid = 43 ssid = >It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)â€¢ Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.</S><S sid = 88 ssid = >IRI9712068.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P99-1004.txt | Citing Article:  W04-2411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos).</S> | Reference Offset:  ['30','72'] | Reference Text:  <S sid = 30 ssid = >Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.</S><S sid = 72 ssid = >For the cosine and the confusion probability, the distribution values on Vqr are key, but other information is also incorporated.</S> | Discourse Facet:  NA | Annotator: Automatic


