Citance Number: 1 | Reference Article:  J96-1002.txt | Citing Article:  W97-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach made use of a maximum entropy model (Berger et al, 1996) formulated from frequency information for various combinations of the observed features.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Natural Language Processing</S><S sid = 8 ssid = >We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J96-1002.txt | Citing Article:  P14-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al, 1996).</S> | Reference Offset:  ['18','278'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 278 ssid = >Features shown here were the first features selected not from template 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J96-1002.txt | Citing Article:  W06-1643.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We performed feature selection by incrementally growing a log-linear model with order0 features f (x,yt) using a forward feature selection procedure similar to (Berger et al, 1996).</S> | Reference Offset:  ['25','166'] | Reference Text:  <S sid = 25 ssid = >The first task is one of feature selection; the second is one of model selection.</S><S sid = 166 ssid = >To find S, we adopt an incremental approach to feature selection, similar to the strategy used for growing decision trees (Bahl et al. 1989).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J96-1002.txt | Citing Article:  D11-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a "maximum entropy" model (Berger et al., 1996) or a log-linear model.</S> | Reference Offset:  ['120','374'] | Reference Text:  <S sid = 120 ssid = >The relevant steps are outlined here; the reader is referred to Della Pietra et al. (1995) for a more thorough discussion of constrained optimization as applied to maximum entropy.</S><S sid = 374 ssid = >Thus two different philosophical approaches— maximum entropy and maximum likelihood—yield the same result: the model with the greatest entropy consistent with the constraints is the same as the exponential model which best predicts the sample of data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J96-1002.txt | Citing Article:  P99-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Berger et al (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.</S> | Reference Offset:  ['18','330'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 330 ssid = >It is at this point that the algorithm should terminate.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J96-1002.txt | Citing Article:  H05-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al, 1996).</S> | Reference Offset:  ['18','294'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 294 ssid = >Thus, a common task in machine translation is to find safe positions at which Example of an unsafe segmentation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J96-1002.txt | Citing Article:  H05-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al, 1996).</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Natural Language Processing</S><S sid = 4 ssid = >We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J96-1002.txt | Citing Article:  D09-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The log-linear model (LLM), or also known as maximum-entropy model (Berger et al, 1996), is a linear classifier widely used in the NLP literature.</S> | Reference Offset:  ['18','129'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 129 ssid = >The most important practical consequence of this result is that any algorithm for finding the maximum A* of W(A) can be used to find the maximum p. of H(p) for p E C. The log-likelihood Lp(p) of the empirical distribution p as predicted by a model p is defined by3 It is easy to check that the dual function lIi(A) of the previous section is, in fact, just the log-likelihood for the exponential model pA; that is With this interpretation, the result of the previous section can be rephrased as: The model p E C with maximum entropy is the model in the parametric family pA(y1x) that maximizes the likelihood of the training sample /3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J96-1002.txt | Citing Article:  P05-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al, 1996).</S> | Reference Offset:  ['18','141'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 141 ssid = >We present here a version of this algorithm specifically designed for the problem at hand; a proof of the monotonicity and convergence of the algorithm is given in Della Pietra et al. (1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J96-1002.txt | Citing Article:  S10-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al, 1996).</S> | Reference Offset:  ['0','4'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Natural Language Processing</S><S sid = 4 ssid = >We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J96-1002.txt | Citing Article:  W11-2708.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Berger et al (1996) propose to estimate the weight wi of the candidate feature fi, while assuming that the weights of features in S stay constant.</S> | Reference Offset:  ['253','350'] | Reference Text:  <S sid = 253 ssid = >Next we define the set of candidate features.</S><S sid = 350 ssid = >We define candidate features based upon the template features shown in Table 8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J96-1002.txt | Citing Article:  W11-2708.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the gain-informed selection method proposed by Berger et al (1996) still recalculates the weights of all the candidate features during every cycle.</S> | Reference Offset:  ['253','359'] | Reference Text:  <S sid = 253 ssid = >Next we define the set of candidate features.</S><S sid = 359 ssid = >We used the feature-selection algorithm of section 4 to construct a maximum entropy model from candidate features derived from templates 1, 2, and 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J96-1002.txt | Citing Article:  C08-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The maximum entropy approach (Berger et al, 1996) is known to be well suited to solve the classification problem.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Natural Language Processing</S><S sid = 8 ssid = >We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J96-1002.txt | Citing Article:  W12-1637.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The NLU uses a maximum-entropy model (Berger et al, 1996) to classify utterances as one of the user SAs using shallow text features.</S> | Reference Offset:  ['18','264'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 264 ssid = >A maximum entropy model that uses only template 1 features predicts each French translation y with the probability p(y) determined by the empirical data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J96-1002.txt | Citing Article:  N07-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >we use the general technique of choosing the maximum entropy (maxent) distribution that estimates the average of each feature over the training data (Berger et al, 1996).</S> | Reference Offset:  ['105','168'] | Reference Text:  <S sid = 105 ssid = >Among the models p E C, the maximum entropy philosophy dictates that we select the most uniform distribution.</S><S sid = 168 ssid = >The choice of feature to add at each step is determined by the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J96-1002.txt | Citing Article:  I08-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Uses Maximum Entropy (Berger et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER).</S> | Reference Offset:  ['18','264'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 264 ssid = >A maximum entropy model that uses only template 1 features predicts each French translation y with the probability p(y) determined by the empirical data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J96-1002.txt | Citing Article:  P02-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >An especially well-founded framework for doing this is maximum entropy (Berger et al, 1996).</S> | Reference Offset:  ['18','146'] | Reference Text:  <S sid = 18 ssid = >In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al.</S><S sid = 146 ssid = >A simple and effective way of doing this is by Newton's method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J96-1002.txt | Citing Article:  W05-0509.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following exponential form (Berger et al 1996).</S> | Reference Offset:  ['115','133'] | Reference Text:  <S sid = 115 ssid = >To select a model from a set C of allowed probability distributions, choose the model p„ E C with maximum entropy H(p): It can be shown that p, is always well-defined; that is, there is always a unique model p„ with maximum entropy in any constrained set C. The maximum entropy principle presents us with a problem in constrained optimization: find the p„ E C that maximizes H(p).</S><S sid = 133 ssid = >In this case, the maximum entropy model will not have the form pA for any A.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J96-1002.txt | Citing Article:  W06-1617.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Since its introduction to the Natural Language Processing (NLP) community (Berger et al, 1996), ME-based classifiers have been shown to be effective in various NLP tasks.</S> | Reference Offset:  ['0','350'] | Reference Text:  <S sid = 0 ssid = >A Maximum Entropy Approach To Natural Language Processing</S><S sid = 350 ssid = >We define candidate features based upon the template features shown in Table 8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J96-1002.txt | Citing Article:  N06-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al, 1996),.</S> | Reference Offset:  ['31','247'] | Reference Text:  <S sid = 31 ssid = >Finally, in Section 5 we describe the application of maximum entropy ideas to several tasks in stochastic language processing: bilingual sense disambiguation, word reordering, and sentence segmentation.</S><S sid = 247 ssid = >This is just a slightly recast version of a longstanding problem in computational linguistics, namely, sense disambiguation—the determination of a word's sense from its context.</S> | Discourse Facet:  NA | Annotator: Automatic


