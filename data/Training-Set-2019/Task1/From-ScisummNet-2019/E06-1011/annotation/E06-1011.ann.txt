Citance Number: 1 | Reference Article:  E06-1011.txt | Citing Article:  P14-2128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013).</S> | Reference Offset:  ['19','101'] | Reference Text:  <S sid = 19 ssid = >Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was proposed by McDonald et al. (2005c).</S><S sid = 101 ssid = >Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1011.txt | Citing Article:  P12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links.</S> | Reference Offset:  ['1','108'] | Reference Text:  <S sid = 1 ssid = >In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.</S><S sid = 108 ssid = >We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1011.txt | Citing Article:  P12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model.</S> | Reference Offset:  ['62','123'] | Reference Text:  <S sid = 62 ssid = >This is done through the creation of a sibling item in part (B).</S><S sid = 123 ssid = >We also include conjunctions between these features and the direction and distance from sibling j to sibling k. We determined the usefulness of these features on the development set, which also helped us find out that features such as the POS tags of words between the two siblings would not improve accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1011.txt | Citing Article:  P12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left.</S> | Reference Offset:  ['40','41'] | Reference Text:  <S sid = 40 ssid = >This independence between left and right descendants allow us to use a O(n3) second-order projective parsing algorithm, as we will see later.</S><S sid = 41 ssid = >We write s(xi, −, xj) when xj is the first left or first right dependent of word xi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1011.txt | Citing Article:  P12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models.</S> | Reference Offset:  ['55','86'] | Reference Text:  <S sid = 55 ssid = >To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm.</S><S sid = 86 ssid = >Another reasonable approach would be to first find the highest scoring first-order non-projective parse, and then re-arrange edges based on second order scores in a similar manner to the algorithm we described.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1011.txt | Citing Article:  P12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model.</S> | Reference Offset:  ['128','143'] | Reference Text:  <S sid = 128 ssid = >The models rely on part-of-speech tags as input and we used the Ratnaparkhi (1996) tagger to provide these for the development and evaluation set.</S><S sid = 143 ssid = >For our experiments we used the Danish Dependency Treebank v1.0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1011.txt | Citing Article:  P10-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic.</S> | Reference Offset:  ['87','159'] | Reference Text:  <S sid = 87 ssid = >We implemented this method and found that the results were slightly worse.</S><S sid = 159 ssid = >We showed that these approximations can be combined with online learning to achieve fast parsing with competitive parsing accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1011.txt | Citing Article:  E09-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest.</S> | Reference Offset:  ['26','96'] | Reference Text:  <S sid = 26 ssid = >We should note that f(i, j) can be based on arbitrary features of the edge and the input sequence x.</S><S sid = 96 ssid = >Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al., 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1011.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig.</S> | Reference Offset:  ['120','165'] | Reference Text:  <S sid = 120 ssid = >As noted earlier, this representation subsumes the first-order representation of McDonald et al. (2005b), so we can incorporate all of their features as well as the new second-order features we now describe.</S><S sid = 165 ssid = >This framework will possibly allow us to include effectively more global features over the dependency structure than those in our current second-order model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1011.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time.</S> | Reference Offset:  ['55','83'] | Reference Text:  <S sid = 55 ssid = >To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm.</S><S sid = 83 ssid = >Furthermore, it is easy to show that each iteration of the loop takes O(n2) time, resulting in a O(n3 + Mn2) runtime algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1011.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['61','166'] | Reference Text:  <S sid = 61 ssid = >The expression y[i → j] denotes the dependency graph identical to y except that xi’s parent is xi instead shows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent of h1 was h2.</S><S sid = 166 ssid = >This work was supported by NSF ITR grants 0205448.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1011.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work.</S> | Reference Offset:  ['37','142'] | Reference Text:  <S sid = 37 ssid = >In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.</S><S sid = 142 ssid = >Indeed, our second-order projective parser analyzes the test set in 16m32s, and the non-projective approximate parser needs 17m03s to parse the entire evaluation set, showing that runtime for the approximation is completely dominated by the initial call to the second-order projective algorithm and that the post-process edge transformation loop typically only iterates a few times per sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1011.txt | Citing Article:  W06-2934.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006).</S> | Reference Offset:  ['11','29'] | Reference Text:  <S sid = 11 ssid = >When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages.</S><S sid = 29 ssid = >Furthermore, it was shown that this formulation can lead to state-of-the-art results when combined with discriminative learning algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1011.txt | Citing Article:  W06-2934.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm.</S> | Reference Offset:  ['13','97'] | Reference Text:  <S sid = 13 ssid = >This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions.</S><S sid = 97 ssid = >To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1011.txt | Citing Article:  W07-2216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods.</S> | Reference Offset:  ['102','137'] | Reference Text:  <S sid = 102 ssid = >In this section, we review the work of McDonald et al. (2005b) for online large-margin dependency parsing.</S><S sid = 137 ssid = >McDonald et al. (2005c) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-order models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E06-1011.txt | Citing Article:  W07-2216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.</S> | Reference Offset:  ['10','54'] | Reference Text:  <S sid = 10 ssid = >Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees.</S><S sid = 54 ssid = >Unfortunately, second-order non-projective MST parsing is NP-hard, as shown in appendix A.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E06-1011.txt | Citing Article:  W07-2216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald and Pereira (2006) define this as a second-order Markov assumption.</S> | Reference Offset:  ['45','120'] | Reference Text:  <S sid = 45 ssid = >The second-order model allows us to condition on the most recent parsing decision, that is, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser (Charniak, 2000).</S><S sid = 120 ssid = >As noted earlier, this representation subsumes the first-order representation of McDonald et al. (2005b), so we can incorporate all of their features as well as the new second-order features we now describe.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E06-1011.txt | Citing Article:  D07-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions.</S> | Reference Offset:  ['37','148'] | Reference Text:  <S sid = 37 ssid = >In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.</S><S sid = 148 ssid = >We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E06-1011.txt | Citing Article:  D07-1127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz.</S> | Reference Offset:  ['37','148'] | Reference Text:  <S sid = 37 ssid = >In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.</S><S sid = 148 ssid = >We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E06-1011.txt | Citing Article:  D11-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively.</S> | Reference Offset:  ['10','110'] | Reference Text:  <S sid = 10 ssid = >Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees.</S><S sid = 110 ssid = >Past work on tree-structured outputs has used constraints for the k-best scoring tree (McDonald et al., 2005b) or even all possible trees by using factored representations (Taskar et al., 2004; McDonald et al., 2005c).</S> | Discourse Facet:  NA | Annotator: Automatic


