Citance Number: 1 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Socher et al (2011a) and Socher et al (2011b) present a framework based on recursive neural net works that learns vector space representations for multi-word phrases and sentences.</S> | Reference Offset:  ['2','222'] | Reference Text:  <S sid = 2 ssid = >Our method learns vector space representations for multi-word phrases.</S><S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D11-1014.txt | Citing Article:  D12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D11-1014.txt | Citing Article:  P14-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al, 2011) and for sentiment domain adaptation (Glo rot et al, 2011).</S> | Reference Offset:  ['222','228'] | Reference Text:  <S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S><S sid = 228 ssid = >Other recent deep learning methods for sentiment analysis include (Maas et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D11-1014.txt | Citing Article:  P13-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We should emphasize that the features induced from the addressee's utterance are unique to this task and are hardly available in the related tasks that predicted the emotion of a reader of news articles (Lin and HsinYihn, 2008) or personal stories (Socher et al, 2011).</S> | Reference Offset:  ['100','222'] | Reference Text:  <S sid = 100 ssid = >5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster.</S><S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D11-1014.txt | Citing Article:  P13-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Analogous to our prediction task, Lin and Hsin Yihn (2008) and Socher et al (2011) investigated predicting the emotion of a reader from the text that s/he reads.</S> | Reference Offset:  ['100','222'] | Reference Text:  <S sid = 100 ssid = >5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster.</S><S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D11-1014.txt | Citing Article:  P14-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unlike Socher et al (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets.</S> | Reference Offset:  ['43','222'] | Reference Text:  <S sid = 43 ssid = >These word vectors are then stacked into a word embedding matrix L E Rn√ó|V |, where |V  |is the size of the vocabulary.</S><S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D11-1014.txt | Citing Article:  P14-1146.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recursive Autoencoder (Socher et al, 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically.</S> | Reference Offset:  ['222','228'] | Reference Text:  <S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S><S sid = 228 ssid = >Other recent deep learning methods for sentiment analysis include (Maas et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D11-1014.txt | Citing Article:  P14-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D11-1014.txt | Citing Article:  P14-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To do this, we use two unsupervised recursive autoencoders (RAE) (Socher et al, 2011b), one for the source phrase and the other for the target phrase.</S> | Reference Offset:  ['39','112'] | Reference Text:  <S sid = 39 ssid = >We first describe neural word representations and then proceed to review a related recursive model based on autoencoders, introduce our recursive autoencoder (RAE) and describe how it can be modified to jointly learn phrase representations, phrase structure and sentiment distributions.</S><S sid = 112 ssid = >So far, the RAE was completely unsupervised and induced general representations that capture the semantics of multi-word phrases.In this section, we extend RAEs to a semi-supervised setting in order to predict a sentence- or phrase-level target distribution t.1 One of the main advantages of the RAE is that each node of the tree built by the RAE has associated with it a distributed vector representation (the parent vector p) which could also be seen as features describing that phrase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D11-1014.txt | Citing Article:  P14-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More details can be found in (Socher et al, 2011b).</S> | Reference Offset:  ['100','222'] | Reference Text:  <S sid = 100 ssid = >5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster.</S><S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D11-1014.txt | Citing Article:  P14-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al, 2011), topic classification (Klementiev et al, 2012) or word-word similarity (Mitchell and Lapata, 2008).</S> | Reference Offset:  ['229','237'] | Reference Text:  <S sid = 229 ssid = >Pang et al. (2002) were one of the first to experiment with sentiment classification.</S><S sid = 237 ssid = >Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D11-1014.txt | Citing Article:  P12-2018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al, 2011).</S> | Reference Offset:  ['222','228'] | Reference Text:  <S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S><S sid = 228 ssid = >Other recent deep learning methods for sentiment analysis include (Maas et al., 2011).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D11-1014.txt | Citing Article:  P12-2018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['97','262'] | Reference Text:  <S sid = 97 ssid = >The tree is then recovered by unfolding the collapsing decisions.</S><S sid = 262 ssid = >We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D11-1014.txt | Citing Article:  P14-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Socher et al (2011) introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.</S> | Reference Offset:  ['0','222'] | Reference Text:  <S sid = 0 ssid = >Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</S><S sid = 222 ssid = >Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


