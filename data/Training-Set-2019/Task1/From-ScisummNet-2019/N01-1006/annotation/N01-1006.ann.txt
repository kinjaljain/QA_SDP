Citance Number: 1 | Reference Article:  N01-1006.txt | Citing Article:  W02-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed.</S> | Reference Offset:  ['10','110'] | Reference Text:  <S sid = 10 ssid = >Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S><S sid = 110 ssid = >Also present in Table 1 are the results of training Brill's tagger on the same data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N01-1006.txt | Citing Article:  W03-0433.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training.</S> | Reference Offset:  ['43','67'] | Reference Text:  <S sid = 43 ssid = >The experiments presented in Section 4 include ICA in the training time and performance comparisonsï¿½.</S><S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N01-1006.txt | Citing Article:  W03-0433.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001).</S> | Reference Offset:  ['15','133'] | Reference Text:  <S sid = 15 ssid = >This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.</S><S sid = 133 ssid = >The following table shows the above sentence with the assigned chunk tags: and the part-of-speech tags were generated by Brill's tagger (Brill, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N01-1006.txt | Citing Article:  W04-3209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.).</S> | Reference Offset:  ['104','132'] | Reference Text:  <S sid = 104 ssid = >The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).</S><S sid = 132 ssid = >Following Ramshaw & Marcus' (1999) work in base noun phrase chunking, each word is assigned a chunk tag corresponding to the phrase to which it belongs .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N01-1006.txt | Citing Article:  P03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001).</S> | Reference Offset:  ['67','117'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 117 ssid = >The problem is cast as a classification task, and the sentence is reduced to a 4-tuple containing the preposition and the non-inflected base forms of the head words of the verb phrase VP and the two noun phrases NP1 and NP2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N01-1006.txt | Citing Article:  C08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001).</S> | Reference Offset:  ['67','146'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 146 ssid = >The experiment was performed with the part-ofspeech data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N01-1006.txt | Citing Article:  W06-1620.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set.</S> | Reference Offset:  ['24','103'] | Reference Text:  <S sid = 24 ssid = >The rules are then applied sequentially to the evaluation set in the order they were learned.</S><S sid = 103 ssid = >A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N01-1006.txt | Citing Article:  P03-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation.</S> | Reference Offset:  ['67','111'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 111 ssid = >The results of this tagger are presented to provide a performance comparison with a widely used tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N01-1006.txt | Citing Article:  W04-0845.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001).</S> | Reference Offset:  ['0','10'] | Reference Text:  <S sid = 0 ssid = >Transformation Based Learning In The Fast Lane</S><S sid = 10 ssid = >Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N01-1006.txt | Citing Article:  W04-2417.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001).</S> | Reference Offset:  ['67','160'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 160 ssid = >Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N01-1006.txt | Citing Article:  W03-1728.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001).</S> | Reference Offset:  ['104','122'] | Reference Text:  <S sid = 104 ssid = >The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).</S><S sid = 122 ssid = >The templates used to generate rules are similar to the ones used by Brill and Resnik (1994) and some include WordNet features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N01-1006.txt | Citing Article:  W03-1728.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001).</S> | Reference Offset:  ['0','15'] | Reference Text:  <S sid = 0 ssid = >Transformation Based Learning In The Fast Lane</S><S sid = 15 ssid = >This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N01-1006.txt | Citing Article:  W05-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001).</S> | Reference Offset:  ['67','111'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 111 ssid = >The results of this tagger are presented to provide a performance comparison with a widely used tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N01-1006.txt | Citing Article:  W03-1812.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC.</S> | Reference Offset:  ['67','111'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 111 ssid = >The results of this tagger are presented to provide a performance comparison with a widely used tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N01-1006.txt | Citing Article:  W03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set.</S> | Reference Offset:  ['10','67'] | Reference Text:  <S sid = 10 ssid = >Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S><S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N01-1006.txt | Citing Article:  W07-2051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC).</S> | Reference Offset:  ['14','34'] | Reference Text:  <S sid = 14 ssid = >For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 million word corpus.</S><S sid = 34 ssid = >For example, a transformation-based text chunker training upon a modestly-sized corpus of 200,000 words has approximately 2 million rules active at each iteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N01-1006.txt | Citing Article:  W07-2074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001).</S> | Reference Offset:  ['67','146'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 146 ssid = >The experiment was performed with the part-ofspeech data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N01-1006.txt | Citing Article:  W03-0806.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging.</S> | Reference Offset:  ['0','125'] | Reference Text:  <S sid = 0 ssid = >Transformation Based Learning In The Fast Lane</S><S sid = 125 ssid = >Again, the ICA algorithm learns the rules very fast, but has a slightly lower performance than the other two TBL systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N01-1006.txt | Citing Article:  N04-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data.</S> | Reference Offset:  ['67','145'] | Reference Text:  <S sid = 67 ssid = >In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = 145 ssid = >Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N01-1006.txt | Citing Article:  W02-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001).</S> | Reference Offset:  ['30','108'] | Reference Text:  <S sid = 30 ssid = >Ramshaw & Marcus (1994) attempted to reduce the training time of the algorithm by making the update process more efficient.</S><S sid = 108 ssid = >The FastTBL algorithm performs very similarly to the regular TBL, while running in an order of magnitude faster.</S> | Discourse Facet:  NA | Annotator: Automatic


