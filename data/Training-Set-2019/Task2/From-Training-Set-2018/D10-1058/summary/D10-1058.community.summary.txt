We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Our model is a coherent generative model that combines the HMM and IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 911 October 2010. Qc 2010 Association for Computational Linguistics estimation. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees. Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (φI , φǫ, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. Our model has only one parameter for each target word, which can be learned more reliably. Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977). The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm. Instead, we do “batch learning”: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). 